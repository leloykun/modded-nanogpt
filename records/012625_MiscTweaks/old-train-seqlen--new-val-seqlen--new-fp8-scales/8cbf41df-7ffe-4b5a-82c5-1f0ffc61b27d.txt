import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**9, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1393 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 06:45:17 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   31C    P0             116W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   31C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1393 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1393 train_time:30939ms step_avg:nanms
step:2/1393 train_time:31407ms step_avg:nanms
step:3/1393 train_time:31529ms step_avg:nanms
step:4/1393 train_time:31649ms step_avg:nanms
step:5/1393 train_time:31769ms step_avg:nanms
step:6/1393 train_time:31890ms step_avg:nanms
step:7/1393 train_time:32011ms step_avg:nanms
step:8/1393 train_time:32132ms step_avg:nanms
step:9/1393 train_time:32253ms step_avg:nanms
step:10/1393 train_time:32374ms step_avg:nanms
step:11/1393 train_time:122ms step_avg:nanms
step:12/1393 train_time:245ms step_avg:nanms
step:13/1393 train_time:367ms step_avg:122.24ms
step:14/1393 train_time:488ms step_avg:122.11ms
step:15/1393 train_time:610ms step_avg:121.92ms
step:16/1393 train_time:731ms step_avg:121.81ms
step:17/1393 train_time:853ms step_avg:121.79ms
step:18/1393 train_time:974ms step_avg:121.78ms
step:19/1393 train_time:1096ms step_avg:121.74ms
step:20/1393 train_time:1217ms step_avg:121.74ms
step:21/1393 train_time:1339ms step_avg:121.70ms
step:22/1393 train_time:1460ms step_avg:121.65ms
step:23/1393 train_time:1581ms step_avg:121.58ms
step:24/1393 train_time:1702ms step_avg:121.58ms
step:25/1393 train_time:1823ms step_avg:121.52ms
step:26/1393 train_time:1944ms step_avg:121.51ms
step:27/1393 train_time:2065ms step_avg:121.50ms
step:28/1393 train_time:2188ms step_avg:121.56ms
step:29/1393 train_time:2309ms step_avg:121.54ms
step:30/1393 train_time:2432ms step_avg:121.58ms
step:31/1393 train_time:2553ms step_avg:121.58ms
step:32/1393 train_time:2675ms step_avg:121.57ms
step:33/1393 train_time:2797ms step_avg:121.59ms
step:34/1393 train_time:2917ms step_avg:121.56ms
step:35/1393 train_time:3039ms step_avg:121.58ms
step:36/1393 train_time:3163ms step_avg:121.64ms
step:37/1393 train_time:3284ms step_avg:121.64ms
step:38/1393 train_time:3406ms step_avg:121.63ms
step:39/1393 train_time:3526ms step_avg:121.59ms
step:40/1393 train_time:3647ms step_avg:121.58ms
step:41/1393 train_time:3769ms step_avg:121.57ms
step:42/1393 train_time:3891ms step_avg:121.58ms
step:43/1393 train_time:4012ms step_avg:121.56ms
step:44/1393 train_time:4133ms step_avg:121.55ms
step:45/1393 train_time:4254ms step_avg:121.55ms
step:46/1393 train_time:4377ms step_avg:121.58ms
step:47/1393 train_time:4500ms step_avg:121.61ms
step:48/1393 train_time:4620ms step_avg:121.58ms
step:49/1393 train_time:4741ms step_avg:121.57ms
step:50/1393 train_time:4862ms step_avg:121.55ms
step:51/1393 train_time:4983ms step_avg:121.55ms
step:52/1393 train_time:5105ms step_avg:121.54ms
step:53/1393 train_time:5226ms step_avg:121.53ms
step:54/1393 train_time:5348ms step_avg:121.55ms
step:55/1393 train_time:5470ms step_avg:121.55ms
step:56/1393 train_time:5593ms step_avg:121.58ms
step:57/1393 train_time:5713ms step_avg:121.55ms
step:58/1393 train_time:5833ms step_avg:121.53ms
step:59/1393 train_time:5955ms step_avg:121.53ms
step:60/1393 train_time:6075ms step_avg:121.51ms
step:61/1393 train_time:6196ms step_avg:121.50ms
step:62/1393 train_time:6318ms step_avg:121.49ms
step:63/1393 train_time:6439ms step_avg:121.49ms
step:64/1393 train_time:6561ms step_avg:121.50ms
step:65/1393 train_time:6683ms step_avg:121.50ms
step:66/1393 train_time:6804ms step_avg:121.50ms
step:67/1393 train_time:6925ms step_avg:121.50ms
step:68/1393 train_time:7048ms step_avg:121.51ms
step:69/1393 train_time:7169ms step_avg:121.50ms
step:70/1393 train_time:7290ms step_avg:121.50ms
step:71/1393 train_time:7412ms step_avg:121.50ms
step:72/1393 train_time:7533ms step_avg:121.51ms
step:73/1393 train_time:7655ms step_avg:121.50ms
step:74/1393 train_time:7776ms step_avg:121.50ms
step:75/1393 train_time:7897ms step_avg:121.50ms
step:76/1393 train_time:8020ms step_avg:121.51ms
step:77/1393 train_time:8140ms step_avg:121.50ms
step:78/1393 train_time:8261ms step_avg:121.48ms
step:79/1393 train_time:8382ms step_avg:121.48ms
step:80/1393 train_time:8504ms step_avg:121.48ms
step:81/1393 train_time:8624ms step_avg:121.47ms
step:82/1393 train_time:8746ms step_avg:121.47ms
step:83/1393 train_time:8867ms step_avg:121.47ms
step:84/1393 train_time:8988ms step_avg:121.47ms
step:85/1393 train_time:9110ms step_avg:121.47ms
step:86/1393 train_time:9232ms step_avg:121.47ms
step:87/1393 train_time:9354ms step_avg:121.48ms
step:88/1393 train_time:9475ms step_avg:121.48ms
step:89/1393 train_time:9596ms step_avg:121.47ms
step:90/1393 train_time:9719ms step_avg:121.49ms
step:91/1393 train_time:9840ms step_avg:121.49ms
step:92/1393 train_time:9962ms step_avg:121.48ms
step:93/1393 train_time:10082ms step_avg:121.48ms
step:94/1393 train_time:10204ms step_avg:121.48ms
step:95/1393 train_time:10325ms step_avg:121.47ms
step:96/1393 train_time:10445ms step_avg:121.45ms
step:97/1393 train_time:10567ms step_avg:121.46ms
step:98/1393 train_time:10689ms step_avg:121.46ms
step:99/1393 train_time:10810ms step_avg:121.46ms
step:100/1393 train_time:10930ms step_avg:121.45ms
step:101/1393 train_time:11051ms step_avg:121.44ms
step:102/1393 train_time:11174ms step_avg:121.45ms
step:103/1393 train_time:11295ms step_avg:121.45ms
step:104/1393 train_time:11416ms step_avg:121.44ms
step:105/1393 train_time:11537ms step_avg:121.44ms
step:106/1393 train_time:11659ms step_avg:121.45ms
step:107/1393 train_time:11781ms step_avg:121.45ms
step:108/1393 train_time:11902ms step_avg:121.45ms
step:109/1393 train_time:12024ms step_avg:121.45ms
step:110/1393 train_time:12146ms step_avg:121.46ms
step:111/1393 train_time:12268ms step_avg:121.47ms
step:112/1393 train_time:12391ms step_avg:121.48ms
step:113/1393 train_time:12513ms step_avg:121.48ms
step:114/1393 train_time:12635ms step_avg:121.49ms
step:115/1393 train_time:12758ms step_avg:121.51ms
step:116/1393 train_time:12880ms step_avg:121.51ms
step:117/1393 train_time:13001ms step_avg:121.51ms
step:118/1393 train_time:13123ms step_avg:121.51ms
step:119/1393 train_time:13244ms step_avg:121.50ms
step:120/1393 train_time:13366ms step_avg:121.51ms
step:121/1393 train_time:13488ms step_avg:121.51ms
step:122/1393 train_time:13610ms step_avg:121.52ms
step:123/1393 train_time:13732ms step_avg:121.52ms
step:124/1393 train_time:13854ms step_avg:121.53ms
step:125/1393 train_time:13976ms step_avg:121.53ms
step:125/1393 val_loss:4.3961 train_time:14096ms step_avg:122.57ms
step:126/1393 train_time:14118ms step_avg:121.70ms
step:127/1393 train_time:14222ms step_avg:121.55ms
step:128/1393 train_time:14348ms step_avg:121.59ms
step:129/1393 train_time:14472ms step_avg:121.62ms
step:130/1393 train_time:14594ms step_avg:121.62ms
step:131/1393 train_time:14715ms step_avg:121.61ms
step:132/1393 train_time:14836ms step_avg:121.61ms
step:133/1393 train_time:14958ms step_avg:121.61ms
step:134/1393 train_time:15082ms step_avg:121.63ms
step:135/1393 train_time:15204ms step_avg:121.63ms
step:136/1393 train_time:15327ms step_avg:121.64ms
step:137/1393 train_time:15450ms step_avg:121.65ms
step:138/1393 train_time:15573ms step_avg:121.66ms
step:139/1393 train_time:15697ms step_avg:121.68ms
step:140/1393 train_time:15819ms step_avg:121.69ms
step:141/1393 train_time:15941ms step_avg:121.69ms
step:142/1393 train_time:16063ms step_avg:121.69ms
step:143/1393 train_time:16185ms step_avg:121.69ms
step:144/1393 train_time:16306ms step_avg:121.69ms
step:145/1393 train_time:16429ms step_avg:121.70ms
step:146/1393 train_time:16552ms step_avg:121.71ms
step:147/1393 train_time:16675ms step_avg:121.71ms
step:148/1393 train_time:16798ms step_avg:121.72ms
step:149/1393 train_time:16919ms step_avg:121.72ms
step:150/1393 train_time:17041ms step_avg:121.72ms
step:151/1393 train_time:17162ms step_avg:121.72ms
step:152/1393 train_time:17284ms step_avg:121.72ms
step:153/1393 train_time:17406ms step_avg:121.72ms
step:154/1393 train_time:17529ms step_avg:121.73ms
step:155/1393 train_time:17652ms step_avg:121.74ms
step:156/1393 train_time:17775ms step_avg:121.75ms
step:157/1393 train_time:17898ms step_avg:121.76ms
step:158/1393 train_time:18020ms step_avg:121.76ms
step:159/1393 train_time:18143ms step_avg:121.76ms
step:160/1393 train_time:18265ms step_avg:121.77ms
step:161/1393 train_time:18386ms step_avg:121.76ms
step:162/1393 train_time:18508ms step_avg:121.76ms
step:163/1393 train_time:18630ms step_avg:121.76ms
step:164/1393 train_time:18753ms step_avg:121.77ms
step:165/1393 train_time:18875ms step_avg:121.78ms
step:166/1393 train_time:18997ms step_avg:121.78ms
step:167/1393 train_time:19120ms step_avg:121.78ms
step:168/1393 train_time:19241ms step_avg:121.78ms
step:169/1393 train_time:19364ms step_avg:121.78ms
step:170/1393 train_time:19486ms step_avg:121.79ms
step:171/1393 train_time:19609ms step_avg:121.79ms
step:172/1393 train_time:19731ms step_avg:121.80ms
step:173/1393 train_time:19853ms step_avg:121.80ms
step:174/1393 train_time:19975ms step_avg:121.80ms
step:175/1393 train_time:20097ms step_avg:121.80ms
step:176/1393 train_time:20219ms step_avg:121.80ms
step:177/1393 train_time:20341ms step_avg:121.80ms
step:178/1393 train_time:20464ms step_avg:121.81ms
step:179/1393 train_time:20586ms step_avg:121.81ms
step:180/1393 train_time:20708ms step_avg:121.81ms
step:181/1393 train_time:20830ms step_avg:121.81ms
step:182/1393 train_time:20952ms step_avg:121.81ms
step:183/1393 train_time:21074ms step_avg:121.81ms
step:184/1393 train_time:21197ms step_avg:121.82ms
step:185/1393 train_time:21319ms step_avg:121.83ms
step:186/1393 train_time:21442ms step_avg:121.83ms
step:187/1393 train_time:21564ms step_avg:121.83ms
step:188/1393 train_time:21686ms step_avg:121.83ms
step:189/1393 train_time:21807ms step_avg:121.83ms
step:190/1393 train_time:21929ms step_avg:121.83ms
step:191/1393 train_time:22050ms step_avg:121.83ms
step:192/1393 train_time:22172ms step_avg:121.83ms
step:193/1393 train_time:22294ms step_avg:121.83ms
step:194/1393 train_time:22416ms step_avg:121.83ms
step:195/1393 train_time:22538ms step_avg:121.83ms
step:196/1393 train_time:22659ms step_avg:121.82ms
step:197/1393 train_time:22782ms step_avg:121.83ms
step:198/1393 train_time:22904ms step_avg:121.83ms
step:199/1393 train_time:23026ms step_avg:121.83ms
step:200/1393 train_time:23149ms step_avg:121.84ms
step:201/1393 train_time:23271ms step_avg:121.84ms
step:202/1393 train_time:23394ms step_avg:121.84ms
step:203/1393 train_time:23516ms step_avg:121.84ms
step:204/1393 train_time:23638ms step_avg:121.84ms
step:205/1393 train_time:23760ms step_avg:121.84ms
step:206/1393 train_time:23882ms step_avg:121.85ms
step:207/1393 train_time:24005ms step_avg:121.85ms
step:208/1393 train_time:24127ms step_avg:121.85ms
step:209/1393 train_time:24249ms step_avg:121.85ms
step:210/1393 train_time:24371ms step_avg:121.85ms
step:211/1393 train_time:24494ms step_avg:121.86ms
step:212/1393 train_time:24617ms step_avg:121.87ms
step:213/1393 train_time:24739ms step_avg:121.87ms
step:214/1393 train_time:24861ms step_avg:121.87ms
step:215/1393 train_time:24984ms step_avg:121.87ms
step:216/1393 train_time:25108ms step_avg:121.88ms
step:217/1393 train_time:25231ms step_avg:121.89ms
step:218/1393 train_time:25353ms step_avg:121.89ms
step:219/1393 train_time:25476ms step_avg:121.89ms
step:220/1393 train_time:25599ms step_avg:121.90ms
step:221/1393 train_time:25722ms step_avg:121.90ms
step:222/1393 train_time:25844ms step_avg:121.91ms
step:223/1393 train_time:25968ms step_avg:121.91ms
step:224/1393 train_time:26090ms step_avg:121.92ms
step:225/1393 train_time:26214ms step_avg:121.92ms
step:226/1393 train_time:26337ms step_avg:121.93ms
step:227/1393 train_time:26460ms step_avg:121.94ms
step:228/1393 train_time:26583ms step_avg:121.94ms
step:229/1393 train_time:26706ms step_avg:121.95ms
step:230/1393 train_time:26830ms step_avg:121.95ms
step:231/1393 train_time:26952ms step_avg:121.96ms
step:232/1393 train_time:27074ms step_avg:121.96ms
step:233/1393 train_time:27197ms step_avg:121.96ms
step:234/1393 train_time:27319ms step_avg:121.96ms
step:235/1393 train_time:27442ms step_avg:121.96ms
step:236/1393 train_time:27565ms step_avg:121.97ms
step:237/1393 train_time:27688ms step_avg:121.97ms
step:238/1393 train_time:27811ms step_avg:121.98ms
step:239/1393 train_time:27935ms step_avg:121.99ms
step:240/1393 train_time:28057ms step_avg:121.99ms
step:241/1393 train_time:28180ms step_avg:121.99ms
step:242/1393 train_time:28302ms step_avg:121.99ms
step:243/1393 train_time:28425ms step_avg:121.99ms
step:244/1393 train_time:28548ms step_avg:122.00ms
step:245/1393 train_time:28670ms step_avg:122.00ms
step:246/1393 train_time:28793ms step_avg:122.00ms
step:247/1393 train_time:28916ms step_avg:122.01ms
step:248/1393 train_time:29039ms step_avg:122.01ms
step:249/1393 train_time:29161ms step_avg:122.01ms
step:250/1393 train_time:29284ms step_avg:122.02ms
step:250/1393 val_loss:3.9783 train_time:29405ms step_avg:122.52ms
step:251/1393 train_time:29426ms step_avg:122.10ms
step:252/1393 train_time:29542ms step_avg:122.07ms
step:253/1393 train_time:29667ms step_avg:122.09ms
step:254/1393 train_time:29790ms step_avg:122.09ms
step:255/1393 train_time:29913ms step_avg:122.09ms
step:256/1393 train_time:30034ms step_avg:122.09ms
step:257/1393 train_time:30156ms step_avg:122.09ms
step:258/1393 train_time:30277ms step_avg:122.09ms
step:259/1393 train_time:30400ms step_avg:122.09ms
step:260/1393 train_time:30524ms step_avg:122.10ms
step:261/1393 train_time:30649ms step_avg:122.11ms
step:262/1393 train_time:30773ms step_avg:122.11ms
step:263/1393 train_time:30896ms step_avg:122.12ms
step:264/1393 train_time:31019ms step_avg:122.12ms
step:265/1393 train_time:31140ms step_avg:122.12ms
step:266/1393 train_time:31262ms step_avg:122.12ms
step:267/1393 train_time:31384ms step_avg:122.11ms
step:268/1393 train_time:31507ms step_avg:122.12ms
step:269/1393 train_time:31630ms step_avg:122.12ms
step:270/1393 train_time:31753ms step_avg:122.13ms
step:271/1393 train_time:31875ms step_avg:122.13ms
step:272/1393 train_time:31999ms step_avg:122.14ms
step:273/1393 train_time:32122ms step_avg:122.14ms
step:274/1393 train_time:32243ms step_avg:122.13ms
step:275/1393 train_time:32366ms step_avg:122.13ms
step:276/1393 train_time:32489ms step_avg:122.14ms
step:277/1393 train_time:32612ms step_avg:122.14ms
step:278/1393 train_time:32734ms step_avg:122.14ms
step:279/1393 train_time:32858ms step_avg:122.15ms
step:280/1393 train_time:32981ms step_avg:122.15ms
step:281/1393 train_time:33104ms step_avg:122.15ms
step:282/1393 train_time:33227ms step_avg:122.16ms
step:283/1393 train_time:33349ms step_avg:122.16ms
step:284/1393 train_time:33473ms step_avg:122.16ms
step:285/1393 train_time:33595ms step_avg:122.16ms
step:286/1393 train_time:33718ms step_avg:122.17ms
step:287/1393 train_time:33841ms step_avg:122.17ms
step:288/1393 train_time:33964ms step_avg:122.17ms
step:289/1393 train_time:34087ms step_avg:122.17ms
step:290/1393 train_time:34210ms step_avg:122.18ms
step:291/1393 train_time:34332ms step_avg:122.18ms
step:292/1393 train_time:34454ms step_avg:122.18ms
step:293/1393 train_time:34576ms step_avg:122.18ms
step:294/1393 train_time:34699ms step_avg:122.18ms
step:295/1393 train_time:34821ms step_avg:122.18ms
step:296/1393 train_time:34945ms step_avg:122.18ms
step:297/1393 train_time:35067ms step_avg:122.19ms
step:298/1393 train_time:35190ms step_avg:122.19ms
step:299/1393 train_time:35313ms step_avg:122.19ms
step:300/1393 train_time:35435ms step_avg:122.19ms
step:301/1393 train_time:35559ms step_avg:122.20ms
step:302/1393 train_time:35682ms step_avg:122.20ms
step:303/1393 train_time:35804ms step_avg:122.20ms
step:304/1393 train_time:35928ms step_avg:122.20ms
step:305/1393 train_time:36050ms step_avg:122.20ms
step:306/1393 train_time:36173ms step_avg:122.21ms
step:307/1393 train_time:36296ms step_avg:122.21ms
step:308/1393 train_time:36420ms step_avg:122.22ms
step:309/1393 train_time:36544ms step_avg:122.22ms
step:310/1393 train_time:36666ms step_avg:122.22ms
step:311/1393 train_time:36789ms step_avg:122.22ms
step:312/1393 train_time:36915ms step_avg:122.23ms
step:313/1393 train_time:37041ms step_avg:122.25ms
step:314/1393 train_time:37168ms step_avg:122.26ms
step:315/1393 train_time:37294ms step_avg:122.28ms
step:316/1393 train_time:37419ms step_avg:122.29ms
step:317/1393 train_time:37545ms step_avg:122.30ms
step:318/1393 train_time:37670ms step_avg:122.31ms
step:319/1393 train_time:37795ms step_avg:122.31ms
step:320/1393 train_time:37920ms step_avg:122.32ms
step:321/1393 train_time:38046ms step_avg:122.33ms
step:322/1393 train_time:38171ms step_avg:122.34ms
step:323/1393 train_time:38296ms step_avg:122.35ms
step:324/1393 train_time:38421ms step_avg:122.36ms
step:325/1393 train_time:38546ms step_avg:122.37ms
step:326/1393 train_time:38672ms step_avg:122.38ms
step:327/1393 train_time:38797ms step_avg:122.39ms
step:328/1393 train_time:38923ms step_avg:122.40ms
step:329/1393 train_time:39048ms step_avg:122.41ms
step:330/1393 train_time:39174ms step_avg:122.42ms
step:331/1393 train_time:39298ms step_avg:122.42ms
step:332/1393 train_time:39424ms step_avg:122.43ms
step:333/1393 train_time:39548ms step_avg:122.44ms
step:334/1393 train_time:39673ms step_avg:122.45ms
step:335/1393 train_time:39799ms step_avg:122.46ms
step:336/1393 train_time:39925ms step_avg:122.47ms
step:337/1393 train_time:40051ms step_avg:122.48ms
step:338/1393 train_time:40176ms step_avg:122.49ms
step:339/1393 train_time:40300ms step_avg:122.49ms
step:340/1393 train_time:40427ms step_avg:122.51ms
step:341/1393 train_time:40551ms step_avg:122.51ms
step:342/1393 train_time:40676ms step_avg:122.52ms
step:343/1393 train_time:40801ms step_avg:122.53ms
step:344/1393 train_time:40926ms step_avg:122.53ms
step:345/1393 train_time:41051ms step_avg:122.54ms
step:346/1393 train_time:41177ms step_avg:122.55ms
step:347/1393 train_time:41303ms step_avg:122.56ms
step:348/1393 train_time:41428ms step_avg:122.57ms
step:349/1393 train_time:41553ms step_avg:122.57ms
step:350/1393 train_time:41678ms step_avg:122.58ms
step:351/1393 train_time:41802ms step_avg:122.59ms
step:352/1393 train_time:41928ms step_avg:122.60ms
step:353/1393 train_time:42052ms step_avg:122.60ms
step:354/1393 train_time:42179ms step_avg:122.61ms
step:355/1393 train_time:42304ms step_avg:122.62ms
step:356/1393 train_time:42430ms step_avg:122.63ms
step:357/1393 train_time:42554ms step_avg:122.63ms
step:358/1393 train_time:42679ms step_avg:122.64ms
step:359/1393 train_time:42805ms step_avg:122.65ms
step:360/1393 train_time:42930ms step_avg:122.66ms
step:361/1393 train_time:43055ms step_avg:122.66ms
step:362/1393 train_time:43180ms step_avg:122.67ms
step:363/1393 train_time:43305ms step_avg:122.68ms
step:364/1393 train_time:43430ms step_avg:122.68ms
step:365/1393 train_time:43555ms step_avg:122.69ms
step:366/1393 train_time:43680ms step_avg:122.70ms
step:367/1393 train_time:43806ms step_avg:122.70ms
step:368/1393 train_time:43930ms step_avg:122.71ms
step:369/1393 train_time:44056ms step_avg:122.72ms
step:370/1393 train_time:44181ms step_avg:122.73ms
step:371/1393 train_time:44305ms step_avg:122.73ms
step:372/1393 train_time:44430ms step_avg:122.73ms
step:373/1393 train_time:44555ms step_avg:122.74ms
step:374/1393 train_time:44680ms step_avg:122.75ms
step:375/1393 train_time:44806ms step_avg:122.75ms
step:375/1393 val_loss:3.7850 train_time:44929ms step_avg:123.09ms
step:376/1393 train_time:44950ms step_avg:122.81ms
step:377/1393 train_time:45067ms step_avg:122.80ms
step:378/1393 train_time:45193ms step_avg:122.81ms
step:379/1393 train_time:45318ms step_avg:122.81ms
step:380/1393 train_time:45442ms step_avg:122.82ms
step:381/1393 train_time:45567ms step_avg:122.82ms
step:382/1393 train_time:45691ms step_avg:122.82ms
step:383/1393 train_time:45815ms step_avg:122.83ms
step:384/1393 train_time:45940ms step_avg:122.84ms
step:385/1393 train_time:46068ms step_avg:122.85ms
step:386/1393 train_time:46194ms step_avg:122.86ms
step:387/1393 train_time:46320ms step_avg:122.86ms
step:388/1393 train_time:46445ms step_avg:122.87ms
step:389/1393 train_time:46570ms step_avg:122.88ms
step:390/1393 train_time:46694ms step_avg:122.88ms
step:391/1393 train_time:46818ms step_avg:122.88ms
step:392/1393 train_time:46944ms step_avg:122.89ms
step:393/1393 train_time:47069ms step_avg:122.90ms
step:394/1393 train_time:47195ms step_avg:122.90ms
step:395/1393 train_time:47320ms step_avg:122.91ms
step:396/1393 train_time:47445ms step_avg:122.91ms
step:397/1393 train_time:47570ms step_avg:122.92ms
step:398/1393 train_time:47694ms step_avg:122.92ms
step:399/1393 train_time:47818ms step_avg:122.93ms
step:400/1393 train_time:47943ms step_avg:122.93ms
step:401/1393 train_time:48070ms step_avg:122.94ms
step:402/1393 train_time:48194ms step_avg:122.94ms
step:403/1393 train_time:48320ms step_avg:122.95ms
step:404/1393 train_time:48445ms step_avg:122.96ms
step:405/1393 train_time:48570ms step_avg:122.96ms
step:406/1393 train_time:48695ms step_avg:122.97ms
step:407/1393 train_time:48820ms step_avg:122.97ms
step:408/1393 train_time:48945ms step_avg:122.98ms
step:409/1393 train_time:49071ms step_avg:122.99ms
step:410/1393 train_time:49196ms step_avg:122.99ms
step:411/1393 train_time:49321ms step_avg:122.99ms
step:412/1393 train_time:49446ms step_avg:123.00ms
step:413/1393 train_time:49570ms step_avg:123.00ms
step:414/1393 train_time:49695ms step_avg:123.01ms
step:415/1393 train_time:49820ms step_avg:123.01ms
step:416/1393 train_time:49945ms step_avg:123.02ms
step:417/1393 train_time:50071ms step_avg:123.02ms
step:418/1393 train_time:50197ms step_avg:123.03ms
step:419/1393 train_time:50324ms step_avg:123.04ms
step:420/1393 train_time:50450ms step_avg:123.05ms
step:421/1393 train_time:50576ms step_avg:123.05ms
step:422/1393 train_time:50700ms step_avg:123.06ms
step:423/1393 train_time:50826ms step_avg:123.07ms
step:424/1393 train_time:50953ms step_avg:123.07ms
step:425/1393 train_time:51078ms step_avg:123.08ms
step:426/1393 train_time:51204ms step_avg:123.09ms
step:427/1393 train_time:51330ms step_avg:123.09ms
step:428/1393 train_time:51455ms step_avg:123.10ms
step:429/1393 train_time:51581ms step_avg:123.11ms
step:430/1393 train_time:51706ms step_avg:123.11ms
step:431/1393 train_time:51833ms step_avg:123.12ms
step:432/1393 train_time:51958ms step_avg:123.12ms
step:433/1393 train_time:52083ms step_avg:123.13ms
step:434/1393 train_time:52208ms step_avg:123.13ms
step:435/1393 train_time:52333ms step_avg:123.14ms
step:436/1393 train_time:52459ms step_avg:123.14ms
step:437/1393 train_time:52584ms step_avg:123.15ms
step:438/1393 train_time:52710ms step_avg:123.15ms
step:439/1393 train_time:52836ms step_avg:123.16ms
step:440/1393 train_time:52961ms step_avg:123.17ms
step:441/1393 train_time:53086ms step_avg:123.17ms
step:442/1393 train_time:53212ms step_avg:123.18ms
step:443/1393 train_time:53338ms step_avg:123.18ms
step:444/1393 train_time:53463ms step_avg:123.19ms
step:445/1393 train_time:53589ms step_avg:123.19ms
step:446/1393 train_time:53715ms step_avg:123.20ms
step:447/1393 train_time:53840ms step_avg:123.20ms
step:448/1393 train_time:53966ms step_avg:123.21ms
step:449/1393 train_time:54092ms step_avg:123.22ms
step:450/1393 train_time:54217ms step_avg:123.22ms
step:451/1393 train_time:54342ms step_avg:123.22ms
step:452/1393 train_time:54468ms step_avg:123.23ms
step:453/1393 train_time:54595ms step_avg:123.24ms
step:454/1393 train_time:54721ms step_avg:123.25ms
step:455/1393 train_time:54847ms step_avg:123.25ms
step:456/1393 train_time:54972ms step_avg:123.26ms
step:457/1393 train_time:55097ms step_avg:123.26ms
step:458/1393 train_time:55222ms step_avg:123.26ms
step:459/1393 train_time:55349ms step_avg:123.27ms
step:460/1393 train_time:55474ms step_avg:123.28ms
step:461/1393 train_time:55599ms step_avg:123.28ms
step:462/1393 train_time:55724ms step_avg:123.28ms
step:463/1393 train_time:55850ms step_avg:123.29ms
step:464/1393 train_time:55976ms step_avg:123.29ms
step:465/1393 train_time:56101ms step_avg:123.30ms
step:466/1393 train_time:56227ms step_avg:123.31ms
step:467/1393 train_time:56353ms step_avg:123.31ms
step:468/1393 train_time:56479ms step_avg:123.32ms
step:469/1393 train_time:56604ms step_avg:123.32ms
step:470/1393 train_time:56730ms step_avg:123.33ms
step:471/1393 train_time:56856ms step_avg:123.33ms
step:472/1393 train_time:56982ms step_avg:123.34ms
step:473/1393 train_time:57107ms step_avg:123.34ms
step:474/1393 train_time:57232ms step_avg:123.35ms
step:475/1393 train_time:57358ms step_avg:123.35ms
step:476/1393 train_time:57484ms step_avg:123.36ms
step:477/1393 train_time:57610ms step_avg:123.36ms
step:478/1393 train_time:57736ms step_avg:123.37ms
step:479/1393 train_time:57863ms step_avg:123.37ms
step:480/1393 train_time:57988ms step_avg:123.38ms
step:481/1393 train_time:58114ms step_avg:123.38ms
step:482/1393 train_time:58239ms step_avg:123.39ms
step:483/1393 train_time:58365ms step_avg:123.39ms
step:484/1393 train_time:58491ms step_avg:123.40ms
step:485/1393 train_time:58616ms step_avg:123.40ms
step:486/1393 train_time:58741ms step_avg:123.41ms
step:487/1393 train_time:58867ms step_avg:123.41ms
step:488/1393 train_time:58993ms step_avg:123.42ms
step:489/1393 train_time:59119ms step_avg:123.42ms
step:490/1393 train_time:59244ms step_avg:123.43ms
step:491/1393 train_time:59370ms step_avg:123.43ms
step:492/1393 train_time:59496ms step_avg:123.44ms
step:493/1393 train_time:59622ms step_avg:123.44ms
step:494/1393 train_time:59747ms step_avg:123.44ms
step:495/1393 train_time:59874ms step_avg:123.45ms
step:496/1393 train_time:59999ms step_avg:123.45ms
step:497/1393 train_time:60124ms step_avg:123.46ms
step:498/1393 train_time:60249ms step_avg:123.46ms
step:499/1393 train_time:60375ms step_avg:123.47ms
step:500/1393 train_time:60501ms step_avg:123.47ms
step:500/1393 val_loss:3.6683 train_time:60625ms step_avg:123.72ms
step:501/1393 train_time:60647ms step_avg:123.52ms
step:502/1393 train_time:60759ms step_avg:123.49ms
step:503/1393 train_time:60885ms step_avg:123.50ms
step:504/1393 train_time:61011ms step_avg:123.50ms
step:505/1393 train_time:61136ms step_avg:123.51ms
step:506/1393 train_time:61261ms step_avg:123.51ms
step:507/1393 train_time:61385ms step_avg:123.51ms
step:508/1393 train_time:61511ms step_avg:123.52ms
step:509/1393 train_time:61638ms step_avg:123.52ms
step:510/1393 train_time:61765ms step_avg:123.53ms
step:511/1393 train_time:61891ms step_avg:123.54ms
step:512/1393 train_time:62017ms step_avg:123.54ms
step:513/1393 train_time:62142ms step_avg:123.54ms
step:514/1393 train_time:62267ms step_avg:123.55ms
step:515/1393 train_time:62392ms step_avg:123.55ms
step:516/1393 train_time:62517ms step_avg:123.55ms
step:517/1393 train_time:62643ms step_avg:123.56ms
step:518/1393 train_time:62770ms step_avg:123.56ms
step:519/1393 train_time:62898ms step_avg:123.57ms
step:520/1393 train_time:63026ms step_avg:123.58ms
step:521/1393 train_time:63154ms step_avg:123.59ms
step:522/1393 train_time:63282ms step_avg:123.60ms
step:523/1393 train_time:63409ms step_avg:123.60ms
step:524/1393 train_time:63537ms step_avg:123.61ms
step:525/1393 train_time:63664ms step_avg:123.62ms
step:526/1393 train_time:63793ms step_avg:123.63ms
step:527/1393 train_time:63920ms step_avg:123.64ms
step:528/1393 train_time:64048ms step_avg:123.64ms
step:529/1393 train_time:64175ms step_avg:123.65ms
step:530/1393 train_time:64303ms step_avg:123.66ms
step:531/1393 train_time:64430ms step_avg:123.67ms
step:532/1393 train_time:64557ms step_avg:123.67ms
step:533/1393 train_time:64685ms step_avg:123.68ms
step:534/1393 train_time:64813ms step_avg:123.69ms
step:535/1393 train_time:64941ms step_avg:123.70ms
step:536/1393 train_time:65068ms step_avg:123.70ms
step:537/1393 train_time:65196ms step_avg:123.71ms
step:538/1393 train_time:65325ms step_avg:123.72ms
step:539/1393 train_time:65453ms step_avg:123.73ms
step:540/1393 train_time:65581ms step_avg:123.74ms
step:541/1393 train_time:65708ms step_avg:123.74ms
step:542/1393 train_time:65836ms step_avg:123.75ms
step:543/1393 train_time:65964ms step_avg:123.76ms
step:544/1393 train_time:66092ms step_avg:123.77ms
step:545/1393 train_time:66219ms step_avg:123.77ms
step:546/1393 train_time:66347ms step_avg:123.78ms
step:547/1393 train_time:66474ms step_avg:123.79ms
step:548/1393 train_time:66602ms step_avg:123.80ms
step:549/1393 train_time:66729ms step_avg:123.80ms
step:550/1393 train_time:66857ms step_avg:123.81ms
step:551/1393 train_time:66984ms step_avg:123.82ms
step:552/1393 train_time:67112ms step_avg:123.82ms
step:553/1393 train_time:67239ms step_avg:123.83ms
step:554/1393 train_time:67367ms step_avg:123.84ms
step:555/1393 train_time:67494ms step_avg:123.84ms
step:556/1393 train_time:67621ms step_avg:123.85ms
step:557/1393 train_time:67749ms step_avg:123.85ms
step:558/1393 train_time:67876ms step_avg:123.86ms
step:559/1393 train_time:68004ms step_avg:123.87ms
step:560/1393 train_time:68132ms step_avg:123.88ms
step:561/1393 train_time:68259ms step_avg:123.88ms
step:562/1393 train_time:68386ms step_avg:123.89ms
step:563/1393 train_time:68514ms step_avg:123.90ms
step:564/1393 train_time:68641ms step_avg:123.90ms
step:565/1393 train_time:68769ms step_avg:123.91ms
step:566/1393 train_time:68897ms step_avg:123.92ms
step:567/1393 train_time:69025ms step_avg:123.92ms
step:568/1393 train_time:69152ms step_avg:123.93ms
step:569/1393 train_time:69279ms step_avg:123.93ms
step:570/1393 train_time:69407ms step_avg:123.94ms
step:571/1393 train_time:69534ms step_avg:123.95ms
step:572/1393 train_time:69662ms step_avg:123.95ms
step:573/1393 train_time:69790ms step_avg:123.96ms
step:574/1393 train_time:69919ms step_avg:123.97ms
step:575/1393 train_time:70047ms step_avg:123.98ms
step:576/1393 train_time:70174ms step_avg:123.98ms
step:577/1393 train_time:70302ms step_avg:123.99ms
step:578/1393 train_time:70429ms step_avg:123.99ms
step:579/1393 train_time:70556ms step_avg:124.00ms
step:580/1393 train_time:70684ms step_avg:124.01ms
step:581/1393 train_time:70813ms step_avg:124.02ms
step:582/1393 train_time:70941ms step_avg:124.02ms
step:583/1393 train_time:71068ms step_avg:124.03ms
step:584/1393 train_time:71195ms step_avg:124.03ms
step:585/1393 train_time:71323ms step_avg:124.04ms
step:586/1393 train_time:71451ms step_avg:124.05ms
step:587/1393 train_time:71579ms step_avg:124.05ms
step:588/1393 train_time:71707ms step_avg:124.06ms
step:589/1393 train_time:71834ms step_avg:124.07ms
step:590/1393 train_time:71962ms step_avg:124.07ms
step:591/1393 train_time:72089ms step_avg:124.08ms
step:592/1393 train_time:72217ms step_avg:124.08ms
step:593/1393 train_time:72345ms step_avg:124.09ms
step:594/1393 train_time:72472ms step_avg:124.10ms
step:595/1393 train_time:72600ms step_avg:124.10ms
step:596/1393 train_time:72728ms step_avg:124.11ms
step:597/1393 train_time:72857ms step_avg:124.12ms
step:598/1393 train_time:72984ms step_avg:124.12ms
step:599/1393 train_time:73112ms step_avg:124.13ms
step:600/1393 train_time:73239ms step_avg:124.13ms
step:601/1393 train_time:73367ms step_avg:124.14ms
step:602/1393 train_time:73494ms step_avg:124.15ms
step:603/1393 train_time:73622ms step_avg:124.15ms
step:604/1393 train_time:73751ms step_avg:124.16ms
step:605/1393 train_time:73878ms step_avg:124.17ms
step:606/1393 train_time:74006ms step_avg:124.17ms
step:607/1393 train_time:74133ms step_avg:124.18ms
step:608/1393 train_time:74260ms step_avg:124.18ms
step:609/1393 train_time:74388ms step_avg:124.19ms
step:610/1393 train_time:74516ms step_avg:124.19ms
step:611/1393 train_time:74644ms step_avg:124.20ms
step:612/1393 train_time:74771ms step_avg:124.20ms
step:613/1393 train_time:74899ms step_avg:124.21ms
step:614/1393 train_time:75027ms step_avg:124.22ms
step:615/1393 train_time:75154ms step_avg:124.22ms
step:616/1393 train_time:75281ms step_avg:124.23ms
step:617/1393 train_time:75409ms step_avg:124.23ms
step:618/1393 train_time:75536ms step_avg:124.24ms
step:619/1393 train_time:75664ms step_avg:124.24ms
step:620/1393 train_time:75791ms step_avg:124.25ms
step:621/1393 train_time:75919ms step_avg:124.25ms
step:622/1393 train_time:76047ms step_avg:124.26ms
step:623/1393 train_time:76175ms step_avg:124.27ms
step:624/1393 train_time:76304ms step_avg:124.27ms
step:625/1393 train_time:76431ms step_avg:124.28ms
step:625/1393 val_loss:3.5860 train_time:76557ms step_avg:124.48ms
step:626/1393 train_time:76580ms step_avg:124.32ms
step:627/1393 train_time:76696ms step_avg:124.30ms
step:628/1393 train_time:76824ms step_avg:124.31ms
step:629/1393 train_time:76951ms step_avg:124.32ms
step:630/1393 train_time:77079ms step_avg:124.32ms
step:631/1393 train_time:77206ms step_avg:124.33ms
step:632/1393 train_time:77333ms step_avg:124.33ms
step:633/1393 train_time:77461ms step_avg:124.34ms
step:634/1393 train_time:77590ms step_avg:124.34ms
step:635/1393 train_time:77720ms step_avg:124.35ms
step:636/1393 train_time:77849ms step_avg:124.36ms
step:637/1393 train_time:77976ms step_avg:124.36ms
step:638/1393 train_time:78103ms step_avg:124.37ms
step:639/1393 train_time:78230ms step_avg:124.37ms
step:640/1393 train_time:78358ms step_avg:124.38ms
step:641/1393 train_time:78485ms step_avg:124.38ms
step:642/1393 train_time:78614ms step_avg:124.39ms
step:643/1393 train_time:78743ms step_avg:124.40ms
step:644/1393 train_time:78871ms step_avg:124.40ms
step:645/1393 train_time:78999ms step_avg:124.41ms
step:646/1393 train_time:79127ms step_avg:124.41ms
step:647/1393 train_time:79254ms step_avg:124.42ms
step:648/1393 train_time:79382ms step_avg:124.42ms
step:649/1393 train_time:79510ms step_avg:124.43ms
step:650/1393 train_time:79638ms step_avg:124.43ms
step:651/1393 train_time:79766ms step_avg:124.44ms
step:652/1393 train_time:79895ms step_avg:124.45ms
step:653/1393 train_time:80023ms step_avg:124.45ms
step:654/1393 train_time:80151ms step_avg:124.46ms
step:655/1393 train_time:80280ms step_avg:124.46ms
step:656/1393 train_time:80407ms step_avg:124.47ms
step:657/1393 train_time:80535ms step_avg:124.47ms
step:658/1393 train_time:80663ms step_avg:124.48ms
step:659/1393 train_time:80792ms step_avg:124.49ms
step:660/1393 train_time:80919ms step_avg:124.49ms
step:661/1393 train_time:81047ms step_avg:124.50ms
step:662/1393 train_time:81175ms step_avg:124.50ms
step:663/1393 train_time:81303ms step_avg:124.51ms
step:664/1393 train_time:81431ms step_avg:124.51ms
step:665/1393 train_time:81560ms step_avg:124.52ms
step:666/1393 train_time:81688ms step_avg:124.52ms
step:667/1393 train_time:81817ms step_avg:124.53ms
step:668/1393 train_time:81944ms step_avg:124.54ms
step:669/1393 train_time:82073ms step_avg:124.54ms
step:670/1393 train_time:82201ms step_avg:124.55ms
step:671/1393 train_time:82330ms step_avg:124.55ms
step:672/1393 train_time:82458ms step_avg:124.56ms
step:673/1393 train_time:82586ms step_avg:124.56ms
step:674/1393 train_time:82714ms step_avg:124.57ms
step:675/1393 train_time:82841ms step_avg:124.57ms
step:676/1393 train_time:82969ms step_avg:124.58ms
step:677/1393 train_time:83098ms step_avg:124.59ms
step:678/1393 train_time:83227ms step_avg:124.59ms
step:679/1393 train_time:83355ms step_avg:124.60ms
step:680/1393 train_time:83483ms step_avg:124.60ms
step:681/1393 train_time:83612ms step_avg:124.61ms
step:682/1393 train_time:83739ms step_avg:124.61ms
step:683/1393 train_time:83867ms step_avg:124.62ms
step:684/1393 train_time:83997ms step_avg:124.62ms
step:685/1393 train_time:84125ms step_avg:124.63ms
step:686/1393 train_time:84254ms step_avg:124.64ms
step:687/1393 train_time:84382ms step_avg:124.64ms
step:688/1393 train_time:84510ms step_avg:124.65ms
step:689/1393 train_time:84639ms step_avg:124.65ms
step:690/1393 train_time:84767ms step_avg:124.66ms
step:691/1393 train_time:84894ms step_avg:124.66ms
step:692/1393 train_time:85023ms step_avg:124.67ms
step:693/1393 train_time:85151ms step_avg:124.67ms
step:694/1393 train_time:85279ms step_avg:124.68ms
step:695/1393 train_time:85407ms step_avg:124.68ms
step:696/1393 train_time:85535ms step_avg:124.69ms
step:697/1393 train_time:85662ms step_avg:124.69ms
step:698/1393 train_time:85791ms step_avg:124.70ms
step:699/1393 train_time:85919ms step_avg:124.70ms
step:700/1393 train_time:86048ms step_avg:124.71ms
step:701/1393 train_time:86176ms step_avg:124.71ms
step:702/1393 train_time:86304ms step_avg:124.72ms
step:703/1393 train_time:86432ms step_avg:124.72ms
step:704/1393 train_time:86560ms step_avg:124.73ms
step:705/1393 train_time:86688ms step_avg:124.73ms
step:706/1393 train_time:86817ms step_avg:124.74ms
step:707/1393 train_time:86945ms step_avg:124.74ms
step:708/1393 train_time:87075ms step_avg:124.75ms
step:709/1393 train_time:87203ms step_avg:124.75ms
step:710/1393 train_time:87331ms step_avg:124.76ms
step:711/1393 train_time:87459ms step_avg:124.76ms
step:712/1393 train_time:87588ms step_avg:124.77ms
step:713/1393 train_time:87715ms step_avg:124.77ms
step:714/1393 train_time:87844ms step_avg:124.78ms
step:715/1393 train_time:87972ms step_avg:124.78ms
step:716/1393 train_time:88100ms step_avg:124.79ms
step:717/1393 train_time:88228ms step_avg:124.79ms
step:718/1393 train_time:88356ms step_avg:124.80ms
step:719/1393 train_time:88483ms step_avg:124.80ms
step:720/1393 train_time:88613ms step_avg:124.81ms
step:721/1393 train_time:88741ms step_avg:124.81ms
step:722/1393 train_time:88870ms step_avg:124.82ms
step:723/1393 train_time:88997ms step_avg:124.82ms
step:724/1393 train_time:89125ms step_avg:124.82ms
step:725/1393 train_time:89255ms step_avg:124.83ms
step:726/1393 train_time:89386ms step_avg:124.84ms
step:727/1393 train_time:89517ms step_avg:124.85ms
step:728/1393 train_time:89647ms step_avg:124.86ms
step:729/1393 train_time:89776ms step_avg:124.86ms
step:730/1393 train_time:89906ms step_avg:124.87ms
step:731/1393 train_time:90036ms step_avg:124.88ms
step:732/1393 train_time:90166ms step_avg:124.88ms
step:733/1393 train_time:90297ms step_avg:124.89ms
step:734/1393 train_time:90427ms step_avg:124.90ms
step:735/1393 train_time:90558ms step_avg:124.91ms
step:736/1393 train_time:90687ms step_avg:124.91ms
step:737/1393 train_time:90818ms step_avg:124.92ms
step:738/1393 train_time:90948ms step_avg:124.93ms
step:739/1393 train_time:91078ms step_avg:124.94ms
step:740/1393 train_time:91208ms step_avg:124.94ms
step:741/1393 train_time:91338ms step_avg:124.95ms
step:742/1393 train_time:91468ms step_avg:124.96ms
step:743/1393 train_time:91598ms step_avg:124.96ms
step:744/1393 train_time:91727ms step_avg:124.97ms
step:745/1393 train_time:91858ms step_avg:124.98ms
step:746/1393 train_time:91988ms step_avg:124.98ms
step:747/1393 train_time:92117ms step_avg:124.99ms
step:748/1393 train_time:92246ms step_avg:125.00ms
step:749/1393 train_time:92376ms step_avg:125.00ms
step:750/1393 train_time:92507ms step_avg:125.01ms
step:750/1393 val_loss:3.5318 train_time:92636ms step_avg:125.18ms
step:751/1393 train_time:92658ms step_avg:125.04ms
step:752/1393 train_time:92774ms step_avg:125.03ms
step:753/1393 train_time:92904ms step_avg:125.04ms
step:754/1393 train_time:93034ms step_avg:125.05ms
step:755/1393 train_time:93164ms step_avg:125.05ms
step:756/1393 train_time:93293ms step_avg:125.06ms
step:757/1393 train_time:93423ms step_avg:125.06ms
step:758/1393 train_time:93552ms step_avg:125.07ms
step:759/1393 train_time:93684ms step_avg:125.08ms
step:760/1393 train_time:93816ms step_avg:125.09ms
step:761/1393 train_time:93946ms step_avg:125.09ms
step:762/1393 train_time:94075ms step_avg:125.10ms
step:763/1393 train_time:94205ms step_avg:125.11ms
step:764/1393 train_time:94335ms step_avg:125.11ms
step:765/1393 train_time:94465ms step_avg:125.12ms
step:766/1393 train_time:94595ms step_avg:125.13ms
step:767/1393 train_time:94726ms step_avg:125.13ms
step:768/1393 train_time:94857ms step_avg:125.14ms
step:769/1393 train_time:94987ms step_avg:125.15ms
step:770/1393 train_time:95117ms step_avg:125.15ms
step:771/1393 train_time:95246ms step_avg:125.16ms
step:772/1393 train_time:95375ms step_avg:125.16ms
step:773/1393 train_time:95505ms step_avg:125.17ms
step:774/1393 train_time:95634ms step_avg:125.18ms
step:775/1393 train_time:95764ms step_avg:125.18ms
step:776/1393 train_time:95894ms step_avg:125.19ms
step:777/1393 train_time:96024ms step_avg:125.19ms
step:778/1393 train_time:96155ms step_avg:125.20ms
step:779/1393 train_time:96285ms step_avg:125.21ms
step:780/1393 train_time:96415ms step_avg:125.21ms
step:781/1393 train_time:96545ms step_avg:125.22ms
step:782/1393 train_time:96675ms step_avg:125.23ms
step:783/1393 train_time:96805ms step_avg:125.23ms
step:784/1393 train_time:96936ms step_avg:125.24ms
step:785/1393 train_time:97066ms step_avg:125.25ms
step:786/1393 train_time:97196ms step_avg:125.25ms
step:787/1393 train_time:97327ms step_avg:125.26ms
step:788/1393 train_time:97456ms step_avg:125.26ms
step:789/1393 train_time:97586ms step_avg:125.27ms
step:790/1393 train_time:97715ms step_avg:125.28ms
step:791/1393 train_time:97845ms step_avg:125.28ms
step:792/1393 train_time:97975ms step_avg:125.29ms
step:793/1393 train_time:98105ms step_avg:125.29ms
step:794/1393 train_time:98235ms step_avg:125.30ms
step:795/1393 train_time:98366ms step_avg:125.31ms
step:796/1393 train_time:98496ms step_avg:125.31ms
step:797/1393 train_time:98626ms step_avg:125.32ms
step:798/1393 train_time:98756ms step_avg:125.32ms
step:799/1393 train_time:98886ms step_avg:125.33ms
step:800/1393 train_time:99016ms step_avg:125.34ms
step:801/1393 train_time:99146ms step_avg:125.34ms
step:802/1393 train_time:99276ms step_avg:125.35ms
step:803/1393 train_time:99406ms step_avg:125.35ms
step:804/1393 train_time:99536ms step_avg:125.36ms
step:805/1393 train_time:99666ms step_avg:125.37ms
step:806/1393 train_time:99795ms step_avg:125.37ms
step:807/1393 train_time:99924ms step_avg:125.37ms
step:808/1393 train_time:100054ms step_avg:125.38ms
step:809/1393 train_time:100184ms step_avg:125.39ms
step:810/1393 train_time:100314ms step_avg:125.39ms
step:811/1393 train_time:100444ms step_avg:125.40ms
step:812/1393 train_time:100574ms step_avg:125.40ms
step:813/1393 train_time:100704ms step_avg:125.41ms
step:814/1393 train_time:100834ms step_avg:125.41ms
step:815/1393 train_time:100964ms step_avg:125.42ms
step:816/1393 train_time:101094ms step_avg:125.43ms
step:817/1393 train_time:101224ms step_avg:125.43ms
step:818/1393 train_time:101354ms step_avg:125.44ms
step:819/1393 train_time:101484ms step_avg:125.44ms
step:820/1393 train_time:101614ms step_avg:125.45ms
step:821/1393 train_time:101745ms step_avg:125.46ms
step:822/1393 train_time:101874ms step_avg:125.46ms
step:823/1393 train_time:102004ms step_avg:125.47ms
step:824/1393 train_time:102134ms step_avg:125.47ms
step:825/1393 train_time:102264ms step_avg:125.48ms
step:826/1393 train_time:102395ms step_avg:125.48ms
step:827/1393 train_time:102526ms step_avg:125.49ms
step:828/1393 train_time:102656ms step_avg:125.50ms
step:829/1393 train_time:102785ms step_avg:125.50ms
step:830/1393 train_time:102916ms step_avg:125.51ms
step:831/1393 train_time:103046ms step_avg:125.51ms
step:832/1393 train_time:103176ms step_avg:125.52ms
step:833/1393 train_time:103306ms step_avg:125.52ms
step:834/1393 train_time:103437ms step_avg:125.53ms
step:835/1393 train_time:103568ms step_avg:125.54ms
step:836/1393 train_time:103698ms step_avg:125.54ms
step:837/1393 train_time:103828ms step_avg:125.55ms
step:838/1393 train_time:103958ms step_avg:125.55ms
step:839/1393 train_time:104089ms step_avg:125.56ms
step:840/1393 train_time:104219ms step_avg:125.56ms
step:841/1393 train_time:104349ms step_avg:125.57ms
step:842/1393 train_time:104478ms step_avg:125.57ms
step:843/1393 train_time:104609ms step_avg:125.58ms
step:844/1393 train_time:104739ms step_avg:125.59ms
step:845/1393 train_time:104869ms step_avg:125.59ms
step:846/1393 train_time:104999ms step_avg:125.60ms
step:847/1393 train_time:105129ms step_avg:125.60ms
step:848/1393 train_time:105261ms step_avg:125.61ms
step:849/1393 train_time:105391ms step_avg:125.62ms
step:850/1393 train_time:105521ms step_avg:125.62ms
step:851/1393 train_time:105652ms step_avg:125.63ms
step:852/1393 train_time:105782ms step_avg:125.63ms
step:853/1393 train_time:105912ms step_avg:125.64ms
step:854/1393 train_time:106043ms step_avg:125.64ms
step:855/1393 train_time:106172ms step_avg:125.65ms
step:856/1393 train_time:106302ms step_avg:125.65ms
step:857/1393 train_time:106432ms step_avg:125.66ms
step:858/1393 train_time:106562ms step_avg:125.66ms
step:859/1393 train_time:106692ms step_avg:125.67ms
step:860/1393 train_time:106823ms step_avg:125.67ms
step:861/1393 train_time:106953ms step_avg:125.68ms
step:862/1393 train_time:107084ms step_avg:125.69ms
step:863/1393 train_time:107215ms step_avg:125.69ms
step:864/1393 train_time:107345ms step_avg:125.70ms
step:865/1393 train_time:107474ms step_avg:125.70ms
step:866/1393 train_time:107605ms step_avg:125.71ms
step:867/1393 train_time:107735ms step_avg:125.71ms
step:868/1393 train_time:107865ms step_avg:125.72ms
step:869/1393 train_time:107994ms step_avg:125.72ms
step:870/1393 train_time:108126ms step_avg:125.73ms
step:871/1393 train_time:108256ms step_avg:125.73ms
step:872/1393 train_time:108388ms step_avg:125.74ms
step:873/1393 train_time:108517ms step_avg:125.74ms
step:874/1393 train_time:108647ms step_avg:125.75ms
step:875/1393 train_time:108778ms step_avg:125.76ms
step:875/1393 val_loss:3.4813 train_time:108908ms step_avg:125.90ms
step:876/1393 train_time:108929ms step_avg:125.78ms
step:877/1393 train_time:109047ms step_avg:125.78ms
step:878/1393 train_time:109177ms step_avg:125.78ms
step:879/1393 train_time:109307ms step_avg:125.78ms
step:880/1393 train_time:109436ms step_avg:125.79ms
step:881/1393 train_time:109566ms step_avg:125.79ms
step:882/1393 train_time:109695ms step_avg:125.80ms
step:883/1393 train_time:109824ms step_avg:125.80ms
step:884/1393 train_time:109957ms step_avg:125.81ms
step:885/1393 train_time:110089ms step_avg:125.82ms
step:886/1393 train_time:110221ms step_avg:125.82ms
step:887/1393 train_time:110350ms step_avg:125.83ms
step:888/1393 train_time:110479ms step_avg:125.83ms
step:889/1393 train_time:110611ms step_avg:125.84ms
step:890/1393 train_time:110741ms step_avg:125.84ms
step:891/1393 train_time:110870ms step_avg:125.85ms
step:892/1393 train_time:111003ms step_avg:125.85ms
step:893/1393 train_time:111133ms step_avg:125.86ms
step:894/1393 train_time:111263ms step_avg:125.86ms
step:895/1393 train_time:111393ms step_avg:125.87ms
step:896/1393 train_time:111523ms step_avg:125.87ms
step:897/1393 train_time:111653ms step_avg:125.88ms
step:898/1393 train_time:111783ms step_avg:125.88ms
step:899/1393 train_time:111914ms step_avg:125.89ms
step:900/1393 train_time:112045ms step_avg:125.89ms
step:901/1393 train_time:112175ms step_avg:125.90ms
step:902/1393 train_time:112305ms step_avg:125.90ms
step:903/1393 train_time:112435ms step_avg:125.91ms
step:904/1393 train_time:112565ms step_avg:125.91ms
step:905/1393 train_time:112695ms step_avg:125.92ms
step:906/1393 train_time:112825ms step_avg:125.92ms
step:907/1393 train_time:112956ms step_avg:125.93ms
step:908/1393 train_time:113086ms step_avg:125.93ms
step:909/1393 train_time:113217ms step_avg:125.94ms
step:910/1393 train_time:113350ms step_avg:125.94ms
step:911/1393 train_time:113480ms step_avg:125.95ms
step:912/1393 train_time:113610ms step_avg:125.95ms
step:913/1393 train_time:113740ms step_avg:125.96ms
step:914/1393 train_time:113870ms step_avg:125.96ms
step:915/1393 train_time:114000ms step_avg:125.97ms
step:916/1393 train_time:114131ms step_avg:125.97ms
step:917/1393 train_time:114263ms step_avg:125.98ms
step:918/1393 train_time:114393ms step_avg:125.98ms
step:919/1393 train_time:114527ms step_avg:125.99ms
step:920/1393 train_time:114658ms step_avg:126.00ms
step:921/1393 train_time:114788ms step_avg:126.00ms
step:922/1393 train_time:114918ms step_avg:126.01ms
step:923/1393 train_time:115048ms step_avg:126.01ms
step:924/1393 train_time:115177ms step_avg:126.01ms
step:925/1393 train_time:115308ms step_avg:126.02ms
step:926/1393 train_time:115438ms step_avg:126.02ms
step:927/1393 train_time:115568ms step_avg:126.03ms
step:928/1393 train_time:115699ms step_avg:126.03ms
step:929/1393 train_time:115830ms step_avg:126.04ms
step:930/1393 train_time:115960ms step_avg:126.04ms
step:931/1393 train_time:116092ms step_avg:126.05ms
step:932/1393 train_time:116223ms step_avg:126.06ms
step:933/1393 train_time:116355ms step_avg:126.06ms
step:934/1393 train_time:116487ms step_avg:126.07ms
step:935/1393 train_time:116620ms step_avg:126.08ms
step:936/1393 train_time:116752ms step_avg:126.08ms
step:937/1393 train_time:116886ms step_avg:126.09ms
step:938/1393 train_time:117019ms step_avg:126.10ms
step:939/1393 train_time:117150ms step_avg:126.10ms
step:940/1393 train_time:117283ms step_avg:126.11ms
step:941/1393 train_time:117414ms step_avg:126.12ms
step:942/1393 train_time:117546ms step_avg:126.12ms
step:943/1393 train_time:117681ms step_avg:126.13ms
step:944/1393 train_time:117814ms step_avg:126.14ms
step:945/1393 train_time:117947ms step_avg:126.15ms
step:946/1393 train_time:118079ms step_avg:126.15ms
step:947/1393 train_time:118211ms step_avg:126.16ms
step:948/1393 train_time:118343ms step_avg:126.17ms
step:949/1393 train_time:118475ms step_avg:126.17ms
step:950/1393 train_time:118608ms step_avg:126.18ms
step:951/1393 train_time:118741ms step_avg:126.19ms
step:952/1393 train_time:118872ms step_avg:126.19ms
step:953/1393 train_time:119005ms step_avg:126.20ms
step:954/1393 train_time:119138ms step_avg:126.21ms
step:955/1393 train_time:119269ms step_avg:126.21ms
step:956/1393 train_time:119403ms step_avg:126.22ms
step:957/1393 train_time:119535ms step_avg:126.22ms
step:958/1393 train_time:119667ms step_avg:126.23ms
step:959/1393 train_time:119799ms step_avg:126.24ms
step:960/1393 train_time:119931ms step_avg:126.24ms
step:961/1393 train_time:120064ms step_avg:126.25ms
step:962/1393 train_time:120195ms step_avg:126.26ms
step:963/1393 train_time:120328ms step_avg:126.26ms
step:964/1393 train_time:120460ms step_avg:126.27ms
step:965/1393 train_time:120592ms step_avg:126.27ms
step:966/1393 train_time:120723ms step_avg:126.28ms
step:967/1393 train_time:120855ms step_avg:126.29ms
step:968/1393 train_time:120987ms step_avg:126.29ms
step:969/1393 train_time:121120ms step_avg:126.30ms
step:970/1393 train_time:121252ms step_avg:126.30ms
step:971/1393 train_time:121384ms step_avg:126.31ms
step:972/1393 train_time:121516ms step_avg:126.32ms
step:973/1393 train_time:121648ms step_avg:126.32ms
step:974/1393 train_time:121779ms step_avg:126.33ms
step:975/1393 train_time:121911ms step_avg:126.33ms
step:976/1393 train_time:122043ms step_avg:126.34ms
step:977/1393 train_time:122175ms step_avg:126.34ms
step:978/1393 train_time:122307ms step_avg:126.35ms
step:979/1393 train_time:122440ms step_avg:126.36ms
step:980/1393 train_time:122571ms step_avg:126.36ms
step:981/1393 train_time:122702ms step_avg:126.37ms
step:982/1393 train_time:122834ms step_avg:126.37ms
step:983/1393 train_time:122965ms step_avg:126.38ms
step:984/1393 train_time:123096ms step_avg:126.38ms
step:985/1393 train_time:123228ms step_avg:126.39ms
step:986/1393 train_time:123362ms step_avg:126.40ms
step:987/1393 train_time:123494ms step_avg:126.40ms
step:988/1393 train_time:123626ms step_avg:126.41ms
step:989/1393 train_time:123759ms step_avg:126.41ms
step:990/1393 train_time:123890ms step_avg:126.42ms
step:991/1393 train_time:124022ms step_avg:126.42ms
step:992/1393 train_time:124154ms step_avg:126.43ms
step:993/1393 train_time:124290ms step_avg:126.44ms
step:994/1393 train_time:124421ms step_avg:126.44ms
step:995/1393 train_time:124553ms step_avg:126.45ms
step:996/1393 train_time:124684ms step_avg:126.45ms
step:997/1393 train_time:124816ms step_avg:126.46ms
step:998/1393 train_time:124948ms step_avg:126.47ms
step:999/1393 train_time:125079ms step_avg:126.47ms
step:1000/1393 train_time:125210ms step_avg:126.48ms
step:1000/1393 val_loss:3.4175 train_time:125341ms step_avg:126.61ms
step:1001/1393 train_time:125364ms step_avg:126.50ms
step:1002/1393 train_time:125480ms step_avg:126.49ms
step:1003/1393 train_time:125613ms step_avg:126.50ms
step:1004/1393 train_time:125744ms step_avg:126.50ms
step:1005/1393 train_time:125875ms step_avg:126.51ms
step:1006/1393 train_time:126005ms step_avg:126.51ms
step:1007/1393 train_time:126136ms step_avg:126.52ms
step:1008/1393 train_time:126268ms step_avg:126.52ms
step:1009/1393 train_time:126405ms step_avg:126.53ms
step:1010/1393 train_time:126538ms step_avg:126.54ms
step:1011/1393 train_time:126672ms step_avg:126.55ms
step:1012/1393 train_time:126803ms step_avg:126.55ms
step:1013/1393 train_time:126936ms step_avg:126.56ms
step:1014/1393 train_time:127066ms step_avg:126.56ms
step:1015/1393 train_time:127197ms step_avg:126.56ms
step:1016/1393 train_time:127329ms step_avg:126.57ms
step:1017/1393 train_time:127462ms step_avg:126.58ms
step:1018/1393 train_time:127594ms step_avg:126.58ms
step:1019/1393 train_time:127727ms step_avg:126.59ms
step:1020/1393 train_time:127859ms step_avg:126.59ms
step:1021/1393 train_time:127990ms step_avg:126.60ms
step:1022/1393 train_time:128121ms step_avg:126.60ms
step:1023/1393 train_time:128254ms step_avg:126.61ms
step:1024/1393 train_time:128386ms step_avg:126.61ms
step:1025/1393 train_time:128518ms step_avg:126.62ms
step:1026/1393 train_time:128650ms step_avg:126.62ms
step:1027/1393 train_time:128783ms step_avg:126.63ms
step:1028/1393 train_time:128916ms step_avg:126.64ms
step:1029/1393 train_time:129048ms step_avg:126.64ms
step:1030/1393 train_time:129179ms step_avg:126.65ms
step:1031/1393 train_time:129310ms step_avg:126.65ms
step:1032/1393 train_time:129443ms step_avg:126.66ms
step:1033/1393 train_time:129574ms step_avg:126.66ms
step:1034/1393 train_time:129707ms step_avg:126.67ms
step:1035/1393 train_time:129840ms step_avg:126.67ms
step:1036/1393 train_time:129972ms step_avg:126.68ms
step:1037/1393 train_time:130104ms step_avg:126.68ms
step:1038/1393 train_time:130237ms step_avg:126.69ms
step:1039/1393 train_time:130369ms step_avg:126.69ms
step:1040/1393 train_time:130501ms step_avg:126.70ms
step:1041/1393 train_time:130633ms step_avg:126.71ms
step:1042/1393 train_time:130765ms step_avg:126.71ms
step:1043/1393 train_time:130897ms step_avg:126.72ms
step:1044/1393 train_time:131031ms step_avg:126.72ms
step:1045/1393 train_time:131163ms step_avg:126.73ms
step:1046/1393 train_time:131296ms step_avg:126.73ms
step:1047/1393 train_time:131428ms step_avg:126.74ms
step:1048/1393 train_time:131559ms step_avg:126.74ms
step:1049/1393 train_time:131692ms step_avg:126.75ms
step:1050/1393 train_time:131825ms step_avg:126.75ms
step:1051/1393 train_time:131959ms step_avg:126.76ms
step:1052/1393 train_time:132091ms step_avg:126.77ms
step:1053/1393 train_time:132224ms step_avg:126.77ms
step:1054/1393 train_time:132356ms step_avg:126.78ms
step:1055/1393 train_time:132488ms step_avg:126.78ms
step:1056/1393 train_time:132620ms step_avg:126.79ms
step:1057/1393 train_time:132751ms step_avg:126.79ms
step:1058/1393 train_time:132883ms step_avg:126.80ms
step:1059/1393 train_time:133015ms step_avg:126.80ms
step:1060/1393 train_time:133150ms step_avg:126.81ms
step:1061/1393 train_time:133282ms step_avg:126.81ms
step:1062/1393 train_time:133415ms step_avg:126.82ms
step:1063/1393 train_time:133546ms step_avg:126.82ms
step:1064/1393 train_time:133678ms step_avg:126.83ms
step:1065/1393 train_time:133810ms step_avg:126.83ms
step:1066/1393 train_time:133942ms step_avg:126.84ms
step:1067/1393 train_time:134074ms step_avg:126.84ms
step:1068/1393 train_time:134206ms step_avg:126.85ms
step:1069/1393 train_time:134340ms step_avg:126.86ms
step:1070/1393 train_time:134472ms step_avg:126.86ms
step:1071/1393 train_time:134606ms step_avg:126.87ms
step:1072/1393 train_time:134737ms step_avg:126.87ms
step:1073/1393 train_time:134869ms step_avg:126.88ms
step:1074/1393 train_time:135000ms step_avg:126.88ms
step:1075/1393 train_time:135134ms step_avg:126.89ms
step:1076/1393 train_time:135265ms step_avg:126.89ms
step:1077/1393 train_time:135398ms step_avg:126.90ms
step:1078/1393 train_time:135530ms step_avg:126.90ms
step:1079/1393 train_time:135664ms step_avg:126.91ms
step:1080/1393 train_time:135795ms step_avg:126.91ms
step:1081/1393 train_time:135927ms step_avg:126.92ms
step:1082/1393 train_time:136059ms step_avg:126.92ms
step:1083/1393 train_time:136191ms step_avg:126.93ms
step:1084/1393 train_time:136324ms step_avg:126.93ms
step:1085/1393 train_time:136457ms step_avg:126.94ms
step:1086/1393 train_time:136590ms step_avg:126.94ms
step:1087/1393 train_time:136723ms step_avg:126.95ms
step:1088/1393 train_time:136855ms step_avg:126.95ms
step:1089/1393 train_time:136990ms step_avg:126.96ms
step:1090/1393 train_time:137123ms step_avg:126.97ms
step:1091/1393 train_time:137255ms step_avg:126.97ms
step:1092/1393 train_time:137388ms step_avg:126.98ms
step:1093/1393 train_time:137520ms step_avg:126.98ms
step:1094/1393 train_time:137651ms step_avg:126.98ms
step:1095/1393 train_time:137783ms step_avg:126.99ms
step:1096/1393 train_time:137918ms step_avg:127.00ms
step:1097/1393 train_time:138051ms step_avg:127.00ms
step:1098/1393 train_time:138184ms step_avg:127.01ms
step:1099/1393 train_time:138316ms step_avg:127.01ms
step:1100/1393 train_time:138448ms step_avg:127.02ms
step:1101/1393 train_time:138581ms step_avg:127.02ms
step:1102/1393 train_time:138713ms step_avg:127.03ms
step:1103/1393 train_time:138845ms step_avg:127.03ms
step:1104/1393 train_time:138977ms step_avg:127.04ms
step:1105/1393 train_time:139111ms step_avg:127.04ms
step:1106/1393 train_time:139243ms step_avg:127.05ms
step:1107/1393 train_time:139375ms step_avg:127.05ms
step:1108/1393 train_time:139510ms step_avg:127.06ms
step:1109/1393 train_time:139641ms step_avg:127.06ms
step:1110/1393 train_time:139774ms step_avg:127.07ms
step:1111/1393 train_time:139907ms step_avg:127.07ms
step:1112/1393 train_time:140039ms step_avg:127.08ms
step:1113/1393 train_time:140170ms step_avg:127.08ms
step:1114/1393 train_time:140303ms step_avg:127.09ms
step:1115/1393 train_time:140434ms step_avg:127.09ms
step:1116/1393 train_time:140566ms step_avg:127.09ms
step:1117/1393 train_time:140698ms step_avg:127.10ms
step:1118/1393 train_time:140832ms step_avg:127.10ms
step:1119/1393 train_time:140964ms step_avg:127.11ms
step:1120/1393 train_time:141096ms step_avg:127.11ms
step:1121/1393 train_time:141228ms step_avg:127.12ms
step:1122/1393 train_time:141359ms step_avg:127.12ms
step:1123/1393 train_time:141491ms step_avg:127.13ms
step:1124/1393 train_time:141624ms step_avg:127.13ms
step:1125/1393 train_time:141756ms step_avg:127.14ms
step:1125/1393 val_loss:3.3671 train_time:141888ms step_avg:127.25ms
step:1126/1393 train_time:141909ms step_avg:127.16ms
step:1127/1393 train_time:142027ms step_avg:127.15ms
step:1128/1393 train_time:142160ms step_avg:127.16ms
step:1129/1393 train_time:142294ms step_avg:127.16ms
step:1130/1393 train_time:142425ms step_avg:127.16ms
step:1131/1393 train_time:142558ms step_avg:127.17ms
step:1132/1393 train_time:142689ms step_avg:127.17ms
step:1133/1393 train_time:142819ms step_avg:127.18ms
step:1134/1393 train_time:142954ms step_avg:127.18ms
step:1135/1393 train_time:143087ms step_avg:127.19ms
step:1136/1393 train_time:143222ms step_avg:127.20ms
step:1137/1393 train_time:143355ms step_avg:127.20ms
step:1138/1393 train_time:143489ms step_avg:127.21ms
step:1139/1393 train_time:143622ms step_avg:127.21ms
step:1140/1393 train_time:143756ms step_avg:127.22ms
step:1141/1393 train_time:143890ms step_avg:127.22ms
step:1142/1393 train_time:144023ms step_avg:127.23ms
step:1143/1393 train_time:144159ms step_avg:127.24ms
step:1144/1393 train_time:144292ms step_avg:127.24ms
step:1145/1393 train_time:144424ms step_avg:127.25ms
step:1146/1393 train_time:144557ms step_avg:127.25ms
step:1147/1393 train_time:144691ms step_avg:127.26ms
step:1148/1393 train_time:144825ms step_avg:127.26ms
step:1149/1393 train_time:144959ms step_avg:127.27ms
step:1150/1393 train_time:145092ms step_avg:127.27ms
step:1151/1393 train_time:145226ms step_avg:127.28ms
step:1152/1393 train_time:145359ms step_avg:127.28ms
step:1153/1393 train_time:145493ms step_avg:127.29ms
step:1154/1393 train_time:145626ms step_avg:127.30ms
step:1155/1393 train_time:145761ms step_avg:127.30ms
step:1156/1393 train_time:145898ms step_avg:127.31ms
step:1157/1393 train_time:146032ms step_avg:127.32ms
step:1158/1393 train_time:146166ms step_avg:127.32ms
step:1159/1393 train_time:146299ms step_avg:127.33ms
step:1160/1393 train_time:146433ms step_avg:127.33ms
step:1161/1393 train_time:146567ms step_avg:127.34ms
step:1162/1393 train_time:146700ms step_avg:127.34ms
step:1163/1393 train_time:146834ms step_avg:127.35ms
step:1164/1393 train_time:146968ms step_avg:127.36ms
step:1165/1393 train_time:147101ms step_avg:127.36ms
step:1166/1393 train_time:147235ms step_avg:127.37ms
step:1167/1393 train_time:147368ms step_avg:127.37ms
step:1168/1393 train_time:147502ms step_avg:127.38ms
step:1169/1393 train_time:147636ms step_avg:127.38ms
step:1170/1393 train_time:147769ms step_avg:127.39ms
step:1171/1393 train_time:147903ms step_avg:127.39ms
step:1172/1393 train_time:148036ms step_avg:127.40ms
step:1173/1393 train_time:148170ms step_avg:127.40ms
step:1174/1393 train_time:148307ms step_avg:127.41ms
step:1175/1393 train_time:148441ms step_avg:127.42ms
step:1176/1393 train_time:148576ms step_avg:127.42ms
step:1177/1393 train_time:148713ms step_avg:127.43ms
step:1178/1393 train_time:148846ms step_avg:127.44ms
step:1179/1393 train_time:148980ms step_avg:127.44ms
step:1180/1393 train_time:149116ms step_avg:127.45ms
step:1181/1393 train_time:149250ms step_avg:127.46ms
step:1182/1393 train_time:149383ms step_avg:127.46ms
step:1183/1393 train_time:149518ms step_avg:127.47ms
step:1184/1393 train_time:149651ms step_avg:127.47ms
step:1185/1393 train_time:149786ms step_avg:127.48ms
step:1186/1393 train_time:149919ms step_avg:127.48ms
step:1187/1393 train_time:150058ms step_avg:127.49ms
step:1188/1393 train_time:150191ms step_avg:127.50ms
step:1189/1393 train_time:150327ms step_avg:127.50ms
step:1190/1393 train_time:150460ms step_avg:127.51ms
step:1191/1393 train_time:150594ms step_avg:127.51ms
step:1192/1393 train_time:150728ms step_avg:127.52ms
step:1193/1393 train_time:150861ms step_avg:127.52ms
step:1194/1393 train_time:150994ms step_avg:127.53ms
step:1195/1393 train_time:151127ms step_avg:127.53ms
step:1196/1393 train_time:151261ms step_avg:127.54ms
step:1197/1393 train_time:151396ms step_avg:127.55ms
step:1198/1393 train_time:151532ms step_avg:127.55ms
step:1199/1393 train_time:151666ms step_avg:127.56ms
step:1200/1393 train_time:151799ms step_avg:127.56ms
step:1201/1393 train_time:151931ms step_avg:127.57ms
step:1202/1393 train_time:152068ms step_avg:127.57ms
step:1203/1393 train_time:152206ms step_avg:127.58ms
step:1204/1393 train_time:152339ms step_avg:127.59ms
step:1205/1393 train_time:152473ms step_avg:127.59ms
step:1206/1393 train_time:152608ms step_avg:127.60ms
step:1207/1393 train_time:152740ms step_avg:127.60ms
step:1208/1393 train_time:152874ms step_avg:127.61ms
step:1209/1393 train_time:153008ms step_avg:127.61ms
step:1210/1393 train_time:153144ms step_avg:127.62ms
step:1211/1393 train_time:153278ms step_avg:127.63ms
step:1212/1393 train_time:153411ms step_avg:127.63ms
step:1213/1393 train_time:153544ms step_avg:127.63ms
step:1214/1393 train_time:153679ms step_avg:127.64ms
step:1215/1393 train_time:153814ms step_avg:127.65ms
step:1216/1393 train_time:153947ms step_avg:127.65ms
step:1217/1393 train_time:154081ms step_avg:127.66ms
step:1218/1393 train_time:154214ms step_avg:127.66ms
step:1219/1393 train_time:154347ms step_avg:127.66ms
step:1220/1393 train_time:154480ms step_avg:127.67ms
step:1221/1393 train_time:154613ms step_avg:127.67ms
step:1222/1393 train_time:154746ms step_avg:127.68ms
step:1223/1393 train_time:154879ms step_avg:127.68ms
step:1224/1393 train_time:155013ms step_avg:127.69ms
step:1225/1393 train_time:155149ms step_avg:127.69ms
step:1226/1393 train_time:155281ms step_avg:127.70ms
step:1227/1393 train_time:155415ms step_avg:127.70ms
step:1228/1393 train_time:155549ms step_avg:127.71ms
step:1229/1393 train_time:155682ms step_avg:127.71ms
step:1230/1393 train_time:155816ms step_avg:127.72ms
step:1231/1393 train_time:155951ms step_avg:127.72ms
step:1232/1393 train_time:156087ms step_avg:127.73ms
step:1233/1393 train_time:156222ms step_avg:127.74ms
step:1234/1393 train_time:156355ms step_avg:127.74ms
step:1235/1393 train_time:156489ms step_avg:127.75ms
step:1236/1393 train_time:156622ms step_avg:127.75ms
step:1237/1393 train_time:156755ms step_avg:127.75ms
step:1238/1393 train_time:156891ms step_avg:127.76ms
step:1239/1393 train_time:157025ms step_avg:127.77ms
step:1240/1393 train_time:157159ms step_avg:127.77ms
step:1241/1393 train_time:157295ms step_avg:127.78ms
step:1242/1393 train_time:157428ms step_avg:127.78ms
step:1243/1393 train_time:157563ms step_avg:127.79ms
step:1244/1393 train_time:157696ms step_avg:127.79ms
step:1245/1393 train_time:157830ms step_avg:127.80ms
step:1246/1393 train_time:157963ms step_avg:127.80ms
step:1247/1393 train_time:158097ms step_avg:127.81ms
step:1248/1393 train_time:158230ms step_avg:127.81ms
step:1249/1393 train_time:158362ms step_avg:127.81ms
step:1250/1393 train_time:158496ms step_avg:127.82ms
step:1250/1393 val_loss:3.3201 train_time:158630ms step_avg:127.93ms
step:1251/1393 train_time:158651ms step_avg:127.84ms
step:1252/1393 train_time:158773ms step_avg:127.84ms
step:1253/1393 train_time:158904ms step_avg:127.84ms
step:1254/1393 train_time:159036ms step_avg:127.84ms
step:1255/1393 train_time:159175ms step_avg:127.85ms
step:1256/1393 train_time:159308ms step_avg:127.86ms
step:1257/1393 train_time:159441ms step_avg:127.86ms
step:1258/1393 train_time:159574ms step_avg:127.86ms
step:1259/1393 train_time:159710ms step_avg:127.87ms
step:1260/1393 train_time:159844ms step_avg:127.88ms
step:1261/1393 train_time:159977ms step_avg:127.88ms
step:1262/1393 train_time:160113ms step_avg:127.89ms
step:1263/1393 train_time:160247ms step_avg:127.89ms
step:1264/1393 train_time:160380ms step_avg:127.89ms
step:1265/1393 train_time:160513ms step_avg:127.90ms
step:1266/1393 train_time:160648ms step_avg:127.90ms
step:1267/1393 train_time:160781ms step_avg:127.91ms
step:1268/1393 train_time:160916ms step_avg:127.91ms
step:1269/1393 train_time:161051ms step_avg:127.92ms
step:1270/1393 train_time:161185ms step_avg:127.92ms
step:1271/1393 train_time:161319ms step_avg:127.93ms
step:1272/1393 train_time:161451ms step_avg:127.93ms
step:1273/1393 train_time:161584ms step_avg:127.94ms
step:1274/1393 train_time:161718ms step_avg:127.94ms
step:1275/1393 train_time:161853ms step_avg:127.95ms
step:1276/1393 train_time:161986ms step_avg:127.95ms
step:1277/1393 train_time:162120ms step_avg:127.96ms
step:1278/1393 train_time:162254ms step_avg:127.96ms
step:1279/1393 train_time:162387ms step_avg:127.96ms
step:1280/1393 train_time:162524ms step_avg:127.97ms
step:1281/1393 train_time:162658ms step_avg:127.98ms
step:1282/1393 train_time:162790ms step_avg:127.98ms
step:1283/1393 train_time:162925ms step_avg:127.99ms
step:1284/1393 train_time:163060ms step_avg:127.99ms
step:1285/1393 train_time:163194ms step_avg:127.99ms
step:1286/1393 train_time:163329ms step_avg:128.00ms
step:1287/1393 train_time:163462ms step_avg:128.00ms
step:1288/1393 train_time:163598ms step_avg:128.01ms
step:1289/1393 train_time:163733ms step_avg:128.02ms
step:1290/1393 train_time:163869ms step_avg:128.02ms
step:1291/1393 train_time:164004ms step_avg:128.03ms
step:1292/1393 train_time:164138ms step_avg:128.03ms
step:1293/1393 train_time:164274ms step_avg:128.04ms
step:1294/1393 train_time:164407ms step_avg:128.04ms
step:1295/1393 train_time:164540ms step_avg:128.05ms
step:1296/1393 train_time:164675ms step_avg:128.05ms
step:1297/1393 train_time:164809ms step_avg:128.06ms
step:1298/1393 train_time:164942ms step_avg:128.06ms
step:1299/1393 train_time:165075ms step_avg:128.06ms
step:1300/1393 train_time:165208ms step_avg:128.07ms
step:1301/1393 train_time:165341ms step_avg:128.07ms
step:1302/1393 train_time:165474ms step_avg:128.08ms
step:1303/1393 train_time:165609ms step_avg:128.08ms
step:1304/1393 train_time:165744ms step_avg:128.09ms
step:1305/1393 train_time:165879ms step_avg:128.09ms
step:1306/1393 train_time:166012ms step_avg:128.10ms
step:1307/1393 train_time:166144ms step_avg:128.10ms
step:1308/1393 train_time:166279ms step_avg:128.10ms
step:1309/1393 train_time:166414ms step_avg:128.11ms
step:1310/1393 train_time:166547ms step_avg:128.11ms
step:1311/1393 train_time:166681ms step_avg:128.12ms
step:1312/1393 train_time:166815ms step_avg:128.12ms
step:1313/1393 train_time:166951ms step_avg:128.13ms
step:1314/1393 train_time:167084ms step_avg:128.13ms
step:1315/1393 train_time:167219ms step_avg:128.14ms
step:1316/1393 train_time:167352ms step_avg:128.14ms
step:1317/1393 train_time:167486ms step_avg:128.15ms
step:1318/1393 train_time:167620ms step_avg:128.15ms
step:1319/1393 train_time:167756ms step_avg:128.16ms
step:1320/1393 train_time:167890ms step_avg:128.16ms
step:1321/1393 train_time:168024ms step_avg:128.16ms
step:1322/1393 train_time:168160ms step_avg:128.17ms
step:1323/1393 train_time:168293ms step_avg:128.17ms
step:1324/1393 train_time:168426ms step_avg:128.18ms
step:1325/1393 train_time:168561ms step_avg:128.18ms
step:1326/1393 train_time:168696ms step_avg:128.19ms
step:1327/1393 train_time:168829ms step_avg:128.19ms
step:1328/1393 train_time:168963ms step_avg:128.20ms
step:1329/1393 train_time:169101ms step_avg:128.20ms
step:1330/1393 train_time:169236ms step_avg:128.21ms
step:1331/1393 train_time:169373ms step_avg:128.22ms
step:1332/1393 train_time:169509ms step_avg:128.22ms
step:1333/1393 train_time:169643ms step_avg:128.23ms
step:1334/1393 train_time:169776ms step_avg:128.23ms
step:1335/1393 train_time:169909ms step_avg:128.23ms
step:1336/1393 train_time:170047ms step_avg:128.24ms
step:1337/1393 train_time:170182ms step_avg:128.25ms
step:1338/1393 train_time:170316ms step_avg:128.25ms
step:1339/1393 train_time:170451ms step_avg:128.25ms
step:1340/1393 train_time:170586ms step_avg:128.26ms
step:1341/1393 train_time:170719ms step_avg:128.26ms
step:1342/1393 train_time:170852ms step_avg:128.27ms
step:1343/1393 train_time:170986ms step_avg:128.27ms
step:1344/1393 train_time:171121ms step_avg:128.28ms
step:1345/1393 train_time:171256ms step_avg:128.28ms
step:1346/1393 train_time:171391ms step_avg:128.29ms
step:1347/1393 train_time:171526ms step_avg:128.29ms
step:1348/1393 train_time:171661ms step_avg:128.30ms
step:1349/1393 train_time:171796ms step_avg:128.30ms
step:1350/1393 train_time:171930ms step_avg:128.31ms
step:1351/1393 train_time:172065ms step_avg:128.31ms
step:1352/1393 train_time:172203ms step_avg:128.32ms
step:1353/1393 train_time:172340ms step_avg:128.32ms
step:1354/1393 train_time:172474ms step_avg:128.33ms
step:1355/1393 train_time:172608ms step_avg:128.33ms
step:1356/1393 train_time:172741ms step_avg:128.34ms
step:1357/1393 train_time:172876ms step_avg:128.34ms
step:1358/1393 train_time:173012ms step_avg:128.35ms
step:1359/1393 train_time:173147ms step_avg:128.35ms
step:1360/1393 train_time:173284ms step_avg:128.36ms
step:1361/1393 train_time:173418ms step_avg:128.36ms
step:1362/1393 train_time:173556ms step_avg:128.37ms
step:1363/1393 train_time:173694ms step_avg:128.38ms
step:1364/1393 train_time:173829ms step_avg:128.38ms
step:1365/1393 train_time:173962ms step_avg:128.39ms
step:1366/1393 train_time:174097ms step_avg:128.39ms
step:1367/1393 train_time:174232ms step_avg:128.40ms
step:1368/1393 train_time:174367ms step_avg:128.40ms
step:1369/1393 train_time:174506ms step_avg:128.41ms
step:1370/1393 train_time:174643ms step_avg:128.41ms
step:1371/1393 train_time:174779ms step_avg:128.42ms
step:1372/1393 train_time:174915ms step_avg:128.43ms
step:1373/1393 train_time:175050ms step_avg:128.43ms
step:1374/1393 train_time:175186ms step_avg:128.44ms
step:1375/1393 train_time:175319ms step_avg:128.44ms
step:1375/1393 val_loss:3.2862 train_time:175452ms step_avg:128.54ms
step:1376/1393 train_time:175473ms step_avg:128.46ms
step:1377/1393 train_time:175596ms step_avg:128.45ms
step:1378/1393 train_time:175731ms step_avg:128.46ms
step:1379/1393 train_time:175865ms step_avg:128.46ms
step:1380/1393 train_time:175999ms step_avg:128.47ms
step:1381/1393 train_time:176135ms step_avg:128.47ms
step:1382/1393 train_time:176271ms step_avg:128.48ms
step:1383/1393 train_time:176405ms step_avg:128.48ms
step:1384/1393 train_time:176543ms step_avg:128.49ms
step:1385/1393 train_time:176677ms step_avg:128.49ms
step:1386/1393 train_time:176811ms step_avg:128.50ms
step:1387/1393 train_time:176947ms step_avg:128.50ms
step:1388/1393 train_time:177082ms step_avg:128.51ms
step:1389/1393 train_time:177218ms step_avg:128.51ms
step:1390/1393 train_time:177352ms step_avg:128.52ms
step:1391/1393 train_time:177487ms step_avg:128.52ms
step:1392/1393 train_time:177623ms step_avg:128.53ms
step:1393/1393 train_time:177758ms step_avg:128.53ms
step:1393/1393 val_loss:3.2827 train_time:177891ms step_avg:128.63ms
peak memory allocated: 38711 MiB reserved: 50198 MiB
