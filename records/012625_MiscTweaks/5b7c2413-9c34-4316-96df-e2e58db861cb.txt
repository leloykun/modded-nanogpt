import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sat Jan 25 22:59:56 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23267ms step_avg:nanms
step:2/1770 train_time:23773ms step_avg:nanms
step:3/1770 train_time:23868ms step_avg:nanms
step:4/1770 train_time:23961ms step_avg:nanms
step:5/1770 train_time:24055ms step_avg:nanms
step:6/1770 train_time:24148ms step_avg:nanms
step:7/1770 train_time:24242ms step_avg:nanms
step:8/1770 train_time:24335ms step_avg:nanms
step:9/1770 train_time:24429ms step_avg:nanms
step:10/1770 train_time:24523ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.23ms
step:14/1770 train_time:378ms step_avg:94.60ms
step:15/1770 train_time:472ms step_avg:94.41ms
step:16/1770 train_time:566ms step_avg:94.33ms
step:17/1770 train_time:660ms step_avg:94.32ms
step:18/1770 train_time:754ms step_avg:94.25ms
step:19/1770 train_time:848ms step_avg:94.24ms
step:20/1770 train_time:942ms step_avg:94.24ms
step:21/1770 train_time:1037ms step_avg:94.24ms
step:22/1770 train_time:1130ms step_avg:94.19ms
step:23/1770 train_time:1224ms step_avg:94.16ms
step:24/1770 train_time:1318ms step_avg:94.14ms
step:25/1770 train_time:1412ms step_avg:94.12ms
step:26/1770 train_time:1506ms step_avg:94.13ms
step:27/1770 train_time:1600ms step_avg:94.10ms
step:28/1770 train_time:1694ms step_avg:94.10ms
step:29/1770 train_time:1788ms step_avg:94.10ms
step:30/1770 train_time:1883ms step_avg:94.13ms
step:31/1770 train_time:1977ms step_avg:94.12ms
step:32/1770 train_time:2071ms step_avg:94.13ms
step:33/1770 train_time:2165ms step_avg:94.11ms
step:34/1770 train_time:2258ms step_avg:94.10ms
step:35/1770 train_time:2352ms step_avg:94.09ms
step:36/1770 train_time:2447ms step_avg:94.11ms
step:37/1770 train_time:2541ms step_avg:94.09ms
step:38/1770 train_time:2634ms step_avg:94.08ms
step:39/1770 train_time:2728ms step_avg:94.07ms
step:40/1770 train_time:2822ms step_avg:94.08ms
step:41/1770 train_time:2917ms step_avg:94.08ms
step:42/1770 train_time:3010ms step_avg:94.07ms
step:43/1770 train_time:3104ms step_avg:94.07ms
step:44/1770 train_time:3198ms step_avg:94.05ms
step:45/1770 train_time:3292ms step_avg:94.05ms
step:46/1770 train_time:3385ms step_avg:94.03ms
step:47/1770 train_time:3479ms step_avg:94.02ms
step:48/1770 train_time:3573ms step_avg:94.02ms
step:49/1770 train_time:3667ms step_avg:94.02ms
step:50/1770 train_time:3761ms step_avg:94.03ms
step:51/1770 train_time:3855ms step_avg:94.02ms
step:52/1770 train_time:3950ms step_avg:94.04ms
step:53/1770 train_time:4043ms step_avg:94.02ms
step:54/1770 train_time:4136ms step_avg:94.01ms
step:55/1770 train_time:4230ms step_avg:94.00ms
step:56/1770 train_time:4324ms step_avg:93.99ms
step:57/1770 train_time:4418ms step_avg:93.99ms
step:58/1770 train_time:4511ms step_avg:93.99ms
step:59/1770 train_time:4605ms step_avg:93.98ms
step:60/1770 train_time:4699ms step_avg:93.98ms
step:61/1770 train_time:4793ms step_avg:93.97ms
step:62/1770 train_time:4887ms step_avg:93.99ms
step:63/1770 train_time:4982ms step_avg:94.00ms
step:64/1770 train_time:5076ms step_avg:94.00ms
step:65/1770 train_time:5170ms step_avg:94.00ms
step:66/1770 train_time:5264ms step_avg:94.00ms
step:67/1770 train_time:5358ms step_avg:93.99ms
step:68/1770 train_time:5451ms step_avg:93.98ms
step:69/1770 train_time:5545ms step_avg:93.98ms
step:70/1770 train_time:5639ms step_avg:93.98ms
step:71/1770 train_time:5733ms step_avg:93.98ms
step:72/1770 train_time:5826ms step_avg:93.97ms
step:73/1770 train_time:5920ms step_avg:93.97ms
step:74/1770 train_time:6014ms step_avg:93.97ms
step:75/1770 train_time:6108ms step_avg:93.96ms
step:76/1770 train_time:6201ms step_avg:93.96ms
step:77/1770 train_time:6295ms step_avg:93.95ms
step:78/1770 train_time:6389ms step_avg:93.95ms
step:79/1770 train_time:6482ms step_avg:93.95ms
step:80/1770 train_time:6576ms step_avg:93.94ms
step:81/1770 train_time:6670ms step_avg:93.95ms
step:82/1770 train_time:6764ms step_avg:93.95ms
step:83/1770 train_time:6858ms step_avg:93.95ms
step:84/1770 train_time:6952ms step_avg:93.95ms
step:85/1770 train_time:7046ms step_avg:93.95ms
step:86/1770 train_time:7140ms step_avg:93.95ms
step:87/1770 train_time:7234ms step_avg:93.95ms
step:88/1770 train_time:7327ms step_avg:93.94ms
step:89/1770 train_time:7421ms step_avg:93.93ms
step:90/1770 train_time:7515ms step_avg:93.93ms
step:91/1770 train_time:7609ms step_avg:93.93ms
step:92/1770 train_time:7702ms step_avg:93.93ms
step:93/1770 train_time:7797ms step_avg:93.94ms
step:94/1770 train_time:7891ms step_avg:93.94ms
step:95/1770 train_time:7985ms step_avg:93.94ms
step:96/1770 train_time:8079ms step_avg:93.95ms
step:97/1770 train_time:8174ms step_avg:93.95ms
step:98/1770 train_time:8268ms step_avg:93.95ms
step:99/1770 train_time:8361ms step_avg:93.94ms
step:100/1770 train_time:8455ms step_avg:93.94ms
step:101/1770 train_time:8549ms step_avg:93.94ms
step:102/1770 train_time:8643ms step_avg:93.94ms
step:103/1770 train_time:8736ms step_avg:93.94ms
step:104/1770 train_time:8830ms step_avg:93.94ms
step:105/1770 train_time:8924ms step_avg:93.94ms
step:106/1770 train_time:9018ms step_avg:93.93ms
step:107/1770 train_time:9111ms step_avg:93.93ms
step:108/1770 train_time:9205ms step_avg:93.93ms
step:109/1770 train_time:9299ms step_avg:93.93ms
step:110/1770 train_time:9393ms step_avg:93.93ms
step:111/1770 train_time:9487ms step_avg:93.93ms
step:112/1770 train_time:9581ms step_avg:93.94ms
step:113/1770 train_time:9676ms step_avg:93.94ms
step:114/1770 train_time:9769ms step_avg:93.93ms
step:115/1770 train_time:9863ms step_avg:93.93ms
step:116/1770 train_time:9957ms step_avg:93.93ms
step:117/1770 train_time:10052ms step_avg:93.94ms
step:118/1770 train_time:10146ms step_avg:93.94ms
step:119/1770 train_time:10240ms step_avg:93.95ms
step:120/1770 train_time:10334ms step_avg:93.94ms
step:121/1770 train_time:10427ms step_avg:93.94ms
step:122/1770 train_time:10521ms step_avg:93.94ms
step:123/1770 train_time:10615ms step_avg:93.93ms
step:124/1770 train_time:10708ms step_avg:93.93ms
step:125/1770 train_time:10804ms step_avg:93.95ms
step:125/1770 val_loss:4.6449 train_time:10895ms step_avg:94.74ms
step:126/1770 train_time:10917ms step_avg:94.11ms
step:127/1770 train_time:10997ms step_avg:93.99ms
step:128/1770 train_time:11095ms step_avg:94.02ms
step:129/1770 train_time:11193ms step_avg:94.06ms
step:130/1770 train_time:11288ms step_avg:94.06ms
step:131/1770 train_time:11382ms step_avg:94.06ms
step:132/1770 train_time:11475ms step_avg:94.06ms
step:133/1770 train_time:11568ms step_avg:94.05ms
step:134/1770 train_time:11663ms step_avg:94.05ms
step:135/1770 train_time:11757ms step_avg:94.05ms
step:136/1770 train_time:11851ms step_avg:94.05ms
step:137/1770 train_time:11945ms step_avg:94.06ms
step:138/1770 train_time:12040ms step_avg:94.06ms
step:139/1770 train_time:12135ms step_avg:94.07ms
step:140/1770 train_time:12229ms step_avg:94.07ms
step:141/1770 train_time:12324ms step_avg:94.08ms
step:142/1770 train_time:12418ms step_avg:94.08ms
step:143/1770 train_time:12512ms step_avg:94.08ms
step:144/1770 train_time:12607ms step_avg:94.08ms
step:145/1770 train_time:12701ms step_avg:94.08ms
step:146/1770 train_time:12795ms step_avg:94.08ms
step:147/1770 train_time:12890ms step_avg:94.09ms
step:148/1770 train_time:12984ms step_avg:94.09ms
step:149/1770 train_time:13079ms step_avg:94.09ms
step:150/1770 train_time:13173ms step_avg:94.10ms
step:151/1770 train_time:13268ms step_avg:94.10ms
step:152/1770 train_time:13363ms step_avg:94.11ms
step:153/1770 train_time:13457ms step_avg:94.11ms
step:154/1770 train_time:13552ms step_avg:94.11ms
step:155/1770 train_time:13646ms step_avg:94.11ms
step:156/1770 train_time:13740ms step_avg:94.11ms
step:157/1770 train_time:13834ms step_avg:94.11ms
step:158/1770 train_time:13928ms step_avg:94.11ms
step:159/1770 train_time:14023ms step_avg:94.12ms
step:160/1770 train_time:14118ms step_avg:94.12ms
step:161/1770 train_time:14213ms step_avg:94.12ms
step:162/1770 train_time:14306ms step_avg:94.12ms
step:163/1770 train_time:14401ms step_avg:94.13ms
step:164/1770 train_time:14496ms step_avg:94.13ms
step:165/1770 train_time:14590ms step_avg:94.13ms
step:166/1770 train_time:14684ms step_avg:94.13ms
step:167/1770 train_time:14779ms step_avg:94.13ms
step:168/1770 train_time:14873ms step_avg:94.13ms
step:169/1770 train_time:14967ms step_avg:94.13ms
step:170/1770 train_time:15062ms step_avg:94.14ms
step:171/1770 train_time:15157ms step_avg:94.14ms
step:172/1770 train_time:15252ms step_avg:94.15ms
step:173/1770 train_time:15346ms step_avg:94.15ms
step:174/1770 train_time:15440ms step_avg:94.15ms
step:175/1770 train_time:15535ms step_avg:94.15ms
step:176/1770 train_time:15631ms step_avg:94.16ms
step:177/1770 train_time:15725ms step_avg:94.16ms
step:178/1770 train_time:15820ms step_avg:94.17ms
step:179/1770 train_time:15915ms step_avg:94.17ms
step:180/1770 train_time:16009ms step_avg:94.17ms
step:181/1770 train_time:16103ms step_avg:94.17ms
step:182/1770 train_time:16198ms step_avg:94.17ms
step:183/1770 train_time:16292ms step_avg:94.17ms
step:184/1770 train_time:16387ms step_avg:94.18ms
step:185/1770 train_time:16481ms step_avg:94.18ms
step:186/1770 train_time:16576ms step_avg:94.18ms
step:187/1770 train_time:16670ms step_avg:94.18ms
step:188/1770 train_time:16766ms step_avg:94.19ms
step:189/1770 train_time:16860ms step_avg:94.19ms
step:190/1770 train_time:16955ms step_avg:94.20ms
step:191/1770 train_time:17049ms step_avg:94.20ms
step:192/1770 train_time:17144ms step_avg:94.20ms
step:193/1770 train_time:17239ms step_avg:94.20ms
step:194/1770 train_time:17333ms step_avg:94.20ms
step:195/1770 train_time:17427ms step_avg:94.20ms
step:196/1770 train_time:17522ms step_avg:94.20ms
step:197/1770 train_time:17617ms step_avg:94.21ms
step:198/1770 train_time:17711ms step_avg:94.21ms
step:199/1770 train_time:17806ms step_avg:94.21ms
step:200/1770 train_time:17900ms step_avg:94.21ms
step:201/1770 train_time:17995ms step_avg:94.22ms
step:202/1770 train_time:18090ms step_avg:94.22ms
step:203/1770 train_time:18185ms step_avg:94.22ms
step:204/1770 train_time:18280ms step_avg:94.22ms
step:205/1770 train_time:18374ms step_avg:94.22ms
step:206/1770 train_time:18468ms step_avg:94.23ms
step:207/1770 train_time:18563ms step_avg:94.23ms
step:208/1770 train_time:18658ms step_avg:94.23ms
step:209/1770 train_time:18752ms step_avg:94.23ms
step:210/1770 train_time:18846ms step_avg:94.23ms
step:211/1770 train_time:18942ms step_avg:94.24ms
step:212/1770 train_time:19037ms step_avg:94.24ms
step:213/1770 train_time:19131ms step_avg:94.24ms
step:214/1770 train_time:19226ms step_avg:94.24ms
step:215/1770 train_time:19320ms step_avg:94.25ms
step:216/1770 train_time:19415ms step_avg:94.25ms
step:217/1770 train_time:19510ms step_avg:94.25ms
step:218/1770 train_time:19605ms step_avg:94.26ms
step:219/1770 train_time:19700ms step_avg:94.26ms
step:220/1770 train_time:19794ms step_avg:94.26ms
step:221/1770 train_time:19889ms step_avg:94.26ms
step:222/1770 train_time:19985ms step_avg:94.27ms
step:223/1770 train_time:20080ms step_avg:94.27ms
step:224/1770 train_time:20174ms step_avg:94.27ms
step:225/1770 train_time:20269ms step_avg:94.27ms
step:226/1770 train_time:20364ms step_avg:94.28ms
step:227/1770 train_time:20459ms step_avg:94.28ms
step:228/1770 train_time:20553ms step_avg:94.28ms
step:229/1770 train_time:20647ms step_avg:94.28ms
step:230/1770 train_time:20742ms step_avg:94.28ms
step:231/1770 train_time:20836ms step_avg:94.28ms
step:232/1770 train_time:20930ms step_avg:94.28ms
step:233/1770 train_time:21025ms step_avg:94.28ms
step:234/1770 train_time:21120ms step_avg:94.29ms
step:235/1770 train_time:21216ms step_avg:94.29ms
step:236/1770 train_time:21310ms step_avg:94.29ms
step:237/1770 train_time:21405ms step_avg:94.30ms
step:238/1770 train_time:21500ms step_avg:94.30ms
step:239/1770 train_time:21595ms step_avg:94.30ms
step:240/1770 train_time:21689ms step_avg:94.30ms
step:241/1770 train_time:21784ms step_avg:94.30ms
step:242/1770 train_time:21878ms step_avg:94.30ms
step:243/1770 train_time:21973ms step_avg:94.30ms
step:244/1770 train_time:22068ms step_avg:94.31ms
step:245/1770 train_time:22163ms step_avg:94.31ms
step:246/1770 train_time:22258ms step_avg:94.31ms
step:247/1770 train_time:22353ms step_avg:94.31ms
step:248/1770 train_time:22447ms step_avg:94.32ms
step:249/1770 train_time:22542ms step_avg:94.32ms
step:250/1770 train_time:22636ms step_avg:94.32ms
step:250/1770 val_loss:4.1072 train_time:22729ms step_avg:94.71ms
step:251/1770 train_time:22751ms step_avg:94.40ms
step:252/1770 train_time:22837ms step_avg:94.37ms
step:253/1770 train_time:22938ms step_avg:94.39ms
step:254/1770 train_time:23033ms step_avg:94.40ms
step:255/1770 train_time:23128ms step_avg:94.40ms
step:256/1770 train_time:23222ms step_avg:94.40ms
step:257/1770 train_time:23315ms step_avg:94.39ms
step:258/1770 train_time:23409ms step_avg:94.39ms
step:259/1770 train_time:23503ms step_avg:94.39ms
step:260/1770 train_time:23597ms step_avg:94.39ms
step:261/1770 train_time:23691ms step_avg:94.39ms
step:262/1770 train_time:23786ms step_avg:94.39ms
step:263/1770 train_time:23883ms step_avg:94.40ms
step:264/1770 train_time:23978ms step_avg:94.40ms
step:265/1770 train_time:24073ms step_avg:94.40ms
step:266/1770 train_time:24168ms step_avg:94.41ms
step:267/1770 train_time:24263ms step_avg:94.41ms
step:268/1770 train_time:24358ms step_avg:94.41ms
step:269/1770 train_time:24453ms step_avg:94.41ms
step:270/1770 train_time:24547ms step_avg:94.41ms
step:271/1770 train_time:24642ms step_avg:94.41ms
step:272/1770 train_time:24737ms step_avg:94.41ms
step:273/1770 train_time:24832ms step_avg:94.42ms
step:274/1770 train_time:24928ms step_avg:94.42ms
step:275/1770 train_time:25023ms step_avg:94.43ms
step:276/1770 train_time:25118ms step_avg:94.43ms
step:277/1770 train_time:25213ms step_avg:94.43ms
step:278/1770 train_time:25308ms step_avg:94.43ms
step:279/1770 train_time:25403ms step_avg:94.44ms
step:280/1770 train_time:25498ms step_avg:94.44ms
step:281/1770 train_time:25593ms step_avg:94.44ms
step:282/1770 train_time:25688ms step_avg:94.44ms
step:283/1770 train_time:25783ms step_avg:94.44ms
step:284/1770 train_time:25878ms step_avg:94.45ms
step:285/1770 train_time:25974ms step_avg:94.45ms
step:286/1770 train_time:26069ms step_avg:94.45ms
step:287/1770 train_time:26164ms step_avg:94.46ms
step:288/1770 train_time:26259ms step_avg:94.46ms
step:289/1770 train_time:26355ms step_avg:94.46ms
step:290/1770 train_time:26450ms step_avg:94.47ms
step:291/1770 train_time:26545ms step_avg:94.47ms
step:292/1770 train_time:26640ms step_avg:94.47ms
step:293/1770 train_time:26735ms step_avg:94.47ms
step:294/1770 train_time:26830ms step_avg:94.47ms
step:295/1770 train_time:26925ms step_avg:94.47ms
step:296/1770 train_time:27021ms step_avg:94.48ms
step:297/1770 train_time:27116ms step_avg:94.48ms
step:298/1770 train_time:27211ms step_avg:94.48ms
step:299/1770 train_time:27306ms step_avg:94.48ms
step:300/1770 train_time:27401ms step_avg:94.49ms
step:301/1770 train_time:27495ms step_avg:94.49ms
step:302/1770 train_time:27590ms step_avg:94.49ms
step:303/1770 train_time:27686ms step_avg:94.49ms
step:304/1770 train_time:27781ms step_avg:94.49ms
step:305/1770 train_time:27875ms step_avg:94.49ms
step:306/1770 train_time:27971ms step_avg:94.50ms
step:307/1770 train_time:28066ms step_avg:94.50ms
step:308/1770 train_time:28161ms step_avg:94.50ms
step:309/1770 train_time:28256ms step_avg:94.50ms
step:310/1770 train_time:28351ms step_avg:94.50ms
step:311/1770 train_time:28446ms step_avg:94.50ms
step:312/1770 train_time:28540ms step_avg:94.50ms
step:313/1770 train_time:28636ms step_avg:94.51ms
step:314/1770 train_time:28731ms step_avg:94.51ms
step:315/1770 train_time:28827ms step_avg:94.51ms
step:316/1770 train_time:28921ms step_avg:94.51ms
step:317/1770 train_time:29016ms step_avg:94.51ms
step:318/1770 train_time:29111ms step_avg:94.52ms
step:319/1770 train_time:29207ms step_avg:94.52ms
step:320/1770 train_time:29301ms step_avg:94.52ms
step:321/1770 train_time:29396ms step_avg:94.52ms
step:322/1770 train_time:29491ms step_avg:94.52ms
step:323/1770 train_time:29586ms step_avg:94.53ms
step:324/1770 train_time:29682ms step_avg:94.53ms
step:325/1770 train_time:29776ms step_avg:94.53ms
step:326/1770 train_time:29871ms step_avg:94.53ms
step:327/1770 train_time:29967ms step_avg:94.53ms
step:328/1770 train_time:30061ms step_avg:94.53ms
step:329/1770 train_time:30156ms step_avg:94.53ms
step:330/1770 train_time:30252ms step_avg:94.54ms
step:331/1770 train_time:30347ms step_avg:94.54ms
step:332/1770 train_time:30442ms step_avg:94.54ms
step:333/1770 train_time:30537ms step_avg:94.54ms
step:334/1770 train_time:30632ms step_avg:94.54ms
step:335/1770 train_time:30728ms step_avg:94.55ms
step:336/1770 train_time:30823ms step_avg:94.55ms
step:337/1770 train_time:30918ms step_avg:94.55ms
step:338/1770 train_time:31013ms step_avg:94.55ms
step:339/1770 train_time:31107ms step_avg:94.55ms
step:340/1770 train_time:31202ms step_avg:94.55ms
step:341/1770 train_time:31296ms step_avg:94.55ms
step:342/1770 train_time:31392ms step_avg:94.55ms
step:343/1770 train_time:31487ms step_avg:94.56ms
step:344/1770 train_time:31582ms step_avg:94.56ms
step:345/1770 train_time:31678ms step_avg:94.56ms
step:346/1770 train_time:31773ms step_avg:94.56ms
step:347/1770 train_time:31869ms step_avg:94.57ms
step:348/1770 train_time:31964ms step_avg:94.57ms
step:349/1770 train_time:32059ms step_avg:94.57ms
step:350/1770 train_time:32155ms step_avg:94.57ms
step:351/1770 train_time:32250ms step_avg:94.58ms
step:352/1770 train_time:32345ms step_avg:94.58ms
step:353/1770 train_time:32440ms step_avg:94.58ms
step:354/1770 train_time:32535ms step_avg:94.58ms
step:355/1770 train_time:32630ms step_avg:94.58ms
step:356/1770 train_time:32725ms step_avg:94.58ms
step:357/1770 train_time:32820ms step_avg:94.58ms
step:358/1770 train_time:32914ms step_avg:94.58ms
step:359/1770 train_time:33010ms step_avg:94.58ms
step:360/1770 train_time:33105ms step_avg:94.59ms
step:361/1770 train_time:33200ms step_avg:94.59ms
step:362/1770 train_time:33296ms step_avg:94.59ms
step:363/1770 train_time:33391ms step_avg:94.59ms
step:364/1770 train_time:33486ms step_avg:94.59ms
step:365/1770 train_time:33581ms step_avg:94.59ms
step:366/1770 train_time:33676ms step_avg:94.59ms
step:367/1770 train_time:33771ms step_avg:94.60ms
step:368/1770 train_time:33866ms step_avg:94.60ms
step:369/1770 train_time:33961ms step_avg:94.60ms
step:370/1770 train_time:34056ms step_avg:94.60ms
step:371/1770 train_time:34151ms step_avg:94.60ms
step:372/1770 train_time:34246ms step_avg:94.60ms
step:373/1770 train_time:34341ms step_avg:94.60ms
step:374/1770 train_time:34436ms step_avg:94.60ms
step:375/1770 train_time:34531ms step_avg:94.61ms
step:375/1770 val_loss:3.8993 train_time:34625ms step_avg:94.86ms
step:376/1770 train_time:34646ms step_avg:94.66ms
step:377/1770 train_time:34728ms step_avg:94.63ms
step:378/1770 train_time:34830ms step_avg:94.65ms
step:379/1770 train_time:34926ms step_avg:94.65ms
step:380/1770 train_time:35021ms step_avg:94.65ms
step:381/1770 train_time:35115ms step_avg:94.65ms
step:382/1770 train_time:35210ms step_avg:94.65ms
step:383/1770 train_time:35304ms step_avg:94.65ms
step:384/1770 train_time:35399ms step_avg:94.65ms
step:385/1770 train_time:35493ms step_avg:94.65ms
step:386/1770 train_time:35588ms step_avg:94.65ms
step:387/1770 train_time:35684ms step_avg:94.65ms
step:388/1770 train_time:35779ms step_avg:94.65ms
step:389/1770 train_time:35875ms step_avg:94.66ms
step:390/1770 train_time:35970ms step_avg:94.66ms
step:391/1770 train_time:36065ms step_avg:94.66ms
step:392/1770 train_time:36161ms step_avg:94.66ms
step:393/1770 train_time:36256ms step_avg:94.66ms
step:394/1770 train_time:36351ms step_avg:94.66ms
step:395/1770 train_time:36446ms step_avg:94.67ms
step:396/1770 train_time:36544ms step_avg:94.67ms
step:397/1770 train_time:36640ms step_avg:94.68ms
step:398/1770 train_time:36737ms step_avg:94.68ms
step:399/1770 train_time:36834ms step_avg:94.69ms
step:400/1770 train_time:36931ms step_avg:94.69ms
step:401/1770 train_time:37028ms step_avg:94.70ms
step:402/1770 train_time:37124ms step_avg:94.70ms
step:403/1770 train_time:37221ms step_avg:94.71ms
step:404/1770 train_time:37318ms step_avg:94.72ms
step:405/1770 train_time:37415ms step_avg:94.72ms
step:406/1770 train_time:37512ms step_avg:94.73ms
step:407/1770 train_time:37610ms step_avg:94.73ms
step:408/1770 train_time:37707ms step_avg:94.74ms
step:409/1770 train_time:37805ms step_avg:94.75ms
step:410/1770 train_time:37902ms step_avg:94.76ms
step:411/1770 train_time:37999ms step_avg:94.76ms
step:412/1770 train_time:38095ms step_avg:94.76ms
step:413/1770 train_time:38192ms step_avg:94.77ms
step:414/1770 train_time:38289ms step_avg:94.77ms
step:415/1770 train_time:38386ms step_avg:94.78ms
step:416/1770 train_time:38484ms step_avg:94.79ms
step:417/1770 train_time:38580ms step_avg:94.79ms
step:418/1770 train_time:38677ms step_avg:94.80ms
step:419/1770 train_time:38774ms step_avg:94.80ms
step:420/1770 train_time:38872ms step_avg:94.81ms
step:421/1770 train_time:38969ms step_avg:94.81ms
step:422/1770 train_time:39067ms step_avg:94.82ms
step:423/1770 train_time:39164ms step_avg:94.83ms
step:424/1770 train_time:39262ms step_avg:94.84ms
step:425/1770 train_time:39359ms step_avg:94.84ms
step:426/1770 train_time:39455ms step_avg:94.84ms
step:427/1770 train_time:39552ms step_avg:94.85ms
step:428/1770 train_time:39649ms step_avg:94.85ms
step:429/1770 train_time:39746ms step_avg:94.86ms
step:430/1770 train_time:39843ms step_avg:94.86ms
step:431/1770 train_time:39940ms step_avg:94.87ms
step:432/1770 train_time:40038ms step_avg:94.88ms
step:433/1770 train_time:40136ms step_avg:94.88ms
step:434/1770 train_time:40233ms step_avg:94.89ms
step:435/1770 train_time:40330ms step_avg:94.89ms
step:436/1770 train_time:40427ms step_avg:94.90ms
step:437/1770 train_time:40523ms step_avg:94.90ms
step:438/1770 train_time:40620ms step_avg:94.91ms
step:439/1770 train_time:40717ms step_avg:94.91ms
step:440/1770 train_time:40814ms step_avg:94.92ms
step:441/1770 train_time:40911ms step_avg:94.92ms
step:442/1770 train_time:41008ms step_avg:94.93ms
step:443/1770 train_time:41106ms step_avg:94.93ms
step:444/1770 train_time:41203ms step_avg:94.94ms
step:445/1770 train_time:41301ms step_avg:94.94ms
step:446/1770 train_time:41398ms step_avg:94.95ms
step:447/1770 train_time:41494ms step_avg:94.95ms
step:448/1770 train_time:41591ms step_avg:94.96ms
step:449/1770 train_time:41688ms step_avg:94.96ms
step:450/1770 train_time:41786ms step_avg:94.97ms
step:451/1770 train_time:41883ms step_avg:94.97ms
step:452/1770 train_time:41980ms step_avg:94.98ms
step:453/1770 train_time:42077ms step_avg:94.98ms
step:454/1770 train_time:42174ms step_avg:94.99ms
step:455/1770 train_time:42271ms step_avg:94.99ms
step:456/1770 train_time:42368ms step_avg:95.00ms
step:457/1770 train_time:42466ms step_avg:95.00ms
step:458/1770 train_time:42563ms step_avg:95.01ms
step:459/1770 train_time:42660ms step_avg:95.01ms
step:460/1770 train_time:42757ms step_avg:95.02ms
step:461/1770 train_time:42854ms step_avg:95.02ms
step:462/1770 train_time:42951ms step_avg:95.03ms
step:463/1770 train_time:43049ms step_avg:95.03ms
step:464/1770 train_time:43146ms step_avg:95.04ms
step:465/1770 train_time:43243ms step_avg:95.04ms
step:466/1770 train_time:43340ms step_avg:95.04ms
step:467/1770 train_time:43437ms step_avg:95.05ms
step:468/1770 train_time:43534ms step_avg:95.05ms
step:469/1770 train_time:43631ms step_avg:95.06ms
step:470/1770 train_time:43729ms step_avg:95.06ms
step:471/1770 train_time:43826ms step_avg:95.07ms
step:472/1770 train_time:43923ms step_avg:95.07ms
step:473/1770 train_time:44020ms step_avg:95.08ms
step:474/1770 train_time:44117ms step_avg:95.08ms
step:475/1770 train_time:44213ms step_avg:95.08ms
step:476/1770 train_time:44310ms step_avg:95.09ms
step:477/1770 train_time:44407ms step_avg:95.09ms
step:478/1770 train_time:44504ms step_avg:95.09ms
step:479/1770 train_time:44601ms step_avg:95.10ms
step:480/1770 train_time:44698ms step_avg:95.10ms
step:481/1770 train_time:44795ms step_avg:95.11ms
step:482/1770 train_time:44892ms step_avg:95.11ms
step:483/1770 train_time:44990ms step_avg:95.12ms
step:484/1770 train_time:45087ms step_avg:95.12ms
step:485/1770 train_time:45184ms step_avg:95.12ms
step:486/1770 train_time:45281ms step_avg:95.13ms
step:487/1770 train_time:45378ms step_avg:95.13ms
step:488/1770 train_time:45475ms step_avg:95.14ms
step:489/1770 train_time:45572ms step_avg:95.14ms
step:490/1770 train_time:45669ms step_avg:95.14ms
step:491/1770 train_time:45766ms step_avg:95.15ms
step:492/1770 train_time:45863ms step_avg:95.15ms
step:493/1770 train_time:45961ms step_avg:95.16ms
step:494/1770 train_time:46058ms step_avg:95.16ms
step:495/1770 train_time:46155ms step_avg:95.16ms
step:496/1770 train_time:46251ms step_avg:95.17ms
step:497/1770 train_time:46348ms step_avg:95.17ms
step:498/1770 train_time:46445ms step_avg:95.18ms
step:499/1770 train_time:46542ms step_avg:95.18ms
step:500/1770 train_time:46639ms step_avg:95.18ms
step:500/1770 val_loss:3.7507 train_time:46734ms step_avg:95.38ms
step:501/1770 train_time:46755ms step_avg:95.23ms
step:502/1770 train_time:46840ms step_avg:95.20ms
step:503/1770 train_time:46939ms step_avg:95.21ms
step:504/1770 train_time:47036ms step_avg:95.21ms
step:505/1770 train_time:47132ms step_avg:95.22ms
step:506/1770 train_time:47229ms step_avg:95.22ms
step:507/1770 train_time:47325ms step_avg:95.22ms
step:508/1770 train_time:47421ms step_avg:95.22ms
step:509/1770 train_time:47519ms step_avg:95.23ms
step:510/1770 train_time:47615ms step_avg:95.23ms
step:511/1770 train_time:47713ms step_avg:95.23ms
step:512/1770 train_time:47812ms step_avg:95.24ms
step:513/1770 train_time:47909ms step_avg:95.25ms
step:514/1770 train_time:48007ms step_avg:95.25ms
step:515/1770 train_time:48104ms step_avg:95.25ms
step:516/1770 train_time:48200ms step_avg:95.26ms
step:517/1770 train_time:48297ms step_avg:95.26ms
step:518/1770 train_time:48394ms step_avg:95.26ms
step:519/1770 train_time:48491ms step_avg:95.27ms
step:520/1770 train_time:48588ms step_avg:95.27ms
step:521/1770 train_time:48684ms step_avg:95.27ms
step:522/1770 train_time:48781ms step_avg:95.28ms
step:523/1770 train_time:48878ms step_avg:95.28ms
step:524/1770 train_time:48976ms step_avg:95.28ms
step:525/1770 train_time:49074ms step_avg:95.29ms
step:526/1770 train_time:49172ms step_avg:95.29ms
step:527/1770 train_time:49269ms step_avg:95.30ms
step:528/1770 train_time:49367ms step_avg:95.30ms
step:529/1770 train_time:49464ms step_avg:95.31ms
step:530/1770 train_time:49561ms step_avg:95.31ms
step:531/1770 train_time:49658ms step_avg:95.31ms
step:532/1770 train_time:49755ms step_avg:95.32ms
step:533/1770 train_time:49852ms step_avg:95.32ms
step:534/1770 train_time:49950ms step_avg:95.32ms
step:535/1770 train_time:50047ms step_avg:95.33ms
step:536/1770 train_time:50145ms step_avg:95.33ms
step:537/1770 train_time:50243ms step_avg:95.34ms
step:538/1770 train_time:50339ms step_avg:95.34ms
step:539/1770 train_time:50437ms step_avg:95.34ms
step:540/1770 train_time:50535ms step_avg:95.35ms
step:541/1770 train_time:50632ms step_avg:95.35ms
step:542/1770 train_time:50730ms step_avg:95.36ms
step:543/1770 train_time:50827ms step_avg:95.36ms
step:544/1770 train_time:50924ms step_avg:95.36ms
step:545/1770 train_time:51021ms step_avg:95.37ms
step:546/1770 train_time:51118ms step_avg:95.37ms
step:547/1770 train_time:51216ms step_avg:95.37ms
step:548/1770 train_time:51314ms step_avg:95.38ms
step:549/1770 train_time:51411ms step_avg:95.38ms
step:550/1770 train_time:51508ms step_avg:95.39ms
step:551/1770 train_time:51605ms step_avg:95.39ms
step:552/1770 train_time:51703ms step_avg:95.39ms
step:553/1770 train_time:51800ms step_avg:95.40ms
step:554/1770 train_time:51898ms step_avg:95.40ms
step:555/1770 train_time:51996ms step_avg:95.41ms
step:556/1770 train_time:52093ms step_avg:95.41ms
step:557/1770 train_time:52190ms step_avg:95.41ms
step:558/1770 train_time:52288ms step_avg:95.42ms
step:559/1770 train_time:52385ms step_avg:95.42ms
step:560/1770 train_time:52482ms step_avg:95.42ms
step:561/1770 train_time:52579ms step_avg:95.42ms
step:562/1770 train_time:52676ms step_avg:95.43ms
step:563/1770 train_time:52774ms step_avg:95.43ms
step:564/1770 train_time:52871ms step_avg:95.44ms
step:565/1770 train_time:52969ms step_avg:95.44ms
step:566/1770 train_time:53067ms step_avg:95.44ms
step:567/1770 train_time:53164ms step_avg:95.45ms
step:568/1770 train_time:53261ms step_avg:95.45ms
step:569/1770 train_time:53358ms step_avg:95.45ms
step:570/1770 train_time:53455ms step_avg:95.46ms
step:571/1770 train_time:53552ms step_avg:95.46ms
step:572/1770 train_time:53650ms step_avg:95.46ms
step:573/1770 train_time:53748ms step_avg:95.47ms
step:574/1770 train_time:53845ms step_avg:95.47ms
step:575/1770 train_time:53943ms step_avg:95.47ms
step:576/1770 train_time:54041ms step_avg:95.48ms
step:577/1770 train_time:54139ms step_avg:95.48ms
step:578/1770 train_time:54236ms step_avg:95.49ms
step:579/1770 train_time:54333ms step_avg:95.49ms
step:580/1770 train_time:54431ms step_avg:95.49ms
step:581/1770 train_time:54528ms step_avg:95.50ms
step:582/1770 train_time:54626ms step_avg:95.50ms
step:583/1770 train_time:54723ms step_avg:95.50ms
step:584/1770 train_time:54820ms step_avg:95.51ms
step:585/1770 train_time:54917ms step_avg:95.51ms
step:586/1770 train_time:55015ms step_avg:95.51ms
step:587/1770 train_time:55112ms step_avg:95.52ms
step:588/1770 train_time:55209ms step_avg:95.52ms
step:589/1770 train_time:55307ms step_avg:95.52ms
step:590/1770 train_time:55403ms step_avg:95.52ms
step:591/1770 train_time:55501ms step_avg:95.53ms
step:592/1770 train_time:55598ms step_avg:95.53ms
step:593/1770 train_time:55696ms step_avg:95.53ms
step:594/1770 train_time:55794ms step_avg:95.54ms
step:595/1770 train_time:55891ms step_avg:95.54ms
step:596/1770 train_time:55988ms step_avg:95.54ms
step:597/1770 train_time:56086ms step_avg:95.55ms
step:598/1770 train_time:56183ms step_avg:95.55ms
step:599/1770 train_time:56280ms step_avg:95.55ms
step:600/1770 train_time:56377ms step_avg:95.55ms
step:601/1770 train_time:56474ms step_avg:95.56ms
step:602/1770 train_time:56572ms step_avg:95.56ms
step:603/1770 train_time:56669ms step_avg:95.56ms
step:604/1770 train_time:56766ms step_avg:95.57ms
step:605/1770 train_time:56864ms step_avg:95.57ms
step:606/1770 train_time:56961ms step_avg:95.57ms
step:607/1770 train_time:57058ms step_avg:95.57ms
step:608/1770 train_time:57156ms step_avg:95.58ms
step:609/1770 train_time:57253ms step_avg:95.58ms
step:610/1770 train_time:57350ms step_avg:95.58ms
step:611/1770 train_time:57448ms step_avg:95.59ms
step:612/1770 train_time:57545ms step_avg:95.59ms
step:613/1770 train_time:57642ms step_avg:95.59ms
step:614/1770 train_time:57740ms step_avg:95.60ms
step:615/1770 train_time:57837ms step_avg:95.60ms
step:616/1770 train_time:57935ms step_avg:95.60ms
step:617/1770 train_time:58033ms step_avg:95.61ms
step:618/1770 train_time:58131ms step_avg:95.61ms
step:619/1770 train_time:58229ms step_avg:95.61ms
step:620/1770 train_time:58326ms step_avg:95.62ms
step:621/1770 train_time:58423ms step_avg:95.62ms
step:622/1770 train_time:58520ms step_avg:95.62ms
step:623/1770 train_time:58618ms step_avg:95.62ms
step:624/1770 train_time:58715ms step_avg:95.63ms
step:625/1770 train_time:58813ms step_avg:95.63ms
step:625/1770 val_loss:3.6650 train_time:58908ms step_avg:95.79ms
step:626/1770 train_time:58930ms step_avg:95.67ms
step:627/1770 train_time:59019ms step_avg:95.66ms
step:628/1770 train_time:59121ms step_avg:95.66ms
step:629/1770 train_time:59218ms step_avg:95.67ms
step:630/1770 train_time:59315ms step_avg:95.67ms
step:631/1770 train_time:59412ms step_avg:95.67ms
step:632/1770 train_time:59508ms step_avg:95.67ms
step:633/1770 train_time:59605ms step_avg:95.67ms
step:634/1770 train_time:59702ms step_avg:95.68ms
step:635/1770 train_time:59800ms step_avg:95.68ms
step:636/1770 train_time:59897ms step_avg:95.68ms
step:637/1770 train_time:59994ms step_avg:95.68ms
step:638/1770 train_time:60092ms step_avg:95.69ms
step:639/1770 train_time:60190ms step_avg:95.69ms
step:640/1770 train_time:60288ms step_avg:95.70ms
step:641/1770 train_time:60385ms step_avg:95.70ms
step:642/1770 train_time:60482ms step_avg:95.70ms
step:643/1770 train_time:60579ms step_avg:95.70ms
step:644/1770 train_time:60676ms step_avg:95.70ms
step:645/1770 train_time:60774ms step_avg:95.71ms
step:646/1770 train_time:60871ms step_avg:95.71ms
step:647/1770 train_time:60968ms step_avg:95.71ms
step:648/1770 train_time:61066ms step_avg:95.71ms
step:649/1770 train_time:61163ms step_avg:95.72ms
step:650/1770 train_time:61261ms step_avg:95.72ms
step:651/1770 train_time:61359ms step_avg:95.72ms
step:652/1770 train_time:61456ms step_avg:95.73ms
step:653/1770 train_time:61553ms step_avg:95.73ms
step:654/1770 train_time:61650ms step_avg:95.73ms
step:655/1770 train_time:61748ms step_avg:95.73ms
step:656/1770 train_time:61845ms step_avg:95.73ms
step:657/1770 train_time:61942ms step_avg:95.74ms
step:658/1770 train_time:62040ms step_avg:95.74ms
step:659/1770 train_time:62140ms step_avg:95.75ms
step:660/1770 train_time:62240ms step_avg:95.75ms
step:661/1770 train_time:62339ms step_avg:95.76ms
step:662/1770 train_time:62438ms step_avg:95.76ms
step:663/1770 train_time:62538ms step_avg:95.77ms
step:664/1770 train_time:62637ms step_avg:95.78ms
step:665/1770 train_time:62736ms step_avg:95.78ms
step:666/1770 train_time:62835ms step_avg:95.79ms
step:667/1770 train_time:62935ms step_avg:95.79ms
step:668/1770 train_time:63034ms step_avg:95.80ms
step:669/1770 train_time:63134ms step_avg:95.80ms
step:670/1770 train_time:63233ms step_avg:95.81ms
step:671/1770 train_time:63332ms step_avg:95.81ms
step:672/1770 train_time:63431ms step_avg:95.82ms
step:673/1770 train_time:63530ms step_avg:95.82ms
step:674/1770 train_time:63628ms step_avg:95.83ms
step:675/1770 train_time:63727ms step_avg:95.83ms
step:676/1770 train_time:63826ms step_avg:95.84ms
step:677/1770 train_time:63926ms step_avg:95.84ms
step:678/1770 train_time:64026ms step_avg:95.85ms
step:679/1770 train_time:64125ms step_avg:95.85ms
step:680/1770 train_time:64224ms step_avg:95.86ms
step:681/1770 train_time:64323ms step_avg:95.86ms
step:682/1770 train_time:64422ms step_avg:95.87ms
step:683/1770 train_time:64522ms step_avg:95.87ms
step:684/1770 train_time:64622ms step_avg:95.88ms
step:685/1770 train_time:64721ms step_avg:95.88ms
step:686/1770 train_time:64821ms step_avg:95.89ms
step:687/1770 train_time:64921ms step_avg:95.89ms
step:688/1770 train_time:65021ms step_avg:95.90ms
step:689/1770 train_time:65120ms step_avg:95.91ms
step:690/1770 train_time:65220ms step_avg:95.91ms
step:691/1770 train_time:65320ms step_avg:95.92ms
step:692/1770 train_time:65419ms step_avg:95.92ms
step:693/1770 train_time:65518ms step_avg:95.93ms
step:694/1770 train_time:65617ms step_avg:95.93ms
step:695/1770 train_time:65716ms step_avg:95.94ms
step:696/1770 train_time:65815ms step_avg:95.94ms
step:697/1770 train_time:65914ms step_avg:95.95ms
step:698/1770 train_time:66013ms step_avg:95.95ms
step:699/1770 train_time:66113ms step_avg:95.95ms
step:700/1770 train_time:66212ms step_avg:95.96ms
step:701/1770 train_time:66310ms step_avg:95.96ms
step:702/1770 train_time:66410ms step_avg:95.97ms
step:703/1770 train_time:66509ms step_avg:95.97ms
step:704/1770 train_time:66608ms step_avg:95.98ms
step:705/1770 train_time:66708ms step_avg:95.98ms
step:706/1770 train_time:66807ms step_avg:95.99ms
step:707/1770 train_time:66907ms step_avg:95.99ms
step:708/1770 train_time:67007ms step_avg:96.00ms
step:709/1770 train_time:67106ms step_avg:96.00ms
step:710/1770 train_time:67206ms step_avg:96.01ms
step:711/1770 train_time:67305ms step_avg:96.01ms
step:712/1770 train_time:67404ms step_avg:96.02ms
step:713/1770 train_time:67504ms step_avg:96.02ms
step:714/1770 train_time:67603ms step_avg:96.03ms
step:715/1770 train_time:67703ms step_avg:96.03ms
step:716/1770 train_time:67802ms step_avg:96.04ms
step:717/1770 train_time:67902ms step_avg:96.04ms
step:718/1770 train_time:68002ms step_avg:96.05ms
step:719/1770 train_time:68101ms step_avg:96.05ms
step:720/1770 train_time:68201ms step_avg:96.06ms
step:721/1770 train_time:68301ms step_avg:96.06ms
step:722/1770 train_time:68400ms step_avg:96.07ms
step:723/1770 train_time:68500ms step_avg:96.07ms
step:724/1770 train_time:68599ms step_avg:96.08ms
step:725/1770 train_time:68699ms step_avg:96.08ms
step:726/1770 train_time:68798ms step_avg:96.09ms
step:727/1770 train_time:68897ms step_avg:96.09ms
step:728/1770 train_time:68997ms step_avg:96.10ms
step:729/1770 train_time:69096ms step_avg:96.10ms
step:730/1770 train_time:69196ms step_avg:96.11ms
step:731/1770 train_time:69295ms step_avg:96.11ms
step:732/1770 train_time:69394ms step_avg:96.11ms
step:733/1770 train_time:69494ms step_avg:96.12ms
step:734/1770 train_time:69593ms step_avg:96.12ms
step:735/1770 train_time:69693ms step_avg:96.13ms
step:736/1770 train_time:69792ms step_avg:96.13ms
step:737/1770 train_time:69891ms step_avg:96.14ms
step:738/1770 train_time:69991ms step_avg:96.14ms
step:739/1770 train_time:70090ms step_avg:96.15ms
step:740/1770 train_time:70190ms step_avg:96.15ms
step:741/1770 train_time:70288ms step_avg:96.15ms
step:742/1770 train_time:70387ms step_avg:96.16ms
step:743/1770 train_time:70487ms step_avg:96.16ms
step:744/1770 train_time:70585ms step_avg:96.17ms
step:745/1770 train_time:70684ms step_avg:96.17ms
step:746/1770 train_time:70783ms step_avg:96.17ms
step:747/1770 train_time:70882ms step_avg:96.18ms
step:748/1770 train_time:70981ms step_avg:96.18ms
step:749/1770 train_time:71082ms step_avg:96.19ms
step:750/1770 train_time:71181ms step_avg:96.19ms
step:750/1770 val_loss:3.5998 train_time:71279ms step_avg:96.32ms
step:751/1770 train_time:71301ms step_avg:96.22ms
step:752/1770 train_time:71389ms step_avg:96.21ms
step:753/1770 train_time:71490ms step_avg:96.22ms
step:754/1770 train_time:71590ms step_avg:96.22ms
step:755/1770 train_time:71689ms step_avg:96.23ms
step:756/1770 train_time:71788ms step_avg:96.23ms
step:757/1770 train_time:71886ms step_avg:96.23ms
step:758/1770 train_time:71985ms step_avg:96.24ms
step:759/1770 train_time:72083ms step_avg:96.24ms
step:760/1770 train_time:72181ms step_avg:96.24ms
step:761/1770 train_time:72279ms step_avg:96.24ms
step:762/1770 train_time:72378ms step_avg:96.25ms
step:763/1770 train_time:72479ms step_avg:96.25ms
step:764/1770 train_time:72579ms step_avg:96.26ms
step:765/1770 train_time:72680ms step_avg:96.26ms
step:766/1770 train_time:72779ms step_avg:96.27ms
step:767/1770 train_time:72879ms step_avg:96.27ms
step:768/1770 train_time:72978ms step_avg:96.28ms
step:769/1770 train_time:73078ms step_avg:96.28ms
step:770/1770 train_time:73177ms step_avg:96.29ms
step:771/1770 train_time:73276ms step_avg:96.29ms
step:772/1770 train_time:73375ms step_avg:96.29ms
step:773/1770 train_time:73473ms step_avg:96.30ms
step:774/1770 train_time:73573ms step_avg:96.30ms
step:775/1770 train_time:73671ms step_avg:96.30ms
step:776/1770 train_time:73770ms step_avg:96.31ms
step:777/1770 train_time:73870ms step_avg:96.31ms
step:778/1770 train_time:73969ms step_avg:96.31ms
step:779/1770 train_time:74068ms step_avg:96.32ms
step:780/1770 train_time:74167ms step_avg:96.32ms
step:781/1770 train_time:74267ms step_avg:96.33ms
step:782/1770 train_time:74367ms step_avg:96.33ms
step:783/1770 train_time:74466ms step_avg:96.33ms
step:784/1770 train_time:74566ms step_avg:96.34ms
step:785/1770 train_time:74665ms step_avg:96.34ms
step:786/1770 train_time:74765ms step_avg:96.35ms
step:787/1770 train_time:74864ms step_avg:96.35ms
step:788/1770 train_time:74963ms step_avg:96.35ms
step:789/1770 train_time:75064ms step_avg:96.36ms
step:790/1770 train_time:75163ms step_avg:96.36ms
step:791/1770 train_time:75263ms step_avg:96.37ms
step:792/1770 train_time:75363ms step_avg:96.37ms
step:793/1770 train_time:75462ms step_avg:96.38ms
step:794/1770 train_time:75562ms step_avg:96.38ms
step:795/1770 train_time:75662ms step_avg:96.38ms
step:796/1770 train_time:75761ms step_avg:96.39ms
step:797/1770 train_time:75861ms step_avg:96.39ms
step:798/1770 train_time:75960ms step_avg:96.40ms
step:799/1770 train_time:76060ms step_avg:96.40ms
step:800/1770 train_time:76160ms step_avg:96.40ms
step:801/1770 train_time:76259ms step_avg:96.41ms
step:802/1770 train_time:76358ms step_avg:96.41ms
step:803/1770 train_time:76457ms step_avg:96.42ms
step:804/1770 train_time:76556ms step_avg:96.42ms
step:805/1770 train_time:76656ms step_avg:96.42ms
step:806/1770 train_time:76756ms step_avg:96.43ms
step:807/1770 train_time:76855ms step_avg:96.43ms
step:808/1770 train_time:76955ms step_avg:96.43ms
step:809/1770 train_time:77055ms step_avg:96.44ms
step:810/1770 train_time:77155ms step_avg:96.44ms
step:811/1770 train_time:77255ms step_avg:96.45ms
step:812/1770 train_time:77354ms step_avg:96.45ms
step:813/1770 train_time:77454ms step_avg:96.46ms
step:814/1770 train_time:77554ms step_avg:96.46ms
step:815/1770 train_time:77653ms step_avg:96.46ms
step:816/1770 train_time:77752ms step_avg:96.47ms
step:817/1770 train_time:77851ms step_avg:96.47ms
step:818/1770 train_time:77951ms step_avg:96.47ms
step:819/1770 train_time:78051ms step_avg:96.48ms
step:820/1770 train_time:78151ms step_avg:96.48ms
step:821/1770 train_time:78251ms step_avg:96.49ms
step:822/1770 train_time:78350ms step_avg:96.49ms
step:823/1770 train_time:78450ms step_avg:96.49ms
step:824/1770 train_time:78550ms step_avg:96.50ms
step:825/1770 train_time:78649ms step_avg:96.50ms
step:826/1770 train_time:78748ms step_avg:96.51ms
step:827/1770 train_time:78848ms step_avg:96.51ms
step:828/1770 train_time:78947ms step_avg:96.51ms
step:829/1770 train_time:79048ms step_avg:96.52ms
step:830/1770 train_time:79148ms step_avg:96.52ms
step:831/1770 train_time:79249ms step_avg:96.53ms
step:832/1770 train_time:79349ms step_avg:96.53ms
step:833/1770 train_time:79449ms step_avg:96.54ms
step:834/1770 train_time:79549ms step_avg:96.54ms
step:835/1770 train_time:79648ms step_avg:96.54ms
step:836/1770 train_time:79747ms step_avg:96.55ms
step:837/1770 train_time:79846ms step_avg:96.55ms
step:838/1770 train_time:79946ms step_avg:96.55ms
step:839/1770 train_time:80045ms step_avg:96.56ms
step:840/1770 train_time:80145ms step_avg:96.56ms
step:841/1770 train_time:80244ms step_avg:96.56ms
step:842/1770 train_time:80343ms step_avg:96.57ms
step:843/1770 train_time:80443ms step_avg:96.57ms
step:844/1770 train_time:80542ms step_avg:96.57ms
step:845/1770 train_time:80641ms step_avg:96.58ms
step:846/1770 train_time:80741ms step_avg:96.58ms
step:847/1770 train_time:80839ms step_avg:96.58ms
step:848/1770 train_time:80939ms step_avg:96.59ms
step:849/1770 train_time:81038ms step_avg:96.59ms
step:850/1770 train_time:81137ms step_avg:96.59ms
step:851/1770 train_time:81237ms step_avg:96.60ms
step:852/1770 train_time:81335ms step_avg:96.60ms
step:853/1770 train_time:81434ms step_avg:96.60ms
step:854/1770 train_time:81534ms step_avg:96.60ms
step:855/1770 train_time:81633ms step_avg:96.61ms
step:856/1770 train_time:81732ms step_avg:96.61ms
step:857/1770 train_time:81832ms step_avg:96.61ms
step:858/1770 train_time:81932ms step_avg:96.62ms
step:859/1770 train_time:82032ms step_avg:96.62ms
step:860/1770 train_time:82131ms step_avg:96.63ms
step:861/1770 train_time:82231ms step_avg:96.63ms
step:862/1770 train_time:82330ms step_avg:96.63ms
step:863/1770 train_time:82430ms step_avg:96.64ms
step:864/1770 train_time:82529ms step_avg:96.64ms
step:865/1770 train_time:82628ms step_avg:96.64ms
step:866/1770 train_time:82728ms step_avg:96.65ms
step:867/1770 train_time:82827ms step_avg:96.65ms
step:868/1770 train_time:82927ms step_avg:96.65ms
step:869/1770 train_time:83027ms step_avg:96.66ms
step:870/1770 train_time:83127ms step_avg:96.66ms
step:871/1770 train_time:83226ms step_avg:96.66ms
step:872/1770 train_time:83326ms step_avg:96.67ms
step:873/1770 train_time:83425ms step_avg:96.67ms
step:874/1770 train_time:83525ms step_avg:96.67ms
step:875/1770 train_time:83624ms step_avg:96.68ms
step:875/1770 val_loss:3.5506 train_time:83721ms step_avg:96.79ms
step:876/1770 train_time:83743ms step_avg:96.70ms
step:877/1770 train_time:83832ms step_avg:96.69ms
step:878/1770 train_time:83932ms step_avg:96.70ms
step:879/1770 train_time:84032ms step_avg:96.70ms
step:880/1770 train_time:84131ms step_avg:96.70ms
step:881/1770 train_time:84230ms step_avg:96.70ms
step:882/1770 train_time:84328ms step_avg:96.71ms
step:883/1770 train_time:84427ms step_avg:96.71ms
step:884/1770 train_time:84526ms step_avg:96.71ms
step:885/1770 train_time:84625ms step_avg:96.71ms
step:886/1770 train_time:84725ms step_avg:96.72ms
step:887/1770 train_time:84827ms step_avg:96.72ms
step:888/1770 train_time:84928ms step_avg:96.73ms
step:889/1770 train_time:85028ms step_avg:96.73ms
step:890/1770 train_time:85128ms step_avg:96.74ms
step:891/1770 train_time:85227ms step_avg:96.74ms
step:892/1770 train_time:85327ms step_avg:96.74ms
step:893/1770 train_time:85426ms step_avg:96.74ms
step:894/1770 train_time:85525ms step_avg:96.75ms
step:895/1770 train_time:85624ms step_avg:96.75ms
step:896/1770 train_time:85724ms step_avg:96.75ms
step:897/1770 train_time:85824ms step_avg:96.76ms
step:898/1770 train_time:85924ms step_avg:96.76ms
step:899/1770 train_time:86024ms step_avg:96.76ms
step:900/1770 train_time:86124ms step_avg:96.77ms
step:901/1770 train_time:86224ms step_avg:96.77ms
step:902/1770 train_time:86324ms step_avg:96.78ms
step:903/1770 train_time:86423ms step_avg:96.78ms
step:904/1770 train_time:86523ms step_avg:96.78ms
step:905/1770 train_time:86623ms step_avg:96.78ms
step:906/1770 train_time:86722ms step_avg:96.79ms
step:907/1770 train_time:86822ms step_avg:96.79ms
step:908/1770 train_time:86922ms step_avg:96.79ms
step:909/1770 train_time:87021ms step_avg:96.80ms
step:910/1770 train_time:87121ms step_avg:96.80ms
step:911/1770 train_time:87221ms step_avg:96.80ms
step:912/1770 train_time:87320ms step_avg:96.81ms
step:913/1770 train_time:87420ms step_avg:96.81ms
step:914/1770 train_time:87520ms step_avg:96.81ms
step:915/1770 train_time:87619ms step_avg:96.82ms
step:916/1770 train_time:87718ms step_avg:96.82ms
step:917/1770 train_time:87818ms step_avg:96.82ms
step:918/1770 train_time:87918ms step_avg:96.83ms
step:919/1770 train_time:88017ms step_avg:96.83ms
step:920/1770 train_time:88117ms step_avg:96.83ms
step:921/1770 train_time:88219ms step_avg:96.84ms
step:922/1770 train_time:88321ms step_avg:96.84ms
step:923/1770 train_time:88421ms step_avg:96.85ms
step:924/1770 train_time:88522ms step_avg:96.85ms
step:925/1770 train_time:88622ms step_avg:96.85ms
step:926/1770 train_time:88723ms step_avg:96.86ms
step:927/1770 train_time:88824ms step_avg:96.86ms
step:928/1770 train_time:88925ms step_avg:96.87ms
step:929/1770 train_time:89026ms step_avg:96.87ms
step:930/1770 train_time:89128ms step_avg:96.88ms
step:931/1770 train_time:89229ms step_avg:96.88ms
step:932/1770 train_time:89329ms step_avg:96.89ms
step:933/1770 train_time:89429ms step_avg:96.89ms
step:934/1770 train_time:89530ms step_avg:96.89ms
step:935/1770 train_time:89630ms step_avg:96.90ms
step:936/1770 train_time:89730ms step_avg:96.90ms
step:937/1770 train_time:89831ms step_avg:96.90ms
step:938/1770 train_time:89932ms step_avg:96.91ms
step:939/1770 train_time:90033ms step_avg:96.91ms
step:940/1770 train_time:90134ms step_avg:96.92ms
step:941/1770 train_time:90235ms step_avg:96.92ms
step:942/1770 train_time:90335ms step_avg:96.93ms
step:943/1770 train_time:90436ms step_avg:96.93ms
step:944/1770 train_time:90536ms step_avg:96.93ms
step:945/1770 train_time:90637ms step_avg:96.94ms
step:946/1770 train_time:90738ms step_avg:96.94ms
step:947/1770 train_time:90839ms step_avg:96.95ms
step:948/1770 train_time:90940ms step_avg:96.95ms
step:949/1770 train_time:91042ms step_avg:96.96ms
step:950/1770 train_time:91143ms step_avg:96.96ms
step:951/1770 train_time:91244ms step_avg:96.97ms
step:952/1770 train_time:91344ms step_avg:96.97ms
step:953/1770 train_time:91447ms step_avg:96.97ms
step:954/1770 train_time:91549ms step_avg:96.98ms
step:955/1770 train_time:91650ms step_avg:96.98ms
step:956/1770 train_time:91752ms step_avg:96.99ms
step:957/1770 train_time:91853ms step_avg:96.99ms
step:958/1770 train_time:91953ms step_avg:97.00ms
step:959/1770 train_time:92054ms step_avg:97.00ms
step:960/1770 train_time:92154ms step_avg:97.00ms
step:961/1770 train_time:92254ms step_avg:97.01ms
step:962/1770 train_time:92355ms step_avg:97.01ms
step:963/1770 train_time:92456ms step_avg:97.02ms
step:964/1770 train_time:92557ms step_avg:97.02ms
step:965/1770 train_time:92659ms step_avg:97.03ms
step:966/1770 train_time:92761ms step_avg:97.03ms
step:967/1770 train_time:92862ms step_avg:97.03ms
step:968/1770 train_time:92963ms step_avg:97.04ms
step:969/1770 train_time:93063ms step_avg:97.04ms
step:970/1770 train_time:93164ms step_avg:97.05ms
step:971/1770 train_time:93266ms step_avg:97.05ms
step:972/1770 train_time:93367ms step_avg:97.06ms
step:973/1770 train_time:93468ms step_avg:97.06ms
step:974/1770 train_time:93570ms step_avg:97.06ms
step:975/1770 train_time:93671ms step_avg:97.07ms
step:976/1770 train_time:93772ms step_avg:97.07ms
step:977/1770 train_time:93873ms step_avg:97.08ms
step:978/1770 train_time:93974ms step_avg:97.08ms
step:979/1770 train_time:94076ms step_avg:97.09ms
step:980/1770 train_time:94176ms step_avg:97.09ms
step:981/1770 train_time:94276ms step_avg:97.09ms
step:982/1770 train_time:94377ms step_avg:97.10ms
step:983/1770 train_time:94479ms step_avg:97.10ms
step:984/1770 train_time:94580ms step_avg:97.10ms
step:985/1770 train_time:94681ms step_avg:97.11ms
step:986/1770 train_time:94783ms step_avg:97.11ms
step:987/1770 train_time:94883ms step_avg:97.12ms
step:988/1770 train_time:94984ms step_avg:97.12ms
step:989/1770 train_time:95087ms step_avg:97.13ms
step:990/1770 train_time:95189ms step_avg:97.13ms
step:991/1770 train_time:95290ms step_avg:97.14ms
step:992/1770 train_time:95391ms step_avg:97.14ms
step:993/1770 train_time:95491ms step_avg:97.14ms
step:994/1770 train_time:95593ms step_avg:97.15ms
step:995/1770 train_time:95694ms step_avg:97.15ms
step:996/1770 train_time:95794ms step_avg:97.15ms
step:997/1770 train_time:95895ms step_avg:97.16ms
step:998/1770 train_time:95995ms step_avg:97.16ms
step:999/1770 train_time:96096ms step_avg:97.16ms
step:1000/1770 train_time:96197ms step_avg:97.17ms
step:1000/1770 val_loss:3.5121 train_time:96296ms step_avg:97.27ms
step:1001/1770 train_time:96318ms step_avg:97.19ms
step:1002/1770 train_time:96405ms step_avg:97.18ms
step:1003/1770 train_time:96509ms step_avg:97.19ms
step:1004/1770 train_time:96610ms step_avg:97.19ms
step:1005/1770 train_time:96710ms step_avg:97.20ms
step:1006/1770 train_time:96810ms step_avg:97.20ms
step:1007/1770 train_time:96910ms step_avg:97.20ms
step:1008/1770 train_time:97010ms step_avg:97.20ms
step:1009/1770 train_time:97110ms step_avg:97.21ms
step:1010/1770 train_time:97210ms step_avg:97.21ms
step:1011/1770 train_time:97312ms step_avg:97.21ms
step:1012/1770 train_time:97415ms step_avg:97.22ms
step:1013/1770 train_time:97518ms step_avg:97.23ms
step:1014/1770 train_time:97619ms step_avg:97.23ms
step:1015/1770 train_time:97719ms step_avg:97.23ms
step:1016/1770 train_time:97820ms step_avg:97.24ms
step:1017/1770 train_time:97921ms step_avg:97.24ms
step:1018/1770 train_time:98020ms step_avg:97.24ms
step:1019/1770 train_time:98120ms step_avg:97.25ms
step:1020/1770 train_time:98221ms step_avg:97.25ms
step:1021/1770 train_time:98324ms step_avg:97.25ms
step:1022/1770 train_time:98426ms step_avg:97.26ms
step:1023/1770 train_time:98528ms step_avg:97.26ms
step:1024/1770 train_time:98629ms step_avg:97.27ms
step:1025/1770 train_time:98730ms step_avg:97.27ms
step:1026/1770 train_time:98832ms step_avg:97.28ms
step:1027/1770 train_time:98933ms step_avg:97.28ms
step:1028/1770 train_time:99034ms step_avg:97.28ms
step:1029/1770 train_time:99136ms step_avg:97.29ms
step:1030/1770 train_time:99237ms step_avg:97.29ms
step:1031/1770 train_time:99337ms step_avg:97.29ms
step:1032/1770 train_time:99438ms step_avg:97.30ms
step:1033/1770 train_time:99539ms step_avg:97.30ms
step:1034/1770 train_time:99639ms step_avg:97.30ms
step:1035/1770 train_time:99739ms step_avg:97.31ms
step:1036/1770 train_time:99839ms step_avg:97.31ms
step:1037/1770 train_time:99940ms step_avg:97.31ms
step:1038/1770 train_time:100041ms step_avg:97.32ms
step:1039/1770 train_time:100142ms step_avg:97.32ms
step:1040/1770 train_time:100242ms step_avg:97.32ms
step:1041/1770 train_time:100344ms step_avg:97.33ms
step:1042/1770 train_time:100446ms step_avg:97.33ms
step:1043/1770 train_time:100547ms step_avg:97.33ms
step:1044/1770 train_time:100647ms step_avg:97.34ms
step:1045/1770 train_time:100748ms step_avg:97.34ms
step:1046/1770 train_time:100849ms step_avg:97.34ms
step:1047/1770 train_time:100950ms step_avg:97.35ms
step:1048/1770 train_time:101051ms step_avg:97.35ms
step:1049/1770 train_time:101152ms step_avg:97.35ms
step:1050/1770 train_time:101253ms step_avg:97.36ms
step:1051/1770 train_time:101354ms step_avg:97.36ms
step:1052/1770 train_time:101455ms step_avg:97.37ms
step:1053/1770 train_time:101556ms step_avg:97.37ms
step:1054/1770 train_time:101657ms step_avg:97.37ms
step:1055/1770 train_time:101759ms step_avg:97.38ms
step:1056/1770 train_time:101860ms step_avg:97.38ms
step:1057/1770 train_time:101962ms step_avg:97.38ms
step:1058/1770 train_time:102063ms step_avg:97.39ms
step:1059/1770 train_time:102163ms step_avg:97.39ms
step:1060/1770 train_time:102265ms step_avg:97.39ms
step:1061/1770 train_time:102366ms step_avg:97.40ms
step:1062/1770 train_time:102468ms step_avg:97.40ms
step:1063/1770 train_time:102570ms step_avg:97.41ms
step:1064/1770 train_time:102671ms step_avg:97.41ms
step:1065/1770 train_time:102774ms step_avg:97.42ms
step:1066/1770 train_time:102875ms step_avg:97.42ms
step:1067/1770 train_time:102975ms step_avg:97.42ms
step:1068/1770 train_time:103077ms step_avg:97.43ms
step:1069/1770 train_time:103177ms step_avg:97.43ms
step:1070/1770 train_time:103277ms step_avg:97.43ms
step:1071/1770 train_time:103379ms step_avg:97.44ms
step:1072/1770 train_time:103482ms step_avg:97.44ms
step:1073/1770 train_time:103583ms step_avg:97.44ms
step:1074/1770 train_time:103684ms step_avg:97.45ms
step:1075/1770 train_time:103786ms step_avg:97.45ms
step:1076/1770 train_time:103887ms step_avg:97.45ms
step:1077/1770 train_time:103988ms step_avg:97.46ms
step:1078/1770 train_time:104088ms step_avg:97.46ms
step:1079/1770 train_time:104188ms step_avg:97.46ms
step:1080/1770 train_time:104290ms step_avg:97.47ms
step:1081/1770 train_time:104391ms step_avg:97.47ms
step:1082/1770 train_time:104494ms step_avg:97.48ms
step:1083/1770 train_time:104596ms step_avg:97.48ms
step:1084/1770 train_time:104697ms step_avg:97.48ms
step:1085/1770 train_time:104797ms step_avg:97.49ms
step:1086/1770 train_time:104897ms step_avg:97.49ms
step:1087/1770 train_time:104997ms step_avg:97.49ms
step:1088/1770 train_time:105098ms step_avg:97.49ms
step:1089/1770 train_time:105199ms step_avg:97.50ms
step:1090/1770 train_time:105302ms step_avg:97.50ms
step:1091/1770 train_time:105402ms step_avg:97.50ms
step:1092/1770 train_time:105503ms step_avg:97.51ms
step:1093/1770 train_time:105604ms step_avg:97.51ms
step:1094/1770 train_time:105705ms step_avg:97.51ms
step:1095/1770 train_time:105808ms step_avg:97.52ms
step:1096/1770 train_time:105909ms step_avg:97.52ms
step:1097/1770 train_time:106009ms step_avg:97.52ms
step:1098/1770 train_time:106110ms step_avg:97.53ms
step:1099/1770 train_time:106211ms step_avg:97.53ms
step:1100/1770 train_time:106314ms step_avg:97.54ms
step:1101/1770 train_time:106415ms step_avg:97.54ms
step:1102/1770 train_time:106515ms step_avg:97.54ms
step:1103/1770 train_time:106617ms step_avg:97.54ms
step:1104/1770 train_time:106718ms step_avg:97.55ms
step:1105/1770 train_time:106819ms step_avg:97.55ms
step:1106/1770 train_time:106920ms step_avg:97.55ms
step:1107/1770 train_time:107020ms step_avg:97.56ms
step:1108/1770 train_time:107123ms step_avg:97.56ms
step:1109/1770 train_time:107223ms step_avg:97.56ms
step:1110/1770 train_time:107324ms step_avg:97.57ms
step:1111/1770 train_time:107427ms step_avg:97.57ms
step:1112/1770 train_time:107528ms step_avg:97.58ms
step:1113/1770 train_time:107627ms step_avg:97.58ms
step:1114/1770 train_time:107728ms step_avg:97.58ms
step:1115/1770 train_time:107830ms step_avg:97.58ms
step:1116/1770 train_time:107933ms step_avg:97.59ms
step:1117/1770 train_time:108035ms step_avg:97.59ms
step:1118/1770 train_time:108137ms step_avg:97.60ms
step:1119/1770 train_time:108238ms step_avg:97.60ms
step:1120/1770 train_time:108338ms step_avg:97.60ms
step:1121/1770 train_time:108439ms step_avg:97.60ms
step:1122/1770 train_time:108540ms step_avg:97.61ms
step:1123/1770 train_time:108640ms step_avg:97.61ms
step:1124/1770 train_time:108742ms step_avg:97.61ms
step:1125/1770 train_time:108842ms step_avg:97.62ms
step:1125/1770 val_loss:3.4713 train_time:108942ms step_avg:97.71ms
step:1126/1770 train_time:108963ms step_avg:97.64ms
step:1127/1770 train_time:109056ms step_avg:97.63ms
step:1128/1770 train_time:109159ms step_avg:97.64ms
step:1129/1770 train_time:109259ms step_avg:97.64ms
step:1130/1770 train_time:109360ms step_avg:97.64ms
step:1131/1770 train_time:109460ms step_avg:97.65ms
step:1132/1770 train_time:109561ms step_avg:97.65ms
step:1133/1770 train_time:109662ms step_avg:97.65ms
step:1134/1770 train_time:109762ms step_avg:97.65ms
step:1135/1770 train_time:109863ms step_avg:97.66ms
step:1136/1770 train_time:109966ms step_avg:97.66ms
step:1137/1770 train_time:110069ms step_avg:97.67ms
step:1138/1770 train_time:110171ms step_avg:97.67ms
step:1139/1770 train_time:110271ms step_avg:97.67ms
step:1140/1770 train_time:110372ms step_avg:97.67ms
step:1141/1770 train_time:110472ms step_avg:97.68ms
step:1142/1770 train_time:110573ms step_avg:97.68ms
step:1143/1770 train_time:110673ms step_avg:97.68ms
step:1144/1770 train_time:110774ms step_avg:97.68ms
step:1145/1770 train_time:110875ms step_avg:97.69ms
step:1146/1770 train_time:110977ms step_avg:97.69ms
step:1147/1770 train_time:111079ms step_avg:97.69ms
step:1148/1770 train_time:111179ms step_avg:97.70ms
step:1149/1770 train_time:111280ms step_avg:97.70ms
step:1150/1770 train_time:111381ms step_avg:97.70ms
step:1151/1770 train_time:111483ms step_avg:97.71ms
step:1152/1770 train_time:111586ms step_avg:97.71ms
step:1153/1770 train_time:111687ms step_avg:97.71ms
step:1154/1770 train_time:111788ms step_avg:97.72ms
step:1155/1770 train_time:111889ms step_avg:97.72ms
step:1156/1770 train_time:111990ms step_avg:97.72ms
step:1157/1770 train_time:112092ms step_avg:97.73ms
step:1158/1770 train_time:112193ms step_avg:97.73ms
step:1159/1770 train_time:112293ms step_avg:97.73ms
step:1160/1770 train_time:112395ms step_avg:97.73ms
step:1161/1770 train_time:112497ms step_avg:97.74ms
step:1162/1770 train_time:112598ms step_avg:97.74ms
step:1163/1770 train_time:112699ms step_avg:97.74ms
step:1164/1770 train_time:112801ms step_avg:97.75ms
step:1165/1770 train_time:112901ms step_avg:97.75ms
step:1166/1770 train_time:113002ms step_avg:97.75ms
step:1167/1770 train_time:113102ms step_avg:97.75ms
step:1168/1770 train_time:113203ms step_avg:97.76ms
step:1169/1770 train_time:113305ms step_avg:97.76ms
step:1170/1770 train_time:113406ms step_avg:97.76ms
step:1171/1770 train_time:113509ms step_avg:97.77ms
step:1172/1770 train_time:113610ms step_avg:97.77ms
step:1173/1770 train_time:113711ms step_avg:97.77ms
step:1174/1770 train_time:113812ms step_avg:97.78ms
step:1175/1770 train_time:113913ms step_avg:97.78ms
step:1176/1770 train_time:114013ms step_avg:97.78ms
step:1177/1770 train_time:114114ms step_avg:97.78ms
step:1178/1770 train_time:114216ms step_avg:97.79ms
step:1179/1770 train_time:114317ms step_avg:97.79ms
step:1180/1770 train_time:114419ms step_avg:97.79ms
step:1181/1770 train_time:114519ms step_avg:97.80ms
step:1182/1770 train_time:114621ms step_avg:97.80ms
step:1183/1770 train_time:114723ms step_avg:97.80ms
step:1184/1770 train_time:114827ms step_avg:97.81ms
step:1185/1770 train_time:114929ms step_avg:97.81ms
step:1186/1770 train_time:115032ms step_avg:97.82ms
step:1187/1770 train_time:115136ms step_avg:97.82ms
step:1188/1770 train_time:115236ms step_avg:97.82ms
step:1189/1770 train_time:115338ms step_avg:97.83ms
step:1190/1770 train_time:115439ms step_avg:97.83ms
step:1191/1770 train_time:115541ms step_avg:97.83ms
step:1192/1770 train_time:115645ms step_avg:97.84ms
step:1193/1770 train_time:115748ms step_avg:97.84ms
step:1194/1770 train_time:115849ms step_avg:97.85ms
step:1195/1770 train_time:115952ms step_avg:97.85ms
step:1196/1770 train_time:116056ms step_avg:97.85ms
step:1197/1770 train_time:116156ms step_avg:97.86ms
step:1198/1770 train_time:116258ms step_avg:97.86ms
step:1199/1770 train_time:116360ms step_avg:97.86ms
step:1200/1770 train_time:116462ms step_avg:97.87ms
step:1201/1770 train_time:116565ms step_avg:97.87ms
step:1202/1770 train_time:116666ms step_avg:97.87ms
step:1203/1770 train_time:116768ms step_avg:97.88ms
step:1204/1770 train_time:116870ms step_avg:97.88ms
step:1205/1770 train_time:116972ms step_avg:97.88ms
step:1206/1770 train_time:117074ms step_avg:97.89ms
step:1207/1770 train_time:117176ms step_avg:97.89ms
step:1208/1770 train_time:117278ms step_avg:97.89ms
step:1209/1770 train_time:117379ms step_avg:97.90ms
step:1210/1770 train_time:117481ms step_avg:97.90ms
step:1211/1770 train_time:117584ms step_avg:97.90ms
step:1212/1770 train_time:117687ms step_avg:97.91ms
step:1213/1770 train_time:117789ms step_avg:97.91ms
step:1214/1770 train_time:117890ms step_avg:97.92ms
step:1215/1770 train_time:117992ms step_avg:97.92ms
step:1216/1770 train_time:118096ms step_avg:97.92ms
step:1217/1770 train_time:118198ms step_avg:97.93ms
step:1218/1770 train_time:118300ms step_avg:97.93ms
step:1219/1770 train_time:118402ms step_avg:97.93ms
step:1220/1770 train_time:118504ms step_avg:97.94ms
step:1221/1770 train_time:118606ms step_avg:97.94ms
step:1222/1770 train_time:118709ms step_avg:97.95ms
step:1223/1770 train_time:118811ms step_avg:97.95ms
step:1224/1770 train_time:118913ms step_avg:97.95ms
step:1225/1770 train_time:119015ms step_avg:97.95ms
step:1226/1770 train_time:119117ms step_avg:97.96ms
step:1227/1770 train_time:119221ms step_avg:97.96ms
step:1228/1770 train_time:119326ms step_avg:97.97ms
step:1229/1770 train_time:119427ms step_avg:97.97ms
step:1230/1770 train_time:119530ms step_avg:97.98ms
step:1231/1770 train_time:119632ms step_avg:97.98ms
step:1232/1770 train_time:119734ms step_avg:97.98ms
step:1233/1770 train_time:119835ms step_avg:97.98ms
step:1234/1770 train_time:119938ms step_avg:97.99ms
step:1235/1770 train_time:120039ms step_avg:97.99ms
step:1236/1770 train_time:120141ms step_avg:97.99ms
step:1237/1770 train_time:120244ms step_avg:98.00ms
step:1238/1770 train_time:120347ms step_avg:98.00ms
step:1239/1770 train_time:120449ms step_avg:98.01ms
step:1240/1770 train_time:120552ms step_avg:98.01ms
step:1241/1770 train_time:120654ms step_avg:98.01ms
step:1242/1770 train_time:120756ms step_avg:98.02ms
step:1243/1770 train_time:120858ms step_avg:98.02ms
step:1244/1770 train_time:120959ms step_avg:98.02ms
step:1245/1770 train_time:121061ms step_avg:98.03ms
step:1246/1770 train_time:121164ms step_avg:98.03ms
step:1247/1770 train_time:121266ms step_avg:98.03ms
step:1248/1770 train_time:121368ms step_avg:98.04ms
step:1249/1770 train_time:121470ms step_avg:98.04ms
step:1250/1770 train_time:121572ms step_avg:98.04ms
step:1250/1770 val_loss:3.4241 train_time:121674ms step_avg:98.12ms
step:1251/1770 train_time:121695ms step_avg:98.06ms
step:1252/1770 train_time:121787ms step_avg:98.06ms
step:1253/1770 train_time:121890ms step_avg:98.06ms
step:1254/1770 train_time:121992ms step_avg:98.06ms
step:1255/1770 train_time:122096ms step_avg:98.07ms
step:1256/1770 train_time:122197ms step_avg:98.07ms
step:1257/1770 train_time:122298ms step_avg:98.07ms
step:1258/1770 train_time:122400ms step_avg:98.08ms
step:1259/1770 train_time:122503ms step_avg:98.08ms
step:1260/1770 train_time:122603ms step_avg:98.08ms
step:1261/1770 train_time:122707ms step_avg:98.09ms
step:1262/1770 train_time:122811ms step_avg:98.09ms
step:1263/1770 train_time:122912ms step_avg:98.09ms
step:1264/1770 train_time:123016ms step_avg:98.10ms
step:1265/1770 train_time:123117ms step_avg:98.10ms
step:1266/1770 train_time:123220ms step_avg:98.10ms
step:1267/1770 train_time:123322ms step_avg:98.11ms
step:1268/1770 train_time:123424ms step_avg:98.11ms
step:1269/1770 train_time:123527ms step_avg:98.11ms
step:1270/1770 train_time:123629ms step_avg:98.12ms
step:1271/1770 train_time:123732ms step_avg:98.12ms
step:1272/1770 train_time:123833ms step_avg:98.12ms
step:1273/1770 train_time:123936ms step_avg:98.13ms
step:1274/1770 train_time:124039ms step_avg:98.13ms
step:1275/1770 train_time:124140ms step_avg:98.13ms
step:1276/1770 train_time:124242ms step_avg:98.14ms
step:1277/1770 train_time:124344ms step_avg:98.14ms
step:1278/1770 train_time:124447ms step_avg:98.14ms
step:1279/1770 train_time:124549ms step_avg:98.15ms
step:1280/1770 train_time:124652ms step_avg:98.15ms
step:1281/1770 train_time:124754ms step_avg:98.15ms
step:1282/1770 train_time:124857ms step_avg:98.16ms
step:1283/1770 train_time:124960ms step_avg:98.16ms
step:1284/1770 train_time:125062ms step_avg:98.16ms
step:1285/1770 train_time:125164ms step_avg:98.17ms
step:1286/1770 train_time:125267ms step_avg:98.17ms
step:1287/1770 train_time:125370ms step_avg:98.18ms
step:1288/1770 train_time:125472ms step_avg:98.18ms
step:1289/1770 train_time:125575ms step_avg:98.18ms
step:1290/1770 train_time:125677ms step_avg:98.18ms
step:1291/1770 train_time:125779ms step_avg:98.19ms
step:1292/1770 train_time:125881ms step_avg:98.19ms
step:1293/1770 train_time:125983ms step_avg:98.19ms
step:1294/1770 train_time:126085ms step_avg:98.20ms
step:1295/1770 train_time:126188ms step_avg:98.20ms
step:1296/1770 train_time:126289ms step_avg:98.20ms
step:1297/1770 train_time:126392ms step_avg:98.21ms
step:1298/1770 train_time:126494ms step_avg:98.21ms
step:1299/1770 train_time:126596ms step_avg:98.21ms
step:1300/1770 train_time:126698ms step_avg:98.22ms
step:1301/1770 train_time:126800ms step_avg:98.22ms
step:1302/1770 train_time:126903ms step_avg:98.22ms
step:1303/1770 train_time:127005ms step_avg:98.22ms
step:1304/1770 train_time:127107ms step_avg:98.23ms
step:1305/1770 train_time:127210ms step_avg:98.23ms
step:1306/1770 train_time:127311ms step_avg:98.23ms
step:1307/1770 train_time:127414ms step_avg:98.24ms
step:1308/1770 train_time:127516ms step_avg:98.24ms
step:1309/1770 train_time:127618ms step_avg:98.24ms
step:1310/1770 train_time:127719ms step_avg:98.25ms
step:1311/1770 train_time:127821ms step_avg:98.25ms
step:1312/1770 train_time:127923ms step_avg:98.25ms
step:1313/1770 train_time:128024ms step_avg:98.25ms
step:1314/1770 train_time:128127ms step_avg:98.26ms
step:1315/1770 train_time:128228ms step_avg:98.26ms
step:1316/1770 train_time:128330ms step_avg:98.26ms
step:1317/1770 train_time:128433ms step_avg:98.27ms
step:1318/1770 train_time:128539ms step_avg:98.27ms
step:1319/1770 train_time:128642ms step_avg:98.28ms
step:1320/1770 train_time:128744ms step_avg:98.28ms
step:1321/1770 train_time:128847ms step_avg:98.28ms
step:1322/1770 train_time:128949ms step_avg:98.28ms
step:1323/1770 train_time:129052ms step_avg:98.29ms
step:1324/1770 train_time:129155ms step_avg:98.29ms
step:1325/1770 train_time:129258ms step_avg:98.30ms
step:1326/1770 train_time:129359ms step_avg:98.30ms
step:1327/1770 train_time:129464ms step_avg:98.30ms
step:1328/1770 train_time:129565ms step_avg:98.30ms
step:1329/1770 train_time:129667ms step_avg:98.31ms
step:1330/1770 train_time:129768ms step_avg:98.31ms
step:1331/1770 train_time:129871ms step_avg:98.31ms
step:1332/1770 train_time:129972ms step_avg:98.32ms
step:1333/1770 train_time:130075ms step_avg:98.32ms
step:1334/1770 train_time:130177ms step_avg:98.32ms
step:1335/1770 train_time:130279ms step_avg:98.32ms
step:1336/1770 train_time:130381ms step_avg:98.33ms
step:1337/1770 train_time:130483ms step_avg:98.33ms
step:1338/1770 train_time:130585ms step_avg:98.33ms
step:1339/1770 train_time:130688ms step_avg:98.34ms
step:1340/1770 train_time:130791ms step_avg:98.34ms
step:1341/1770 train_time:130893ms step_avg:98.34ms
step:1342/1770 train_time:130995ms step_avg:98.34ms
step:1343/1770 train_time:131098ms step_avg:98.35ms
step:1344/1770 train_time:131201ms step_avg:98.35ms
step:1345/1770 train_time:131302ms step_avg:98.35ms
step:1346/1770 train_time:131404ms step_avg:98.36ms
step:1347/1770 train_time:131506ms step_avg:98.36ms
step:1348/1770 train_time:131610ms step_avg:98.36ms
step:1349/1770 train_time:131713ms step_avg:98.37ms
step:1350/1770 train_time:131815ms step_avg:98.37ms
step:1351/1770 train_time:131917ms step_avg:98.37ms
step:1352/1770 train_time:132018ms step_avg:98.37ms
step:1353/1770 train_time:132121ms step_avg:98.38ms
step:1354/1770 train_time:132223ms step_avg:98.38ms
step:1355/1770 train_time:132325ms step_avg:98.38ms
step:1356/1770 train_time:132427ms step_avg:98.39ms
step:1357/1770 train_time:132529ms step_avg:98.39ms
step:1358/1770 train_time:132632ms step_avg:98.39ms
step:1359/1770 train_time:132735ms step_avg:98.40ms
step:1360/1770 train_time:132838ms step_avg:98.40ms
step:1361/1770 train_time:132940ms step_avg:98.40ms
step:1362/1770 train_time:133042ms step_avg:98.40ms
step:1363/1770 train_time:133145ms step_avg:98.41ms
step:1364/1770 train_time:133247ms step_avg:98.41ms
step:1365/1770 train_time:133349ms step_avg:98.41ms
step:1366/1770 train_time:133450ms step_avg:98.41ms
step:1367/1770 train_time:133553ms step_avg:98.42ms
step:1368/1770 train_time:133655ms step_avg:98.42ms
step:1369/1770 train_time:133758ms step_avg:98.42ms
step:1370/1770 train_time:133861ms step_avg:98.43ms
step:1371/1770 train_time:133964ms step_avg:98.43ms
step:1372/1770 train_time:134066ms step_avg:98.43ms
step:1373/1770 train_time:134168ms step_avg:98.44ms
step:1374/1770 train_time:134271ms step_avg:98.44ms
step:1375/1770 train_time:134373ms step_avg:98.44ms
step:1375/1770 val_loss:3.3808 train_time:134474ms step_avg:98.52ms
step:1376/1770 train_time:134496ms step_avg:98.46ms
step:1377/1770 train_time:134588ms step_avg:98.46ms
step:1378/1770 train_time:134690ms step_avg:98.46ms
step:1379/1770 train_time:134791ms step_avg:98.46ms
step:1380/1770 train_time:134893ms step_avg:98.46ms
step:1381/1770 train_time:134995ms step_avg:98.46ms
step:1382/1770 train_time:135097ms step_avg:98.47ms
step:1383/1770 train_time:135200ms step_avg:98.47ms
step:1384/1770 train_time:135301ms step_avg:98.47ms
step:1385/1770 train_time:135403ms step_avg:98.47ms
step:1386/1770 train_time:135506ms step_avg:98.48ms
step:1387/1770 train_time:135610ms step_avg:98.48ms
step:1388/1770 train_time:135713ms step_avg:98.49ms
step:1389/1770 train_time:135815ms step_avg:98.49ms
step:1390/1770 train_time:135917ms step_avg:98.49ms
step:1391/1770 train_time:136019ms step_avg:98.49ms
step:1392/1770 train_time:136121ms step_avg:98.50ms
step:1393/1770 train_time:136222ms step_avg:98.50ms
step:1394/1770 train_time:136324ms step_avg:98.50ms
step:1395/1770 train_time:136427ms step_avg:98.50ms
step:1396/1770 train_time:136530ms step_avg:98.51ms
step:1397/1770 train_time:136633ms step_avg:98.51ms
step:1398/1770 train_time:136735ms step_avg:98.51ms
step:1399/1770 train_time:136838ms step_avg:98.52ms
step:1400/1770 train_time:136941ms step_avg:98.52ms
step:1401/1770 train_time:137043ms step_avg:98.52ms
step:1402/1770 train_time:137145ms step_avg:98.52ms
step:1403/1770 train_time:137247ms step_avg:98.53ms
step:1404/1770 train_time:137349ms step_avg:98.53ms
step:1405/1770 train_time:137451ms step_avg:98.53ms
step:1406/1770 train_time:137554ms step_avg:98.53ms
step:1407/1770 train_time:137656ms step_avg:98.54ms
step:1408/1770 train_time:137758ms step_avg:98.54ms
step:1409/1770 train_time:137860ms step_avg:98.54ms
step:1410/1770 train_time:137962ms step_avg:98.54ms
step:1411/1770 train_time:138065ms step_avg:98.55ms
step:1412/1770 train_time:138167ms step_avg:98.55ms
step:1413/1770 train_time:138268ms step_avg:98.55ms
step:1414/1770 train_time:138371ms step_avg:98.55ms
step:1415/1770 train_time:138474ms step_avg:98.56ms
step:1416/1770 train_time:138577ms step_avg:98.56ms
step:1417/1770 train_time:138679ms step_avg:98.56ms
step:1418/1770 train_time:138781ms step_avg:98.57ms
step:1419/1770 train_time:138884ms step_avg:98.57ms
step:1420/1770 train_time:138986ms step_avg:98.57ms
step:1421/1770 train_time:139088ms step_avg:98.57ms
step:1422/1770 train_time:139190ms step_avg:98.58ms
step:1423/1770 train_time:139291ms step_avg:98.58ms
step:1424/1770 train_time:139394ms step_avg:98.58ms
step:1425/1770 train_time:139496ms step_avg:98.58ms
step:1426/1770 train_time:139599ms step_avg:98.59ms
step:1427/1770 train_time:139701ms step_avg:98.59ms
step:1428/1770 train_time:139804ms step_avg:98.59ms
step:1429/1770 train_time:139907ms step_avg:98.60ms
step:1430/1770 train_time:140009ms step_avg:98.60ms
step:1431/1770 train_time:140113ms step_avg:98.60ms
step:1432/1770 train_time:140215ms step_avg:98.60ms
step:1433/1770 train_time:140316ms step_avg:98.61ms
step:1434/1770 train_time:140418ms step_avg:98.61ms
step:1435/1770 train_time:140519ms step_avg:98.61ms
step:1436/1770 train_time:140624ms step_avg:98.61ms
step:1437/1770 train_time:140726ms step_avg:98.62ms
step:1438/1770 train_time:140827ms step_avg:98.62ms
step:1439/1770 train_time:140929ms step_avg:98.62ms
step:1440/1770 train_time:141032ms step_avg:98.62ms
step:1441/1770 train_time:141137ms step_avg:98.63ms
step:1442/1770 train_time:141238ms step_avg:98.63ms
step:1443/1770 train_time:141341ms step_avg:98.63ms
step:1444/1770 train_time:141443ms step_avg:98.64ms
step:1445/1770 train_time:141545ms step_avg:98.64ms
step:1446/1770 train_time:141648ms step_avg:98.64ms
step:1447/1770 train_time:141751ms step_avg:98.64ms
step:1448/1770 train_time:141855ms step_avg:98.65ms
step:1449/1770 train_time:141959ms step_avg:98.65ms
step:1450/1770 train_time:142062ms step_avg:98.65ms
step:1451/1770 train_time:142165ms step_avg:98.66ms
step:1452/1770 train_time:142268ms step_avg:98.66ms
step:1453/1770 train_time:142372ms step_avg:98.66ms
step:1454/1770 train_time:142474ms step_avg:98.67ms
step:1455/1770 train_time:142578ms step_avg:98.67ms
step:1456/1770 train_time:142682ms step_avg:98.67ms
step:1457/1770 train_time:142786ms step_avg:98.68ms
step:1458/1770 train_time:142889ms step_avg:98.68ms
step:1459/1770 train_time:142993ms step_avg:98.68ms
step:1460/1770 train_time:143096ms step_avg:98.69ms
step:1461/1770 train_time:143199ms step_avg:98.69ms
step:1462/1770 train_time:143303ms step_avg:98.69ms
step:1463/1770 train_time:143407ms step_avg:98.70ms
step:1464/1770 train_time:143512ms step_avg:98.70ms
step:1465/1770 train_time:143614ms step_avg:98.70ms
step:1466/1770 train_time:143718ms step_avg:98.71ms
step:1467/1770 train_time:143822ms step_avg:98.71ms
step:1468/1770 train_time:143926ms step_avg:98.71ms
step:1469/1770 train_time:144028ms step_avg:98.72ms
step:1470/1770 train_time:144132ms step_avg:98.72ms
step:1471/1770 train_time:144237ms step_avg:98.72ms
step:1472/1770 train_time:144340ms step_avg:98.73ms
step:1473/1770 train_time:144444ms step_avg:98.73ms
step:1474/1770 train_time:144548ms step_avg:98.73ms
step:1475/1770 train_time:144651ms step_avg:98.74ms
step:1476/1770 train_time:144753ms step_avg:98.74ms
step:1477/1770 train_time:144859ms step_avg:98.74ms
step:1478/1770 train_time:144963ms step_avg:98.75ms
step:1479/1770 train_time:145066ms step_avg:98.75ms
step:1480/1770 train_time:145169ms step_avg:98.75ms
step:1481/1770 train_time:145276ms step_avg:98.76ms
step:1482/1770 train_time:145379ms step_avg:98.76ms
step:1483/1770 train_time:145483ms step_avg:98.77ms
step:1484/1770 train_time:145586ms step_avg:98.77ms
step:1485/1770 train_time:145689ms step_avg:98.77ms
step:1486/1770 train_time:145791ms step_avg:98.77ms
step:1487/1770 train_time:145893ms step_avg:98.78ms
step:1488/1770 train_time:145998ms step_avg:98.78ms
step:1489/1770 train_time:146102ms step_avg:98.78ms
step:1490/1770 train_time:146206ms step_avg:98.79ms
step:1491/1770 train_time:146309ms step_avg:98.79ms
step:1492/1770 train_time:146413ms step_avg:98.79ms
step:1493/1770 train_time:146519ms step_avg:98.80ms
step:1494/1770 train_time:146625ms step_avg:98.80ms
step:1495/1770 train_time:146728ms step_avg:98.81ms
step:1496/1770 train_time:146831ms step_avg:98.81ms
step:1497/1770 train_time:146934ms step_avg:98.81ms
step:1498/1770 train_time:147036ms step_avg:98.81ms
step:1499/1770 train_time:147139ms step_avg:98.82ms
step:1500/1770 train_time:147242ms step_avg:98.82ms
step:1500/1770 val_loss:3.3425 train_time:147344ms step_avg:98.89ms
step:1501/1770 train_time:147365ms step_avg:98.84ms
step:1502/1770 train_time:147456ms step_avg:98.83ms
step:1503/1770 train_time:147559ms step_avg:98.83ms
step:1504/1770 train_time:147662ms step_avg:98.84ms
step:1505/1770 train_time:147768ms step_avg:98.84ms
step:1506/1770 train_time:147871ms step_avg:98.84ms
step:1507/1770 train_time:147975ms step_avg:98.85ms
step:1508/1770 train_time:148079ms step_avg:98.85ms
step:1509/1770 train_time:148181ms step_avg:98.85ms
step:1510/1770 train_time:148284ms step_avg:98.86ms
step:1511/1770 train_time:148391ms step_avg:98.86ms
step:1512/1770 train_time:148494ms step_avg:98.86ms
step:1513/1770 train_time:148598ms step_avg:98.87ms
step:1514/1770 train_time:148702ms step_avg:98.87ms
step:1515/1770 train_time:148805ms step_avg:98.87ms
step:1516/1770 train_time:148909ms step_avg:98.88ms
step:1517/1770 train_time:149012ms step_avg:98.88ms
step:1518/1770 train_time:149117ms step_avg:98.88ms
step:1519/1770 train_time:149219ms step_avg:98.89ms
step:1520/1770 train_time:149323ms step_avg:98.89ms
step:1521/1770 train_time:149427ms step_avg:98.89ms
step:1522/1770 train_time:149531ms step_avg:98.90ms
step:1523/1770 train_time:149635ms step_avg:98.90ms
step:1524/1770 train_time:149739ms step_avg:98.90ms
step:1525/1770 train_time:149842ms step_avg:98.91ms
step:1526/1770 train_time:149945ms step_avg:98.91ms
step:1527/1770 train_time:150050ms step_avg:98.91ms
step:1528/1770 train_time:150155ms step_avg:98.92ms
step:1529/1770 train_time:150257ms step_avg:98.92ms
step:1530/1770 train_time:150360ms step_avg:98.92ms
step:1531/1770 train_time:150464ms step_avg:98.92ms
step:1532/1770 train_time:150569ms step_avg:98.93ms
step:1533/1770 train_time:150672ms step_avg:98.93ms
step:1534/1770 train_time:150776ms step_avg:98.93ms
step:1535/1770 train_time:150878ms step_avg:98.94ms
step:1536/1770 train_time:150981ms step_avg:98.94ms
step:1537/1770 train_time:151085ms step_avg:98.94ms
step:1538/1770 train_time:151190ms step_avg:98.95ms
step:1539/1770 train_time:151293ms step_avg:98.95ms
step:1540/1770 train_time:151401ms step_avg:98.95ms
step:1541/1770 train_time:151506ms step_avg:98.96ms
step:1542/1770 train_time:151610ms step_avg:98.96ms
step:1543/1770 train_time:151712ms step_avg:98.96ms
step:1544/1770 train_time:151818ms step_avg:98.97ms
step:1545/1770 train_time:151920ms step_avg:98.97ms
step:1546/1770 train_time:152024ms step_avg:98.97ms
step:1547/1770 train_time:152127ms step_avg:98.98ms
step:1548/1770 train_time:152231ms step_avg:98.98ms
step:1549/1770 train_time:152334ms step_avg:98.98ms
step:1550/1770 train_time:152439ms step_avg:98.99ms
step:1551/1770 train_time:152542ms step_avg:98.99ms
step:1552/1770 train_time:152647ms step_avg:98.99ms
step:1553/1770 train_time:152750ms step_avg:99.00ms
step:1554/1770 train_time:152853ms step_avg:99.00ms
step:1555/1770 train_time:152957ms step_avg:99.00ms
step:1556/1770 train_time:153059ms step_avg:99.00ms
step:1557/1770 train_time:153161ms step_avg:99.01ms
step:1558/1770 train_time:153266ms step_avg:99.01ms
step:1559/1770 train_time:153370ms step_avg:99.01ms
step:1560/1770 train_time:153472ms step_avg:99.01ms
step:1561/1770 train_time:153578ms step_avg:99.02ms
step:1562/1770 train_time:153681ms step_avg:99.02ms
step:1563/1770 train_time:153785ms step_avg:99.02ms
step:1564/1770 train_time:153887ms step_avg:99.03ms
step:1565/1770 train_time:153991ms step_avg:99.03ms
step:1566/1770 train_time:154095ms step_avg:99.03ms
step:1567/1770 train_time:154198ms step_avg:99.04ms
step:1568/1770 train_time:154301ms step_avg:99.04ms
step:1569/1770 train_time:154408ms step_avg:99.04ms
step:1570/1770 train_time:154511ms step_avg:99.05ms
step:1571/1770 train_time:154614ms step_avg:99.05ms
step:1572/1770 train_time:154719ms step_avg:99.05ms
step:1573/1770 train_time:154824ms step_avg:99.06ms
step:1574/1770 train_time:154927ms step_avg:99.06ms
step:1575/1770 train_time:155029ms step_avg:99.06ms
step:1576/1770 train_time:155133ms step_avg:99.06ms
step:1577/1770 train_time:155238ms step_avg:99.07ms
step:1578/1770 train_time:155343ms step_avg:99.07ms
step:1579/1770 train_time:155447ms step_avg:99.07ms
step:1580/1770 train_time:155550ms step_avg:99.08ms
step:1581/1770 train_time:155655ms step_avg:99.08ms
step:1582/1770 train_time:155760ms step_avg:99.08ms
step:1583/1770 train_time:155864ms step_avg:99.09ms
step:1584/1770 train_time:155968ms step_avg:99.09ms
step:1585/1770 train_time:156073ms step_avg:99.09ms
step:1586/1770 train_time:156180ms step_avg:99.10ms
step:1587/1770 train_time:156284ms step_avg:99.10ms
step:1588/1770 train_time:156387ms step_avg:99.10ms
step:1589/1770 train_time:156493ms step_avg:99.11ms
step:1590/1770 train_time:156596ms step_avg:99.11ms
step:1591/1770 train_time:156699ms step_avg:99.11ms
step:1592/1770 train_time:156802ms step_avg:99.12ms
step:1593/1770 train_time:156905ms step_avg:99.12ms
step:1594/1770 train_time:157008ms step_avg:99.12ms
step:1595/1770 train_time:157111ms step_avg:99.12ms
step:1596/1770 train_time:157217ms step_avg:99.13ms
step:1597/1770 train_time:157320ms step_avg:99.13ms
step:1598/1770 train_time:157423ms step_avg:99.13ms
step:1599/1770 train_time:157528ms step_avg:99.14ms
step:1600/1770 train_time:157633ms step_avg:99.14ms
step:1601/1770 train_time:157737ms step_avg:99.14ms
step:1602/1770 train_time:157842ms step_avg:99.15ms
step:1603/1770 train_time:157945ms step_avg:99.15ms
step:1604/1770 train_time:158049ms step_avg:99.15ms
step:1605/1770 train_time:158151ms step_avg:99.15ms
step:1606/1770 train_time:158255ms step_avg:99.16ms
step:1607/1770 train_time:158362ms step_avg:99.16ms
step:1608/1770 train_time:158465ms step_avg:99.16ms
step:1609/1770 train_time:158568ms step_avg:99.17ms
step:1610/1770 train_time:158672ms step_avg:99.17ms
step:1611/1770 train_time:158777ms step_avg:99.17ms
step:1612/1770 train_time:158881ms step_avg:99.18ms
step:1613/1770 train_time:158984ms step_avg:99.18ms
step:1614/1770 train_time:159087ms step_avg:99.18ms
step:1615/1770 train_time:159192ms step_avg:99.18ms
step:1616/1770 train_time:159294ms step_avg:99.19ms
step:1617/1770 train_time:159399ms step_avg:99.19ms
step:1618/1770 train_time:159503ms step_avg:99.19ms
step:1619/1770 train_time:159607ms step_avg:99.20ms
step:1620/1770 train_time:159712ms step_avg:99.20ms
step:1621/1770 train_time:159816ms step_avg:99.20ms
step:1622/1770 train_time:159921ms step_avg:99.21ms
step:1623/1770 train_time:160026ms step_avg:99.21ms
step:1624/1770 train_time:160129ms step_avg:99.21ms
step:1625/1770 train_time:160231ms step_avg:99.21ms
step:1625/1770 val_loss:3.3084 train_time:160333ms step_avg:99.28ms
step:1626/1770 train_time:160354ms step_avg:99.23ms
step:1627/1770 train_time:160446ms step_avg:99.22ms
step:1628/1770 train_time:160549ms step_avg:99.23ms
step:1629/1770 train_time:160652ms step_avg:99.23ms
step:1630/1770 train_time:160755ms step_avg:99.23ms
step:1631/1770 train_time:160857ms step_avg:99.23ms
step:1632/1770 train_time:160960ms step_avg:99.24ms
step:1633/1770 train_time:161064ms step_avg:99.24ms
step:1634/1770 train_time:161166ms step_avg:99.24ms
step:1635/1770 train_time:161269ms step_avg:99.24ms
step:1636/1770 train_time:161373ms step_avg:99.25ms
step:1637/1770 train_time:161479ms step_avg:99.25ms
step:1638/1770 train_time:161582ms step_avg:99.25ms
step:1639/1770 train_time:161686ms step_avg:99.25ms
step:1640/1770 train_time:161790ms step_avg:99.26ms
step:1641/1770 train_time:161893ms step_avg:99.26ms
step:1642/1770 train_time:161996ms step_avg:99.26ms
step:1643/1770 train_time:162099ms step_avg:99.26ms
step:1644/1770 train_time:162205ms step_avg:99.27ms
step:1645/1770 train_time:162307ms step_avg:99.27ms
step:1646/1770 train_time:162412ms step_avg:99.27ms
step:1647/1770 train_time:162518ms step_avg:99.28ms
step:1648/1770 train_time:162621ms step_avg:99.28ms
step:1649/1770 train_time:162725ms step_avg:99.28ms
step:1650/1770 train_time:162828ms step_avg:99.29ms
step:1651/1770 train_time:162930ms step_avg:99.29ms
step:1652/1770 train_time:163034ms step_avg:99.29ms
step:1653/1770 train_time:163138ms step_avg:99.29ms
step:1654/1770 train_time:163245ms step_avg:99.30ms
step:1655/1770 train_time:163350ms step_avg:99.30ms
step:1656/1770 train_time:163454ms step_avg:99.30ms
step:1657/1770 train_time:163559ms step_avg:99.31ms
step:1658/1770 train_time:163663ms step_avg:99.31ms
step:1659/1770 train_time:163768ms step_avg:99.31ms
step:1660/1770 train_time:163871ms step_avg:99.32ms
step:1661/1770 train_time:163976ms step_avg:99.32ms
step:1662/1770 train_time:164080ms step_avg:99.32ms
step:1663/1770 train_time:164183ms step_avg:99.32ms
step:1664/1770 train_time:164286ms step_avg:99.33ms
step:1665/1770 train_time:164389ms step_avg:99.33ms
step:1666/1770 train_time:164493ms step_avg:99.33ms
step:1667/1770 train_time:164596ms step_avg:99.33ms
step:1668/1770 train_time:164699ms step_avg:99.34ms
step:1669/1770 train_time:164801ms step_avg:99.34ms
step:1670/1770 train_time:164904ms step_avg:99.34ms
step:1671/1770 train_time:165008ms step_avg:99.34ms
step:1672/1770 train_time:165113ms step_avg:99.35ms
step:1673/1770 train_time:165218ms step_avg:99.35ms
step:1674/1770 train_time:165321ms step_avg:99.35ms
step:1675/1770 train_time:165423ms step_avg:99.35ms
step:1676/1770 train_time:165527ms step_avg:99.36ms
step:1677/1770 train_time:165635ms step_avg:99.36ms
step:1678/1770 train_time:165737ms step_avg:99.36ms
step:1679/1770 train_time:165841ms step_avg:99.37ms
step:1680/1770 train_time:165944ms step_avg:99.37ms
step:1681/1770 train_time:166048ms step_avg:99.37ms
step:1682/1770 train_time:166154ms step_avg:99.37ms
step:1683/1770 train_time:166257ms step_avg:99.38ms
step:1684/1770 train_time:166360ms step_avg:99.38ms
step:1685/1770 train_time:166464ms step_avg:99.38ms
step:1686/1770 train_time:166569ms step_avg:99.38ms
step:1687/1770 train_time:166674ms step_avg:99.39ms
step:1688/1770 train_time:166777ms step_avg:99.39ms
step:1689/1770 train_time:166880ms step_avg:99.39ms
step:1690/1770 train_time:166984ms step_avg:99.40ms
step:1691/1770 train_time:167088ms step_avg:99.40ms
step:1692/1770 train_time:167191ms step_avg:99.40ms
step:1693/1770 train_time:167295ms step_avg:99.40ms
step:1694/1770 train_time:167398ms step_avg:99.41ms
step:1695/1770 train_time:167503ms step_avg:99.41ms
step:1696/1770 train_time:167609ms step_avg:99.41ms
step:1697/1770 train_time:167714ms step_avg:99.42ms
step:1698/1770 train_time:167818ms step_avg:99.42ms
step:1699/1770 train_time:167921ms step_avg:99.42ms
step:1700/1770 train_time:168025ms step_avg:99.42ms
step:1701/1770 train_time:168128ms step_avg:99.43ms
step:1702/1770 train_time:168231ms step_avg:99.43ms
step:1703/1770 train_time:168334ms step_avg:99.43ms
step:1704/1770 train_time:168439ms step_avg:99.43ms
step:1705/1770 train_time:168543ms step_avg:99.44ms
step:1706/1770 train_time:168646ms step_avg:99.44ms
step:1707/1770 train_time:168750ms step_avg:99.44ms
step:1708/1770 train_time:168854ms step_avg:99.44ms
step:1709/1770 train_time:168959ms step_avg:99.45ms
step:1710/1770 train_time:169066ms step_avg:99.45ms
step:1711/1770 train_time:169171ms step_avg:99.45ms
step:1712/1770 train_time:169275ms step_avg:99.46ms
step:1713/1770 train_time:169378ms step_avg:99.46ms
step:1714/1770 train_time:169483ms step_avg:99.46ms
step:1715/1770 train_time:169586ms step_avg:99.46ms
step:1716/1770 train_time:169691ms step_avg:99.47ms
step:1717/1770 train_time:169794ms step_avg:99.47ms
step:1718/1770 train_time:169899ms step_avg:99.47ms
step:1719/1770 train_time:170005ms step_avg:99.48ms
step:1720/1770 train_time:170110ms step_avg:99.48ms
step:1721/1770 train_time:170213ms step_avg:99.48ms
step:1722/1770 train_time:170320ms step_avg:99.49ms
step:1723/1770 train_time:170427ms step_avg:99.49ms
step:1724/1770 train_time:170534ms step_avg:99.50ms
step:1725/1770 train_time:170641ms step_avg:99.50ms
step:1726/1770 train_time:170747ms step_avg:99.50ms
step:1727/1770 train_time:170850ms step_avg:99.51ms
step:1728/1770 train_time:170956ms step_avg:99.51ms
step:1729/1770 train_time:171059ms step_avg:99.51ms
step:1730/1770 train_time:171165ms step_avg:99.51ms
step:1731/1770 train_time:171272ms step_avg:99.52ms
step:1732/1770 train_time:171375ms step_avg:99.52ms
step:1733/1770 train_time:171481ms step_avg:99.52ms
step:1734/1770 train_time:171585ms step_avg:99.53ms
step:1735/1770 train_time:171690ms step_avg:99.53ms
step:1736/1770 train_time:171794ms step_avg:99.53ms
step:1737/1770 train_time:171898ms step_avg:99.54ms
step:1738/1770 train_time:172002ms step_avg:99.54ms
step:1739/1770 train_time:172107ms step_avg:99.54ms
step:1740/1770 train_time:172210ms step_avg:99.54ms
step:1741/1770 train_time:172317ms step_avg:99.55ms
step:1742/1770 train_time:172423ms step_avg:99.55ms
step:1743/1770 train_time:172529ms step_avg:99.55ms
step:1744/1770 train_time:172633ms step_avg:99.56ms
step:1745/1770 train_time:172737ms step_avg:99.56ms
step:1746/1770 train_time:172844ms step_avg:99.56ms
step:1747/1770 train_time:172947ms step_avg:99.57ms
step:1748/1770 train_time:173053ms step_avg:99.57ms
step:1749/1770 train_time:173158ms step_avg:99.57ms
step:1750/1770 train_time:173262ms step_avg:99.58ms
step:1750/1770 val_loss:3.2811 train_time:173365ms step_avg:99.63ms
step:1751/1770 train_time:173386ms step_avg:99.59ms
step:1752/1770 train_time:173479ms step_avg:99.59ms
step:1753/1770 train_time:173583ms step_avg:99.59ms
step:1754/1770 train_time:173688ms step_avg:99.59ms
step:1755/1770 train_time:173792ms step_avg:99.59ms
step:1756/1770 train_time:173896ms step_avg:99.60ms
step:1757/1770 train_time:174001ms step_avg:99.60ms
step:1758/1770 train_time:174104ms step_avg:99.60ms
step:1759/1770 train_time:174209ms step_avg:99.60ms
step:1760/1770 train_time:174313ms step_avg:99.61ms
step:1761/1770 train_time:174420ms step_avg:99.61ms
step:1762/1770 train_time:174528ms step_avg:99.62ms
step:1763/1770 train_time:174631ms step_avg:99.62ms
step:1764/1770 train_time:174736ms step_avg:99.62ms
step:1765/1770 train_time:174839ms step_avg:99.62ms
step:1766/1770 train_time:174947ms step_avg:99.63ms
step:1767/1770 train_time:175050ms step_avg:99.63ms
step:1768/1770 train_time:175155ms step_avg:99.63ms
step:1769/1770 train_time:175258ms step_avg:99.64ms
step:1770/1770 train_time:175361ms step_avg:99.64ms
step:1770/1770 val_loss:3.2782 train_time:175466ms step_avg:99.70ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
