import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sat Jan 25 22:33:30 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23305ms step_avg:nanms
step:2/1770 train_time:23827ms step_avg:nanms
step:3/1770 train_time:23921ms step_avg:nanms
step:4/1770 train_time:24014ms step_avg:nanms
step:5/1770 train_time:24108ms step_avg:nanms
step:6/1770 train_time:24201ms step_avg:nanms
step:7/1770 train_time:24295ms step_avg:nanms
step:8/1770 train_time:24389ms step_avg:nanms
step:9/1770 train_time:24482ms step_avg:nanms
step:10/1770 train_time:24576ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.26ms
step:14/1770 train_time:377ms step_avg:94.20ms
step:15/1770 train_time:471ms step_avg:94.11ms
step:16/1770 train_time:565ms step_avg:94.15ms
step:17/1770 train_time:659ms step_avg:94.08ms
step:18/1770 train_time:752ms step_avg:94.04ms
step:19/1770 train_time:846ms step_avg:94.00ms
step:20/1770 train_time:939ms step_avg:93.95ms
step:21/1770 train_time:1033ms step_avg:93.94ms
step:22/1770 train_time:1127ms step_avg:93.89ms
step:23/1770 train_time:1221ms step_avg:93.91ms
step:24/1770 train_time:1316ms step_avg:93.97ms
step:25/1770 train_time:1410ms step_avg:93.97ms
step:26/1770 train_time:1503ms step_avg:93.96ms
step:27/1770 train_time:1597ms step_avg:93.95ms
step:28/1770 train_time:1691ms step_avg:93.97ms
step:29/1770 train_time:1785ms step_avg:93.95ms
step:30/1770 train_time:1879ms step_avg:93.95ms
step:31/1770 train_time:1973ms step_avg:93.94ms
step:32/1770 train_time:2067ms step_avg:93.96ms
step:33/1770 train_time:2161ms step_avg:93.97ms
step:34/1770 train_time:2256ms step_avg:93.98ms
step:35/1770 train_time:2350ms step_avg:93.98ms
step:36/1770 train_time:2444ms step_avg:93.99ms
step:37/1770 train_time:2538ms step_avg:94.00ms
step:38/1770 train_time:2632ms step_avg:93.98ms
step:39/1770 train_time:2726ms step_avg:93.99ms
step:40/1770 train_time:2820ms step_avg:93.99ms
step:41/1770 train_time:2914ms step_avg:93.99ms
step:42/1770 train_time:3008ms step_avg:93.99ms
step:43/1770 train_time:3102ms step_avg:94.01ms
step:44/1770 train_time:3196ms step_avg:94.00ms
step:45/1770 train_time:3290ms step_avg:94.00ms
step:46/1770 train_time:3384ms step_avg:94.01ms
step:47/1770 train_time:3478ms step_avg:94.01ms
step:48/1770 train_time:3572ms step_avg:94.01ms
step:49/1770 train_time:3667ms step_avg:94.02ms
step:50/1770 train_time:3761ms step_avg:94.02ms
step:51/1770 train_time:3854ms step_avg:94.01ms
step:52/1770 train_time:3949ms step_avg:94.02ms
step:53/1770 train_time:4043ms step_avg:94.02ms
step:54/1770 train_time:4137ms step_avg:94.01ms
step:55/1770 train_time:4230ms step_avg:94.01ms
step:56/1770 train_time:4324ms step_avg:94.01ms
step:57/1770 train_time:4418ms step_avg:94.00ms
step:58/1770 train_time:4512ms step_avg:94.01ms
step:59/1770 train_time:4606ms step_avg:94.00ms
step:60/1770 train_time:4700ms step_avg:93.99ms
step:61/1770 train_time:4794ms step_avg:93.99ms
step:62/1770 train_time:4888ms step_avg:94.00ms
step:63/1770 train_time:4982ms step_avg:94.00ms
step:64/1770 train_time:5075ms step_avg:93.99ms
step:65/1770 train_time:5169ms step_avg:93.98ms
step:66/1770 train_time:5263ms step_avg:93.98ms
step:67/1770 train_time:5356ms step_avg:93.97ms
step:68/1770 train_time:5451ms step_avg:93.98ms
step:69/1770 train_time:5544ms step_avg:93.97ms
step:70/1770 train_time:5638ms step_avg:93.96ms
step:71/1770 train_time:5732ms step_avg:93.96ms
step:72/1770 train_time:5826ms step_avg:93.97ms
step:73/1770 train_time:5920ms step_avg:93.96ms
step:74/1770 train_time:6014ms step_avg:93.97ms
step:75/1770 train_time:6108ms step_avg:93.97ms
step:76/1770 train_time:6202ms step_avg:93.97ms
step:77/1770 train_time:6296ms step_avg:93.97ms
step:78/1770 train_time:6390ms step_avg:93.97ms
step:79/1770 train_time:6484ms step_avg:93.97ms
step:80/1770 train_time:6577ms step_avg:93.96ms
step:81/1770 train_time:6671ms step_avg:93.96ms
step:82/1770 train_time:6765ms step_avg:93.96ms
step:83/1770 train_time:6859ms step_avg:93.95ms
step:84/1770 train_time:6953ms step_avg:93.96ms
step:85/1770 train_time:7047ms step_avg:93.96ms
step:86/1770 train_time:7141ms step_avg:93.96ms
step:87/1770 train_time:7235ms step_avg:93.95ms
step:88/1770 train_time:7328ms step_avg:93.95ms
step:89/1770 train_time:7422ms step_avg:93.95ms
step:90/1770 train_time:7516ms step_avg:93.95ms
step:91/1770 train_time:7610ms step_avg:93.95ms
step:92/1770 train_time:7704ms step_avg:93.95ms
step:93/1770 train_time:7797ms step_avg:93.94ms
step:94/1770 train_time:7891ms step_avg:93.94ms
step:95/1770 train_time:7985ms step_avg:93.94ms
step:96/1770 train_time:8080ms step_avg:93.95ms
step:97/1770 train_time:8173ms step_avg:93.95ms
step:98/1770 train_time:8268ms step_avg:93.95ms
step:99/1770 train_time:8362ms step_avg:93.96ms
step:100/1770 train_time:8456ms step_avg:93.95ms
step:101/1770 train_time:8549ms step_avg:93.95ms
step:102/1770 train_time:8644ms step_avg:93.95ms
step:103/1770 train_time:8737ms step_avg:93.94ms
step:104/1770 train_time:8830ms step_avg:93.94ms
step:105/1770 train_time:8924ms step_avg:93.94ms
step:106/1770 train_time:9018ms step_avg:93.94ms
step:107/1770 train_time:9112ms step_avg:93.94ms
step:108/1770 train_time:9206ms step_avg:93.94ms
step:109/1770 train_time:9300ms step_avg:93.94ms
step:110/1770 train_time:9394ms step_avg:93.94ms
step:111/1770 train_time:9488ms step_avg:93.94ms
step:112/1770 train_time:9582ms step_avg:93.95ms
step:113/1770 train_time:9676ms step_avg:93.94ms
step:114/1770 train_time:9771ms step_avg:93.95ms
step:115/1770 train_time:9864ms step_avg:93.94ms
step:116/1770 train_time:9957ms step_avg:93.94ms
step:117/1770 train_time:10051ms step_avg:93.93ms
step:118/1770 train_time:10145ms step_avg:93.93ms
step:119/1770 train_time:10239ms step_avg:93.93ms
step:120/1770 train_time:10332ms step_avg:93.93ms
step:121/1770 train_time:10426ms step_avg:93.93ms
step:122/1770 train_time:10520ms step_avg:93.93ms
step:123/1770 train_time:10614ms step_avg:93.93ms
step:124/1770 train_time:10708ms step_avg:93.93ms
step:125/1770 train_time:10803ms step_avg:93.94ms
step:125/1770 val_loss:4.6438 train_time:10895ms step_avg:94.74ms
step:126/1770 train_time:10918ms step_avg:94.12ms
step:127/1770 train_time:10994ms step_avg:93.97ms
step:128/1770 train_time:11092ms step_avg:94.00ms
step:129/1770 train_time:11195ms step_avg:94.07ms
step:130/1770 train_time:11290ms step_avg:94.08ms
step:131/1770 train_time:11384ms step_avg:94.08ms
step:132/1770 train_time:11478ms step_avg:94.08ms
step:133/1770 train_time:11571ms step_avg:94.07ms
step:134/1770 train_time:11665ms step_avg:94.07ms
step:135/1770 train_time:11759ms step_avg:94.08ms
step:136/1770 train_time:11854ms step_avg:94.08ms
step:137/1770 train_time:11947ms step_avg:94.07ms
step:138/1770 train_time:12042ms step_avg:94.08ms
step:139/1770 train_time:12137ms step_avg:94.09ms
step:140/1770 train_time:12232ms step_avg:94.09ms
step:141/1770 train_time:12327ms step_avg:94.10ms
step:142/1770 train_time:12422ms step_avg:94.10ms
step:143/1770 train_time:12516ms step_avg:94.11ms
step:144/1770 train_time:12610ms step_avg:94.11ms
step:145/1770 train_time:12704ms step_avg:94.11ms
step:146/1770 train_time:12799ms step_avg:94.11ms
step:147/1770 train_time:12893ms step_avg:94.11ms
step:148/1770 train_time:12988ms step_avg:94.11ms
step:149/1770 train_time:13083ms step_avg:94.12ms
step:150/1770 train_time:13178ms step_avg:94.13ms
step:151/1770 train_time:13272ms step_avg:94.13ms
step:152/1770 train_time:13366ms step_avg:94.13ms
step:153/1770 train_time:13462ms step_avg:94.14ms
step:154/1770 train_time:13557ms step_avg:94.14ms
step:155/1770 train_time:13651ms step_avg:94.14ms
step:156/1770 train_time:13745ms step_avg:94.14ms
step:157/1770 train_time:13840ms step_avg:94.15ms
step:158/1770 train_time:13935ms step_avg:94.15ms
step:159/1770 train_time:14029ms step_avg:94.15ms
step:160/1770 train_time:14124ms step_avg:94.16ms
step:161/1770 train_time:14218ms step_avg:94.16ms
step:162/1770 train_time:14313ms step_avg:94.16ms
step:163/1770 train_time:14408ms step_avg:94.17ms
step:164/1770 train_time:14502ms step_avg:94.17ms
step:165/1770 train_time:14597ms step_avg:94.18ms
step:166/1770 train_time:14691ms step_avg:94.18ms
step:167/1770 train_time:14786ms step_avg:94.18ms
step:168/1770 train_time:14881ms step_avg:94.18ms
step:169/1770 train_time:14976ms step_avg:94.19ms
step:170/1770 train_time:15071ms step_avg:94.19ms
step:171/1770 train_time:15164ms step_avg:94.19ms
step:172/1770 train_time:15259ms step_avg:94.19ms
step:173/1770 train_time:15354ms step_avg:94.19ms
step:174/1770 train_time:15449ms step_avg:94.20ms
step:175/1770 train_time:15544ms step_avg:94.20ms
step:176/1770 train_time:15639ms step_avg:94.21ms
step:177/1770 train_time:15733ms step_avg:94.21ms
step:178/1770 train_time:15828ms step_avg:94.22ms
step:179/1770 train_time:15923ms step_avg:94.22ms
step:180/1770 train_time:16017ms step_avg:94.22ms
step:181/1770 train_time:16112ms step_avg:94.22ms
step:182/1770 train_time:16207ms step_avg:94.22ms
step:183/1770 train_time:16301ms step_avg:94.23ms
step:184/1770 train_time:16396ms step_avg:94.23ms
step:185/1770 train_time:16490ms step_avg:94.23ms
step:186/1770 train_time:16584ms step_avg:94.23ms
step:187/1770 train_time:16680ms step_avg:94.24ms
step:188/1770 train_time:16774ms step_avg:94.24ms
step:189/1770 train_time:16869ms step_avg:94.24ms
step:190/1770 train_time:16963ms step_avg:94.24ms
step:191/1770 train_time:17058ms step_avg:94.24ms
step:192/1770 train_time:17152ms step_avg:94.24ms
step:193/1770 train_time:17246ms step_avg:94.24ms
step:194/1770 train_time:17341ms step_avg:94.25ms
step:195/1770 train_time:17436ms step_avg:94.25ms
step:196/1770 train_time:17530ms step_avg:94.25ms
step:197/1770 train_time:17625ms step_avg:94.25ms
step:198/1770 train_time:17720ms step_avg:94.25ms
step:199/1770 train_time:17814ms step_avg:94.26ms
step:200/1770 train_time:17909ms step_avg:94.26ms
step:201/1770 train_time:18004ms step_avg:94.26ms
step:202/1770 train_time:18099ms step_avg:94.26ms
step:203/1770 train_time:18193ms step_avg:94.26ms
step:204/1770 train_time:18288ms step_avg:94.27ms
step:205/1770 train_time:18382ms step_avg:94.27ms
step:206/1770 train_time:18477ms step_avg:94.27ms
step:207/1770 train_time:18570ms step_avg:94.27ms
step:208/1770 train_time:18665ms step_avg:94.27ms
step:209/1770 train_time:18760ms step_avg:94.27ms
step:210/1770 train_time:18854ms step_avg:94.27ms
step:211/1770 train_time:18949ms step_avg:94.27ms
step:212/1770 train_time:19044ms step_avg:94.27ms
step:213/1770 train_time:19138ms step_avg:94.28ms
step:214/1770 train_time:19232ms step_avg:94.28ms
step:215/1770 train_time:19328ms step_avg:94.28ms
step:216/1770 train_time:19422ms step_avg:94.28ms
step:217/1770 train_time:19517ms step_avg:94.29ms
step:218/1770 train_time:19611ms step_avg:94.28ms
step:219/1770 train_time:19706ms step_avg:94.29ms
step:220/1770 train_time:19800ms step_avg:94.29ms
step:221/1770 train_time:19895ms step_avg:94.29ms
step:222/1770 train_time:19990ms step_avg:94.29ms
step:223/1770 train_time:20085ms step_avg:94.29ms
step:224/1770 train_time:20179ms step_avg:94.29ms
step:225/1770 train_time:20273ms step_avg:94.29ms
step:226/1770 train_time:20368ms step_avg:94.29ms
step:227/1770 train_time:20462ms step_avg:94.30ms
step:228/1770 train_time:20558ms step_avg:94.30ms
step:229/1770 train_time:20652ms step_avg:94.30ms
step:230/1770 train_time:20747ms step_avg:94.31ms
step:231/1770 train_time:20842ms step_avg:94.31ms
step:232/1770 train_time:20937ms step_avg:94.31ms
step:233/1770 train_time:21031ms step_avg:94.31ms
step:234/1770 train_time:21126ms step_avg:94.31ms
step:235/1770 train_time:21221ms step_avg:94.31ms
step:236/1770 train_time:21315ms step_avg:94.32ms
step:237/1770 train_time:21410ms step_avg:94.32ms
step:238/1770 train_time:21505ms step_avg:94.32ms
step:239/1770 train_time:21601ms step_avg:94.33ms
step:240/1770 train_time:21695ms step_avg:94.32ms
step:241/1770 train_time:21790ms step_avg:94.33ms
step:242/1770 train_time:21884ms step_avg:94.33ms
step:243/1770 train_time:21979ms step_avg:94.33ms
step:244/1770 train_time:22074ms step_avg:94.33ms
step:245/1770 train_time:22169ms step_avg:94.34ms
step:246/1770 train_time:22264ms step_avg:94.34ms
step:247/1770 train_time:22359ms step_avg:94.34ms
step:248/1770 train_time:22453ms step_avg:94.34ms
step:249/1770 train_time:22547ms step_avg:94.34ms
step:250/1770 train_time:22642ms step_avg:94.34ms
step:250/1770 val_loss:4.1097 train_time:22735ms step_avg:94.73ms
step:251/1770 train_time:22758ms step_avg:94.43ms
step:252/1770 train_time:22839ms step_avg:94.38ms
step:253/1770 train_time:22936ms step_avg:94.39ms
step:254/1770 train_time:23031ms step_avg:94.39ms
step:255/1770 train_time:23125ms step_avg:94.39ms
step:256/1770 train_time:23219ms step_avg:94.38ms
step:257/1770 train_time:23313ms step_avg:94.38ms
step:258/1770 train_time:23407ms step_avg:94.38ms
step:259/1770 train_time:23501ms step_avg:94.38ms
step:260/1770 train_time:23596ms step_avg:94.38ms
step:261/1770 train_time:23690ms step_avg:94.38ms
step:262/1770 train_time:23785ms step_avg:94.39ms
step:263/1770 train_time:23881ms step_avg:94.39ms
step:264/1770 train_time:23976ms step_avg:94.39ms
step:265/1770 train_time:24071ms step_avg:94.40ms
step:266/1770 train_time:24166ms step_avg:94.40ms
step:267/1770 train_time:24261ms step_avg:94.40ms
step:268/1770 train_time:24356ms step_avg:94.40ms
step:269/1770 train_time:24452ms step_avg:94.41ms
step:270/1770 train_time:24546ms step_avg:94.41ms
step:271/1770 train_time:24641ms step_avg:94.41ms
step:272/1770 train_time:24736ms step_avg:94.41ms
step:273/1770 train_time:24831ms step_avg:94.42ms
step:274/1770 train_time:24927ms step_avg:94.42ms
step:275/1770 train_time:25022ms step_avg:94.42ms
step:276/1770 train_time:25118ms step_avg:94.43ms
step:277/1770 train_time:25213ms step_avg:94.43ms
step:278/1770 train_time:25308ms step_avg:94.43ms
step:279/1770 train_time:25403ms step_avg:94.43ms
step:280/1770 train_time:25499ms step_avg:94.44ms
step:281/1770 train_time:25594ms step_avg:94.44ms
step:282/1770 train_time:25689ms step_avg:94.45ms
step:283/1770 train_time:25784ms step_avg:94.45ms
step:284/1770 train_time:25879ms step_avg:94.45ms
step:285/1770 train_time:25975ms step_avg:94.46ms
step:286/1770 train_time:26071ms step_avg:94.46ms
step:287/1770 train_time:26165ms step_avg:94.46ms
step:288/1770 train_time:26261ms step_avg:94.46ms
step:289/1770 train_time:26356ms step_avg:94.46ms
step:290/1770 train_time:26451ms step_avg:94.47ms
step:291/1770 train_time:26546ms step_avg:94.47ms
step:292/1770 train_time:26642ms step_avg:94.48ms
step:293/1770 train_time:26738ms step_avg:94.48ms
step:294/1770 train_time:26833ms step_avg:94.48ms
step:295/1770 train_time:26928ms step_avg:94.48ms
step:296/1770 train_time:27023ms step_avg:94.48ms
step:297/1770 train_time:27118ms step_avg:94.49ms
step:298/1770 train_time:27213ms step_avg:94.49ms
step:299/1770 train_time:27308ms step_avg:94.49ms
step:300/1770 train_time:27403ms step_avg:94.49ms
step:301/1770 train_time:27498ms step_avg:94.49ms
step:302/1770 train_time:27594ms step_avg:94.50ms
step:303/1770 train_time:27689ms step_avg:94.50ms
step:304/1770 train_time:27784ms step_avg:94.50ms
step:305/1770 train_time:27879ms step_avg:94.51ms
step:306/1770 train_time:27974ms step_avg:94.51ms
step:307/1770 train_time:28070ms step_avg:94.51ms
step:308/1770 train_time:28165ms step_avg:94.51ms
step:309/1770 train_time:28260ms step_avg:94.52ms
step:310/1770 train_time:28355ms step_avg:94.52ms
step:311/1770 train_time:28450ms step_avg:94.52ms
step:312/1770 train_time:28545ms step_avg:94.52ms
step:313/1770 train_time:28641ms step_avg:94.52ms
step:314/1770 train_time:28736ms step_avg:94.53ms
step:315/1770 train_time:28832ms step_avg:94.53ms
step:316/1770 train_time:28926ms step_avg:94.53ms
step:317/1770 train_time:29021ms step_avg:94.53ms
step:318/1770 train_time:29117ms step_avg:94.53ms
step:319/1770 train_time:29213ms step_avg:94.54ms
step:320/1770 train_time:29308ms step_avg:94.54ms
step:321/1770 train_time:29403ms step_avg:94.54ms
step:322/1770 train_time:29498ms step_avg:94.54ms
step:323/1770 train_time:29593ms step_avg:94.55ms
step:324/1770 train_time:29689ms step_avg:94.55ms
step:325/1770 train_time:29783ms step_avg:94.55ms
step:326/1770 train_time:29879ms step_avg:94.55ms
step:327/1770 train_time:29975ms step_avg:94.56ms
step:328/1770 train_time:30070ms step_avg:94.56ms
step:329/1770 train_time:30165ms step_avg:94.56ms
step:330/1770 train_time:30260ms step_avg:94.56ms
step:331/1770 train_time:30356ms step_avg:94.57ms
step:332/1770 train_time:30451ms step_avg:94.57ms
step:333/1770 train_time:30546ms step_avg:94.57ms
step:334/1770 train_time:30641ms step_avg:94.57ms
step:335/1770 train_time:30737ms step_avg:94.57ms
step:336/1770 train_time:30832ms step_avg:94.58ms
step:337/1770 train_time:30927ms step_avg:94.58ms
step:338/1770 train_time:31023ms step_avg:94.58ms
step:339/1770 train_time:31118ms step_avg:94.58ms
step:340/1770 train_time:31213ms step_avg:94.58ms
step:341/1770 train_time:31308ms step_avg:94.59ms
step:342/1770 train_time:31403ms step_avg:94.59ms
step:343/1770 train_time:31499ms step_avg:94.59ms
step:344/1770 train_time:31595ms step_avg:94.59ms
step:345/1770 train_time:31690ms step_avg:94.60ms
step:346/1770 train_time:31785ms step_avg:94.60ms
step:347/1770 train_time:31881ms step_avg:94.60ms
step:348/1770 train_time:31977ms step_avg:94.61ms
step:349/1770 train_time:32071ms step_avg:94.60ms
step:350/1770 train_time:32166ms step_avg:94.61ms
step:351/1770 train_time:32261ms step_avg:94.61ms
step:352/1770 train_time:32356ms step_avg:94.61ms
step:353/1770 train_time:32451ms step_avg:94.61ms
step:354/1770 train_time:32545ms step_avg:94.61ms
step:355/1770 train_time:32640ms step_avg:94.61ms
step:356/1770 train_time:32735ms step_avg:94.61ms
step:357/1770 train_time:32830ms step_avg:94.61ms
step:358/1770 train_time:32925ms step_avg:94.61ms
step:359/1770 train_time:33021ms step_avg:94.62ms
step:360/1770 train_time:33116ms step_avg:94.62ms
step:361/1770 train_time:33211ms step_avg:94.62ms
step:362/1770 train_time:33306ms step_avg:94.62ms
step:363/1770 train_time:33402ms step_avg:94.62ms
step:364/1770 train_time:33497ms step_avg:94.62ms
step:365/1770 train_time:33592ms step_avg:94.63ms
step:366/1770 train_time:33687ms step_avg:94.63ms
step:367/1770 train_time:33782ms step_avg:94.63ms
step:368/1770 train_time:33877ms step_avg:94.63ms
step:369/1770 train_time:33973ms step_avg:94.63ms
step:370/1770 train_time:34068ms step_avg:94.63ms
step:371/1770 train_time:34163ms step_avg:94.63ms
step:372/1770 train_time:34259ms step_avg:94.64ms
step:373/1770 train_time:34354ms step_avg:94.64ms
step:374/1770 train_time:34449ms step_avg:94.64ms
step:375/1770 train_time:34544ms step_avg:94.64ms
step:375/1770 val_loss:3.9075 train_time:34638ms step_avg:94.90ms
step:376/1770 train_time:34659ms step_avg:94.70ms
step:377/1770 train_time:34742ms step_avg:94.66ms
step:378/1770 train_time:34840ms step_avg:94.67ms
step:379/1770 train_time:34936ms step_avg:94.68ms
step:380/1770 train_time:35031ms step_avg:94.68ms
step:381/1770 train_time:35126ms step_avg:94.68ms
step:382/1770 train_time:35221ms step_avg:94.68ms
step:383/1770 train_time:35315ms step_avg:94.68ms
step:384/1770 train_time:35409ms step_avg:94.68ms
step:385/1770 train_time:35504ms step_avg:94.68ms
step:386/1770 train_time:35598ms step_avg:94.68ms
step:387/1770 train_time:35693ms step_avg:94.68ms
step:388/1770 train_time:35790ms step_avg:94.68ms
step:389/1770 train_time:35885ms step_avg:94.68ms
step:390/1770 train_time:35981ms step_avg:94.69ms
step:391/1770 train_time:36075ms step_avg:94.69ms
step:392/1770 train_time:36170ms step_avg:94.69ms
step:393/1770 train_time:36265ms step_avg:94.69ms
step:394/1770 train_time:36360ms step_avg:94.69ms
step:395/1770 train_time:36455ms step_avg:94.69ms
step:396/1770 train_time:36551ms step_avg:94.69ms
step:397/1770 train_time:36648ms step_avg:94.70ms
step:398/1770 train_time:36745ms step_avg:94.70ms
step:399/1770 train_time:36842ms step_avg:94.71ms
step:400/1770 train_time:36939ms step_avg:94.72ms
step:401/1770 train_time:37036ms step_avg:94.72ms
step:402/1770 train_time:37133ms step_avg:94.73ms
step:403/1770 train_time:37230ms step_avg:94.73ms
step:404/1770 train_time:37327ms step_avg:94.74ms
step:405/1770 train_time:37424ms step_avg:94.74ms
step:406/1770 train_time:37521ms step_avg:94.75ms
step:407/1770 train_time:37619ms step_avg:94.76ms
step:408/1770 train_time:37715ms step_avg:94.76ms
step:409/1770 train_time:37812ms step_avg:94.77ms
step:410/1770 train_time:37909ms step_avg:94.77ms
step:411/1770 train_time:38006ms step_avg:94.78ms
step:412/1770 train_time:38104ms step_avg:94.79ms
step:413/1770 train_time:38201ms step_avg:94.79ms
step:414/1770 train_time:38298ms step_avg:94.80ms
step:415/1770 train_time:38395ms step_avg:94.80ms
step:416/1770 train_time:38492ms step_avg:94.81ms
step:417/1770 train_time:38589ms step_avg:94.81ms
step:418/1770 train_time:38686ms step_avg:94.82ms
step:419/1770 train_time:38784ms step_avg:94.83ms
step:420/1770 train_time:38881ms step_avg:94.83ms
step:421/1770 train_time:38977ms step_avg:94.84ms
step:422/1770 train_time:39074ms step_avg:94.84ms
step:423/1770 train_time:39172ms step_avg:94.85ms
step:424/1770 train_time:39268ms step_avg:94.85ms
step:425/1770 train_time:39365ms step_avg:94.86ms
step:426/1770 train_time:39462ms step_avg:94.86ms
step:427/1770 train_time:39560ms step_avg:94.87ms
step:428/1770 train_time:39656ms step_avg:94.87ms
step:429/1770 train_time:39753ms step_avg:94.88ms
step:430/1770 train_time:39850ms step_avg:94.88ms
step:431/1770 train_time:39947ms step_avg:94.89ms
step:432/1770 train_time:40044ms step_avg:94.89ms
step:433/1770 train_time:40141ms step_avg:94.90ms
step:434/1770 train_time:40238ms step_avg:94.90ms
step:435/1770 train_time:40334ms step_avg:94.90ms
step:436/1770 train_time:40431ms step_avg:94.91ms
step:437/1770 train_time:40528ms step_avg:94.91ms
step:438/1770 train_time:40626ms step_avg:94.92ms
step:439/1770 train_time:40724ms step_avg:94.93ms
step:440/1770 train_time:40820ms step_avg:94.93ms
step:441/1770 train_time:40916ms step_avg:94.93ms
step:442/1770 train_time:41014ms step_avg:94.94ms
step:443/1770 train_time:41110ms step_avg:94.94ms
step:444/1770 train_time:41208ms step_avg:94.95ms
step:445/1770 train_time:41305ms step_avg:94.95ms
step:446/1770 train_time:41403ms step_avg:94.96ms
step:447/1770 train_time:41500ms step_avg:94.97ms
step:448/1770 train_time:41596ms step_avg:94.97ms
step:449/1770 train_time:41693ms step_avg:94.97ms
step:450/1770 train_time:41790ms step_avg:94.98ms
step:451/1770 train_time:41887ms step_avg:94.98ms
step:452/1770 train_time:41984ms step_avg:94.99ms
step:453/1770 train_time:42081ms step_avg:94.99ms
step:454/1770 train_time:42178ms step_avg:95.00ms
step:455/1770 train_time:42275ms step_avg:95.00ms
step:456/1770 train_time:42372ms step_avg:95.00ms
step:457/1770 train_time:42469ms step_avg:95.01ms
step:458/1770 train_time:42566ms step_avg:95.01ms
step:459/1770 train_time:42664ms step_avg:95.02ms
step:460/1770 train_time:42761ms step_avg:95.02ms
step:461/1770 train_time:42858ms step_avg:95.03ms
step:462/1770 train_time:42954ms step_avg:95.03ms
step:463/1770 train_time:43051ms step_avg:95.04ms
step:464/1770 train_time:43148ms step_avg:95.04ms
step:465/1770 train_time:43245ms step_avg:95.04ms
step:466/1770 train_time:43343ms step_avg:95.05ms
step:467/1770 train_time:43440ms step_avg:95.05ms
step:468/1770 train_time:43537ms step_avg:95.06ms
step:469/1770 train_time:43635ms step_avg:95.06ms
step:470/1770 train_time:43732ms step_avg:95.07ms
step:471/1770 train_time:43830ms step_avg:95.08ms
step:472/1770 train_time:43927ms step_avg:95.08ms
step:473/1770 train_time:44025ms step_avg:95.09ms
step:474/1770 train_time:44121ms step_avg:95.09ms
step:475/1770 train_time:44218ms step_avg:95.09ms
step:476/1770 train_time:44314ms step_avg:95.09ms
step:477/1770 train_time:44411ms step_avg:95.10ms
step:478/1770 train_time:44508ms step_avg:95.10ms
step:479/1770 train_time:44606ms step_avg:95.11ms
step:480/1770 train_time:44703ms step_avg:95.11ms
step:481/1770 train_time:44800ms step_avg:95.12ms
step:482/1770 train_time:44896ms step_avg:95.12ms
step:483/1770 train_time:44994ms step_avg:95.12ms
step:484/1770 train_time:45091ms step_avg:95.13ms
step:485/1770 train_time:45188ms step_avg:95.13ms
step:486/1770 train_time:45285ms step_avg:95.14ms
step:487/1770 train_time:45382ms step_avg:95.14ms
step:488/1770 train_time:45479ms step_avg:95.14ms
step:489/1770 train_time:45576ms step_avg:95.15ms
step:490/1770 train_time:45673ms step_avg:95.15ms
step:491/1770 train_time:45770ms step_avg:95.16ms
step:492/1770 train_time:45867ms step_avg:95.16ms
step:493/1770 train_time:45964ms step_avg:95.16ms
step:494/1770 train_time:46061ms step_avg:95.17ms
step:495/1770 train_time:46158ms step_avg:95.17ms
step:496/1770 train_time:46255ms step_avg:95.17ms
step:497/1770 train_time:46352ms step_avg:95.18ms
step:498/1770 train_time:46449ms step_avg:95.18ms
step:499/1770 train_time:46546ms step_avg:95.19ms
step:500/1770 train_time:46644ms step_avg:95.19ms
step:500/1770 val_loss:3.7529 train_time:46739ms step_avg:95.39ms
step:501/1770 train_time:46762ms step_avg:95.24ms
step:502/1770 train_time:46847ms step_avg:95.22ms
step:503/1770 train_time:46948ms step_avg:95.23ms
step:504/1770 train_time:47046ms step_avg:95.23ms
step:505/1770 train_time:47143ms step_avg:95.24ms
step:506/1770 train_time:47240ms step_avg:95.24ms
step:507/1770 train_time:47336ms step_avg:95.24ms
step:508/1770 train_time:47432ms step_avg:95.24ms
step:509/1770 train_time:47528ms step_avg:95.25ms
step:510/1770 train_time:47625ms step_avg:95.25ms
step:511/1770 train_time:47721ms step_avg:95.25ms
step:512/1770 train_time:47819ms step_avg:95.26ms
step:513/1770 train_time:47917ms step_avg:95.26ms
step:514/1770 train_time:48014ms step_avg:95.27ms
step:515/1770 train_time:48111ms step_avg:95.27ms
step:516/1770 train_time:48208ms step_avg:95.27ms
step:517/1770 train_time:48304ms step_avg:95.28ms
step:518/1770 train_time:48402ms step_avg:95.28ms
step:519/1770 train_time:48499ms step_avg:95.28ms
step:520/1770 train_time:48596ms step_avg:95.29ms
step:521/1770 train_time:48692ms step_avg:95.29ms
step:522/1770 train_time:48789ms step_avg:95.29ms
step:523/1770 train_time:48886ms step_avg:95.29ms
step:524/1770 train_time:48984ms step_avg:95.30ms
step:525/1770 train_time:49082ms step_avg:95.30ms
step:526/1770 train_time:49179ms step_avg:95.31ms
step:527/1770 train_time:49277ms step_avg:95.31ms
step:528/1770 train_time:49374ms step_avg:95.32ms
step:529/1770 train_time:49471ms step_avg:95.32ms
step:530/1770 train_time:49569ms step_avg:95.32ms
step:531/1770 train_time:49666ms step_avg:95.33ms
step:532/1770 train_time:49762ms step_avg:95.33ms
step:533/1770 train_time:49860ms step_avg:95.33ms
step:534/1770 train_time:49957ms step_avg:95.34ms
step:535/1770 train_time:50054ms step_avg:95.34ms
step:536/1770 train_time:50151ms step_avg:95.34ms
step:537/1770 train_time:50249ms step_avg:95.35ms
step:538/1770 train_time:50346ms step_avg:95.35ms
step:539/1770 train_time:50444ms step_avg:95.36ms
step:540/1770 train_time:50541ms step_avg:95.36ms
step:541/1770 train_time:50638ms step_avg:95.36ms
step:542/1770 train_time:50736ms step_avg:95.37ms
step:543/1770 train_time:50832ms step_avg:95.37ms
step:544/1770 train_time:50929ms step_avg:95.37ms
step:545/1770 train_time:51026ms step_avg:95.38ms
step:546/1770 train_time:51123ms step_avg:95.38ms
step:547/1770 train_time:51221ms step_avg:95.38ms
step:548/1770 train_time:51319ms step_avg:95.39ms
step:549/1770 train_time:51416ms step_avg:95.39ms
step:550/1770 train_time:51513ms step_avg:95.40ms
step:551/1770 train_time:51611ms step_avg:95.40ms
step:552/1770 train_time:51708ms step_avg:95.40ms
step:553/1770 train_time:51806ms step_avg:95.41ms
step:554/1770 train_time:51903ms step_avg:95.41ms
step:555/1770 train_time:52001ms step_avg:95.41ms
step:556/1770 train_time:52098ms step_avg:95.42ms
step:557/1770 train_time:52196ms step_avg:95.42ms
step:558/1770 train_time:52293ms step_avg:95.42ms
step:559/1770 train_time:52390ms step_avg:95.43ms
step:560/1770 train_time:52487ms step_avg:95.43ms
step:561/1770 train_time:52584ms step_avg:95.43ms
step:562/1770 train_time:52682ms step_avg:95.44ms
step:563/1770 train_time:52779ms step_avg:95.44ms
step:564/1770 train_time:52877ms step_avg:95.45ms
step:565/1770 train_time:52974ms step_avg:95.45ms
step:566/1770 train_time:53071ms step_avg:95.45ms
step:567/1770 train_time:53169ms step_avg:95.46ms
step:568/1770 train_time:53267ms step_avg:95.46ms
step:569/1770 train_time:53364ms step_avg:95.46ms
step:570/1770 train_time:53461ms step_avg:95.47ms
step:571/1770 train_time:53558ms step_avg:95.47ms
step:572/1770 train_time:53656ms step_avg:95.47ms
step:573/1770 train_time:53753ms step_avg:95.48ms
step:574/1770 train_time:53850ms step_avg:95.48ms
step:575/1770 train_time:53947ms step_avg:95.48ms
step:576/1770 train_time:54045ms step_avg:95.49ms
step:577/1770 train_time:54143ms step_avg:95.49ms
step:578/1770 train_time:54240ms step_avg:95.49ms
step:579/1770 train_time:54337ms step_avg:95.50ms
step:580/1770 train_time:54434ms step_avg:95.50ms
step:581/1770 train_time:54531ms step_avg:95.50ms
step:582/1770 train_time:54629ms step_avg:95.51ms
step:583/1770 train_time:54727ms step_avg:95.51ms
step:584/1770 train_time:54824ms step_avg:95.51ms
step:585/1770 train_time:54922ms step_avg:95.52ms
step:586/1770 train_time:55019ms step_avg:95.52ms
step:587/1770 train_time:55117ms step_avg:95.52ms
step:588/1770 train_time:55214ms step_avg:95.53ms
step:589/1770 train_time:55311ms step_avg:95.53ms
step:590/1770 train_time:55409ms step_avg:95.53ms
step:591/1770 train_time:55506ms step_avg:95.54ms
step:592/1770 train_time:55604ms step_avg:95.54ms
step:593/1770 train_time:55701ms step_avg:95.54ms
step:594/1770 train_time:55799ms step_avg:95.55ms
step:595/1770 train_time:55896ms step_avg:95.55ms
step:596/1770 train_time:55993ms step_avg:95.55ms
step:597/1770 train_time:56090ms step_avg:95.55ms
step:598/1770 train_time:56187ms step_avg:95.56ms
step:599/1770 train_time:56284ms step_avg:95.56ms
step:600/1770 train_time:56382ms step_avg:95.56ms
step:601/1770 train_time:56480ms step_avg:95.57ms
step:602/1770 train_time:56577ms step_avg:95.57ms
step:603/1770 train_time:56674ms step_avg:95.57ms
step:604/1770 train_time:56771ms step_avg:95.57ms
step:605/1770 train_time:56869ms step_avg:95.58ms
step:606/1770 train_time:56966ms step_avg:95.58ms
step:607/1770 train_time:57063ms step_avg:95.58ms
step:608/1770 train_time:57161ms step_avg:95.59ms
step:609/1770 train_time:57258ms step_avg:95.59ms
step:610/1770 train_time:57356ms step_avg:95.59ms
step:611/1770 train_time:57453ms step_avg:95.60ms
step:612/1770 train_time:57550ms step_avg:95.60ms
step:613/1770 train_time:57647ms step_avg:95.60ms
step:614/1770 train_time:57744ms step_avg:95.60ms
step:615/1770 train_time:57842ms step_avg:95.61ms
step:616/1770 train_time:57939ms step_avg:95.61ms
step:617/1770 train_time:58037ms step_avg:95.61ms
step:618/1770 train_time:58134ms step_avg:95.62ms
step:619/1770 train_time:58231ms step_avg:95.62ms
step:620/1770 train_time:58329ms step_avg:95.62ms
step:621/1770 train_time:58426ms step_avg:95.62ms
step:622/1770 train_time:58523ms step_avg:95.63ms
step:623/1770 train_time:58620ms step_avg:95.63ms
step:624/1770 train_time:58718ms step_avg:95.63ms
step:625/1770 train_time:58815ms step_avg:95.63ms
step:625/1770 val_loss:3.6634 train_time:58910ms step_avg:95.79ms
step:626/1770 train_time:58931ms step_avg:95.67ms
step:627/1770 train_time:59019ms step_avg:95.65ms
step:628/1770 train_time:59119ms step_avg:95.66ms
step:629/1770 train_time:59216ms step_avg:95.66ms
step:630/1770 train_time:59314ms step_avg:95.67ms
step:631/1770 train_time:59411ms step_avg:95.67ms
step:632/1770 train_time:59508ms step_avg:95.67ms
step:633/1770 train_time:59605ms step_avg:95.67ms
step:634/1770 train_time:59701ms step_avg:95.67ms
step:635/1770 train_time:59798ms step_avg:95.68ms
step:636/1770 train_time:59895ms step_avg:95.68ms
step:637/1770 train_time:59994ms step_avg:95.68ms
step:638/1770 train_time:60093ms step_avg:95.69ms
step:639/1770 train_time:60192ms step_avg:95.69ms
step:640/1770 train_time:60289ms step_avg:95.70ms
step:641/1770 train_time:60386ms step_avg:95.70ms
step:642/1770 train_time:60483ms step_avg:95.70ms
step:643/1770 train_time:60579ms step_avg:95.70ms
step:644/1770 train_time:60676ms step_avg:95.70ms
step:645/1770 train_time:60773ms step_avg:95.71ms
step:646/1770 train_time:60871ms step_avg:95.71ms
step:647/1770 train_time:60969ms step_avg:95.71ms
step:648/1770 train_time:61066ms step_avg:95.71ms
step:649/1770 train_time:61164ms step_avg:95.72ms
step:650/1770 train_time:61262ms step_avg:95.72ms
step:651/1770 train_time:61359ms step_avg:95.72ms
step:652/1770 train_time:61457ms step_avg:95.73ms
step:653/1770 train_time:61554ms step_avg:95.73ms
step:654/1770 train_time:61652ms step_avg:95.73ms
step:655/1770 train_time:61749ms step_avg:95.73ms
step:656/1770 train_time:61846ms step_avg:95.74ms
step:657/1770 train_time:61943ms step_avg:95.74ms
step:658/1770 train_time:62041ms step_avg:95.74ms
step:659/1770 train_time:62141ms step_avg:95.75ms
step:660/1770 train_time:62241ms step_avg:95.75ms
step:661/1770 train_time:62340ms step_avg:95.76ms
step:662/1770 train_time:62439ms step_avg:95.77ms
step:663/1770 train_time:62538ms step_avg:95.77ms
step:664/1770 train_time:62637ms step_avg:95.78ms
step:665/1770 train_time:62737ms step_avg:95.78ms
step:666/1770 train_time:62837ms step_avg:95.79ms
step:667/1770 train_time:62936ms step_avg:95.79ms
step:668/1770 train_time:63035ms step_avg:95.80ms
step:669/1770 train_time:63134ms step_avg:95.80ms
step:670/1770 train_time:63233ms step_avg:95.81ms
step:671/1770 train_time:63333ms step_avg:95.81ms
step:672/1770 train_time:63432ms step_avg:95.82ms
step:673/1770 train_time:63532ms step_avg:95.82ms
step:674/1770 train_time:63631ms step_avg:95.83ms
step:675/1770 train_time:63730ms step_avg:95.83ms
step:676/1770 train_time:63829ms step_avg:95.84ms
step:677/1770 train_time:63928ms step_avg:95.84ms
step:678/1770 train_time:64028ms step_avg:95.85ms
step:679/1770 train_time:64126ms step_avg:95.85ms
step:680/1770 train_time:64225ms step_avg:95.86ms
step:681/1770 train_time:64324ms step_avg:95.86ms
step:682/1770 train_time:64423ms step_avg:95.87ms
step:683/1770 train_time:64522ms step_avg:95.87ms
step:684/1770 train_time:64621ms step_avg:95.88ms
step:685/1770 train_time:64720ms step_avg:95.88ms
step:686/1770 train_time:64820ms step_avg:95.89ms
step:687/1770 train_time:64920ms step_avg:95.89ms
step:688/1770 train_time:65020ms step_avg:95.90ms
step:689/1770 train_time:65120ms step_avg:95.91ms
step:690/1770 train_time:65220ms step_avg:95.91ms
step:691/1770 train_time:65319ms step_avg:95.92ms
step:692/1770 train_time:65418ms step_avg:95.92ms
step:693/1770 train_time:65517ms step_avg:95.93ms
step:694/1770 train_time:65616ms step_avg:95.93ms
step:695/1770 train_time:65716ms step_avg:95.94ms
step:696/1770 train_time:65815ms step_avg:95.94ms
step:697/1770 train_time:65915ms step_avg:95.95ms
step:698/1770 train_time:66014ms step_avg:95.95ms
step:699/1770 train_time:66114ms step_avg:95.96ms
step:700/1770 train_time:66214ms step_avg:95.96ms
step:701/1770 train_time:66314ms step_avg:95.97ms
step:702/1770 train_time:66414ms step_avg:95.97ms
step:703/1770 train_time:66514ms step_avg:95.98ms
step:704/1770 train_time:66613ms step_avg:95.98ms
step:705/1770 train_time:66713ms step_avg:95.99ms
step:706/1770 train_time:66812ms step_avg:95.99ms
step:707/1770 train_time:66911ms step_avg:96.00ms
step:708/1770 train_time:67011ms step_avg:96.00ms
step:709/1770 train_time:67111ms step_avg:96.01ms
step:710/1770 train_time:67211ms step_avg:96.02ms
step:711/1770 train_time:67311ms step_avg:96.02ms
step:712/1770 train_time:67411ms step_avg:96.03ms
step:713/1770 train_time:67511ms step_avg:96.03ms
step:714/1770 train_time:67610ms step_avg:96.04ms
step:715/1770 train_time:67710ms step_avg:96.04ms
step:716/1770 train_time:67809ms step_avg:96.05ms
step:717/1770 train_time:67907ms step_avg:96.05ms
step:718/1770 train_time:68005ms step_avg:96.05ms
step:719/1770 train_time:68104ms step_avg:96.06ms
step:720/1770 train_time:68204ms step_avg:96.06ms
step:721/1770 train_time:68303ms step_avg:96.07ms
step:722/1770 train_time:68401ms step_avg:96.07ms
step:723/1770 train_time:68500ms step_avg:96.07ms
step:724/1770 train_time:68600ms step_avg:96.08ms
step:725/1770 train_time:68699ms step_avg:96.08ms
step:726/1770 train_time:68799ms step_avg:96.09ms
step:727/1770 train_time:68899ms step_avg:96.09ms
step:728/1770 train_time:68999ms step_avg:96.10ms
step:729/1770 train_time:69099ms step_avg:96.10ms
step:730/1770 train_time:69198ms step_avg:96.11ms
step:731/1770 train_time:69297ms step_avg:96.11ms
step:732/1770 train_time:69396ms step_avg:96.12ms
step:733/1770 train_time:69496ms step_avg:96.12ms
step:734/1770 train_time:69595ms step_avg:96.13ms
step:735/1770 train_time:69695ms step_avg:96.13ms
step:736/1770 train_time:69794ms step_avg:96.14ms
step:737/1770 train_time:69894ms step_avg:96.14ms
step:738/1770 train_time:69994ms step_avg:96.15ms
step:739/1770 train_time:70094ms step_avg:96.15ms
step:740/1770 train_time:70194ms step_avg:96.16ms
step:741/1770 train_time:70293ms step_avg:96.16ms
step:742/1770 train_time:70392ms step_avg:96.16ms
step:743/1770 train_time:70491ms step_avg:96.17ms
step:744/1770 train_time:70591ms step_avg:96.17ms
step:745/1770 train_time:70690ms step_avg:96.18ms
step:746/1770 train_time:70789ms step_avg:96.18ms
step:747/1770 train_time:70888ms step_avg:96.18ms
step:748/1770 train_time:70986ms step_avg:96.19ms
step:749/1770 train_time:71085ms step_avg:96.19ms
step:750/1770 train_time:71184ms step_avg:96.20ms
step:750/1770 val_loss:3.6004 train_time:71282ms step_avg:96.33ms
step:751/1770 train_time:71303ms step_avg:96.23ms
step:752/1770 train_time:71391ms step_avg:96.21ms
step:753/1770 train_time:71492ms step_avg:96.22ms
step:754/1770 train_time:71592ms step_avg:96.23ms
step:755/1770 train_time:71691ms step_avg:96.23ms
step:756/1770 train_time:71789ms step_avg:96.23ms
step:757/1770 train_time:71888ms step_avg:96.24ms
step:758/1770 train_time:71987ms step_avg:96.24ms
step:759/1770 train_time:72085ms step_avg:96.24ms
step:760/1770 train_time:72184ms step_avg:96.24ms
step:761/1770 train_time:72283ms step_avg:96.25ms
step:762/1770 train_time:72382ms step_avg:96.25ms
step:763/1770 train_time:72482ms step_avg:96.26ms
step:764/1770 train_time:72580ms step_avg:96.26ms
step:765/1770 train_time:72680ms step_avg:96.27ms
step:766/1770 train_time:72781ms step_avg:96.27ms
step:767/1770 train_time:72879ms step_avg:96.27ms
step:768/1770 train_time:72978ms step_avg:96.28ms
step:769/1770 train_time:73078ms step_avg:96.28ms
step:770/1770 train_time:73176ms step_avg:96.28ms
step:771/1770 train_time:73275ms step_avg:96.29ms
step:772/1770 train_time:73375ms step_avg:96.29ms
step:773/1770 train_time:73474ms step_avg:96.30ms
step:774/1770 train_time:73573ms step_avg:96.30ms
step:775/1770 train_time:73672ms step_avg:96.30ms
step:776/1770 train_time:73771ms step_avg:96.31ms
step:777/1770 train_time:73871ms step_avg:96.31ms
step:778/1770 train_time:73972ms step_avg:96.32ms
step:779/1770 train_time:74071ms step_avg:96.32ms
step:780/1770 train_time:74170ms step_avg:96.32ms
step:781/1770 train_time:74270ms step_avg:96.33ms
step:782/1770 train_time:74369ms step_avg:96.33ms
step:783/1770 train_time:74468ms step_avg:96.34ms
step:784/1770 train_time:74567ms step_avg:96.34ms
step:785/1770 train_time:74665ms step_avg:96.34ms
step:786/1770 train_time:74765ms step_avg:96.35ms
step:787/1770 train_time:74864ms step_avg:96.35ms
step:788/1770 train_time:74963ms step_avg:96.35ms
step:789/1770 train_time:75062ms step_avg:96.36ms
step:790/1770 train_time:75161ms step_avg:96.36ms
step:791/1770 train_time:75260ms step_avg:96.36ms
step:792/1770 train_time:75359ms step_avg:96.37ms
step:793/1770 train_time:75459ms step_avg:96.37ms
step:794/1770 train_time:75559ms step_avg:96.38ms
step:795/1770 train_time:75659ms step_avg:96.38ms
step:796/1770 train_time:75760ms step_avg:96.39ms
step:797/1770 train_time:75860ms step_avg:96.39ms
step:798/1770 train_time:75960ms step_avg:96.40ms
step:799/1770 train_time:76059ms step_avg:96.40ms
step:800/1770 train_time:76158ms step_avg:96.40ms
step:801/1770 train_time:76258ms step_avg:96.41ms
step:802/1770 train_time:76358ms step_avg:96.41ms
step:803/1770 train_time:76457ms step_avg:96.42ms
step:804/1770 train_time:76557ms step_avg:96.42ms
step:805/1770 train_time:76658ms step_avg:96.42ms
step:806/1770 train_time:76758ms step_avg:96.43ms
step:807/1770 train_time:76858ms step_avg:96.43ms
step:808/1770 train_time:76958ms step_avg:96.44ms
step:809/1770 train_time:77059ms step_avg:96.44ms
step:810/1770 train_time:77159ms step_avg:96.45ms
step:811/1770 train_time:77258ms step_avg:96.45ms
step:812/1770 train_time:77358ms step_avg:96.46ms
step:813/1770 train_time:77457ms step_avg:96.46ms
step:814/1770 train_time:77557ms step_avg:96.46ms
step:815/1770 train_time:77657ms step_avg:96.47ms
step:816/1770 train_time:77757ms step_avg:96.47ms
step:817/1770 train_time:77857ms step_avg:96.48ms
step:818/1770 train_time:77957ms step_avg:96.48ms
step:819/1770 train_time:78057ms step_avg:96.49ms
step:820/1770 train_time:78157ms step_avg:96.49ms
step:821/1770 train_time:78257ms step_avg:96.49ms
step:822/1770 train_time:78357ms step_avg:96.50ms
step:823/1770 train_time:78456ms step_avg:96.50ms
step:824/1770 train_time:78556ms step_avg:96.51ms
step:825/1770 train_time:78655ms step_avg:96.51ms
step:826/1770 train_time:78755ms step_avg:96.51ms
step:827/1770 train_time:78855ms step_avg:96.52ms
step:828/1770 train_time:78954ms step_avg:96.52ms
step:829/1770 train_time:79053ms step_avg:96.52ms
step:830/1770 train_time:79152ms step_avg:96.53ms
step:831/1770 train_time:79253ms step_avg:96.53ms
step:832/1770 train_time:79353ms step_avg:96.54ms
step:833/1770 train_time:79454ms step_avg:96.54ms
step:834/1770 train_time:79553ms step_avg:96.54ms
step:835/1770 train_time:79652ms step_avg:96.55ms
step:836/1770 train_time:79753ms step_avg:96.55ms
step:837/1770 train_time:79852ms step_avg:96.56ms
step:838/1770 train_time:79952ms step_avg:96.56ms
step:839/1770 train_time:80051ms step_avg:96.56ms
step:840/1770 train_time:80150ms step_avg:96.57ms
step:841/1770 train_time:80249ms step_avg:96.57ms
step:842/1770 train_time:80350ms step_avg:96.57ms
step:843/1770 train_time:80449ms step_avg:96.58ms
step:844/1770 train_time:80549ms step_avg:96.58ms
step:845/1770 train_time:80649ms step_avg:96.59ms
step:846/1770 train_time:80749ms step_avg:96.59ms
step:847/1770 train_time:80849ms step_avg:96.59ms
step:848/1770 train_time:80949ms step_avg:96.60ms
step:849/1770 train_time:81048ms step_avg:96.60ms
step:850/1770 train_time:81148ms step_avg:96.60ms
step:851/1770 train_time:81247ms step_avg:96.61ms
step:852/1770 train_time:81346ms step_avg:96.61ms
step:853/1770 train_time:81445ms step_avg:96.61ms
step:854/1770 train_time:81544ms step_avg:96.62ms
step:855/1770 train_time:81644ms step_avg:96.62ms
step:856/1770 train_time:81743ms step_avg:96.62ms
step:857/1770 train_time:81842ms step_avg:96.63ms
step:858/1770 train_time:81940ms step_avg:96.63ms
step:859/1770 train_time:82039ms step_avg:96.63ms
step:860/1770 train_time:82138ms step_avg:96.63ms
step:861/1770 train_time:82238ms step_avg:96.64ms
step:862/1770 train_time:82337ms step_avg:96.64ms
step:863/1770 train_time:82437ms step_avg:96.64ms
step:864/1770 train_time:82537ms step_avg:96.65ms
step:865/1770 train_time:82637ms step_avg:96.65ms
step:866/1770 train_time:82737ms step_avg:96.66ms
step:867/1770 train_time:82837ms step_avg:96.66ms
step:868/1770 train_time:82937ms step_avg:96.66ms
step:869/1770 train_time:83037ms step_avg:96.67ms
step:870/1770 train_time:83136ms step_avg:96.67ms
step:871/1770 train_time:83235ms step_avg:96.67ms
step:872/1770 train_time:83335ms step_avg:96.68ms
step:873/1770 train_time:83435ms step_avg:96.68ms
step:874/1770 train_time:83535ms step_avg:96.68ms
step:875/1770 train_time:83635ms step_avg:96.69ms
step:875/1770 val_loss:3.5514 train_time:83733ms step_avg:96.80ms
step:876/1770 train_time:83755ms step_avg:96.71ms
step:877/1770 train_time:83844ms step_avg:96.71ms
step:878/1770 train_time:83947ms step_avg:96.71ms
step:879/1770 train_time:84047ms step_avg:96.72ms
step:880/1770 train_time:84145ms step_avg:96.72ms
step:881/1770 train_time:84245ms step_avg:96.72ms
step:882/1770 train_time:84344ms step_avg:96.72ms
step:883/1770 train_time:84442ms step_avg:96.73ms
step:884/1770 train_time:84541ms step_avg:96.73ms
step:885/1770 train_time:84639ms step_avg:96.73ms
step:886/1770 train_time:84739ms step_avg:96.73ms
step:887/1770 train_time:84839ms step_avg:96.74ms
step:888/1770 train_time:84939ms step_avg:96.74ms
step:889/1770 train_time:85038ms step_avg:96.74ms
step:890/1770 train_time:85137ms step_avg:96.75ms
step:891/1770 train_time:85236ms step_avg:96.75ms
step:892/1770 train_time:85335ms step_avg:96.75ms
step:893/1770 train_time:85435ms step_avg:96.75ms
step:894/1770 train_time:85533ms step_avg:96.76ms
step:895/1770 train_time:85633ms step_avg:96.76ms
step:896/1770 train_time:85733ms step_avg:96.76ms
step:897/1770 train_time:85834ms step_avg:96.77ms
step:898/1770 train_time:85934ms step_avg:96.77ms
step:899/1770 train_time:86034ms step_avg:96.78ms
step:900/1770 train_time:86135ms step_avg:96.78ms
step:901/1770 train_time:86234ms step_avg:96.78ms
step:902/1770 train_time:86334ms step_avg:96.79ms
step:903/1770 train_time:86433ms step_avg:96.79ms
step:904/1770 train_time:86532ms step_avg:96.79ms
step:905/1770 train_time:86632ms step_avg:96.80ms
step:906/1770 train_time:86732ms step_avg:96.80ms
step:907/1770 train_time:86832ms step_avg:96.80ms
step:908/1770 train_time:86932ms step_avg:96.81ms
step:909/1770 train_time:87032ms step_avg:96.81ms
step:910/1770 train_time:87132ms step_avg:96.81ms
step:911/1770 train_time:87233ms step_avg:96.82ms
step:912/1770 train_time:87332ms step_avg:96.82ms
step:913/1770 train_time:87431ms step_avg:96.82ms
step:914/1770 train_time:87530ms step_avg:96.83ms
step:915/1770 train_time:87629ms step_avg:96.83ms
step:916/1770 train_time:87729ms step_avg:96.83ms
step:917/1770 train_time:87828ms step_avg:96.83ms
step:918/1770 train_time:87928ms step_avg:96.84ms
step:919/1770 train_time:88028ms step_avg:96.84ms
step:920/1770 train_time:88130ms step_avg:96.85ms
step:921/1770 train_time:88232ms step_avg:96.85ms
step:922/1770 train_time:88333ms step_avg:96.86ms
step:923/1770 train_time:88433ms step_avg:96.86ms
step:924/1770 train_time:88534ms step_avg:96.86ms
step:925/1770 train_time:88635ms step_avg:96.87ms
step:926/1770 train_time:88735ms step_avg:96.87ms
step:927/1770 train_time:88836ms step_avg:96.88ms
step:928/1770 train_time:88938ms step_avg:96.88ms
step:929/1770 train_time:89038ms step_avg:96.89ms
step:930/1770 train_time:89139ms step_avg:96.89ms
step:931/1770 train_time:89239ms step_avg:96.89ms
step:932/1770 train_time:89340ms step_avg:96.90ms
step:933/1770 train_time:89440ms step_avg:96.90ms
step:934/1770 train_time:89540ms step_avg:96.90ms
step:935/1770 train_time:89640ms step_avg:96.91ms
step:936/1770 train_time:89741ms step_avg:96.91ms
step:937/1770 train_time:89841ms step_avg:96.92ms
step:938/1770 train_time:89942ms step_avg:96.92ms
step:939/1770 train_time:90043ms step_avg:96.92ms
step:940/1770 train_time:90144ms step_avg:96.93ms
step:941/1770 train_time:90244ms step_avg:96.93ms
step:942/1770 train_time:90346ms step_avg:96.94ms
step:943/1770 train_time:90447ms step_avg:96.94ms
step:944/1770 train_time:90548ms step_avg:96.95ms
step:945/1770 train_time:90649ms step_avg:96.95ms
step:946/1770 train_time:90751ms step_avg:96.96ms
step:947/1770 train_time:90851ms step_avg:96.96ms
step:948/1770 train_time:90952ms step_avg:96.96ms
step:949/1770 train_time:91054ms step_avg:96.97ms
step:950/1770 train_time:91155ms step_avg:96.97ms
step:951/1770 train_time:91257ms step_avg:96.98ms
step:952/1770 train_time:91357ms step_avg:96.98ms
step:953/1770 train_time:91458ms step_avg:96.99ms
step:954/1770 train_time:91558ms step_avg:96.99ms
step:955/1770 train_time:91658ms step_avg:96.99ms
step:956/1770 train_time:91759ms step_avg:97.00ms
step:957/1770 train_time:91859ms step_avg:97.00ms
step:958/1770 train_time:91960ms step_avg:97.00ms
step:959/1770 train_time:92061ms step_avg:97.01ms
step:960/1770 train_time:92162ms step_avg:97.01ms
step:961/1770 train_time:92263ms step_avg:97.02ms
step:962/1770 train_time:92365ms step_avg:97.02ms
step:963/1770 train_time:92466ms step_avg:97.03ms
step:964/1770 train_time:92567ms step_avg:97.03ms
step:965/1770 train_time:92668ms step_avg:97.03ms
step:966/1770 train_time:92769ms step_avg:97.04ms
step:967/1770 train_time:92870ms step_avg:97.04ms
step:968/1770 train_time:92971ms step_avg:97.05ms
step:969/1770 train_time:93073ms step_avg:97.05ms
step:970/1770 train_time:93174ms step_avg:97.06ms
step:971/1770 train_time:93277ms step_avg:97.06ms
step:972/1770 train_time:93378ms step_avg:97.07ms
step:973/1770 train_time:93478ms step_avg:97.07ms
step:974/1770 train_time:93578ms step_avg:97.07ms
step:975/1770 train_time:93679ms step_avg:97.08ms
step:976/1770 train_time:93779ms step_avg:97.08ms
step:977/1770 train_time:93879ms step_avg:97.08ms
step:978/1770 train_time:93978ms step_avg:97.09ms
step:979/1770 train_time:94079ms step_avg:97.09ms
step:980/1770 train_time:94180ms step_avg:97.09ms
step:981/1770 train_time:94281ms step_avg:97.10ms
step:982/1770 train_time:94382ms step_avg:97.10ms
step:983/1770 train_time:94482ms step_avg:97.10ms
step:984/1770 train_time:94583ms step_avg:97.11ms
step:985/1770 train_time:94683ms step_avg:97.11ms
step:986/1770 train_time:94785ms step_avg:97.12ms
step:987/1770 train_time:94886ms step_avg:97.12ms
step:988/1770 train_time:94987ms step_avg:97.12ms
step:989/1770 train_time:95091ms step_avg:97.13ms
step:990/1770 train_time:95191ms step_avg:97.13ms
step:991/1770 train_time:95292ms step_avg:97.14ms
step:992/1770 train_time:95393ms step_avg:97.14ms
step:993/1770 train_time:95494ms step_avg:97.14ms
step:994/1770 train_time:95594ms step_avg:97.15ms
step:995/1770 train_time:95695ms step_avg:97.15ms
step:996/1770 train_time:95796ms step_avg:97.16ms
step:997/1770 train_time:95897ms step_avg:97.16ms
step:998/1770 train_time:95998ms step_avg:97.16ms
step:999/1770 train_time:96098ms step_avg:97.17ms
step:1000/1770 train_time:96199ms step_avg:97.17ms
step:1000/1770 val_loss:3.5136 train_time:96298ms step_avg:97.27ms
step:1001/1770 train_time:96319ms step_avg:97.19ms
step:1002/1770 train_time:96407ms step_avg:97.18ms
step:1003/1770 train_time:96509ms step_avg:97.19ms
step:1004/1770 train_time:96610ms step_avg:97.19ms
step:1005/1770 train_time:96710ms step_avg:97.20ms
step:1006/1770 train_time:96810ms step_avg:97.20ms
step:1007/1770 train_time:96910ms step_avg:97.20ms
step:1008/1770 train_time:97010ms step_avg:97.20ms
step:1009/1770 train_time:97111ms step_avg:97.21ms
step:1010/1770 train_time:97212ms step_avg:97.21ms
step:1011/1770 train_time:97316ms step_avg:97.22ms
step:1012/1770 train_time:97418ms step_avg:97.22ms
step:1013/1770 train_time:97518ms step_avg:97.23ms
step:1014/1770 train_time:97619ms step_avg:97.23ms
step:1015/1770 train_time:97719ms step_avg:97.23ms
step:1016/1770 train_time:97819ms step_avg:97.24ms
step:1017/1770 train_time:97919ms step_avg:97.24ms
step:1018/1770 train_time:98019ms step_avg:97.24ms
step:1019/1770 train_time:98119ms step_avg:97.24ms
step:1020/1770 train_time:98220ms step_avg:97.25ms
step:1021/1770 train_time:98321ms step_avg:97.25ms
step:1022/1770 train_time:98422ms step_avg:97.26ms
step:1023/1770 train_time:98523ms step_avg:97.26ms
step:1024/1770 train_time:98624ms step_avg:97.26ms
step:1025/1770 train_time:98724ms step_avg:97.27ms
step:1026/1770 train_time:98826ms step_avg:97.27ms
step:1027/1770 train_time:98927ms step_avg:97.27ms
step:1028/1770 train_time:99028ms step_avg:97.28ms
step:1029/1770 train_time:99128ms step_avg:97.28ms
step:1030/1770 train_time:99229ms step_avg:97.28ms
step:1031/1770 train_time:99330ms step_avg:97.29ms
step:1032/1770 train_time:99430ms step_avg:97.29ms
step:1033/1770 train_time:99531ms step_avg:97.29ms
step:1034/1770 train_time:99632ms step_avg:97.30ms
step:1035/1770 train_time:99733ms step_avg:97.30ms
step:1036/1770 train_time:99834ms step_avg:97.30ms
step:1037/1770 train_time:99934ms step_avg:97.31ms
step:1038/1770 train_time:100035ms step_avg:97.31ms
step:1039/1770 train_time:100136ms step_avg:97.31ms
step:1040/1770 train_time:100236ms step_avg:97.32ms
step:1041/1770 train_time:100336ms step_avg:97.32ms
step:1042/1770 train_time:100437ms step_avg:97.32ms
step:1043/1770 train_time:100537ms step_avg:97.33ms
step:1044/1770 train_time:100638ms step_avg:97.33ms
step:1045/1770 train_time:100738ms step_avg:97.33ms
step:1046/1770 train_time:100838ms step_avg:97.33ms
step:1047/1770 train_time:100939ms step_avg:97.34ms
step:1048/1770 train_time:101040ms step_avg:97.34ms
step:1049/1770 train_time:101142ms step_avg:97.35ms
step:1050/1770 train_time:101243ms step_avg:97.35ms
step:1051/1770 train_time:101345ms step_avg:97.35ms
step:1052/1770 train_time:101446ms step_avg:97.36ms
step:1053/1770 train_time:101547ms step_avg:97.36ms
step:1054/1770 train_time:101648ms step_avg:97.36ms
step:1055/1770 train_time:101749ms step_avg:97.37ms
step:1056/1770 train_time:101850ms step_avg:97.37ms
step:1057/1770 train_time:101951ms step_avg:97.37ms
step:1058/1770 train_time:102052ms step_avg:97.38ms
step:1059/1770 train_time:102153ms step_avg:97.38ms
step:1060/1770 train_time:102256ms step_avg:97.39ms
step:1061/1770 train_time:102358ms step_avg:97.39ms
step:1062/1770 train_time:102459ms step_avg:97.39ms
step:1063/1770 train_time:102560ms step_avg:97.40ms
step:1064/1770 train_time:102661ms step_avg:97.40ms
step:1065/1770 train_time:102762ms step_avg:97.40ms
step:1066/1770 train_time:102863ms step_avg:97.41ms
step:1067/1770 train_time:102965ms step_avg:97.41ms
step:1068/1770 train_time:103066ms step_avg:97.42ms
step:1069/1770 train_time:103168ms step_avg:97.42ms
step:1070/1770 train_time:103271ms step_avg:97.43ms
step:1071/1770 train_time:103371ms step_avg:97.43ms
step:1072/1770 train_time:103472ms step_avg:97.43ms
step:1073/1770 train_time:103574ms step_avg:97.44ms
step:1074/1770 train_time:103675ms step_avg:97.44ms
step:1075/1770 train_time:103776ms step_avg:97.44ms
step:1076/1770 train_time:103877ms step_avg:97.45ms
step:1077/1770 train_time:103978ms step_avg:97.45ms
step:1078/1770 train_time:104079ms step_avg:97.45ms
step:1079/1770 train_time:104180ms step_avg:97.46ms
step:1080/1770 train_time:104280ms step_avg:97.46ms
step:1081/1770 train_time:104380ms step_avg:97.46ms
step:1082/1770 train_time:104482ms step_avg:97.46ms
step:1083/1770 train_time:104582ms step_avg:97.47ms
step:1084/1770 train_time:104683ms step_avg:97.47ms
step:1085/1770 train_time:104785ms step_avg:97.47ms
step:1086/1770 train_time:104887ms step_avg:97.48ms
step:1087/1770 train_time:104988ms step_avg:97.48ms
step:1088/1770 train_time:105089ms step_avg:97.49ms
step:1089/1770 train_time:105190ms step_avg:97.49ms
step:1090/1770 train_time:105291ms step_avg:97.49ms
step:1091/1770 train_time:105392ms step_avg:97.50ms
step:1092/1770 train_time:105494ms step_avg:97.50ms
step:1093/1770 train_time:105595ms step_avg:97.50ms
step:1094/1770 train_time:105697ms step_avg:97.51ms
step:1095/1770 train_time:105797ms step_avg:97.51ms
step:1096/1770 train_time:105897ms step_avg:97.51ms
step:1097/1770 train_time:105998ms step_avg:97.51ms
step:1098/1770 train_time:106099ms step_avg:97.52ms
step:1099/1770 train_time:106200ms step_avg:97.52ms
step:1100/1770 train_time:106301ms step_avg:97.52ms
step:1101/1770 train_time:106402ms step_avg:97.53ms
step:1102/1770 train_time:106503ms step_avg:97.53ms
step:1103/1770 train_time:106604ms step_avg:97.53ms
step:1104/1770 train_time:106706ms step_avg:97.54ms
step:1105/1770 train_time:106807ms step_avg:97.54ms
step:1106/1770 train_time:106909ms step_avg:97.54ms
step:1107/1770 train_time:107010ms step_avg:97.55ms
step:1108/1770 train_time:107110ms step_avg:97.55ms
step:1109/1770 train_time:107212ms step_avg:97.55ms
step:1110/1770 train_time:107314ms step_avg:97.56ms
step:1111/1770 train_time:107415ms step_avg:97.56ms
step:1112/1770 train_time:107516ms step_avg:97.56ms
step:1113/1770 train_time:107617ms step_avg:97.57ms
step:1114/1770 train_time:107717ms step_avg:97.57ms
step:1115/1770 train_time:107818ms step_avg:97.57ms
step:1116/1770 train_time:107918ms step_avg:97.58ms
step:1117/1770 train_time:108019ms step_avg:97.58ms
step:1118/1770 train_time:108119ms step_avg:97.58ms
step:1119/1770 train_time:108220ms step_avg:97.58ms
step:1120/1770 train_time:108320ms step_avg:97.59ms
step:1121/1770 train_time:108421ms step_avg:97.59ms
step:1122/1770 train_time:108522ms step_avg:97.59ms
step:1123/1770 train_time:108623ms step_avg:97.60ms
step:1124/1770 train_time:108725ms step_avg:97.60ms
step:1125/1770 train_time:108827ms step_avg:97.60ms
step:1125/1770 val_loss:3.4734 train_time:108926ms step_avg:97.69ms
step:1126/1770 train_time:108948ms step_avg:97.62ms
step:1127/1770 train_time:109041ms step_avg:97.62ms
step:1128/1770 train_time:109143ms step_avg:97.62ms
step:1129/1770 train_time:109243ms step_avg:97.63ms
step:1130/1770 train_time:109344ms step_avg:97.63ms
step:1131/1770 train_time:109444ms step_avg:97.63ms
step:1132/1770 train_time:109545ms step_avg:97.63ms
step:1133/1770 train_time:109645ms step_avg:97.64ms
step:1134/1770 train_time:109746ms step_avg:97.64ms
step:1135/1770 train_time:109846ms step_avg:97.64ms
step:1136/1770 train_time:109948ms step_avg:97.64ms
step:1137/1770 train_time:110051ms step_avg:97.65ms
step:1138/1770 train_time:110153ms step_avg:97.65ms
step:1139/1770 train_time:110254ms step_avg:97.66ms
step:1140/1770 train_time:110354ms step_avg:97.66ms
step:1141/1770 train_time:110454ms step_avg:97.66ms
step:1142/1770 train_time:110555ms step_avg:97.66ms
step:1143/1770 train_time:110656ms step_avg:97.67ms
step:1144/1770 train_time:110757ms step_avg:97.67ms
step:1145/1770 train_time:110858ms step_avg:97.67ms
step:1146/1770 train_time:110960ms step_avg:97.68ms
step:1147/1770 train_time:111061ms step_avg:97.68ms
step:1148/1770 train_time:111162ms step_avg:97.68ms
step:1149/1770 train_time:111262ms step_avg:97.68ms
step:1150/1770 train_time:111363ms step_avg:97.69ms
step:1151/1770 train_time:111464ms step_avg:97.69ms
step:1152/1770 train_time:111565ms step_avg:97.69ms
step:1153/1770 train_time:111666ms step_avg:97.70ms
step:1154/1770 train_time:111767ms step_avg:97.70ms
step:1155/1770 train_time:111868ms step_avg:97.70ms
step:1156/1770 train_time:111970ms step_avg:97.71ms
step:1157/1770 train_time:112074ms step_avg:97.71ms
step:1158/1770 train_time:112175ms step_avg:97.71ms
step:1159/1770 train_time:112275ms step_avg:97.72ms
step:1160/1770 train_time:112376ms step_avg:97.72ms
step:1161/1770 train_time:112477ms step_avg:97.72ms
step:1162/1770 train_time:112579ms step_avg:97.72ms
step:1163/1770 train_time:112680ms step_avg:97.73ms
step:1164/1770 train_time:112781ms step_avg:97.73ms
step:1165/1770 train_time:112883ms step_avg:97.73ms
step:1166/1770 train_time:112983ms step_avg:97.74ms
step:1167/1770 train_time:113084ms step_avg:97.74ms
step:1168/1770 train_time:113186ms step_avg:97.74ms
step:1169/1770 train_time:113287ms step_avg:97.75ms
step:1170/1770 train_time:113388ms step_avg:97.75ms
step:1171/1770 train_time:113490ms step_avg:97.75ms
step:1172/1770 train_time:113591ms step_avg:97.75ms
step:1173/1770 train_time:113693ms step_avg:97.76ms
step:1174/1770 train_time:113794ms step_avg:97.76ms
step:1175/1770 train_time:113895ms step_avg:97.76ms
step:1176/1770 train_time:113997ms step_avg:97.77ms
step:1177/1770 train_time:114099ms step_avg:97.77ms
step:1178/1770 train_time:114200ms step_avg:97.77ms
step:1179/1770 train_time:114302ms step_avg:97.78ms
step:1180/1770 train_time:114403ms step_avg:97.78ms
step:1181/1770 train_time:114504ms step_avg:97.78ms
step:1182/1770 train_time:114605ms step_avg:97.79ms
step:1183/1770 train_time:114707ms step_avg:97.79ms
step:1184/1770 train_time:114810ms step_avg:97.79ms
step:1185/1770 train_time:114912ms step_avg:97.80ms
step:1186/1770 train_time:115015ms step_avg:97.80ms
step:1187/1770 train_time:115119ms step_avg:97.81ms
step:1188/1770 train_time:115221ms step_avg:97.81ms
step:1189/1770 train_time:115323ms step_avg:97.81ms
step:1190/1770 train_time:115424ms step_avg:97.82ms
step:1191/1770 train_time:115527ms step_avg:97.82ms
step:1192/1770 train_time:115629ms step_avg:97.82ms
step:1193/1770 train_time:115731ms step_avg:97.83ms
step:1194/1770 train_time:115833ms step_avg:97.83ms
step:1195/1770 train_time:115937ms step_avg:97.84ms
step:1196/1770 train_time:116040ms step_avg:97.84ms
step:1197/1770 train_time:116142ms step_avg:97.85ms
step:1198/1770 train_time:116244ms step_avg:97.85ms
step:1199/1770 train_time:116346ms step_avg:97.85ms
step:1200/1770 train_time:116448ms step_avg:97.86ms
step:1201/1770 train_time:116550ms step_avg:97.86ms
step:1202/1770 train_time:116652ms step_avg:97.86ms
step:1203/1770 train_time:116754ms step_avg:97.87ms
step:1204/1770 train_time:116857ms step_avg:97.87ms
step:1205/1770 train_time:116958ms step_avg:97.87ms
step:1206/1770 train_time:117061ms step_avg:97.88ms
step:1207/1770 train_time:117163ms step_avg:97.88ms
step:1208/1770 train_time:117265ms step_avg:97.88ms
step:1209/1770 train_time:117366ms step_avg:97.89ms
step:1210/1770 train_time:117467ms step_avg:97.89ms
step:1211/1770 train_time:117570ms step_avg:97.89ms
step:1212/1770 train_time:117674ms step_avg:97.90ms
step:1213/1770 train_time:117776ms step_avg:97.90ms
step:1214/1770 train_time:117877ms step_avg:97.90ms
step:1215/1770 train_time:117979ms step_avg:97.91ms
step:1216/1770 train_time:118084ms step_avg:97.91ms
step:1217/1770 train_time:118186ms step_avg:97.92ms
step:1218/1770 train_time:118287ms step_avg:97.92ms
step:1219/1770 train_time:118390ms step_avg:97.92ms
step:1220/1770 train_time:118492ms step_avg:97.93ms
step:1221/1770 train_time:118594ms step_avg:97.93ms
step:1222/1770 train_time:118699ms step_avg:97.94ms
step:1223/1770 train_time:118801ms step_avg:97.94ms
step:1224/1770 train_time:118904ms step_avg:97.94ms
step:1225/1770 train_time:119006ms step_avg:97.95ms
step:1226/1770 train_time:119109ms step_avg:97.95ms
step:1227/1770 train_time:119213ms step_avg:97.96ms
step:1228/1770 train_time:119316ms step_avg:97.96ms
step:1229/1770 train_time:119417ms step_avg:97.96ms
step:1230/1770 train_time:119519ms step_avg:97.97ms
step:1231/1770 train_time:119621ms step_avg:97.97ms
step:1232/1770 train_time:119723ms step_avg:97.97ms
step:1233/1770 train_time:119824ms step_avg:97.98ms
step:1234/1770 train_time:119927ms step_avg:97.98ms
step:1235/1770 train_time:120029ms step_avg:97.98ms
step:1236/1770 train_time:120132ms step_avg:97.99ms
step:1237/1770 train_time:120235ms step_avg:97.99ms
step:1238/1770 train_time:120338ms step_avg:97.99ms
step:1239/1770 train_time:120440ms step_avg:98.00ms
step:1240/1770 train_time:120542ms step_avg:98.00ms
step:1241/1770 train_time:120644ms step_avg:98.00ms
step:1242/1770 train_time:120745ms step_avg:98.01ms
step:1243/1770 train_time:120847ms step_avg:98.01ms
step:1244/1770 train_time:120949ms step_avg:98.01ms
step:1245/1770 train_time:121051ms step_avg:98.02ms
step:1246/1770 train_time:121154ms step_avg:98.02ms
step:1247/1770 train_time:121256ms step_avg:98.02ms
step:1248/1770 train_time:121359ms step_avg:98.03ms
step:1249/1770 train_time:121461ms step_avg:98.03ms
step:1250/1770 train_time:121563ms step_avg:98.03ms
step:1250/1770 val_loss:3.4252 train_time:121665ms step_avg:98.12ms
step:1251/1770 train_time:121686ms step_avg:98.06ms
step:1252/1770 train_time:121777ms step_avg:98.05ms
step:1253/1770 train_time:121879ms step_avg:98.05ms
step:1254/1770 train_time:121982ms step_avg:98.06ms
step:1255/1770 train_time:122086ms step_avg:98.06ms
step:1256/1770 train_time:122188ms step_avg:98.06ms
step:1257/1770 train_time:122289ms step_avg:98.07ms
step:1258/1770 train_time:122391ms step_avg:98.07ms
step:1259/1770 train_time:122493ms step_avg:98.07ms
step:1260/1770 train_time:122594ms step_avg:98.08ms
step:1261/1770 train_time:122697ms step_avg:98.08ms
step:1262/1770 train_time:122800ms step_avg:98.08ms
step:1263/1770 train_time:122902ms step_avg:98.09ms
step:1264/1770 train_time:123007ms step_avg:98.09ms
step:1265/1770 train_time:123108ms step_avg:98.09ms
step:1266/1770 train_time:123210ms step_avg:98.10ms
step:1267/1770 train_time:123312ms step_avg:98.10ms
step:1268/1770 train_time:123415ms step_avg:98.10ms
step:1269/1770 train_time:123517ms step_avg:98.11ms
step:1270/1770 train_time:123618ms step_avg:98.11ms
step:1271/1770 train_time:123720ms step_avg:98.11ms
step:1272/1770 train_time:123822ms step_avg:98.12ms
step:1273/1770 train_time:123925ms step_avg:98.12ms
step:1274/1770 train_time:124028ms step_avg:98.12ms
step:1275/1770 train_time:124129ms step_avg:98.13ms
step:1276/1770 train_time:124232ms step_avg:98.13ms
step:1277/1770 train_time:124334ms step_avg:98.13ms
step:1278/1770 train_time:124436ms step_avg:98.14ms
step:1279/1770 train_time:124538ms step_avg:98.14ms
step:1280/1770 train_time:124642ms step_avg:98.14ms
step:1281/1770 train_time:124743ms step_avg:98.15ms
step:1282/1770 train_time:124846ms step_avg:98.15ms
step:1283/1770 train_time:124949ms step_avg:98.15ms
step:1284/1770 train_time:125053ms step_avg:98.16ms
step:1285/1770 train_time:125155ms step_avg:98.16ms
step:1286/1770 train_time:125258ms step_avg:98.16ms
step:1287/1770 train_time:125362ms step_avg:98.17ms
step:1288/1770 train_time:125465ms step_avg:98.17ms
step:1289/1770 train_time:125567ms step_avg:98.18ms
step:1290/1770 train_time:125669ms step_avg:98.18ms
step:1291/1770 train_time:125772ms step_avg:98.18ms
step:1292/1770 train_time:125873ms step_avg:98.19ms
step:1293/1770 train_time:125976ms step_avg:98.19ms
step:1294/1770 train_time:126077ms step_avg:98.19ms
step:1295/1770 train_time:126179ms step_avg:98.19ms
step:1296/1770 train_time:126281ms step_avg:98.20ms
step:1297/1770 train_time:126384ms step_avg:98.20ms
step:1298/1770 train_time:126486ms step_avg:98.20ms
step:1299/1770 train_time:126589ms step_avg:98.21ms
step:1300/1770 train_time:126692ms step_avg:98.21ms
step:1301/1770 train_time:126794ms step_avg:98.21ms
step:1302/1770 train_time:126896ms step_avg:98.22ms
step:1303/1770 train_time:126998ms step_avg:98.22ms
step:1304/1770 train_time:127100ms step_avg:98.22ms
step:1305/1770 train_time:127202ms step_avg:98.23ms
step:1306/1770 train_time:127304ms step_avg:98.23ms
step:1307/1770 train_time:127406ms step_avg:98.23ms
step:1308/1770 train_time:127509ms step_avg:98.24ms
step:1309/1770 train_time:127611ms step_avg:98.24ms
step:1310/1770 train_time:127713ms step_avg:98.24ms
step:1311/1770 train_time:127814ms step_avg:98.24ms
step:1312/1770 train_time:127916ms step_avg:98.25ms
step:1313/1770 train_time:128017ms step_avg:98.25ms
step:1314/1770 train_time:128119ms step_avg:98.25ms
step:1315/1770 train_time:128220ms step_avg:98.25ms
step:1316/1770 train_time:128323ms step_avg:98.26ms
step:1317/1770 train_time:128425ms step_avg:98.26ms
step:1318/1770 train_time:128531ms step_avg:98.27ms
step:1319/1770 train_time:128634ms step_avg:98.27ms
step:1320/1770 train_time:128738ms step_avg:98.27ms
step:1321/1770 train_time:128841ms step_avg:98.28ms
step:1322/1770 train_time:128943ms step_avg:98.28ms
step:1323/1770 train_time:129046ms step_avg:98.28ms
step:1324/1770 train_time:129149ms step_avg:98.29ms
step:1325/1770 train_time:129251ms step_avg:98.29ms
step:1326/1770 train_time:129354ms step_avg:98.29ms
step:1327/1770 train_time:129459ms step_avg:98.30ms
step:1328/1770 train_time:129561ms step_avg:98.30ms
step:1329/1770 train_time:129664ms step_avg:98.30ms
step:1330/1770 train_time:129766ms step_avg:98.31ms
step:1331/1770 train_time:129868ms step_avg:98.31ms
step:1332/1770 train_time:129971ms step_avg:98.31ms
step:1333/1770 train_time:130072ms step_avg:98.32ms
step:1334/1770 train_time:130174ms step_avg:98.32ms
step:1335/1770 train_time:130276ms step_avg:98.32ms
step:1336/1770 train_time:130377ms step_avg:98.32ms
step:1337/1770 train_time:130479ms step_avg:98.33ms
step:1338/1770 train_time:130581ms step_avg:98.33ms
step:1339/1770 train_time:130684ms step_avg:98.33ms
step:1340/1770 train_time:130788ms step_avg:98.34ms
step:1341/1770 train_time:130889ms step_avg:98.34ms
step:1342/1770 train_time:130992ms step_avg:98.34ms
step:1343/1770 train_time:131095ms step_avg:98.35ms
step:1344/1770 train_time:131197ms step_avg:98.35ms
step:1345/1770 train_time:131299ms step_avg:98.35ms
step:1346/1770 train_time:131401ms step_avg:98.35ms
step:1347/1770 train_time:131503ms step_avg:98.36ms
step:1348/1770 train_time:131608ms step_avg:98.36ms
step:1349/1770 train_time:131710ms step_avg:98.36ms
step:1350/1770 train_time:131812ms step_avg:98.37ms
step:1351/1770 train_time:131915ms step_avg:98.37ms
step:1352/1770 train_time:132017ms step_avg:98.37ms
step:1353/1770 train_time:132121ms step_avg:98.38ms
step:1354/1770 train_time:132223ms step_avg:98.38ms
step:1355/1770 train_time:132325ms step_avg:98.38ms
step:1356/1770 train_time:132427ms step_avg:98.39ms
step:1357/1770 train_time:132529ms step_avg:98.39ms
step:1358/1770 train_time:132632ms step_avg:98.39ms
step:1359/1770 train_time:132734ms step_avg:98.39ms
step:1360/1770 train_time:132837ms step_avg:98.40ms
step:1361/1770 train_time:132939ms step_avg:98.40ms
step:1362/1770 train_time:133042ms step_avg:98.40ms
step:1363/1770 train_time:133145ms step_avg:98.41ms
step:1364/1770 train_time:133247ms step_avg:98.41ms
step:1365/1770 train_time:133349ms step_avg:98.41ms
step:1366/1770 train_time:133451ms step_avg:98.42ms
step:1367/1770 train_time:133554ms step_avg:98.42ms
step:1368/1770 train_time:133655ms step_avg:98.42ms
step:1369/1770 train_time:133757ms step_avg:98.42ms
step:1370/1770 train_time:133860ms step_avg:98.43ms
step:1371/1770 train_time:133963ms step_avg:98.43ms
step:1372/1770 train_time:134064ms step_avg:98.43ms
step:1373/1770 train_time:134167ms step_avg:98.44ms
step:1374/1770 train_time:134270ms step_avg:98.44ms
step:1375/1770 train_time:134373ms step_avg:98.44ms
step:1375/1770 val_loss:3.3822 train_time:134475ms step_avg:98.52ms
step:1376/1770 train_time:134496ms step_avg:98.46ms
step:1377/1770 train_time:134587ms step_avg:98.45ms
step:1378/1770 train_time:134690ms step_avg:98.46ms
step:1379/1770 train_time:134792ms step_avg:98.46ms
step:1380/1770 train_time:134894ms step_avg:98.46ms
step:1381/1770 train_time:134996ms step_avg:98.47ms
step:1382/1770 train_time:135097ms step_avg:98.47ms
step:1383/1770 train_time:135199ms step_avg:98.47ms
step:1384/1770 train_time:135301ms step_avg:98.47ms
step:1385/1770 train_time:135403ms step_avg:98.48ms
step:1386/1770 train_time:135506ms step_avg:98.48ms
step:1387/1770 train_time:135610ms step_avg:98.48ms
step:1388/1770 train_time:135712ms step_avg:98.48ms
step:1389/1770 train_time:135815ms step_avg:98.49ms
step:1390/1770 train_time:135917ms step_avg:98.49ms
step:1391/1770 train_time:136019ms step_avg:98.49ms
step:1392/1770 train_time:136121ms step_avg:98.50ms
step:1393/1770 train_time:136222ms step_avg:98.50ms
step:1394/1770 train_time:136324ms step_avg:98.50ms
step:1395/1770 train_time:136428ms step_avg:98.50ms
step:1396/1770 train_time:136531ms step_avg:98.51ms
step:1397/1770 train_time:136634ms step_avg:98.51ms
step:1398/1770 train_time:136736ms step_avg:98.51ms
step:1399/1770 train_time:136837ms step_avg:98.52ms
step:1400/1770 train_time:136940ms step_avg:98.52ms
step:1401/1770 train_time:137042ms step_avg:98.52ms
step:1402/1770 train_time:137145ms step_avg:98.52ms
step:1403/1770 train_time:137247ms step_avg:98.53ms
step:1404/1770 train_time:137350ms step_avg:98.53ms
step:1405/1770 train_time:137451ms step_avg:98.53ms
step:1406/1770 train_time:137554ms step_avg:98.53ms
step:1407/1770 train_time:137656ms step_avg:98.54ms
step:1408/1770 train_time:137758ms step_avg:98.54ms
step:1409/1770 train_time:137861ms step_avg:98.54ms
step:1410/1770 train_time:137963ms step_avg:98.54ms
step:1411/1770 train_time:138065ms step_avg:98.55ms
step:1412/1770 train_time:138166ms step_avg:98.55ms
step:1413/1770 train_time:138267ms step_avg:98.55ms
step:1414/1770 train_time:138371ms step_avg:98.55ms
step:1415/1770 train_time:138475ms step_avg:98.56ms
step:1416/1770 train_time:138579ms step_avg:98.56ms
step:1417/1770 train_time:138681ms step_avg:98.57ms
step:1418/1770 train_time:138783ms step_avg:98.57ms
step:1419/1770 train_time:138886ms step_avg:98.57ms
step:1420/1770 train_time:138987ms step_avg:98.57ms
step:1421/1770 train_time:139089ms step_avg:98.58ms
step:1422/1770 train_time:139191ms step_avg:98.58ms
step:1423/1770 train_time:139293ms step_avg:98.58ms
step:1424/1770 train_time:139396ms step_avg:98.58ms
step:1425/1770 train_time:139498ms step_avg:98.58ms
step:1426/1770 train_time:139600ms step_avg:98.59ms
step:1427/1770 train_time:139702ms step_avg:98.59ms
step:1428/1770 train_time:139806ms step_avg:98.59ms
step:1429/1770 train_time:139908ms step_avg:98.60ms
step:1430/1770 train_time:140010ms step_avg:98.60ms
step:1431/1770 train_time:140113ms step_avg:98.60ms
step:1432/1770 train_time:140214ms step_avg:98.60ms
step:1433/1770 train_time:140317ms step_avg:98.61ms
step:1434/1770 train_time:140418ms step_avg:98.61ms
step:1435/1770 train_time:140521ms step_avg:98.61ms
step:1436/1770 train_time:140625ms step_avg:98.62ms
step:1437/1770 train_time:140728ms step_avg:98.62ms
step:1438/1770 train_time:140830ms step_avg:98.62ms
step:1439/1770 train_time:140933ms step_avg:98.62ms
step:1440/1770 train_time:141035ms step_avg:98.63ms
step:1441/1770 train_time:141139ms step_avg:98.63ms
step:1442/1770 train_time:141241ms step_avg:98.63ms
step:1443/1770 train_time:141342ms step_avg:98.63ms
step:1444/1770 train_time:141445ms step_avg:98.64ms
step:1445/1770 train_time:141548ms step_avg:98.64ms
step:1446/1770 train_time:141652ms step_avg:98.64ms
step:1447/1770 train_time:141756ms step_avg:98.65ms
step:1448/1770 train_time:141859ms step_avg:98.65ms
step:1449/1770 train_time:141964ms step_avg:98.65ms
step:1450/1770 train_time:142067ms step_avg:98.66ms
step:1451/1770 train_time:142172ms step_avg:98.66ms
step:1452/1770 train_time:142276ms step_avg:98.67ms
step:1453/1770 train_time:142379ms step_avg:98.67ms
step:1454/1770 train_time:142482ms step_avg:98.67ms
step:1455/1770 train_time:142586ms step_avg:98.68ms
step:1456/1770 train_time:142690ms step_avg:98.68ms
step:1457/1770 train_time:142793ms step_avg:98.68ms
step:1458/1770 train_time:142898ms step_avg:98.69ms
step:1459/1770 train_time:143002ms step_avg:98.69ms
step:1460/1770 train_time:143105ms step_avg:98.69ms
step:1461/1770 train_time:143208ms step_avg:98.70ms
step:1462/1770 train_time:143311ms step_avg:98.70ms
step:1463/1770 train_time:143416ms step_avg:98.70ms
step:1464/1770 train_time:143521ms step_avg:98.71ms
step:1465/1770 train_time:143624ms step_avg:98.71ms
step:1466/1770 train_time:143728ms step_avg:98.71ms
step:1467/1770 train_time:143833ms step_avg:98.72ms
step:1468/1770 train_time:143936ms step_avg:98.72ms
step:1469/1770 train_time:144039ms step_avg:98.72ms
step:1470/1770 train_time:144143ms step_avg:98.73ms
step:1471/1770 train_time:144247ms step_avg:98.73ms
step:1472/1770 train_time:144350ms step_avg:98.73ms
step:1473/1770 train_time:144455ms step_avg:98.74ms
step:1474/1770 train_time:144559ms step_avg:98.74ms
step:1475/1770 train_time:144663ms step_avg:98.75ms
step:1476/1770 train_time:144765ms step_avg:98.75ms
step:1477/1770 train_time:144870ms step_avg:98.75ms
step:1478/1770 train_time:144974ms step_avg:98.76ms
step:1479/1770 train_time:145077ms step_avg:98.76ms
step:1480/1770 train_time:145181ms step_avg:98.76ms
step:1481/1770 train_time:145289ms step_avg:98.77ms
step:1482/1770 train_time:145392ms step_avg:98.77ms
step:1483/1770 train_time:145495ms step_avg:98.77ms
step:1484/1770 train_time:145597ms step_avg:98.78ms
step:1485/1770 train_time:145701ms step_avg:98.78ms
step:1486/1770 train_time:145804ms step_avg:98.78ms
step:1487/1770 train_time:145907ms step_avg:98.79ms
step:1488/1770 train_time:146011ms step_avg:98.79ms
step:1489/1770 train_time:146116ms step_avg:98.79ms
step:1490/1770 train_time:146219ms step_avg:98.80ms
step:1491/1770 train_time:146322ms step_avg:98.80ms
step:1492/1770 train_time:146426ms step_avg:98.80ms
step:1493/1770 train_time:146532ms step_avg:98.81ms
step:1494/1770 train_time:146640ms step_avg:98.81ms
step:1495/1770 train_time:146742ms step_avg:98.82ms
step:1496/1770 train_time:146845ms step_avg:98.82ms
step:1497/1770 train_time:146948ms step_avg:98.82ms
step:1498/1770 train_time:147051ms step_avg:98.82ms
step:1499/1770 train_time:147154ms step_avg:98.83ms
step:1500/1770 train_time:147256ms step_avg:98.83ms
step:1500/1770 val_loss:3.3434 train_time:147358ms step_avg:98.90ms
step:1501/1770 train_time:147380ms step_avg:98.85ms
step:1502/1770 train_time:147472ms step_avg:98.84ms
step:1503/1770 train_time:147575ms step_avg:98.84ms
step:1504/1770 train_time:147679ms step_avg:98.85ms
step:1505/1770 train_time:147785ms step_avg:98.85ms
step:1506/1770 train_time:147888ms step_avg:98.86ms
step:1507/1770 train_time:147991ms step_avg:98.86ms
step:1508/1770 train_time:148096ms step_avg:98.86ms
step:1509/1770 train_time:148199ms step_avg:98.87ms
step:1510/1770 train_time:148302ms step_avg:98.87ms
step:1511/1770 train_time:148408ms step_avg:98.87ms
step:1512/1770 train_time:148512ms step_avg:98.88ms
step:1513/1770 train_time:148616ms step_avg:98.88ms
step:1514/1770 train_time:148720ms step_avg:98.88ms
step:1515/1770 train_time:148823ms step_avg:98.89ms
step:1516/1770 train_time:148927ms step_avg:98.89ms
step:1517/1770 train_time:149030ms step_avg:98.89ms
step:1518/1770 train_time:149135ms step_avg:98.90ms
step:1519/1770 train_time:149237ms step_avg:98.90ms
step:1520/1770 train_time:149341ms step_avg:98.90ms
step:1521/1770 train_time:149445ms step_avg:98.90ms
step:1522/1770 train_time:149549ms step_avg:98.91ms
step:1523/1770 train_time:149653ms step_avg:98.91ms
step:1524/1770 train_time:149756ms step_avg:98.91ms
step:1525/1770 train_time:149859ms step_avg:98.92ms
step:1526/1770 train_time:149963ms step_avg:98.92ms
step:1527/1770 train_time:150067ms step_avg:98.92ms
step:1528/1770 train_time:150172ms step_avg:98.93ms
step:1529/1770 train_time:150275ms step_avg:98.93ms
step:1530/1770 train_time:150378ms step_avg:98.93ms
step:1531/1770 train_time:150481ms step_avg:98.94ms
step:1532/1770 train_time:150586ms step_avg:98.94ms
step:1533/1770 train_time:150689ms step_avg:98.94ms
step:1534/1770 train_time:150794ms step_avg:98.95ms
step:1535/1770 train_time:150896ms step_avg:98.95ms
step:1536/1770 train_time:151001ms step_avg:98.95ms
step:1537/1770 train_time:151104ms step_avg:98.95ms
step:1538/1770 train_time:151208ms step_avg:98.96ms
step:1539/1770 train_time:151311ms step_avg:98.96ms
step:1540/1770 train_time:151419ms step_avg:98.97ms
step:1541/1770 train_time:151523ms step_avg:98.97ms
step:1542/1770 train_time:151627ms step_avg:98.97ms
step:1543/1770 train_time:151729ms step_avg:98.98ms
step:1544/1770 train_time:151834ms step_avg:98.98ms
step:1545/1770 train_time:151938ms step_avg:98.98ms
step:1546/1770 train_time:152041ms step_avg:98.99ms
step:1547/1770 train_time:152144ms step_avg:98.99ms
step:1548/1770 train_time:152247ms step_avg:98.99ms
step:1549/1770 train_time:152350ms step_avg:98.99ms
step:1550/1770 train_time:152454ms step_avg:99.00ms
step:1551/1770 train_time:152558ms step_avg:99.00ms
step:1552/1770 train_time:152663ms step_avg:99.00ms
step:1553/1770 train_time:152765ms step_avg:99.01ms
step:1554/1770 train_time:152868ms step_avg:99.01ms
step:1555/1770 train_time:152972ms step_avg:99.01ms
step:1556/1770 train_time:153075ms step_avg:99.01ms
step:1557/1770 train_time:153179ms step_avg:99.02ms
step:1558/1770 train_time:153283ms step_avg:99.02ms
step:1559/1770 train_time:153386ms step_avg:99.02ms
step:1560/1770 train_time:153489ms step_avg:99.03ms
step:1561/1770 train_time:153596ms step_avg:99.03ms
step:1562/1770 train_time:153699ms step_avg:99.03ms
step:1563/1770 train_time:153803ms step_avg:99.04ms
step:1564/1770 train_time:153905ms step_avg:99.04ms
step:1565/1770 train_time:154008ms step_avg:99.04ms
step:1566/1770 train_time:154110ms step_avg:99.04ms
step:1567/1770 train_time:154215ms step_avg:99.05ms
step:1568/1770 train_time:154319ms step_avg:99.05ms
step:1569/1770 train_time:154426ms step_avg:99.05ms
step:1570/1770 train_time:154529ms step_avg:99.06ms
step:1571/1770 train_time:154632ms step_avg:99.06ms
step:1572/1770 train_time:154736ms step_avg:99.06ms
step:1573/1770 train_time:154842ms step_avg:99.07ms
step:1574/1770 train_time:154945ms step_avg:99.07ms
step:1575/1770 train_time:155048ms step_avg:99.07ms
step:1576/1770 train_time:155151ms step_avg:99.07ms
step:1577/1770 train_time:155257ms step_avg:99.08ms
step:1578/1770 train_time:155362ms step_avg:99.08ms
step:1579/1770 train_time:155465ms step_avg:99.09ms
step:1580/1770 train_time:155568ms step_avg:99.09ms
step:1581/1770 train_time:155673ms step_avg:99.09ms
step:1582/1770 train_time:155779ms step_avg:99.10ms
step:1583/1770 train_time:155882ms step_avg:99.10ms
step:1584/1770 train_time:155987ms step_avg:99.10ms
step:1585/1770 train_time:156090ms step_avg:99.10ms
step:1586/1770 train_time:156197ms step_avg:99.11ms
step:1587/1770 train_time:156301ms step_avg:99.11ms
step:1588/1770 train_time:156404ms step_avg:99.12ms
step:1589/1770 train_time:156509ms step_avg:99.12ms
step:1590/1770 train_time:156612ms step_avg:99.12ms
step:1591/1770 train_time:156715ms step_avg:99.12ms
step:1592/1770 train_time:156819ms step_avg:99.13ms
step:1593/1770 train_time:156922ms step_avg:99.13ms
step:1594/1770 train_time:157026ms step_avg:99.13ms
step:1595/1770 train_time:157129ms step_avg:99.14ms
step:1596/1770 train_time:157234ms step_avg:99.14ms
step:1597/1770 train_time:157337ms step_avg:99.14ms
step:1598/1770 train_time:157440ms step_avg:99.14ms
step:1599/1770 train_time:157545ms step_avg:99.15ms
step:1600/1770 train_time:157650ms step_avg:99.15ms
step:1601/1770 train_time:157754ms step_avg:99.15ms
step:1602/1770 train_time:157858ms step_avg:99.16ms
step:1603/1770 train_time:157962ms step_avg:99.16ms
step:1604/1770 train_time:158065ms step_avg:99.16ms
step:1605/1770 train_time:158167ms step_avg:99.16ms
step:1606/1770 train_time:158271ms step_avg:99.17ms
step:1607/1770 train_time:158378ms step_avg:99.17ms
step:1608/1770 train_time:158481ms step_avg:99.17ms
step:1609/1770 train_time:158584ms step_avg:99.18ms
step:1610/1770 train_time:158688ms step_avg:99.18ms
step:1611/1770 train_time:158793ms step_avg:99.18ms
step:1612/1770 train_time:158898ms step_avg:99.19ms
step:1613/1770 train_time:159001ms step_avg:99.19ms
step:1614/1770 train_time:159105ms step_avg:99.19ms
step:1615/1770 train_time:159209ms step_avg:99.20ms
step:1616/1770 train_time:159311ms step_avg:99.20ms
step:1617/1770 train_time:159417ms step_avg:99.20ms
step:1618/1770 train_time:159521ms step_avg:99.20ms
step:1619/1770 train_time:159625ms step_avg:99.21ms
step:1620/1770 train_time:159730ms step_avg:99.21ms
step:1621/1770 train_time:159833ms step_avg:99.21ms
step:1622/1770 train_time:159938ms step_avg:99.22ms
step:1623/1770 train_time:160045ms step_avg:99.22ms
step:1624/1770 train_time:160148ms step_avg:99.22ms
step:1625/1770 train_time:160251ms step_avg:99.23ms
step:1625/1770 val_loss:3.3092 train_time:160353ms step_avg:99.29ms
step:1626/1770 train_time:160375ms step_avg:99.24ms
step:1627/1770 train_time:160465ms step_avg:99.24ms
step:1628/1770 train_time:160567ms step_avg:99.24ms
step:1629/1770 train_time:160669ms step_avg:99.24ms
step:1630/1770 train_time:160773ms step_avg:99.24ms
step:1631/1770 train_time:160875ms step_avg:99.24ms
step:1632/1770 train_time:160978ms step_avg:99.25ms
step:1633/1770 train_time:161081ms step_avg:99.25ms
step:1634/1770 train_time:161184ms step_avg:99.25ms
step:1635/1770 train_time:161287ms step_avg:99.25ms
step:1636/1770 train_time:161394ms step_avg:99.26ms
step:1637/1770 train_time:161499ms step_avg:99.26ms
step:1638/1770 train_time:161603ms step_avg:99.26ms
step:1639/1770 train_time:161706ms step_avg:99.27ms
step:1640/1770 train_time:161809ms step_avg:99.27ms
step:1641/1770 train_time:161913ms step_avg:99.27ms
step:1642/1770 train_time:162015ms step_avg:99.27ms
step:1643/1770 train_time:162118ms step_avg:99.28ms
step:1644/1770 train_time:162224ms step_avg:99.28ms
step:1645/1770 train_time:162326ms step_avg:99.28ms
step:1646/1770 train_time:162432ms step_avg:99.29ms
step:1647/1770 train_time:162537ms step_avg:99.29ms
step:1648/1770 train_time:162641ms step_avg:99.29ms
step:1649/1770 train_time:162744ms step_avg:99.29ms
step:1650/1770 train_time:162847ms step_avg:99.30ms
step:1651/1770 train_time:162949ms step_avg:99.30ms
step:1652/1770 train_time:163053ms step_avg:99.30ms
step:1653/1770 train_time:163156ms step_avg:99.30ms
step:1654/1770 train_time:163262ms step_avg:99.31ms
step:1655/1770 train_time:163368ms step_avg:99.31ms
step:1656/1770 train_time:163472ms step_avg:99.31ms
step:1657/1770 train_time:163577ms step_avg:99.32ms
step:1658/1770 train_time:163681ms step_avg:99.32ms
step:1659/1770 train_time:163786ms step_avg:99.32ms
step:1660/1770 train_time:163890ms step_avg:99.33ms
step:1661/1770 train_time:163994ms step_avg:99.33ms
step:1662/1770 train_time:164098ms step_avg:99.33ms
step:1663/1770 train_time:164201ms step_avg:99.34ms
step:1664/1770 train_time:164304ms step_avg:99.34ms
step:1665/1770 train_time:164407ms step_avg:99.34ms
step:1666/1770 train_time:164512ms step_avg:99.34ms
step:1667/1770 train_time:164615ms step_avg:99.35ms
step:1668/1770 train_time:164718ms step_avg:99.35ms
step:1669/1770 train_time:164820ms step_avg:99.35ms
step:1670/1770 train_time:164923ms step_avg:99.35ms
step:1671/1770 train_time:165027ms step_avg:99.35ms
step:1672/1770 train_time:165131ms step_avg:99.36ms
step:1673/1770 train_time:165237ms step_avg:99.36ms
step:1674/1770 train_time:165339ms step_avg:99.36ms
step:1675/1770 train_time:165443ms step_avg:99.37ms
step:1676/1770 train_time:165547ms step_avg:99.37ms
step:1677/1770 train_time:165654ms step_avg:99.37ms
step:1678/1770 train_time:165756ms step_avg:99.37ms
step:1679/1770 train_time:165860ms step_avg:99.38ms
step:1680/1770 train_time:165963ms step_avg:99.38ms
step:1681/1770 train_time:166067ms step_avg:99.38ms
step:1682/1770 train_time:166172ms step_avg:99.39ms
step:1683/1770 train_time:166275ms step_avg:99.39ms
step:1684/1770 train_time:166379ms step_avg:99.39ms
step:1685/1770 train_time:166483ms step_avg:99.39ms
step:1686/1770 train_time:166587ms step_avg:99.40ms
step:1687/1770 train_time:166692ms step_avg:99.40ms
step:1688/1770 train_time:166796ms step_avg:99.40ms
step:1689/1770 train_time:166899ms step_avg:99.40ms
step:1690/1770 train_time:167003ms step_avg:99.41ms
step:1691/1770 train_time:167106ms step_avg:99.41ms
step:1692/1770 train_time:167209ms step_avg:99.41ms
step:1693/1770 train_time:167314ms step_avg:99.41ms
step:1694/1770 train_time:167418ms step_avg:99.42ms
step:1695/1770 train_time:167523ms step_avg:99.42ms
step:1696/1770 train_time:167628ms step_avg:99.42ms
step:1697/1770 train_time:167733ms step_avg:99.43ms
step:1698/1770 train_time:167837ms step_avg:99.43ms
step:1699/1770 train_time:167940ms step_avg:99.43ms
step:1700/1770 train_time:168043ms step_avg:99.43ms
step:1701/1770 train_time:168146ms step_avg:99.44ms
step:1702/1770 train_time:168250ms step_avg:99.44ms
step:1703/1770 train_time:168353ms step_avg:99.44ms
step:1704/1770 train_time:168457ms step_avg:99.44ms
step:1705/1770 train_time:168560ms step_avg:99.45ms
step:1706/1770 train_time:168663ms step_avg:99.45ms
step:1707/1770 train_time:168768ms step_avg:99.45ms
step:1708/1770 train_time:168873ms step_avg:99.45ms
step:1709/1770 train_time:168978ms step_avg:99.46ms
step:1710/1770 train_time:169085ms step_avg:99.46ms
step:1711/1770 train_time:169191ms step_avg:99.47ms
step:1712/1770 train_time:169295ms step_avg:99.47ms
step:1713/1770 train_time:169399ms step_avg:99.47ms
step:1714/1770 train_time:169504ms step_avg:99.47ms
step:1715/1770 train_time:169607ms step_avg:99.48ms
step:1716/1770 train_time:169712ms step_avg:99.48ms
step:1717/1770 train_time:169815ms step_avg:99.48ms
step:1718/1770 train_time:169921ms step_avg:99.49ms
step:1719/1770 train_time:170026ms step_avg:99.49ms
step:1720/1770 train_time:170131ms step_avg:99.49ms
step:1721/1770 train_time:170235ms step_avg:99.49ms
step:1722/1770 train_time:170342ms step_avg:99.50ms
step:1723/1770 train_time:170447ms step_avg:99.50ms
step:1724/1770 train_time:170553ms step_avg:99.51ms
step:1725/1770 train_time:170659ms step_avg:99.51ms
step:1726/1770 train_time:170765ms step_avg:99.51ms
step:1727/1770 train_time:170869ms step_avg:99.52ms
step:1728/1770 train_time:170974ms step_avg:99.52ms
step:1729/1770 train_time:171078ms step_avg:99.52ms
step:1730/1770 train_time:171184ms step_avg:99.53ms
step:1731/1770 train_time:171290ms step_avg:99.53ms
step:1732/1770 train_time:171394ms step_avg:99.53ms
step:1733/1770 train_time:171500ms step_avg:99.54ms
step:1734/1770 train_time:171603ms step_avg:99.54ms
step:1735/1770 train_time:171708ms step_avg:99.54ms
step:1736/1770 train_time:171812ms step_avg:99.54ms
step:1737/1770 train_time:171918ms step_avg:99.55ms
step:1738/1770 train_time:172023ms step_avg:99.55ms
step:1739/1770 train_time:172126ms step_avg:99.55ms
step:1740/1770 train_time:172230ms step_avg:99.55ms
step:1741/1770 train_time:172336ms step_avg:99.56ms
step:1742/1770 train_time:172443ms step_avg:99.56ms
step:1743/1770 train_time:172548ms step_avg:99.57ms
step:1744/1770 train_time:172653ms step_avg:99.57ms
step:1745/1770 train_time:172756ms step_avg:99.57ms
step:1746/1770 train_time:172863ms step_avg:99.58ms
step:1747/1770 train_time:172966ms step_avg:99.58ms
step:1748/1770 train_time:173073ms step_avg:99.58ms
step:1749/1770 train_time:173178ms step_avg:99.59ms
step:1750/1770 train_time:173282ms step_avg:99.59ms
step:1750/1770 val_loss:3.2826 train_time:173385ms step_avg:99.65ms
step:1751/1770 train_time:173406ms step_avg:99.60ms
step:1752/1770 train_time:173501ms step_avg:99.60ms
step:1753/1770 train_time:173604ms step_avg:99.60ms
step:1754/1770 train_time:173709ms step_avg:99.60ms
step:1755/1770 train_time:173813ms step_avg:99.61ms
step:1756/1770 train_time:173917ms step_avg:99.61ms
step:1757/1770 train_time:174022ms step_avg:99.61ms
step:1758/1770 train_time:174126ms step_avg:99.61ms
step:1759/1770 train_time:174230ms step_avg:99.62ms
step:1760/1770 train_time:174335ms step_avg:99.62ms
step:1761/1770 train_time:174442ms step_avg:99.62ms
step:1762/1770 train_time:174550ms step_avg:99.63ms
step:1763/1770 train_time:174652ms step_avg:99.63ms
step:1764/1770 train_time:174757ms step_avg:99.63ms
step:1765/1770 train_time:174862ms step_avg:99.64ms
step:1766/1770 train_time:174970ms step_avg:99.64ms
step:1767/1770 train_time:175073ms step_avg:99.64ms
step:1768/1770 train_time:175177ms step_avg:99.65ms
step:1769/1770 train_time:175280ms step_avg:99.65ms
step:1770/1770 train_time:175384ms step_avg:99.65ms
step:1770/1770 val_loss:3.2797 train_time:175489ms step_avg:99.71ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
