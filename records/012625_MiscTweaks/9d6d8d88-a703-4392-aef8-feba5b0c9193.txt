import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sat Jan 25 22:51:07 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23201ms step_avg:nanms
step:2/1770 train_time:23760ms step_avg:nanms
step:3/1770 train_time:23855ms step_avg:nanms
step:4/1770 train_time:23948ms step_avg:nanms
step:5/1770 train_time:24042ms step_avg:nanms
step:6/1770 train_time:24135ms step_avg:nanms
step:7/1770 train_time:24229ms step_avg:nanms
step:8/1770 train_time:24323ms step_avg:nanms
step:9/1770 train_time:24416ms step_avg:nanms
step:10/1770 train_time:24510ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.25ms
step:14/1770 train_time:377ms step_avg:94.23ms
step:15/1770 train_time:471ms step_avg:94.18ms
step:16/1770 train_time:565ms step_avg:94.15ms
step:17/1770 train_time:664ms step_avg:94.86ms
step:18/1770 train_time:753ms step_avg:94.11ms
step:19/1770 train_time:847ms step_avg:94.07ms
step:20/1770 train_time:941ms step_avg:94.10ms
step:21/1770 train_time:1034ms step_avg:94.01ms
step:22/1770 train_time:1128ms step_avg:94.03ms
step:23/1770 train_time:1223ms step_avg:94.04ms
step:24/1770 train_time:1317ms step_avg:94.05ms
step:25/1770 train_time:1411ms step_avg:94.05ms
step:26/1770 train_time:1505ms step_avg:94.06ms
step:27/1770 train_time:1599ms step_avg:94.03ms
step:28/1770 train_time:1692ms step_avg:94.02ms
step:29/1770 train_time:1787ms step_avg:94.05ms
step:30/1770 train_time:1881ms step_avg:94.04ms
step:31/1770 train_time:1975ms step_avg:94.03ms
step:32/1770 train_time:2068ms step_avg:94.02ms
step:33/1770 train_time:2162ms step_avg:94.02ms
step:34/1770 train_time:2258ms step_avg:94.09ms
step:35/1770 train_time:2350ms step_avg:94.01ms
step:36/1770 train_time:2444ms step_avg:94.02ms
step:37/1770 train_time:2538ms step_avg:94.01ms
step:38/1770 train_time:2632ms step_avg:94.01ms
step:39/1770 train_time:2726ms step_avg:94.01ms
step:40/1770 train_time:2820ms step_avg:94.01ms
step:41/1770 train_time:2914ms step_avg:94.00ms
step:42/1770 train_time:3008ms step_avg:94.00ms
step:43/1770 train_time:3102ms step_avg:94.01ms
step:44/1770 train_time:3196ms step_avg:94.01ms
step:45/1770 train_time:3290ms step_avg:94.00ms
step:46/1770 train_time:3384ms step_avg:94.01ms
step:47/1770 train_time:3479ms step_avg:94.02ms
step:48/1770 train_time:3572ms step_avg:94.01ms
step:49/1770 train_time:3666ms step_avg:94.01ms
step:50/1770 train_time:3760ms step_avg:94.00ms
step:51/1770 train_time:3854ms step_avg:94.00ms
step:52/1770 train_time:3948ms step_avg:94.00ms
step:53/1770 train_time:4042ms step_avg:93.99ms
step:54/1770 train_time:4136ms step_avg:93.99ms
step:55/1770 train_time:4229ms step_avg:93.99ms
step:56/1770 train_time:4323ms step_avg:93.98ms
step:57/1770 train_time:4417ms step_avg:93.98ms
step:58/1770 train_time:4511ms step_avg:93.99ms
step:59/1770 train_time:4606ms step_avg:94.00ms
step:60/1770 train_time:4700ms step_avg:93.99ms
step:61/1770 train_time:4793ms step_avg:93.98ms
step:62/1770 train_time:4887ms step_avg:93.98ms
step:63/1770 train_time:4981ms step_avg:93.98ms
step:64/1770 train_time:5075ms step_avg:93.97ms
step:65/1770 train_time:5168ms step_avg:93.97ms
step:66/1770 train_time:5263ms step_avg:93.98ms
step:67/1770 train_time:5356ms step_avg:93.97ms
step:68/1770 train_time:5451ms step_avg:93.97ms
step:69/1770 train_time:5544ms step_avg:93.97ms
step:70/1770 train_time:5638ms step_avg:93.97ms
step:71/1770 train_time:5732ms step_avg:93.96ms
step:72/1770 train_time:5825ms step_avg:93.96ms
step:73/1770 train_time:5919ms step_avg:93.95ms
step:74/1770 train_time:6013ms step_avg:93.95ms
step:75/1770 train_time:6107ms step_avg:93.95ms
step:76/1770 train_time:6201ms step_avg:93.95ms
step:77/1770 train_time:6294ms step_avg:93.95ms
step:78/1770 train_time:6388ms step_avg:93.95ms
step:79/1770 train_time:6483ms step_avg:93.96ms
step:80/1770 train_time:6577ms step_avg:93.96ms
step:81/1770 train_time:6671ms step_avg:93.96ms
step:82/1770 train_time:6765ms step_avg:93.96ms
step:83/1770 train_time:6859ms step_avg:93.95ms
step:84/1770 train_time:6953ms step_avg:93.95ms
step:85/1770 train_time:7046ms step_avg:93.95ms
step:86/1770 train_time:7140ms step_avg:93.94ms
step:87/1770 train_time:7233ms step_avg:93.93ms
step:88/1770 train_time:7326ms step_avg:93.92ms
step:89/1770 train_time:7420ms step_avg:93.92ms
step:90/1770 train_time:7513ms step_avg:93.92ms
step:91/1770 train_time:7607ms step_avg:93.92ms
step:92/1770 train_time:7702ms step_avg:93.92ms
step:93/1770 train_time:7795ms step_avg:93.91ms
step:94/1770 train_time:7888ms step_avg:93.91ms
step:95/1770 train_time:7982ms step_avg:93.91ms
step:96/1770 train_time:8076ms step_avg:93.91ms
step:97/1770 train_time:8171ms step_avg:93.91ms
step:98/1770 train_time:8264ms step_avg:93.91ms
step:99/1770 train_time:8357ms step_avg:93.90ms
step:100/1770 train_time:8451ms step_avg:93.90ms
step:101/1770 train_time:8544ms step_avg:93.89ms
step:102/1770 train_time:8638ms step_avg:93.89ms
step:103/1770 train_time:8732ms step_avg:93.89ms
step:104/1770 train_time:8826ms step_avg:93.90ms
step:105/1770 train_time:8920ms step_avg:93.89ms
step:106/1770 train_time:9014ms step_avg:93.89ms
step:107/1770 train_time:9108ms step_avg:93.89ms
step:108/1770 train_time:9201ms step_avg:93.89ms
step:109/1770 train_time:9295ms step_avg:93.89ms
step:110/1770 train_time:9389ms step_avg:93.89ms
step:111/1770 train_time:9483ms step_avg:93.89ms
step:112/1770 train_time:9579ms step_avg:93.91ms
step:113/1770 train_time:9670ms step_avg:93.88ms
step:114/1770 train_time:9764ms step_avg:93.88ms
step:115/1770 train_time:9858ms step_avg:93.89ms
step:116/1770 train_time:9952ms step_avg:93.89ms
step:117/1770 train_time:10046ms step_avg:93.89ms
step:118/1770 train_time:10140ms step_avg:93.89ms
step:119/1770 train_time:10234ms step_avg:93.89ms
step:120/1770 train_time:10328ms step_avg:93.89ms
step:121/1770 train_time:10422ms step_avg:93.89ms
step:122/1770 train_time:10515ms step_avg:93.89ms
step:123/1770 train_time:10609ms step_avg:93.88ms
step:124/1770 train_time:10703ms step_avg:93.88ms
step:125/1770 train_time:10797ms step_avg:93.89ms
step:125/1770 val_loss:4.6644 train_time:10889ms step_avg:94.69ms
step:126/1770 train_time:10911ms step_avg:94.06ms
step:127/1770 train_time:10990ms step_avg:93.93ms
step:128/1770 train_time:11088ms step_avg:93.96ms
step:129/1770 train_time:11190ms step_avg:94.03ms
step:130/1770 train_time:11286ms step_avg:94.05ms
step:131/1770 train_time:11380ms step_avg:94.05ms
step:132/1770 train_time:11473ms step_avg:94.04ms
step:133/1770 train_time:11567ms step_avg:94.04ms
step:134/1770 train_time:11660ms step_avg:94.04ms
step:135/1770 train_time:11755ms step_avg:94.04ms
step:136/1770 train_time:11849ms step_avg:94.04ms
step:137/1770 train_time:11943ms step_avg:94.04ms
step:138/1770 train_time:12037ms step_avg:94.04ms
step:139/1770 train_time:12133ms step_avg:94.05ms
step:140/1770 train_time:12229ms step_avg:94.07ms
step:141/1770 train_time:12323ms step_avg:94.07ms
step:142/1770 train_time:12417ms step_avg:94.07ms
step:143/1770 train_time:12512ms step_avg:94.08ms
step:144/1770 train_time:12606ms step_avg:94.08ms
step:145/1770 train_time:12701ms step_avg:94.08ms
step:146/1770 train_time:12795ms step_avg:94.08ms
step:147/1770 train_time:12889ms step_avg:94.08ms
step:148/1770 train_time:12984ms step_avg:94.09ms
step:149/1770 train_time:13078ms step_avg:94.09ms
step:150/1770 train_time:13174ms step_avg:94.10ms
step:151/1770 train_time:13269ms step_avg:94.10ms
step:152/1770 train_time:13363ms step_avg:94.11ms
step:153/1770 train_time:13458ms step_avg:94.11ms
step:154/1770 train_time:13553ms step_avg:94.12ms
step:155/1770 train_time:13647ms step_avg:94.12ms
step:156/1770 train_time:13741ms step_avg:94.12ms
step:157/1770 train_time:13836ms step_avg:94.13ms
step:158/1770 train_time:13931ms step_avg:94.13ms
step:159/1770 train_time:14025ms step_avg:94.13ms
step:160/1770 train_time:14120ms step_avg:94.14ms
step:161/1770 train_time:14215ms step_avg:94.14ms
step:162/1770 train_time:14310ms step_avg:94.14ms
step:163/1770 train_time:14404ms step_avg:94.14ms
step:164/1770 train_time:14503ms step_avg:94.17ms
step:165/1770 train_time:14594ms step_avg:94.15ms
step:166/1770 train_time:14688ms step_avg:94.15ms
step:167/1770 train_time:14782ms step_avg:94.16ms
step:168/1770 train_time:14877ms step_avg:94.16ms
step:169/1770 train_time:14971ms step_avg:94.16ms
step:170/1770 train_time:15066ms step_avg:94.16ms
step:171/1770 train_time:15160ms step_avg:94.16ms
step:172/1770 train_time:15255ms step_avg:94.17ms
step:173/1770 train_time:15350ms step_avg:94.17ms
step:174/1770 train_time:15444ms step_avg:94.17ms
step:175/1770 train_time:15538ms step_avg:94.17ms
step:176/1770 train_time:15633ms step_avg:94.18ms
step:177/1770 train_time:15727ms step_avg:94.17ms
step:178/1770 train_time:15822ms step_avg:94.18ms
step:179/1770 train_time:15917ms step_avg:94.19ms
step:180/1770 train_time:16012ms step_avg:94.19ms
step:181/1770 train_time:16108ms step_avg:94.20ms
step:182/1770 train_time:16200ms step_avg:94.19ms
step:183/1770 train_time:16295ms step_avg:94.19ms
step:184/1770 train_time:16390ms step_avg:94.20ms
step:185/1770 train_time:16484ms step_avg:94.19ms
step:186/1770 train_time:16579ms step_avg:94.20ms
step:187/1770 train_time:16674ms step_avg:94.20ms
step:188/1770 train_time:16767ms step_avg:94.20ms
step:189/1770 train_time:16861ms step_avg:94.20ms
step:190/1770 train_time:16956ms step_avg:94.20ms
step:191/1770 train_time:17051ms step_avg:94.20ms
step:192/1770 train_time:17146ms step_avg:94.21ms
step:193/1770 train_time:17240ms step_avg:94.21ms
step:194/1770 train_time:17335ms step_avg:94.21ms
step:195/1770 train_time:17430ms step_avg:94.22ms
step:196/1770 train_time:17524ms step_avg:94.22ms
step:197/1770 train_time:17619ms step_avg:94.22ms
step:198/1770 train_time:17714ms step_avg:94.22ms
step:199/1770 train_time:17808ms step_avg:94.22ms
step:200/1770 train_time:17903ms step_avg:94.22ms
step:201/1770 train_time:18001ms step_avg:94.25ms
step:202/1770 train_time:18092ms step_avg:94.23ms
step:203/1770 train_time:18186ms step_avg:94.23ms
step:204/1770 train_time:18281ms step_avg:94.23ms
step:205/1770 train_time:18376ms step_avg:94.23ms
step:206/1770 train_time:18470ms step_avg:94.24ms
step:207/1770 train_time:18564ms step_avg:94.23ms
step:208/1770 train_time:18659ms step_avg:94.24ms
step:209/1770 train_time:18753ms step_avg:94.24ms
step:210/1770 train_time:18849ms step_avg:94.24ms
step:211/1770 train_time:18943ms step_avg:94.24ms
step:212/1770 train_time:19037ms step_avg:94.24ms
step:213/1770 train_time:19132ms step_avg:94.25ms
step:214/1770 train_time:19226ms step_avg:94.25ms
step:215/1770 train_time:19320ms step_avg:94.25ms
step:216/1770 train_time:19415ms step_avg:94.25ms
step:217/1770 train_time:19511ms step_avg:94.25ms
step:218/1770 train_time:19605ms step_avg:94.25ms
step:219/1770 train_time:19700ms step_avg:94.26ms
step:220/1770 train_time:19794ms step_avg:94.26ms
step:221/1770 train_time:19888ms step_avg:94.26ms
step:222/1770 train_time:19983ms step_avg:94.26ms
step:223/1770 train_time:20077ms step_avg:94.26ms
step:224/1770 train_time:20172ms step_avg:94.26ms
step:225/1770 train_time:20266ms step_avg:94.26ms
step:226/1770 train_time:20360ms step_avg:94.26ms
step:227/1770 train_time:20455ms step_avg:94.26ms
step:228/1770 train_time:20550ms step_avg:94.26ms
step:229/1770 train_time:20644ms step_avg:94.27ms
step:230/1770 train_time:20739ms step_avg:94.27ms
step:231/1770 train_time:20834ms step_avg:94.27ms
step:232/1770 train_time:20928ms step_avg:94.27ms
step:233/1770 train_time:21023ms step_avg:94.27ms
step:234/1770 train_time:21117ms step_avg:94.27ms
step:235/1770 train_time:21211ms step_avg:94.27ms
step:236/1770 train_time:21306ms step_avg:94.27ms
step:237/1770 train_time:21401ms step_avg:94.28ms
step:238/1770 train_time:21496ms step_avg:94.28ms
step:239/1770 train_time:21595ms step_avg:94.30ms
step:240/1770 train_time:21685ms step_avg:94.28ms
step:241/1770 train_time:21779ms step_avg:94.28ms
step:242/1770 train_time:21874ms step_avg:94.28ms
step:243/1770 train_time:21968ms step_avg:94.28ms
step:244/1770 train_time:22062ms step_avg:94.28ms
step:245/1770 train_time:22157ms step_avg:94.29ms
step:246/1770 train_time:22252ms step_avg:94.29ms
step:247/1770 train_time:22346ms step_avg:94.29ms
step:248/1770 train_time:22441ms step_avg:94.29ms
step:249/1770 train_time:22536ms step_avg:94.29ms
step:250/1770 train_time:22630ms step_avg:94.29ms
step:250/1770 val_loss:4.1066 train_time:22723ms step_avg:94.68ms
step:251/1770 train_time:22744ms step_avg:94.37ms
step:252/1770 train_time:22830ms step_avg:94.34ms
step:253/1770 train_time:22927ms step_avg:94.35ms
step:254/1770 train_time:23022ms step_avg:94.35ms
step:255/1770 train_time:23116ms step_avg:94.35ms
step:256/1770 train_time:23210ms step_avg:94.35ms
step:257/1770 train_time:23304ms step_avg:94.35ms
step:258/1770 train_time:23399ms step_avg:94.35ms
step:259/1770 train_time:23492ms step_avg:94.35ms
step:260/1770 train_time:23586ms step_avg:94.35ms
step:261/1770 train_time:23681ms step_avg:94.35ms
step:262/1770 train_time:23776ms step_avg:94.35ms
step:263/1770 train_time:23871ms step_avg:94.35ms
step:264/1770 train_time:23966ms step_avg:94.35ms
step:265/1770 train_time:24061ms step_avg:94.36ms
step:266/1770 train_time:24157ms step_avg:94.36ms
step:267/1770 train_time:24251ms step_avg:94.36ms
step:268/1770 train_time:24346ms step_avg:94.36ms
step:269/1770 train_time:24441ms step_avg:94.37ms
step:270/1770 train_time:24536ms step_avg:94.37ms
step:271/1770 train_time:24630ms step_avg:94.37ms
step:272/1770 train_time:24725ms step_avg:94.37ms
step:273/1770 train_time:24820ms step_avg:94.37ms
step:274/1770 train_time:24916ms step_avg:94.38ms
step:275/1770 train_time:25010ms step_avg:94.38ms
step:276/1770 train_time:25106ms step_avg:94.38ms
step:277/1770 train_time:25201ms step_avg:94.38ms
step:278/1770 train_time:25295ms step_avg:94.38ms
step:279/1770 train_time:25390ms step_avg:94.39ms
step:280/1770 train_time:25485ms step_avg:94.39ms
step:281/1770 train_time:25579ms step_avg:94.39ms
step:282/1770 train_time:25674ms step_avg:94.39ms
step:283/1770 train_time:25769ms step_avg:94.39ms
step:284/1770 train_time:25864ms step_avg:94.39ms
step:285/1770 train_time:25959ms step_avg:94.40ms
step:286/1770 train_time:26054ms step_avg:94.40ms
step:287/1770 train_time:26149ms step_avg:94.40ms
step:288/1770 train_time:26244ms step_avg:94.40ms
step:289/1770 train_time:26338ms step_avg:94.40ms
step:290/1770 train_time:26433ms step_avg:94.41ms
step:291/1770 train_time:26528ms step_avg:94.41ms
step:292/1770 train_time:26623ms step_avg:94.41ms
step:293/1770 train_time:26718ms step_avg:94.41ms
step:294/1770 train_time:26813ms step_avg:94.41ms
step:295/1770 train_time:26907ms step_avg:94.41ms
step:296/1770 train_time:27003ms step_avg:94.41ms
step:297/1770 train_time:27098ms step_avg:94.42ms
step:298/1770 train_time:27193ms step_avg:94.42ms
step:299/1770 train_time:27287ms step_avg:94.42ms
step:300/1770 train_time:27383ms step_avg:94.42ms
step:301/1770 train_time:27478ms step_avg:94.43ms
step:302/1770 train_time:27573ms step_avg:94.43ms
step:303/1770 train_time:27667ms step_avg:94.43ms
step:304/1770 train_time:27763ms step_avg:94.43ms
step:305/1770 train_time:27857ms step_avg:94.43ms
step:306/1770 train_time:27952ms step_avg:94.43ms
step:307/1770 train_time:28047ms step_avg:94.43ms
step:308/1770 train_time:28142ms step_avg:94.44ms
step:309/1770 train_time:28237ms step_avg:94.44ms
step:310/1770 train_time:28332ms step_avg:94.44ms
step:311/1770 train_time:28427ms step_avg:94.44ms
step:312/1770 train_time:28523ms step_avg:94.45ms
step:313/1770 train_time:28618ms step_avg:94.45ms
step:314/1770 train_time:28712ms step_avg:94.45ms
step:315/1770 train_time:28807ms step_avg:94.45ms
step:316/1770 train_time:28902ms step_avg:94.45ms
step:317/1770 train_time:28997ms step_avg:94.45ms
step:318/1770 train_time:29092ms step_avg:94.45ms
step:319/1770 train_time:29186ms step_avg:94.45ms
step:320/1770 train_time:29281ms step_avg:94.46ms
step:321/1770 train_time:29377ms step_avg:94.46ms
step:322/1770 train_time:29472ms step_avg:94.46ms
step:323/1770 train_time:29567ms step_avg:94.46ms
step:324/1770 train_time:29662ms step_avg:94.46ms
step:325/1770 train_time:29757ms step_avg:94.47ms
step:326/1770 train_time:29852ms step_avg:94.47ms
step:327/1770 train_time:29947ms step_avg:94.47ms
step:328/1770 train_time:30042ms step_avg:94.47ms
step:329/1770 train_time:30136ms step_avg:94.47ms
step:330/1770 train_time:30231ms step_avg:94.47ms
step:331/1770 train_time:30327ms step_avg:94.48ms
step:332/1770 train_time:30421ms step_avg:94.48ms
step:333/1770 train_time:30516ms step_avg:94.48ms
step:334/1770 train_time:30611ms step_avg:94.48ms
step:335/1770 train_time:30706ms step_avg:94.48ms
step:336/1770 train_time:30801ms step_avg:94.48ms
step:337/1770 train_time:30896ms step_avg:94.48ms
step:338/1770 train_time:30991ms step_avg:94.48ms
step:339/1770 train_time:31086ms step_avg:94.49ms
step:340/1770 train_time:31181ms step_avg:94.49ms
step:341/1770 train_time:31276ms step_avg:94.49ms
step:342/1770 train_time:31370ms step_avg:94.49ms
step:343/1770 train_time:31466ms step_avg:94.49ms
step:344/1770 train_time:31561ms step_avg:94.49ms
step:345/1770 train_time:31655ms step_avg:94.49ms
step:346/1770 train_time:31750ms step_avg:94.49ms
step:347/1770 train_time:31844ms step_avg:94.49ms
step:348/1770 train_time:31939ms step_avg:94.49ms
step:349/1770 train_time:32035ms step_avg:94.50ms
step:350/1770 train_time:32129ms step_avg:94.50ms
step:351/1770 train_time:32224ms step_avg:94.50ms
step:352/1770 train_time:32319ms step_avg:94.50ms
step:353/1770 train_time:32414ms step_avg:94.50ms
step:354/1770 train_time:32508ms step_avg:94.50ms
step:355/1770 train_time:32604ms step_avg:94.50ms
step:356/1770 train_time:32698ms step_avg:94.50ms
step:357/1770 train_time:32793ms step_avg:94.51ms
step:358/1770 train_time:32888ms step_avg:94.51ms
step:359/1770 train_time:32984ms step_avg:94.51ms
step:360/1770 train_time:33079ms step_avg:94.51ms
step:361/1770 train_time:33174ms step_avg:94.51ms
step:362/1770 train_time:33269ms step_avg:94.52ms
step:363/1770 train_time:33364ms step_avg:94.52ms
step:364/1770 train_time:33459ms step_avg:94.52ms
step:365/1770 train_time:33554ms step_avg:94.52ms
step:366/1770 train_time:33648ms step_avg:94.52ms
step:367/1770 train_time:33744ms step_avg:94.52ms
step:368/1770 train_time:33838ms step_avg:94.52ms
step:369/1770 train_time:33933ms step_avg:94.52ms
step:370/1770 train_time:34028ms step_avg:94.52ms
step:371/1770 train_time:34123ms step_avg:94.52ms
step:372/1770 train_time:34219ms step_avg:94.53ms
step:373/1770 train_time:34313ms step_avg:94.53ms
step:374/1770 train_time:34408ms step_avg:94.53ms
step:375/1770 train_time:34503ms step_avg:94.53ms
step:375/1770 val_loss:3.9051 train_time:34597ms step_avg:94.79ms
step:376/1770 train_time:34618ms step_avg:94.58ms
step:377/1770 train_time:34699ms step_avg:94.55ms
step:378/1770 train_time:34797ms step_avg:94.56ms
step:379/1770 train_time:34892ms step_avg:94.56ms
step:380/1770 train_time:34988ms step_avg:94.56ms
step:381/1770 train_time:35082ms step_avg:94.56ms
step:382/1770 train_time:35177ms step_avg:94.56ms
step:383/1770 train_time:35271ms step_avg:94.56ms
step:384/1770 train_time:35367ms step_avg:94.56ms
step:385/1770 train_time:35460ms step_avg:94.56ms
step:386/1770 train_time:35555ms step_avg:94.56ms
step:387/1770 train_time:35651ms step_avg:94.56ms
step:388/1770 train_time:35748ms step_avg:94.57ms
step:389/1770 train_time:35843ms step_avg:94.57ms
step:390/1770 train_time:35938ms step_avg:94.57ms
step:391/1770 train_time:36033ms step_avg:94.57ms
step:392/1770 train_time:36128ms step_avg:94.58ms
step:393/1770 train_time:36222ms step_avg:94.58ms
step:394/1770 train_time:36317ms step_avg:94.57ms
step:395/1770 train_time:36412ms step_avg:94.58ms
step:396/1770 train_time:36508ms step_avg:94.58ms
step:397/1770 train_time:36604ms step_avg:94.58ms
step:398/1770 train_time:36701ms step_avg:94.59ms
step:399/1770 train_time:36798ms step_avg:94.60ms
step:400/1770 train_time:36895ms step_avg:94.60ms
step:401/1770 train_time:36992ms step_avg:94.61ms
step:402/1770 train_time:37089ms step_avg:94.61ms
step:403/1770 train_time:37186ms step_avg:94.62ms
step:404/1770 train_time:37282ms step_avg:94.62ms
step:405/1770 train_time:37379ms step_avg:94.63ms
step:406/1770 train_time:37476ms step_avg:94.64ms
step:407/1770 train_time:37573ms step_avg:94.64ms
step:408/1770 train_time:37670ms step_avg:94.65ms
step:409/1770 train_time:37767ms step_avg:94.65ms
step:410/1770 train_time:37864ms step_avg:94.66ms
step:411/1770 train_time:37961ms step_avg:94.66ms
step:412/1770 train_time:38058ms step_avg:94.67ms
step:413/1770 train_time:38155ms step_avg:94.68ms
step:414/1770 train_time:38251ms step_avg:94.68ms
step:415/1770 train_time:38348ms step_avg:94.69ms
step:416/1770 train_time:38445ms step_avg:94.69ms
step:417/1770 train_time:38542ms step_avg:94.70ms
step:418/1770 train_time:38639ms step_avg:94.70ms
step:419/1770 train_time:38736ms step_avg:94.71ms
step:420/1770 train_time:38834ms step_avg:94.72ms
step:421/1770 train_time:38932ms step_avg:94.72ms
step:422/1770 train_time:39029ms step_avg:94.73ms
step:423/1770 train_time:39127ms step_avg:94.74ms
step:424/1770 train_time:39224ms step_avg:94.74ms
step:425/1770 train_time:39320ms step_avg:94.75ms
step:426/1770 train_time:39417ms step_avg:94.75ms
step:427/1770 train_time:39513ms step_avg:94.76ms
step:428/1770 train_time:39610ms step_avg:94.76ms
step:429/1770 train_time:39707ms step_avg:94.77ms
step:430/1770 train_time:39804ms step_avg:94.77ms
step:431/1770 train_time:39900ms step_avg:94.78ms
step:432/1770 train_time:39997ms step_avg:94.78ms
step:433/1770 train_time:40094ms step_avg:94.79ms
step:434/1770 train_time:40191ms step_avg:94.79ms
step:435/1770 train_time:40289ms step_avg:94.80ms
step:436/1770 train_time:40385ms step_avg:94.80ms
step:437/1770 train_time:40482ms step_avg:94.81ms
step:438/1770 train_time:40579ms step_avg:94.81ms
step:439/1770 train_time:40676ms step_avg:94.82ms
step:440/1770 train_time:40773ms step_avg:94.82ms
step:441/1770 train_time:40870ms step_avg:94.83ms
step:442/1770 train_time:40967ms step_avg:94.83ms
step:443/1770 train_time:41064ms step_avg:94.84ms
step:444/1770 train_time:41161ms step_avg:94.84ms
step:445/1770 train_time:41258ms step_avg:94.84ms
step:446/1770 train_time:41354ms step_avg:94.85ms
step:447/1770 train_time:41452ms step_avg:94.86ms
step:448/1770 train_time:41549ms step_avg:94.86ms
step:449/1770 train_time:41646ms step_avg:94.87ms
step:450/1770 train_time:41742ms step_avg:94.87ms
step:451/1770 train_time:41839ms step_avg:94.87ms
step:452/1770 train_time:41937ms step_avg:94.88ms
step:453/1770 train_time:42034ms step_avg:94.88ms
step:454/1770 train_time:42131ms step_avg:94.89ms
step:455/1770 train_time:42228ms step_avg:94.90ms
step:456/1770 train_time:42326ms step_avg:94.90ms
step:457/1770 train_time:42423ms step_avg:94.91ms
step:458/1770 train_time:42519ms step_avg:94.91ms
step:459/1770 train_time:42616ms step_avg:94.91ms
step:460/1770 train_time:42713ms step_avg:94.92ms
step:461/1770 train_time:42810ms step_avg:94.92ms
step:462/1770 train_time:42907ms step_avg:94.93ms
step:463/1770 train_time:43003ms step_avg:94.93ms
step:464/1770 train_time:43099ms step_avg:94.93ms
step:465/1770 train_time:43196ms step_avg:94.94ms
step:466/1770 train_time:43293ms step_avg:94.94ms
step:467/1770 train_time:43390ms step_avg:94.94ms
step:468/1770 train_time:43487ms step_avg:94.95ms
step:469/1770 train_time:43584ms step_avg:94.96ms
step:470/1770 train_time:43681ms step_avg:94.96ms
step:471/1770 train_time:43777ms step_avg:94.96ms
step:472/1770 train_time:43875ms step_avg:94.97ms
step:473/1770 train_time:43972ms step_avg:94.97ms
step:474/1770 train_time:44069ms step_avg:94.98ms
step:475/1770 train_time:44166ms step_avg:94.98ms
step:476/1770 train_time:44263ms step_avg:94.98ms
step:477/1770 train_time:44359ms step_avg:94.99ms
step:478/1770 train_time:44456ms step_avg:94.99ms
step:479/1770 train_time:44553ms step_avg:95.00ms
step:480/1770 train_time:44651ms step_avg:95.00ms
step:481/1770 train_time:44747ms step_avg:95.00ms
step:482/1770 train_time:44844ms step_avg:95.01ms
step:483/1770 train_time:44940ms step_avg:95.01ms
step:484/1770 train_time:45037ms step_avg:95.02ms
step:485/1770 train_time:45134ms step_avg:95.02ms
step:486/1770 train_time:45231ms step_avg:95.02ms
step:487/1770 train_time:45329ms step_avg:95.03ms
step:488/1770 train_time:45426ms step_avg:95.03ms
step:489/1770 train_time:45523ms step_avg:95.04ms
step:490/1770 train_time:45620ms step_avg:95.04ms
step:491/1770 train_time:45716ms step_avg:95.04ms
step:492/1770 train_time:45813ms step_avg:95.05ms
step:493/1770 train_time:45910ms step_avg:95.05ms
step:494/1770 train_time:46007ms step_avg:95.06ms
step:495/1770 train_time:46104ms step_avg:95.06ms
step:496/1770 train_time:46200ms step_avg:95.06ms
step:497/1770 train_time:46297ms step_avg:95.07ms
step:498/1770 train_time:46394ms step_avg:95.07ms
step:499/1770 train_time:46492ms step_avg:95.07ms
step:500/1770 train_time:46589ms step_avg:95.08ms
step:500/1770 val_loss:3.7542 train_time:46685ms step_avg:95.27ms
step:501/1770 train_time:46705ms step_avg:95.12ms
step:502/1770 train_time:46793ms step_avg:95.11ms
step:503/1770 train_time:46892ms step_avg:95.11ms
step:504/1770 train_time:46989ms step_avg:95.12ms
step:505/1770 train_time:47086ms step_avg:95.12ms
step:506/1770 train_time:47183ms step_avg:95.13ms
step:507/1770 train_time:47279ms step_avg:95.13ms
step:508/1770 train_time:47375ms step_avg:95.13ms
step:509/1770 train_time:47471ms step_avg:95.13ms
step:510/1770 train_time:47567ms step_avg:95.13ms
step:511/1770 train_time:47664ms step_avg:95.14ms
step:512/1770 train_time:47762ms step_avg:95.14ms
step:513/1770 train_time:47859ms step_avg:95.15ms
step:514/1770 train_time:47956ms step_avg:95.15ms
step:515/1770 train_time:48053ms step_avg:95.15ms
step:516/1770 train_time:48150ms step_avg:95.16ms
step:517/1770 train_time:48247ms step_avg:95.16ms
step:518/1770 train_time:48344ms step_avg:95.17ms
step:519/1770 train_time:48441ms step_avg:95.17ms
step:520/1770 train_time:48537ms step_avg:95.17ms
step:521/1770 train_time:48634ms step_avg:95.17ms
step:522/1770 train_time:48731ms step_avg:95.18ms
step:523/1770 train_time:48828ms step_avg:95.18ms
step:524/1770 train_time:48926ms step_avg:95.19ms
step:525/1770 train_time:49024ms step_avg:95.19ms
step:526/1770 train_time:49121ms step_avg:95.19ms
step:527/1770 train_time:49218ms step_avg:95.20ms
step:528/1770 train_time:49315ms step_avg:95.20ms
step:529/1770 train_time:49412ms step_avg:95.21ms
step:530/1770 train_time:49509ms step_avg:95.21ms
step:531/1770 train_time:49606ms step_avg:95.21ms
step:532/1770 train_time:49705ms step_avg:95.22ms
step:533/1770 train_time:49802ms step_avg:95.22ms
step:534/1770 train_time:49899ms step_avg:95.23ms
step:535/1770 train_time:49996ms step_avg:95.23ms
step:536/1770 train_time:50093ms step_avg:95.23ms
step:537/1770 train_time:50191ms step_avg:95.24ms
step:538/1770 train_time:50289ms step_avg:95.24ms
step:539/1770 train_time:50386ms step_avg:95.25ms
step:540/1770 train_time:50483ms step_avg:95.25ms
step:541/1770 train_time:50581ms step_avg:95.26ms
step:542/1770 train_time:50678ms step_avg:95.26ms
step:543/1770 train_time:50774ms step_avg:95.26ms
step:544/1770 train_time:50872ms step_avg:95.27ms
step:545/1770 train_time:50969ms step_avg:95.27ms
step:546/1770 train_time:51066ms step_avg:95.27ms
step:547/1770 train_time:51163ms step_avg:95.28ms
step:548/1770 train_time:51260ms step_avg:95.28ms
step:549/1770 train_time:51358ms step_avg:95.28ms
step:550/1770 train_time:51456ms step_avg:95.29ms
step:551/1770 train_time:51553ms step_avg:95.29ms
step:552/1770 train_time:51650ms step_avg:95.30ms
step:553/1770 train_time:51748ms step_avg:95.30ms
step:554/1770 train_time:51845ms step_avg:95.30ms
step:555/1770 train_time:51943ms step_avg:95.31ms
step:556/1770 train_time:52040ms step_avg:95.31ms
step:557/1770 train_time:52137ms step_avg:95.32ms
step:558/1770 train_time:52234ms step_avg:95.32ms
step:559/1770 train_time:52331ms step_avg:95.32ms
step:560/1770 train_time:52429ms step_avg:95.33ms
step:561/1770 train_time:52526ms step_avg:95.33ms
step:562/1770 train_time:52623ms step_avg:95.33ms
step:563/1770 train_time:52720ms step_avg:95.33ms
step:564/1770 train_time:52817ms step_avg:95.34ms
step:565/1770 train_time:52914ms step_avg:95.34ms
step:566/1770 train_time:53011ms step_avg:95.34ms
step:567/1770 train_time:53108ms step_avg:95.35ms
step:568/1770 train_time:53206ms step_avg:95.35ms
step:569/1770 train_time:53303ms step_avg:95.35ms
step:570/1770 train_time:53400ms step_avg:95.36ms
step:571/1770 train_time:53498ms step_avg:95.36ms
step:572/1770 train_time:53595ms step_avg:95.37ms
step:573/1770 train_time:53693ms step_avg:95.37ms
step:574/1770 train_time:53790ms step_avg:95.37ms
step:575/1770 train_time:53888ms step_avg:95.38ms
step:576/1770 train_time:53985ms step_avg:95.38ms
step:577/1770 train_time:54083ms step_avg:95.38ms
step:578/1770 train_time:54180ms step_avg:95.39ms
step:579/1770 train_time:54276ms step_avg:95.39ms
step:580/1770 train_time:54374ms step_avg:95.39ms
step:581/1770 train_time:54471ms step_avg:95.40ms
step:582/1770 train_time:54569ms step_avg:95.40ms
step:583/1770 train_time:54666ms step_avg:95.40ms
step:584/1770 train_time:54763ms step_avg:95.41ms
step:585/1770 train_time:54860ms step_avg:95.41ms
step:586/1770 train_time:54957ms step_avg:95.41ms
step:587/1770 train_time:55054ms step_avg:95.41ms
step:588/1770 train_time:55152ms step_avg:95.42ms
step:589/1770 train_time:55249ms step_avg:95.42ms
step:590/1770 train_time:55346ms step_avg:95.42ms
step:591/1770 train_time:55444ms step_avg:95.43ms
step:592/1770 train_time:55541ms step_avg:95.43ms
step:593/1770 train_time:55638ms step_avg:95.43ms
step:594/1770 train_time:55736ms step_avg:95.44ms
step:595/1770 train_time:55833ms step_avg:95.44ms
step:596/1770 train_time:55930ms step_avg:95.44ms
step:597/1770 train_time:56028ms step_avg:95.45ms
step:598/1770 train_time:56125ms step_avg:95.45ms
step:599/1770 train_time:56222ms step_avg:95.45ms
step:600/1770 train_time:56319ms step_avg:95.46ms
step:601/1770 train_time:56416ms step_avg:95.46ms
step:602/1770 train_time:56513ms step_avg:95.46ms
step:603/1770 train_time:56611ms step_avg:95.46ms
step:604/1770 train_time:56708ms step_avg:95.47ms
step:605/1770 train_time:56806ms step_avg:95.47ms
step:606/1770 train_time:56904ms step_avg:95.48ms
step:607/1770 train_time:57001ms step_avg:95.48ms
step:608/1770 train_time:57098ms step_avg:95.48ms
step:609/1770 train_time:57196ms step_avg:95.49ms
step:610/1770 train_time:57292ms step_avg:95.49ms
step:611/1770 train_time:57390ms step_avg:95.49ms
step:612/1770 train_time:57488ms step_avg:95.49ms
step:613/1770 train_time:57585ms step_avg:95.50ms
step:614/1770 train_time:57682ms step_avg:95.50ms
step:615/1770 train_time:57779ms step_avg:95.50ms
step:616/1770 train_time:57876ms step_avg:95.51ms
step:617/1770 train_time:57973ms step_avg:95.51ms
step:618/1770 train_time:58070ms step_avg:95.51ms
step:619/1770 train_time:58168ms step_avg:95.51ms
step:620/1770 train_time:58266ms step_avg:95.52ms
step:621/1770 train_time:58363ms step_avg:95.52ms
step:622/1770 train_time:58461ms step_avg:95.52ms
step:623/1770 train_time:58558ms step_avg:95.53ms
step:624/1770 train_time:58656ms step_avg:95.53ms
step:625/1770 train_time:58753ms step_avg:95.53ms
step:625/1770 val_loss:3.6665 train_time:58849ms step_avg:95.69ms
step:626/1770 train_time:58871ms step_avg:95.57ms
step:627/1770 train_time:58961ms step_avg:95.56ms
step:628/1770 train_time:59060ms step_avg:95.57ms
step:629/1770 train_time:59157ms step_avg:95.57ms
step:630/1770 train_time:59254ms step_avg:95.57ms
step:631/1770 train_time:59351ms step_avg:95.57ms
step:632/1770 train_time:59448ms step_avg:95.58ms
step:633/1770 train_time:59545ms step_avg:95.58ms
step:634/1770 train_time:59642ms step_avg:95.58ms
step:635/1770 train_time:59739ms step_avg:95.58ms
step:636/1770 train_time:59835ms step_avg:95.58ms
step:637/1770 train_time:59934ms step_avg:95.59ms
step:638/1770 train_time:60033ms step_avg:95.59ms
step:639/1770 train_time:60131ms step_avg:95.60ms
step:640/1770 train_time:60229ms step_avg:95.60ms
step:641/1770 train_time:60326ms step_avg:95.60ms
step:642/1770 train_time:60423ms step_avg:95.61ms
step:643/1770 train_time:60520ms step_avg:95.61ms
step:644/1770 train_time:60616ms step_avg:95.61ms
step:645/1770 train_time:60713ms step_avg:95.61ms
step:646/1770 train_time:60810ms step_avg:95.61ms
step:647/1770 train_time:60907ms step_avg:95.62ms
step:648/1770 train_time:61005ms step_avg:95.62ms
step:649/1770 train_time:61102ms step_avg:95.62ms
step:650/1770 train_time:61200ms step_avg:95.62ms
step:651/1770 train_time:61297ms step_avg:95.63ms
step:652/1770 train_time:61394ms step_avg:95.63ms
step:653/1770 train_time:61492ms step_avg:95.63ms
step:654/1770 train_time:61588ms step_avg:95.63ms
step:655/1770 train_time:61685ms step_avg:95.64ms
step:656/1770 train_time:61782ms step_avg:95.64ms
step:657/1770 train_time:61879ms step_avg:95.64ms
step:658/1770 train_time:61978ms step_avg:95.65ms
step:659/1770 train_time:62077ms step_avg:95.65ms
step:660/1770 train_time:62177ms step_avg:95.66ms
step:661/1770 train_time:62276ms step_avg:95.66ms
step:662/1770 train_time:62375ms step_avg:95.67ms
step:663/1770 train_time:62474ms step_avg:95.67ms
step:664/1770 train_time:62573ms step_avg:95.68ms
step:665/1770 train_time:62672ms step_avg:95.68ms
step:666/1770 train_time:62770ms step_avg:95.69ms
step:667/1770 train_time:62869ms step_avg:95.69ms
step:668/1770 train_time:62969ms step_avg:95.70ms
step:669/1770 train_time:63067ms step_avg:95.70ms
step:670/1770 train_time:63166ms step_avg:95.71ms
step:671/1770 train_time:63265ms step_avg:95.71ms
step:672/1770 train_time:63364ms step_avg:95.72ms
step:673/1770 train_time:63464ms step_avg:95.72ms
step:674/1770 train_time:63564ms step_avg:95.73ms
step:675/1770 train_time:63663ms step_avg:95.73ms
step:676/1770 train_time:63762ms step_avg:95.74ms
step:677/1770 train_time:63860ms step_avg:95.74ms
step:678/1770 train_time:63959ms step_avg:95.75ms
step:679/1770 train_time:64058ms step_avg:95.75ms
step:680/1770 train_time:64156ms step_avg:95.76ms
step:681/1770 train_time:64256ms step_avg:95.76ms
step:682/1770 train_time:64356ms step_avg:95.77ms
step:683/1770 train_time:64456ms step_avg:95.77ms
step:684/1770 train_time:64556ms step_avg:95.78ms
step:685/1770 train_time:64656ms step_avg:95.79ms
step:686/1770 train_time:64755ms step_avg:95.79ms
step:687/1770 train_time:64855ms step_avg:95.80ms
step:688/1770 train_time:64954ms step_avg:95.80ms
step:689/1770 train_time:65054ms step_avg:95.81ms
step:690/1770 train_time:65152ms step_avg:95.81ms
step:691/1770 train_time:65251ms step_avg:95.82ms
step:692/1770 train_time:65350ms step_avg:95.82ms
step:693/1770 train_time:65449ms step_avg:95.83ms
step:694/1770 train_time:65549ms step_avg:95.83ms
step:695/1770 train_time:65649ms step_avg:95.84ms
step:696/1770 train_time:65749ms step_avg:95.84ms
step:697/1770 train_time:65848ms step_avg:95.85ms
step:698/1770 train_time:65947ms step_avg:95.85ms
step:699/1770 train_time:66047ms step_avg:95.86ms
step:700/1770 train_time:66145ms step_avg:95.86ms
step:701/1770 train_time:66244ms step_avg:95.87ms
step:702/1770 train_time:66343ms step_avg:95.87ms
step:703/1770 train_time:66441ms step_avg:95.87ms
step:704/1770 train_time:66540ms step_avg:95.88ms
step:705/1770 train_time:66638ms step_avg:95.88ms
step:706/1770 train_time:66737ms step_avg:95.89ms
step:707/1770 train_time:66837ms step_avg:95.89ms
step:708/1770 train_time:66936ms step_avg:95.90ms
step:709/1770 train_time:67035ms step_avg:95.90ms
step:710/1770 train_time:67134ms step_avg:95.91ms
step:711/1770 train_time:67234ms step_avg:95.91ms
step:712/1770 train_time:67333ms step_avg:95.92ms
step:713/1770 train_time:67433ms step_avg:95.92ms
step:714/1770 train_time:67533ms step_avg:95.93ms
step:715/1770 train_time:67632ms step_avg:95.93ms
step:716/1770 train_time:67730ms step_avg:95.94ms
step:717/1770 train_time:67830ms step_avg:95.94ms
step:718/1770 train_time:67930ms step_avg:95.95ms
step:719/1770 train_time:68029ms step_avg:95.95ms
step:720/1770 train_time:68129ms step_avg:95.96ms
step:721/1770 train_time:68228ms step_avg:95.96ms
step:722/1770 train_time:68327ms step_avg:95.97ms
step:723/1770 train_time:68427ms step_avg:95.97ms
step:724/1770 train_time:68526ms step_avg:95.97ms
step:725/1770 train_time:68624ms step_avg:95.98ms
step:726/1770 train_time:68723ms step_avg:95.98ms
step:727/1770 train_time:68822ms step_avg:95.99ms
step:728/1770 train_time:68920ms step_avg:95.99ms
step:729/1770 train_time:69018ms step_avg:95.99ms
step:730/1770 train_time:69117ms step_avg:96.00ms
step:731/1770 train_time:69216ms step_avg:96.00ms
step:732/1770 train_time:69316ms step_avg:96.01ms
step:733/1770 train_time:69415ms step_avg:96.01ms
step:734/1770 train_time:69515ms step_avg:96.02ms
step:735/1770 train_time:69615ms step_avg:96.02ms
step:736/1770 train_time:69714ms step_avg:96.03ms
step:737/1770 train_time:69814ms step_avg:96.03ms
step:738/1770 train_time:69913ms step_avg:96.03ms
step:739/1770 train_time:70013ms step_avg:96.04ms
step:740/1770 train_time:70112ms step_avg:96.04ms
step:741/1770 train_time:70211ms step_avg:96.05ms
step:742/1770 train_time:70311ms step_avg:96.05ms
step:743/1770 train_time:70409ms step_avg:96.06ms
step:744/1770 train_time:70509ms step_avg:96.06ms
step:745/1770 train_time:70609ms step_avg:96.07ms
step:746/1770 train_time:70710ms step_avg:96.07ms
step:747/1770 train_time:70809ms step_avg:96.08ms
step:748/1770 train_time:70908ms step_avg:96.08ms
step:749/1770 train_time:71007ms step_avg:96.09ms
step:750/1770 train_time:71106ms step_avg:96.09ms
step:750/1770 val_loss:3.6030 train_time:71203ms step_avg:96.22ms
step:751/1770 train_time:71223ms step_avg:96.12ms
step:752/1770 train_time:71314ms step_avg:96.11ms
step:753/1770 train_time:71416ms step_avg:96.12ms
step:754/1770 train_time:71515ms step_avg:96.12ms
step:755/1770 train_time:71613ms step_avg:96.12ms
step:756/1770 train_time:71712ms step_avg:96.13ms
step:757/1770 train_time:71810ms step_avg:96.13ms
step:758/1770 train_time:71908ms step_avg:96.13ms
step:759/1770 train_time:72007ms step_avg:96.14ms
step:760/1770 train_time:72105ms step_avg:96.14ms
step:761/1770 train_time:72204ms step_avg:96.14ms
step:762/1770 train_time:72303ms step_avg:96.15ms
step:763/1770 train_time:72403ms step_avg:96.15ms
step:764/1770 train_time:72503ms step_avg:96.16ms
step:765/1770 train_time:72603ms step_avg:96.16ms
step:766/1770 train_time:72703ms step_avg:96.17ms
step:767/1770 train_time:72803ms step_avg:96.17ms
step:768/1770 train_time:72903ms step_avg:96.18ms
step:769/1770 train_time:73002ms step_avg:96.18ms
step:770/1770 train_time:73100ms step_avg:96.18ms
step:771/1770 train_time:73200ms step_avg:96.19ms
step:772/1770 train_time:73298ms step_avg:96.19ms
step:773/1770 train_time:73397ms step_avg:96.20ms
step:774/1770 train_time:73496ms step_avg:96.20ms
step:775/1770 train_time:73595ms step_avg:96.20ms
step:776/1770 train_time:73695ms step_avg:96.21ms
step:777/1770 train_time:73794ms step_avg:96.21ms
step:778/1770 train_time:73893ms step_avg:96.21ms
step:779/1770 train_time:73992ms step_avg:96.22ms
step:780/1770 train_time:74091ms step_avg:96.22ms
step:781/1770 train_time:74190ms step_avg:96.23ms
step:782/1770 train_time:74289ms step_avg:96.23ms
step:783/1770 train_time:74388ms step_avg:96.23ms
step:784/1770 train_time:74487ms step_avg:96.24ms
step:785/1770 train_time:74587ms step_avg:96.24ms
step:786/1770 train_time:74686ms step_avg:96.25ms
step:787/1770 train_time:74786ms step_avg:96.25ms
step:788/1770 train_time:74885ms step_avg:96.25ms
step:789/1770 train_time:74985ms step_avg:96.26ms
step:790/1770 train_time:75085ms step_avg:96.26ms
step:791/1770 train_time:75184ms step_avg:96.27ms
step:792/1770 train_time:75284ms step_avg:96.27ms
step:793/1770 train_time:75384ms step_avg:96.28ms
step:794/1770 train_time:75483ms step_avg:96.28ms
step:795/1770 train_time:75583ms step_avg:96.28ms
step:796/1770 train_time:75682ms step_avg:96.29ms
step:797/1770 train_time:75782ms step_avg:96.29ms
step:798/1770 train_time:75881ms step_avg:96.30ms
step:799/1770 train_time:75981ms step_avg:96.30ms
step:800/1770 train_time:76082ms step_avg:96.31ms
step:801/1770 train_time:76180ms step_avg:96.31ms
step:802/1770 train_time:76279ms step_avg:96.31ms
step:803/1770 train_time:76379ms step_avg:96.32ms
step:804/1770 train_time:76478ms step_avg:96.32ms
step:805/1770 train_time:76578ms step_avg:96.32ms
step:806/1770 train_time:76677ms step_avg:96.33ms
step:807/1770 train_time:76777ms step_avg:96.33ms
step:808/1770 train_time:76876ms step_avg:96.34ms
step:809/1770 train_time:76975ms step_avg:96.34ms
step:810/1770 train_time:77075ms step_avg:96.34ms
step:811/1770 train_time:77175ms step_avg:96.35ms
step:812/1770 train_time:77273ms step_avg:96.35ms
step:813/1770 train_time:77372ms step_avg:96.35ms
step:814/1770 train_time:77472ms step_avg:96.36ms
step:815/1770 train_time:77570ms step_avg:96.36ms
step:816/1770 train_time:77669ms step_avg:96.36ms
step:817/1770 train_time:77772ms step_avg:96.37ms
step:818/1770 train_time:77867ms step_avg:96.37ms
step:819/1770 train_time:77966ms step_avg:96.37ms
step:820/1770 train_time:78066ms step_avg:96.38ms
step:821/1770 train_time:78167ms step_avg:96.38ms
step:822/1770 train_time:78266ms step_avg:96.39ms
step:823/1770 train_time:78365ms step_avg:96.39ms
step:824/1770 train_time:78464ms step_avg:96.39ms
step:825/1770 train_time:78564ms step_avg:96.40ms
step:826/1770 train_time:78663ms step_avg:96.40ms
step:827/1770 train_time:78762ms step_avg:96.40ms
step:828/1770 train_time:78861ms step_avg:96.41ms
step:829/1770 train_time:78960ms step_avg:96.41ms
step:830/1770 train_time:79060ms step_avg:96.41ms
step:831/1770 train_time:79159ms step_avg:96.42ms
step:832/1770 train_time:79259ms step_avg:96.42ms
step:833/1770 train_time:79359ms step_avg:96.43ms
step:834/1770 train_time:79458ms step_avg:96.43ms
step:835/1770 train_time:79558ms step_avg:96.43ms
step:836/1770 train_time:79658ms step_avg:96.44ms
step:837/1770 train_time:79758ms step_avg:96.44ms
step:838/1770 train_time:79857ms step_avg:96.45ms
step:839/1770 train_time:79956ms step_avg:96.45ms
step:840/1770 train_time:80054ms step_avg:96.45ms
step:841/1770 train_time:80154ms step_avg:96.45ms
step:842/1770 train_time:80253ms step_avg:96.46ms
step:843/1770 train_time:80352ms step_avg:96.46ms
step:844/1770 train_time:80451ms step_avg:96.46ms
step:845/1770 train_time:80550ms step_avg:96.47ms
step:846/1770 train_time:80649ms step_avg:96.47ms
step:847/1770 train_time:80749ms step_avg:96.47ms
step:848/1770 train_time:80848ms step_avg:96.48ms
step:849/1770 train_time:80947ms step_avg:96.48ms
step:850/1770 train_time:81046ms step_avg:96.48ms
step:851/1770 train_time:81146ms step_avg:96.49ms
step:852/1770 train_time:81246ms step_avg:96.49ms
step:853/1770 train_time:81346ms step_avg:96.50ms
step:854/1770 train_time:81446ms step_avg:96.50ms
step:855/1770 train_time:81546ms step_avg:96.50ms
step:856/1770 train_time:81646ms step_avg:96.51ms
step:857/1770 train_time:81745ms step_avg:96.51ms
step:858/1770 train_time:81845ms step_avg:96.52ms
step:859/1770 train_time:81944ms step_avg:96.52ms
step:860/1770 train_time:82044ms step_avg:96.52ms
step:861/1770 train_time:82143ms step_avg:96.53ms
step:862/1770 train_time:82243ms step_avg:96.53ms
step:863/1770 train_time:82342ms step_avg:96.53ms
step:864/1770 train_time:82441ms step_avg:96.54ms
step:865/1770 train_time:82540ms step_avg:96.54ms
step:866/1770 train_time:82640ms step_avg:96.54ms
step:867/1770 train_time:82739ms step_avg:96.54ms
step:868/1770 train_time:82839ms step_avg:96.55ms
step:869/1770 train_time:82939ms step_avg:96.55ms
step:870/1770 train_time:83039ms step_avg:96.56ms
step:871/1770 train_time:83138ms step_avg:96.56ms
step:872/1770 train_time:83238ms step_avg:96.56ms
step:873/1770 train_time:83337ms step_avg:96.57ms
step:874/1770 train_time:83436ms step_avg:96.57ms
step:875/1770 train_time:83535ms step_avg:96.57ms
step:875/1770 val_loss:3.5535 train_time:83632ms step_avg:96.68ms
step:876/1770 train_time:83653ms step_avg:96.60ms
step:877/1770 train_time:83743ms step_avg:96.59ms
step:878/1770 train_time:83843ms step_avg:96.59ms
step:879/1770 train_time:83943ms step_avg:96.60ms
step:880/1770 train_time:84042ms step_avg:96.60ms
step:881/1770 train_time:84141ms step_avg:96.60ms
step:882/1770 train_time:84240ms step_avg:96.61ms
step:883/1770 train_time:84339ms step_avg:96.61ms
step:884/1770 train_time:84438ms step_avg:96.61ms
step:885/1770 train_time:84537ms step_avg:96.61ms
step:886/1770 train_time:84637ms step_avg:96.62ms
step:887/1770 train_time:84738ms step_avg:96.62ms
step:888/1770 train_time:84839ms step_avg:96.63ms
step:889/1770 train_time:84939ms step_avg:96.63ms
step:890/1770 train_time:85038ms step_avg:96.63ms
step:891/1770 train_time:85138ms step_avg:96.64ms
step:892/1770 train_time:85237ms step_avg:96.64ms
step:893/1770 train_time:85337ms step_avg:96.64ms
step:894/1770 train_time:85436ms step_avg:96.65ms
step:895/1770 train_time:85535ms step_avg:96.65ms
step:896/1770 train_time:85634ms step_avg:96.65ms
step:897/1770 train_time:85733ms step_avg:96.65ms
step:898/1770 train_time:85832ms step_avg:96.66ms
step:899/1770 train_time:85933ms step_avg:96.66ms
step:900/1770 train_time:86031ms step_avg:96.66ms
step:901/1770 train_time:86131ms step_avg:96.67ms
step:902/1770 train_time:86230ms step_avg:96.67ms
step:903/1770 train_time:86330ms step_avg:96.67ms
step:904/1770 train_time:86430ms step_avg:96.68ms
step:905/1770 train_time:86530ms step_avg:96.68ms
step:906/1770 train_time:86629ms step_avg:96.68ms
step:907/1770 train_time:86728ms step_avg:96.69ms
step:908/1770 train_time:86827ms step_avg:96.69ms
step:909/1770 train_time:86926ms step_avg:96.69ms
step:910/1770 train_time:87025ms step_avg:96.69ms
step:911/1770 train_time:87124ms step_avg:96.70ms
step:912/1770 train_time:87224ms step_avg:96.70ms
step:913/1770 train_time:87323ms step_avg:96.70ms
step:914/1770 train_time:87423ms step_avg:96.71ms
step:915/1770 train_time:87523ms step_avg:96.71ms
step:916/1770 train_time:87624ms step_avg:96.72ms
step:917/1770 train_time:87723ms step_avg:96.72ms
step:918/1770 train_time:87823ms step_avg:96.72ms
step:919/1770 train_time:87924ms step_avg:96.73ms
step:920/1770 train_time:88025ms step_avg:96.73ms
step:921/1770 train_time:88127ms step_avg:96.74ms
step:922/1770 train_time:88227ms step_avg:96.74ms
step:923/1770 train_time:88327ms step_avg:96.74ms
step:924/1770 train_time:88428ms step_avg:96.75ms
step:925/1770 train_time:88529ms step_avg:96.75ms
step:926/1770 train_time:88629ms step_avg:96.76ms
step:927/1770 train_time:88731ms step_avg:96.76ms
step:928/1770 train_time:88832ms step_avg:96.77ms
step:929/1770 train_time:88933ms step_avg:96.77ms
step:930/1770 train_time:89034ms step_avg:96.78ms
step:931/1770 train_time:89134ms step_avg:96.78ms
step:932/1770 train_time:89234ms step_avg:96.78ms
step:933/1770 train_time:89335ms step_avg:96.79ms
step:934/1770 train_time:89435ms step_avg:96.79ms
step:935/1770 train_time:89536ms step_avg:96.80ms
step:936/1770 train_time:89637ms step_avg:96.80ms
step:937/1770 train_time:89738ms step_avg:96.80ms
step:938/1770 train_time:89839ms step_avg:96.81ms
step:939/1770 train_time:89940ms step_avg:96.81ms
step:940/1770 train_time:90041ms step_avg:96.82ms
step:941/1770 train_time:90141ms step_avg:96.82ms
step:942/1770 train_time:90242ms step_avg:96.83ms
step:943/1770 train_time:90344ms step_avg:96.83ms
step:944/1770 train_time:90445ms step_avg:96.84ms
step:945/1770 train_time:90546ms step_avg:96.84ms
step:946/1770 train_time:90648ms step_avg:96.85ms
step:947/1770 train_time:90749ms step_avg:96.85ms
step:948/1770 train_time:90850ms step_avg:96.85ms
step:949/1770 train_time:90951ms step_avg:96.86ms
step:950/1770 train_time:91052ms step_avg:96.86ms
step:951/1770 train_time:91154ms step_avg:96.87ms
step:952/1770 train_time:91254ms step_avg:96.87ms
step:953/1770 train_time:91354ms step_avg:96.88ms
step:954/1770 train_time:91454ms step_avg:96.88ms
step:955/1770 train_time:91555ms step_avg:96.88ms
step:956/1770 train_time:91656ms step_avg:96.89ms
step:957/1770 train_time:91757ms step_avg:96.89ms
step:958/1770 train_time:91858ms step_avg:96.90ms
step:959/1770 train_time:91959ms step_avg:96.90ms
step:960/1770 train_time:92059ms step_avg:96.90ms
step:961/1770 train_time:92160ms step_avg:96.91ms
step:962/1770 train_time:92262ms step_avg:96.91ms
step:963/1770 train_time:92362ms step_avg:96.92ms
step:964/1770 train_time:92463ms step_avg:96.92ms
step:965/1770 train_time:92565ms step_avg:96.93ms
step:966/1770 train_time:92666ms step_avg:96.93ms
step:967/1770 train_time:92768ms step_avg:96.94ms
step:968/1770 train_time:92868ms step_avg:96.94ms
step:969/1770 train_time:92972ms step_avg:96.95ms
step:970/1770 train_time:93069ms step_avg:96.95ms
step:971/1770 train_time:93169ms step_avg:96.95ms
step:972/1770 train_time:93270ms step_avg:96.95ms
step:973/1770 train_time:93372ms step_avg:96.96ms
step:974/1770 train_time:93473ms step_avg:96.96ms
step:975/1770 train_time:93575ms step_avg:96.97ms
step:976/1770 train_time:93676ms step_avg:96.97ms
step:977/1770 train_time:93776ms step_avg:96.98ms
step:978/1770 train_time:93877ms step_avg:96.98ms
step:979/1770 train_time:93977ms step_avg:96.98ms
step:980/1770 train_time:94078ms step_avg:96.99ms
step:981/1770 train_time:94179ms step_avg:96.99ms
step:982/1770 train_time:94280ms step_avg:97.00ms
step:983/1770 train_time:94381ms step_avg:97.00ms
step:984/1770 train_time:94483ms step_avg:97.00ms
step:985/1770 train_time:94584ms step_avg:97.01ms
step:986/1770 train_time:94685ms step_avg:97.01ms
step:987/1770 train_time:94786ms step_avg:97.02ms
step:988/1770 train_time:94888ms step_avg:97.02ms
step:989/1770 train_time:94989ms step_avg:97.03ms
step:990/1770 train_time:95089ms step_avg:97.03ms
step:991/1770 train_time:95190ms step_avg:97.03ms
step:992/1770 train_time:95291ms step_avg:97.04ms
step:993/1770 train_time:95393ms step_avg:97.04ms
step:994/1770 train_time:95494ms step_avg:97.05ms
step:995/1770 train_time:95596ms step_avg:97.05ms
step:996/1770 train_time:95695ms step_avg:97.05ms
step:997/1770 train_time:95797ms step_avg:97.06ms
step:998/1770 train_time:95897ms step_avg:97.06ms
step:999/1770 train_time:95997ms step_avg:97.07ms
step:1000/1770 train_time:96099ms step_avg:97.07ms
step:1000/1770 val_loss:3.5142 train_time:96198ms step_avg:97.17ms
step:1001/1770 train_time:96219ms step_avg:97.09ms
step:1002/1770 train_time:96312ms step_avg:97.09ms
step:1003/1770 train_time:96415ms step_avg:97.09ms
step:1004/1770 train_time:96515ms step_avg:97.10ms
step:1005/1770 train_time:96615ms step_avg:97.10ms
step:1006/1770 train_time:96715ms step_avg:97.10ms
step:1007/1770 train_time:96815ms step_avg:97.11ms
step:1008/1770 train_time:96915ms step_avg:97.11ms
step:1009/1770 train_time:97015ms step_avg:97.11ms
step:1010/1770 train_time:97114ms step_avg:97.11ms
step:1011/1770 train_time:97217ms step_avg:97.12ms
step:1012/1770 train_time:97319ms step_avg:97.13ms
step:1013/1770 train_time:97421ms step_avg:97.13ms
step:1014/1770 train_time:97522ms step_avg:97.13ms
step:1015/1770 train_time:97622ms step_avg:97.14ms
step:1016/1770 train_time:97723ms step_avg:97.14ms
step:1017/1770 train_time:97824ms step_avg:97.14ms
step:1018/1770 train_time:97923ms step_avg:97.15ms
step:1019/1770 train_time:98024ms step_avg:97.15ms
step:1020/1770 train_time:98125ms step_avg:97.15ms
step:1021/1770 train_time:98227ms step_avg:97.16ms
step:1022/1770 train_time:98328ms step_avg:97.16ms
step:1023/1770 train_time:98430ms step_avg:97.17ms
step:1024/1770 train_time:98531ms step_avg:97.17ms
step:1025/1770 train_time:98632ms step_avg:97.17ms
step:1026/1770 train_time:98733ms step_avg:97.18ms
step:1027/1770 train_time:98834ms step_avg:97.18ms
step:1028/1770 train_time:98935ms step_avg:97.19ms
step:1029/1770 train_time:99036ms step_avg:97.19ms
step:1030/1770 train_time:99137ms step_avg:97.19ms
step:1031/1770 train_time:99237ms step_avg:97.20ms
step:1032/1770 train_time:99338ms step_avg:97.20ms
step:1033/1770 train_time:99439ms step_avg:97.20ms
step:1034/1770 train_time:99539ms step_avg:97.21ms
step:1035/1770 train_time:99640ms step_avg:97.21ms
step:1036/1770 train_time:99741ms step_avg:97.21ms
step:1037/1770 train_time:99842ms step_avg:97.22ms
step:1038/1770 train_time:99942ms step_avg:97.22ms
step:1039/1770 train_time:100042ms step_avg:97.22ms
step:1040/1770 train_time:100142ms step_avg:97.23ms
step:1041/1770 train_time:100242ms step_avg:97.23ms
step:1042/1770 train_time:100343ms step_avg:97.23ms
step:1043/1770 train_time:100445ms step_avg:97.24ms
step:1044/1770 train_time:100545ms step_avg:97.24ms
step:1045/1770 train_time:100647ms step_avg:97.24ms
step:1046/1770 train_time:100747ms step_avg:97.25ms
step:1047/1770 train_time:100848ms step_avg:97.25ms
step:1048/1770 train_time:100949ms step_avg:97.25ms
step:1049/1770 train_time:101050ms step_avg:97.26ms
step:1050/1770 train_time:101151ms step_avg:97.26ms
step:1051/1770 train_time:101254ms step_avg:97.27ms
step:1052/1770 train_time:101354ms step_avg:97.27ms
step:1053/1770 train_time:101456ms step_avg:97.27ms
step:1054/1770 train_time:101556ms step_avg:97.28ms
step:1055/1770 train_time:101658ms step_avg:97.28ms
step:1056/1770 train_time:101759ms step_avg:97.28ms
step:1057/1770 train_time:101860ms step_avg:97.29ms
step:1058/1770 train_time:101961ms step_avg:97.29ms
step:1059/1770 train_time:102063ms step_avg:97.30ms
step:1060/1770 train_time:102165ms step_avg:97.30ms
step:1061/1770 train_time:102267ms step_avg:97.30ms
step:1062/1770 train_time:102368ms step_avg:97.31ms
step:1063/1770 train_time:102470ms step_avg:97.31ms
step:1064/1770 train_time:102573ms step_avg:97.32ms
step:1065/1770 train_time:102675ms step_avg:97.32ms
step:1066/1770 train_time:102774ms step_avg:97.32ms
step:1067/1770 train_time:102875ms step_avg:97.33ms
step:1068/1770 train_time:102978ms step_avg:97.33ms
step:1069/1770 train_time:103078ms step_avg:97.34ms
step:1070/1770 train_time:103179ms step_avg:97.34ms
step:1071/1770 train_time:103281ms step_avg:97.34ms
step:1072/1770 train_time:103382ms step_avg:97.35ms
step:1073/1770 train_time:103483ms step_avg:97.35ms
step:1074/1770 train_time:103584ms step_avg:97.35ms
step:1075/1770 train_time:103685ms step_avg:97.36ms
step:1076/1770 train_time:103785ms step_avg:97.36ms
step:1077/1770 train_time:103886ms step_avg:97.36ms
step:1078/1770 train_time:103988ms step_avg:97.37ms
step:1079/1770 train_time:104089ms step_avg:97.37ms
step:1080/1770 train_time:104190ms step_avg:97.37ms
step:1081/1770 train_time:104291ms step_avg:97.38ms
step:1082/1770 train_time:104392ms step_avg:97.38ms
step:1083/1770 train_time:104493ms step_avg:97.38ms
step:1084/1770 train_time:104594ms step_avg:97.39ms
step:1085/1770 train_time:104696ms step_avg:97.39ms
step:1086/1770 train_time:104797ms step_avg:97.40ms
step:1087/1770 train_time:104898ms step_avg:97.40ms
step:1088/1770 train_time:104999ms step_avg:97.40ms
step:1089/1770 train_time:105100ms step_avg:97.40ms
step:1090/1770 train_time:105202ms step_avg:97.41ms
step:1091/1770 train_time:105303ms step_avg:97.41ms
step:1092/1770 train_time:105403ms step_avg:97.41ms
step:1093/1770 train_time:105504ms step_avg:97.42ms
step:1094/1770 train_time:105605ms step_avg:97.42ms
step:1095/1770 train_time:105706ms step_avg:97.43ms
step:1096/1770 train_time:105808ms step_avg:97.43ms
step:1097/1770 train_time:105908ms step_avg:97.43ms
step:1098/1770 train_time:106009ms step_avg:97.43ms
step:1099/1770 train_time:106111ms step_avg:97.44ms
step:1100/1770 train_time:106213ms step_avg:97.44ms
step:1101/1770 train_time:106314ms step_avg:97.45ms
step:1102/1770 train_time:106414ms step_avg:97.45ms
step:1103/1770 train_time:106515ms step_avg:97.45ms
step:1104/1770 train_time:106617ms step_avg:97.46ms
step:1105/1770 train_time:106717ms step_avg:97.46ms
step:1106/1770 train_time:106818ms step_avg:97.46ms
step:1107/1770 train_time:106919ms step_avg:97.46ms
step:1108/1770 train_time:107020ms step_avg:97.47ms
step:1109/1770 train_time:107122ms step_avg:97.47ms
step:1110/1770 train_time:107223ms step_avg:97.48ms
step:1111/1770 train_time:107325ms step_avg:97.48ms
step:1112/1770 train_time:107427ms step_avg:97.48ms
step:1113/1770 train_time:107528ms step_avg:97.49ms
step:1114/1770 train_time:107629ms step_avg:97.49ms
step:1115/1770 train_time:107731ms step_avg:97.49ms
step:1116/1770 train_time:107832ms step_avg:97.50ms
step:1117/1770 train_time:107933ms step_avg:97.50ms
step:1118/1770 train_time:108034ms step_avg:97.50ms
step:1119/1770 train_time:108137ms step_avg:97.51ms
step:1120/1770 train_time:108239ms step_avg:97.51ms
step:1121/1770 train_time:108340ms step_avg:97.52ms
step:1122/1770 train_time:108440ms step_avg:97.52ms
step:1123/1770 train_time:108541ms step_avg:97.52ms
step:1124/1770 train_time:108643ms step_avg:97.52ms
step:1125/1770 train_time:108743ms step_avg:97.53ms
step:1125/1770 val_loss:3.4725 train_time:108842ms step_avg:97.62ms
step:1126/1770 train_time:108863ms step_avg:97.55ms
step:1127/1770 train_time:108959ms step_avg:97.55ms
step:1128/1770 train_time:109060ms step_avg:97.55ms
step:1129/1770 train_time:109161ms step_avg:97.55ms
step:1130/1770 train_time:109262ms step_avg:97.56ms
step:1131/1770 train_time:109362ms step_avg:97.56ms
step:1132/1770 train_time:109463ms step_avg:97.56ms
step:1133/1770 train_time:109564ms step_avg:97.56ms
step:1134/1770 train_time:109665ms step_avg:97.57ms
step:1135/1770 train_time:109765ms step_avg:97.57ms
step:1136/1770 train_time:109867ms step_avg:97.57ms
step:1137/1770 train_time:109971ms step_avg:97.58ms
step:1138/1770 train_time:110071ms step_avg:97.58ms
step:1139/1770 train_time:110172ms step_avg:97.58ms
step:1140/1770 train_time:110273ms step_avg:97.59ms
step:1141/1770 train_time:110373ms step_avg:97.59ms
step:1142/1770 train_time:110474ms step_avg:97.59ms
step:1143/1770 train_time:110575ms step_avg:97.60ms
step:1144/1770 train_time:110676ms step_avg:97.60ms
step:1145/1770 train_time:110777ms step_avg:97.60ms
step:1146/1770 train_time:110878ms step_avg:97.60ms
step:1147/1770 train_time:110979ms step_avg:97.61ms
step:1148/1770 train_time:111080ms step_avg:97.61ms
step:1149/1770 train_time:111181ms step_avg:97.61ms
step:1150/1770 train_time:111282ms step_avg:97.62ms
step:1151/1770 train_time:111384ms step_avg:97.62ms
step:1152/1770 train_time:111486ms step_avg:97.62ms
step:1153/1770 train_time:111588ms step_avg:97.63ms
step:1154/1770 train_time:111689ms step_avg:97.63ms
step:1155/1770 train_time:111790ms step_avg:97.63ms
step:1156/1770 train_time:111891ms step_avg:97.64ms
step:1157/1770 train_time:111992ms step_avg:97.64ms
step:1158/1770 train_time:112094ms step_avg:97.64ms
step:1159/1770 train_time:112195ms step_avg:97.65ms
step:1160/1770 train_time:112296ms step_avg:97.65ms
step:1161/1770 train_time:112397ms step_avg:97.65ms
step:1162/1770 train_time:112498ms step_avg:97.65ms
step:1163/1770 train_time:112599ms step_avg:97.66ms
step:1164/1770 train_time:112701ms step_avg:97.66ms
step:1165/1770 train_time:112802ms step_avg:97.66ms
step:1166/1770 train_time:112904ms step_avg:97.67ms
step:1167/1770 train_time:113005ms step_avg:97.67ms
step:1168/1770 train_time:113106ms step_avg:97.67ms
step:1169/1770 train_time:113207ms step_avg:97.68ms
step:1170/1770 train_time:113308ms step_avg:97.68ms
step:1171/1770 train_time:113410ms step_avg:97.68ms
step:1172/1770 train_time:113512ms step_avg:97.69ms
step:1173/1770 train_time:113612ms step_avg:97.69ms
step:1174/1770 train_time:113713ms step_avg:97.69ms
step:1175/1770 train_time:113814ms step_avg:97.69ms
step:1176/1770 train_time:113916ms step_avg:97.70ms
step:1177/1770 train_time:114017ms step_avg:97.70ms
step:1178/1770 train_time:114118ms step_avg:97.70ms
step:1179/1770 train_time:114219ms step_avg:97.71ms
step:1180/1770 train_time:114319ms step_avg:97.71ms
step:1181/1770 train_time:114420ms step_avg:97.71ms
step:1182/1770 train_time:114521ms step_avg:97.71ms
step:1183/1770 train_time:114623ms step_avg:97.72ms
step:1184/1770 train_time:114727ms step_avg:97.72ms
step:1185/1770 train_time:114828ms step_avg:97.73ms
step:1186/1770 train_time:114931ms step_avg:97.73ms
step:1187/1770 train_time:115035ms step_avg:97.74ms
step:1188/1770 train_time:115136ms step_avg:97.74ms
step:1189/1770 train_time:115238ms step_avg:97.74ms
step:1190/1770 train_time:115339ms step_avg:97.75ms
step:1191/1770 train_time:115442ms step_avg:97.75ms
step:1192/1770 train_time:115545ms step_avg:97.75ms
step:1193/1770 train_time:115648ms step_avg:97.76ms
step:1194/1770 train_time:115750ms step_avg:97.76ms
step:1195/1770 train_time:115853ms step_avg:97.77ms
step:1196/1770 train_time:115956ms step_avg:97.77ms
step:1197/1770 train_time:116058ms step_avg:97.77ms
step:1198/1770 train_time:116160ms step_avg:97.78ms
step:1199/1770 train_time:116262ms step_avg:97.78ms
step:1200/1770 train_time:116365ms step_avg:97.79ms
step:1201/1770 train_time:116468ms step_avg:97.79ms
step:1202/1770 train_time:116569ms step_avg:97.79ms
step:1203/1770 train_time:116671ms step_avg:97.80ms
step:1204/1770 train_time:116774ms step_avg:97.80ms
step:1205/1770 train_time:116875ms step_avg:97.80ms
step:1206/1770 train_time:116977ms step_avg:97.81ms
step:1207/1770 train_time:117079ms step_avg:97.81ms
step:1208/1770 train_time:117180ms step_avg:97.81ms
step:1209/1770 train_time:117281ms step_avg:97.82ms
step:1210/1770 train_time:117384ms step_avg:97.82ms
step:1211/1770 train_time:117486ms step_avg:97.82ms
step:1212/1770 train_time:117589ms step_avg:97.83ms
step:1213/1770 train_time:117691ms step_avg:97.83ms
step:1214/1770 train_time:117793ms step_avg:97.83ms
step:1215/1770 train_time:117895ms step_avg:97.84ms
step:1216/1770 train_time:117999ms step_avg:97.84ms
step:1217/1770 train_time:118100ms step_avg:97.85ms
step:1218/1770 train_time:118202ms step_avg:97.85ms
step:1219/1770 train_time:118304ms step_avg:97.85ms
step:1220/1770 train_time:118406ms step_avg:97.86ms
step:1221/1770 train_time:118507ms step_avg:97.86ms
step:1222/1770 train_time:118611ms step_avg:97.86ms
step:1223/1770 train_time:118713ms step_avg:97.87ms
step:1224/1770 train_time:118815ms step_avg:97.87ms
step:1225/1770 train_time:118918ms step_avg:97.87ms
step:1226/1770 train_time:119020ms step_avg:97.88ms
step:1227/1770 train_time:119124ms step_avg:97.88ms
step:1228/1770 train_time:119228ms step_avg:97.89ms
step:1229/1770 train_time:119330ms step_avg:97.89ms
step:1230/1770 train_time:119432ms step_avg:97.89ms
step:1231/1770 train_time:119534ms step_avg:97.90ms
step:1232/1770 train_time:119636ms step_avg:97.90ms
step:1233/1770 train_time:119737ms step_avg:97.90ms
step:1234/1770 train_time:119839ms step_avg:97.91ms
step:1235/1770 train_time:119940ms step_avg:97.91ms
step:1236/1770 train_time:120043ms step_avg:97.91ms
step:1237/1770 train_time:120145ms step_avg:97.92ms
step:1238/1770 train_time:120247ms step_avg:97.92ms
step:1239/1770 train_time:120349ms step_avg:97.92ms
step:1240/1770 train_time:120452ms step_avg:97.93ms
step:1241/1770 train_time:120554ms step_avg:97.93ms
step:1242/1770 train_time:120655ms step_avg:97.93ms
step:1243/1770 train_time:120758ms step_avg:97.94ms
step:1244/1770 train_time:120859ms step_avg:97.94ms
step:1245/1770 train_time:120961ms step_avg:97.94ms
step:1246/1770 train_time:121063ms step_avg:97.95ms
step:1247/1770 train_time:121166ms step_avg:97.95ms
step:1248/1770 train_time:121269ms step_avg:97.96ms
step:1249/1770 train_time:121371ms step_avg:97.96ms
step:1250/1770 train_time:121472ms step_avg:97.96ms
step:1250/1770 val_loss:3.4247 train_time:121575ms step_avg:98.04ms
step:1251/1770 train_time:121596ms step_avg:97.98ms
step:1252/1770 train_time:121689ms step_avg:97.98ms
step:1253/1770 train_time:121792ms step_avg:97.98ms
step:1254/1770 train_time:121895ms step_avg:97.99ms
step:1255/1770 train_time:121998ms step_avg:97.99ms
step:1256/1770 train_time:122099ms step_avg:97.99ms
step:1257/1770 train_time:122200ms step_avg:98.00ms
step:1258/1770 train_time:122303ms step_avg:98.00ms
step:1259/1770 train_time:122405ms step_avg:98.00ms
step:1260/1770 train_time:122506ms step_avg:98.00ms
step:1261/1770 train_time:122608ms step_avg:98.01ms
step:1262/1770 train_time:122713ms step_avg:98.01ms
step:1263/1770 train_time:122815ms step_avg:98.02ms
step:1264/1770 train_time:122918ms step_avg:98.02ms
step:1265/1770 train_time:123019ms step_avg:98.02ms
step:1266/1770 train_time:123121ms step_avg:98.03ms
step:1267/1770 train_time:123223ms step_avg:98.03ms
step:1268/1770 train_time:123325ms step_avg:98.03ms
step:1269/1770 train_time:123427ms step_avg:98.04ms
step:1270/1770 train_time:123530ms step_avg:98.04ms
step:1271/1770 train_time:123632ms step_avg:98.04ms
step:1272/1770 train_time:123733ms step_avg:98.05ms
step:1273/1770 train_time:123836ms step_avg:98.05ms
step:1274/1770 train_time:123940ms step_avg:98.05ms
step:1275/1770 train_time:124041ms step_avg:98.06ms
step:1276/1770 train_time:124144ms step_avg:98.06ms
step:1277/1770 train_time:124245ms step_avg:98.06ms
step:1278/1770 train_time:124348ms step_avg:98.07ms
step:1279/1770 train_time:124450ms step_avg:98.07ms
step:1280/1770 train_time:124554ms step_avg:98.07ms
step:1281/1770 train_time:124655ms step_avg:98.08ms
step:1282/1770 train_time:124758ms step_avg:98.08ms
step:1283/1770 train_time:124861ms step_avg:98.08ms
step:1284/1770 train_time:124963ms step_avg:98.09ms
step:1285/1770 train_time:125065ms step_avg:98.09ms
step:1286/1770 train_time:125168ms step_avg:98.09ms
step:1287/1770 train_time:125271ms step_avg:98.10ms
step:1288/1770 train_time:125374ms step_avg:98.10ms
step:1289/1770 train_time:125477ms step_avg:98.11ms
step:1290/1770 train_time:125578ms step_avg:98.11ms
step:1291/1770 train_time:125680ms step_avg:98.11ms
step:1292/1770 train_time:125782ms step_avg:98.11ms
step:1293/1770 train_time:125884ms step_avg:98.12ms
step:1294/1770 train_time:125985ms step_avg:98.12ms
step:1295/1770 train_time:126087ms step_avg:98.12ms
step:1296/1770 train_time:126189ms step_avg:98.13ms
step:1297/1770 train_time:126291ms step_avg:98.13ms
step:1298/1770 train_time:126393ms step_avg:98.13ms
step:1299/1770 train_time:126495ms step_avg:98.13ms
step:1300/1770 train_time:126597ms step_avg:98.14ms
step:1301/1770 train_time:126700ms step_avg:98.14ms
step:1302/1770 train_time:126802ms step_avg:98.14ms
step:1303/1770 train_time:126903ms step_avg:98.15ms
step:1304/1770 train_time:127004ms step_avg:98.15ms
step:1305/1770 train_time:127106ms step_avg:98.15ms
step:1306/1770 train_time:127208ms step_avg:98.15ms
step:1307/1770 train_time:127310ms step_avg:98.16ms
step:1308/1770 train_time:127411ms step_avg:98.16ms
step:1309/1770 train_time:127513ms step_avg:98.16ms
step:1310/1770 train_time:127616ms step_avg:98.17ms
step:1311/1770 train_time:127718ms step_avg:98.17ms
step:1312/1770 train_time:127820ms step_avg:98.17ms
step:1313/1770 train_time:127921ms step_avg:98.17ms
step:1314/1770 train_time:128023ms step_avg:98.18ms
step:1315/1770 train_time:128124ms step_avg:98.18ms
step:1316/1770 train_time:128227ms step_avg:98.18ms
step:1317/1770 train_time:128329ms step_avg:98.19ms
step:1318/1770 train_time:128434ms step_avg:98.19ms
step:1319/1770 train_time:128537ms step_avg:98.19ms
step:1320/1770 train_time:128638ms step_avg:98.20ms
step:1321/1770 train_time:128741ms step_avg:98.20ms
step:1322/1770 train_time:128842ms step_avg:98.20ms
step:1323/1770 train_time:128945ms step_avg:98.21ms
step:1324/1770 train_time:129047ms step_avg:98.21ms
step:1325/1770 train_time:129150ms step_avg:98.21ms
step:1326/1770 train_time:129252ms step_avg:98.22ms
step:1327/1770 train_time:129356ms step_avg:98.22ms
step:1328/1770 train_time:129458ms step_avg:98.22ms
step:1329/1770 train_time:129559ms step_avg:98.23ms
step:1330/1770 train_time:129661ms step_avg:98.23ms
step:1331/1770 train_time:129762ms step_avg:98.23ms
step:1332/1770 train_time:129864ms step_avg:98.23ms
step:1333/1770 train_time:129965ms step_avg:98.23ms
step:1334/1770 train_time:130067ms step_avg:98.24ms
step:1335/1770 train_time:130169ms step_avg:98.24ms
step:1336/1770 train_time:130272ms step_avg:98.24ms
step:1337/1770 train_time:130375ms step_avg:98.25ms
step:1338/1770 train_time:130477ms step_avg:98.25ms
step:1339/1770 train_time:130579ms step_avg:98.25ms
step:1340/1770 train_time:130683ms step_avg:98.26ms
step:1341/1770 train_time:130785ms step_avg:98.26ms
step:1342/1770 train_time:130887ms step_avg:98.26ms
step:1343/1770 train_time:130990ms step_avg:98.27ms
step:1344/1770 train_time:131094ms step_avg:98.27ms
step:1345/1770 train_time:131195ms step_avg:98.27ms
step:1346/1770 train_time:131297ms step_avg:98.28ms
step:1347/1770 train_time:131400ms step_avg:98.28ms
step:1348/1770 train_time:131504ms step_avg:98.28ms
step:1349/1770 train_time:131606ms step_avg:98.29ms
step:1350/1770 train_time:131708ms step_avg:98.29ms
step:1351/1770 train_time:131810ms step_avg:98.29ms
step:1352/1770 train_time:131912ms step_avg:98.29ms
step:1353/1770 train_time:132015ms step_avg:98.30ms
step:1354/1770 train_time:132116ms step_avg:98.30ms
step:1355/1770 train_time:132218ms step_avg:98.30ms
step:1356/1770 train_time:132321ms step_avg:98.31ms
step:1357/1770 train_time:132423ms step_avg:98.31ms
step:1358/1770 train_time:132525ms step_avg:98.31ms
step:1359/1770 train_time:132627ms step_avg:98.32ms
step:1360/1770 train_time:132730ms step_avg:98.32ms
step:1361/1770 train_time:132832ms step_avg:98.32ms
step:1362/1770 train_time:132934ms step_avg:98.32ms
step:1363/1770 train_time:133036ms step_avg:98.33ms
step:1364/1770 train_time:133139ms step_avg:98.33ms
step:1365/1770 train_time:133241ms step_avg:98.33ms
step:1366/1770 train_time:133343ms step_avg:98.34ms
step:1367/1770 train_time:133446ms step_avg:98.34ms
step:1368/1770 train_time:133547ms step_avg:98.34ms
step:1369/1770 train_time:133650ms step_avg:98.34ms
step:1370/1770 train_time:133753ms step_avg:98.35ms
step:1371/1770 train_time:133855ms step_avg:98.35ms
step:1372/1770 train_time:133956ms step_avg:98.35ms
step:1373/1770 train_time:134059ms step_avg:98.36ms
step:1374/1770 train_time:134162ms step_avg:98.36ms
step:1375/1770 train_time:134265ms step_avg:98.36ms
step:1375/1770 val_loss:3.3816 train_time:134365ms step_avg:98.44ms
step:1376/1770 train_time:134386ms step_avg:98.38ms
step:1377/1770 train_time:134480ms step_avg:98.38ms
step:1378/1770 train_time:134585ms step_avg:98.38ms
step:1379/1770 train_time:134685ms step_avg:98.38ms
step:1380/1770 train_time:134786ms step_avg:98.38ms
step:1381/1770 train_time:134889ms step_avg:98.39ms
step:1382/1770 train_time:134990ms step_avg:98.39ms
step:1383/1770 train_time:135092ms step_avg:98.39ms
step:1384/1770 train_time:135193ms step_avg:98.39ms
step:1385/1770 train_time:135295ms step_avg:98.40ms
step:1386/1770 train_time:135398ms step_avg:98.40ms
step:1387/1770 train_time:135502ms step_avg:98.40ms
step:1388/1770 train_time:135604ms step_avg:98.41ms
step:1389/1770 train_time:135707ms step_avg:98.41ms
step:1390/1770 train_time:135808ms step_avg:98.41ms
step:1391/1770 train_time:135910ms step_avg:98.41ms
step:1392/1770 train_time:136012ms step_avg:98.42ms
step:1393/1770 train_time:136114ms step_avg:98.42ms
step:1394/1770 train_time:136216ms step_avg:98.42ms
step:1395/1770 train_time:136319ms step_avg:98.43ms
step:1396/1770 train_time:136422ms step_avg:98.43ms
step:1397/1770 train_time:136524ms step_avg:98.43ms
step:1398/1770 train_time:136627ms step_avg:98.43ms
step:1399/1770 train_time:136729ms step_avg:98.44ms
step:1400/1770 train_time:136832ms step_avg:98.44ms
step:1401/1770 train_time:136933ms step_avg:98.44ms
step:1402/1770 train_time:137035ms step_avg:98.44ms
step:1403/1770 train_time:137137ms step_avg:98.45ms
step:1404/1770 train_time:137240ms step_avg:98.45ms
step:1405/1770 train_time:137341ms step_avg:98.45ms
step:1406/1770 train_time:137444ms step_avg:98.46ms
step:1407/1770 train_time:137546ms step_avg:98.46ms
step:1408/1770 train_time:137648ms step_avg:98.46ms
step:1409/1770 train_time:137751ms step_avg:98.46ms
step:1410/1770 train_time:137853ms step_avg:98.47ms
step:1411/1770 train_time:137955ms step_avg:98.47ms
step:1412/1770 train_time:138056ms step_avg:98.47ms
step:1413/1770 train_time:138158ms step_avg:98.47ms
step:1414/1770 train_time:138261ms step_avg:98.48ms
step:1415/1770 train_time:138365ms step_avg:98.48ms
step:1416/1770 train_time:138468ms step_avg:98.48ms
step:1417/1770 train_time:138570ms step_avg:98.49ms
step:1418/1770 train_time:138672ms step_avg:98.49ms
step:1419/1770 train_time:138774ms step_avg:98.49ms
step:1420/1770 train_time:138876ms step_avg:98.49ms
step:1421/1770 train_time:138977ms step_avg:98.50ms
step:1422/1770 train_time:139079ms step_avg:98.50ms
step:1423/1770 train_time:139181ms step_avg:98.50ms
step:1424/1770 train_time:139284ms step_avg:98.50ms
step:1425/1770 train_time:139386ms step_avg:98.51ms
step:1426/1770 train_time:139489ms step_avg:98.51ms
step:1427/1770 train_time:139591ms step_avg:98.51ms
step:1428/1770 train_time:139694ms step_avg:98.52ms
step:1429/1770 train_time:139796ms step_avg:98.52ms
step:1430/1770 train_time:139898ms step_avg:98.52ms
step:1431/1770 train_time:140001ms step_avg:98.52ms
step:1432/1770 train_time:140104ms step_avg:98.53ms
step:1433/1770 train_time:140206ms step_avg:98.53ms
step:1434/1770 train_time:140307ms step_avg:98.53ms
step:1435/1770 train_time:140409ms step_avg:98.53ms
step:1436/1770 train_time:140512ms step_avg:98.54ms
step:1437/1770 train_time:140615ms step_avg:98.54ms
step:1438/1770 train_time:140716ms step_avg:98.54ms
step:1439/1770 train_time:140819ms step_avg:98.54ms
step:1440/1770 train_time:140921ms step_avg:98.55ms
step:1441/1770 train_time:141025ms step_avg:98.55ms
step:1442/1770 train_time:141128ms step_avg:98.55ms
step:1443/1770 train_time:141230ms step_avg:98.56ms
step:1444/1770 train_time:141332ms step_avg:98.56ms
step:1445/1770 train_time:141435ms step_avg:98.56ms
step:1446/1770 train_time:141538ms step_avg:98.56ms
step:1447/1770 train_time:141641ms step_avg:98.57ms
step:1448/1770 train_time:141745ms step_avg:98.57ms
step:1449/1770 train_time:141849ms step_avg:98.57ms
step:1450/1770 train_time:141952ms step_avg:98.58ms
step:1451/1770 train_time:142055ms step_avg:98.58ms
step:1452/1770 train_time:142159ms step_avg:98.58ms
step:1453/1770 train_time:142262ms step_avg:98.59ms
step:1454/1770 train_time:142365ms step_avg:98.59ms
step:1455/1770 train_time:142469ms step_avg:98.59ms
step:1456/1770 train_time:142574ms step_avg:98.60ms
step:1457/1770 train_time:142678ms step_avg:98.60ms
step:1458/1770 train_time:142781ms step_avg:98.61ms
step:1459/1770 train_time:142885ms step_avg:98.61ms
step:1460/1770 train_time:142988ms step_avg:98.61ms
step:1461/1770 train_time:143092ms step_avg:98.62ms
step:1462/1770 train_time:143196ms step_avg:98.62ms
step:1463/1770 train_time:143300ms step_avg:98.62ms
step:1464/1770 train_time:143407ms step_avg:98.63ms
step:1465/1770 train_time:143511ms step_avg:98.63ms
step:1466/1770 train_time:143614ms step_avg:98.64ms
step:1467/1770 train_time:143719ms step_avg:98.64ms
step:1468/1770 train_time:143821ms step_avg:98.64ms
step:1469/1770 train_time:143923ms step_avg:98.65ms
step:1470/1770 train_time:144026ms step_avg:98.65ms
step:1471/1770 train_time:144128ms step_avg:98.65ms
step:1472/1770 train_time:144232ms step_avg:98.65ms
step:1473/1770 train_time:144336ms step_avg:98.66ms
step:1474/1770 train_time:144440ms step_avg:98.66ms
step:1475/1770 train_time:144543ms step_avg:98.66ms
step:1476/1770 train_time:144647ms step_avg:98.67ms
step:1477/1770 train_time:144753ms step_avg:98.67ms
step:1478/1770 train_time:144857ms step_avg:98.68ms
step:1479/1770 train_time:144960ms step_avg:98.68ms
step:1480/1770 train_time:145062ms step_avg:98.68ms
step:1481/1770 train_time:145169ms step_avg:98.69ms
step:1482/1770 train_time:145272ms step_avg:98.69ms
step:1483/1770 train_time:145375ms step_avg:98.69ms
step:1484/1770 train_time:145478ms step_avg:98.70ms
step:1485/1770 train_time:145581ms step_avg:98.70ms
step:1486/1770 train_time:145685ms step_avg:98.70ms
step:1487/1770 train_time:145788ms step_avg:98.71ms
step:1488/1770 train_time:145891ms step_avg:98.71ms
step:1489/1770 train_time:145996ms step_avg:98.71ms
step:1490/1770 train_time:146101ms step_avg:98.72ms
step:1491/1770 train_time:146204ms step_avg:98.72ms
step:1492/1770 train_time:146308ms step_avg:98.72ms
step:1493/1770 train_time:146413ms step_avg:98.73ms
step:1494/1770 train_time:146520ms step_avg:98.73ms
step:1495/1770 train_time:146622ms step_avg:98.74ms
step:1496/1770 train_time:146725ms step_avg:98.74ms
step:1497/1770 train_time:146829ms step_avg:98.74ms
step:1498/1770 train_time:146932ms step_avg:98.74ms
step:1499/1770 train_time:147035ms step_avg:98.75ms
step:1500/1770 train_time:147137ms step_avg:98.75ms
step:1500/1770 val_loss:3.3436 train_time:147239ms step_avg:98.82ms
step:1501/1770 train_time:147259ms step_avg:98.77ms
step:1502/1770 train_time:147352ms step_avg:98.76ms
step:1503/1770 train_time:147455ms step_avg:98.76ms
step:1504/1770 train_time:147559ms step_avg:98.77ms
step:1505/1770 train_time:147664ms step_avg:98.77ms
step:1506/1770 train_time:147767ms step_avg:98.77ms
step:1507/1770 train_time:147870ms step_avg:98.78ms
step:1508/1770 train_time:147975ms step_avg:98.78ms
step:1509/1770 train_time:148078ms step_avg:98.78ms
step:1510/1770 train_time:148180ms step_avg:98.79ms
step:1511/1770 train_time:148284ms step_avg:98.79ms
step:1512/1770 train_time:148389ms step_avg:98.79ms
step:1513/1770 train_time:148494ms step_avg:98.80ms
step:1514/1770 train_time:148597ms step_avg:98.80ms
step:1515/1770 train_time:148700ms step_avg:98.80ms
step:1516/1770 train_time:148803ms step_avg:98.81ms
step:1517/1770 train_time:148906ms step_avg:98.81ms
step:1518/1770 train_time:149012ms step_avg:98.81ms
step:1519/1770 train_time:149114ms step_avg:98.82ms
step:1520/1770 train_time:149218ms step_avg:98.82ms
step:1521/1770 train_time:149321ms step_avg:98.82ms
step:1522/1770 train_time:149425ms step_avg:98.83ms
step:1523/1770 train_time:149529ms step_avg:98.83ms
step:1524/1770 train_time:149632ms step_avg:98.83ms
step:1525/1770 train_time:149735ms step_avg:98.84ms
step:1526/1770 train_time:149838ms step_avg:98.84ms
step:1527/1770 train_time:149941ms step_avg:98.84ms
step:1528/1770 train_time:150047ms step_avg:98.84ms
step:1529/1770 train_time:150149ms step_avg:98.85ms
step:1530/1770 train_time:150253ms step_avg:98.85ms
step:1531/1770 train_time:150356ms step_avg:98.85ms
step:1532/1770 train_time:150459ms step_avg:98.86ms
step:1533/1770 train_time:150563ms step_avg:98.86ms
step:1534/1770 train_time:150668ms step_avg:98.86ms
step:1535/1770 train_time:150771ms step_avg:98.87ms
step:1536/1770 train_time:150873ms step_avg:98.87ms
step:1537/1770 train_time:150977ms step_avg:98.87ms
step:1538/1770 train_time:151082ms step_avg:98.88ms
step:1539/1770 train_time:151185ms step_avg:98.88ms
step:1540/1770 train_time:151291ms step_avg:98.88ms
step:1541/1770 train_time:151395ms step_avg:98.89ms
step:1542/1770 train_time:151499ms step_avg:98.89ms
step:1543/1770 train_time:151601ms step_avg:98.89ms
step:1544/1770 train_time:151707ms step_avg:98.90ms
step:1545/1770 train_time:151810ms step_avg:98.90ms
step:1546/1770 train_time:151913ms step_avg:98.90ms
step:1547/1770 train_time:152016ms step_avg:98.90ms
step:1548/1770 train_time:152118ms step_avg:98.91ms
step:1549/1770 train_time:152221ms step_avg:98.91ms
step:1550/1770 train_time:152324ms step_avg:98.91ms
step:1551/1770 train_time:152427ms step_avg:98.91ms
step:1552/1770 train_time:152532ms step_avg:98.92ms
step:1553/1770 train_time:152635ms step_avg:98.92ms
step:1554/1770 train_time:152738ms step_avg:98.92ms
step:1555/1770 train_time:152841ms step_avg:98.93ms
step:1556/1770 train_time:152945ms step_avg:98.93ms
step:1557/1770 train_time:153049ms step_avg:98.93ms
step:1558/1770 train_time:153152ms step_avg:98.94ms
step:1559/1770 train_time:153255ms step_avg:98.94ms
step:1560/1770 train_time:153358ms step_avg:98.94ms
step:1561/1770 train_time:153463ms step_avg:98.94ms
step:1562/1770 train_time:153566ms step_avg:98.95ms
step:1563/1770 train_time:153669ms step_avg:98.95ms
step:1564/1770 train_time:153773ms step_avg:98.95ms
step:1565/1770 train_time:153877ms step_avg:98.96ms
step:1566/1770 train_time:153980ms step_avg:98.96ms
step:1567/1770 train_time:154084ms step_avg:98.96ms
step:1568/1770 train_time:154186ms step_avg:98.96ms
step:1569/1770 train_time:154293ms step_avg:98.97ms
step:1570/1770 train_time:154396ms step_avg:98.97ms
step:1571/1770 train_time:154500ms step_avg:98.97ms
step:1572/1770 train_time:154604ms step_avg:98.98ms
step:1573/1770 train_time:154709ms step_avg:98.98ms
step:1574/1770 train_time:154814ms step_avg:98.99ms
step:1575/1770 train_time:154916ms step_avg:98.99ms
step:1576/1770 train_time:155019ms step_avg:98.99ms
step:1577/1770 train_time:155123ms step_avg:98.99ms
step:1578/1770 train_time:155228ms step_avg:99.00ms
step:1579/1770 train_time:155332ms step_avg:99.00ms
step:1580/1770 train_time:155435ms step_avg:99.00ms
step:1581/1770 train_time:155541ms step_avg:99.01ms
step:1582/1770 train_time:155647ms step_avg:99.01ms
step:1583/1770 train_time:155750ms step_avg:99.01ms
step:1584/1770 train_time:155855ms step_avg:99.02ms
step:1585/1770 train_time:155957ms step_avg:99.02ms
step:1586/1770 train_time:156065ms step_avg:99.03ms
step:1587/1770 train_time:156169ms step_avg:99.03ms
step:1588/1770 train_time:156272ms step_avg:99.03ms
step:1589/1770 train_time:156377ms step_avg:99.04ms
step:1590/1770 train_time:156480ms step_avg:99.04ms
step:1591/1770 train_time:156582ms step_avg:99.04ms
step:1592/1770 train_time:156687ms step_avg:99.04ms
step:1593/1770 train_time:156789ms step_avg:99.05ms
step:1594/1770 train_time:156892ms step_avg:99.05ms
step:1595/1770 train_time:156996ms step_avg:99.05ms
step:1596/1770 train_time:157100ms step_avg:99.05ms
step:1597/1770 train_time:157204ms step_avg:99.06ms
step:1598/1770 train_time:157307ms step_avg:99.06ms
step:1599/1770 train_time:157411ms step_avg:99.06ms
step:1600/1770 train_time:157517ms step_avg:99.07ms
step:1601/1770 train_time:157620ms step_avg:99.07ms
step:1602/1770 train_time:157724ms step_avg:99.07ms
step:1603/1770 train_time:157827ms step_avg:99.08ms
step:1604/1770 train_time:157931ms step_avg:99.08ms
step:1605/1770 train_time:158038ms step_avg:99.08ms
step:1606/1770 train_time:158137ms step_avg:99.08ms
step:1607/1770 train_time:158243ms step_avg:99.09ms
step:1608/1770 train_time:158347ms step_avg:99.09ms
step:1609/1770 train_time:158450ms step_avg:99.09ms
step:1610/1770 train_time:158555ms step_avg:99.10ms
step:1611/1770 train_time:158660ms step_avg:99.10ms
step:1612/1770 train_time:158764ms step_avg:99.10ms
step:1613/1770 train_time:158867ms step_avg:99.11ms
step:1614/1770 train_time:158971ms step_avg:99.11ms
step:1615/1770 train_time:159074ms step_avg:99.11ms
step:1616/1770 train_time:159177ms step_avg:99.11ms
step:1617/1770 train_time:159282ms step_avg:99.12ms
step:1618/1770 train_time:159387ms step_avg:99.12ms
step:1619/1770 train_time:159490ms step_avg:99.12ms
step:1620/1770 train_time:159595ms step_avg:99.13ms
step:1621/1770 train_time:159700ms step_avg:99.13ms
step:1622/1770 train_time:159803ms step_avg:99.13ms
step:1623/1770 train_time:159909ms step_avg:99.14ms
step:1624/1770 train_time:160012ms step_avg:99.14ms
step:1625/1770 train_time:160114ms step_avg:99.14ms
step:1625/1770 val_loss:3.3090 train_time:160216ms step_avg:99.20ms
step:1626/1770 train_time:160237ms step_avg:99.16ms
step:1627/1770 train_time:160330ms step_avg:99.15ms
step:1628/1770 train_time:160433ms step_avg:99.16ms
step:1629/1770 train_time:160536ms step_avg:99.16ms
step:1630/1770 train_time:160639ms step_avg:99.16ms
step:1631/1770 train_time:160742ms step_avg:99.16ms
step:1632/1770 train_time:160845ms step_avg:99.16ms
step:1633/1770 train_time:160948ms step_avg:99.17ms
step:1634/1770 train_time:161051ms step_avg:99.17ms
step:1635/1770 train_time:161154ms step_avg:99.17ms
step:1636/1770 train_time:161258ms step_avg:99.17ms
step:1637/1770 train_time:161364ms step_avg:99.18ms
step:1638/1770 train_time:161468ms step_avg:99.18ms
step:1639/1770 train_time:161571ms step_avg:99.18ms
step:1640/1770 train_time:161676ms step_avg:99.19ms
step:1641/1770 train_time:161778ms step_avg:99.19ms
step:1642/1770 train_time:161881ms step_avg:99.19ms
step:1643/1770 train_time:161984ms step_avg:99.19ms
step:1644/1770 train_time:162089ms step_avg:99.20ms
step:1645/1770 train_time:162192ms step_avg:99.20ms
step:1646/1770 train_time:162297ms step_avg:99.20ms
step:1647/1770 train_time:162403ms step_avg:99.21ms
step:1648/1770 train_time:162507ms step_avg:99.21ms
step:1649/1770 train_time:162610ms step_avg:99.21ms
step:1650/1770 train_time:162713ms step_avg:99.22ms
step:1651/1770 train_time:162815ms step_avg:99.22ms
step:1652/1770 train_time:162919ms step_avg:99.22ms
step:1653/1770 train_time:163022ms step_avg:99.22ms
step:1654/1770 train_time:163129ms step_avg:99.23ms
step:1655/1770 train_time:163236ms step_avg:99.23ms
step:1656/1770 train_time:163339ms step_avg:99.23ms
step:1657/1770 train_time:163445ms step_avg:99.24ms
step:1658/1770 train_time:163549ms step_avg:99.24ms
step:1659/1770 train_time:163653ms step_avg:99.24ms
step:1660/1770 train_time:163756ms step_avg:99.25ms
step:1661/1770 train_time:163860ms step_avg:99.25ms
step:1662/1770 train_time:163964ms step_avg:99.25ms
step:1663/1770 train_time:164067ms step_avg:99.25ms
step:1664/1770 train_time:164170ms step_avg:99.26ms
step:1665/1770 train_time:164272ms step_avg:99.26ms
step:1666/1770 train_time:164376ms step_avg:99.26ms
step:1667/1770 train_time:164479ms step_avg:99.26ms
step:1668/1770 train_time:164584ms step_avg:99.27ms
step:1669/1770 train_time:164686ms step_avg:99.27ms
step:1670/1770 train_time:164788ms step_avg:99.27ms
step:1671/1770 train_time:164892ms step_avg:99.27ms
step:1672/1770 train_time:164996ms step_avg:99.28ms
step:1673/1770 train_time:165101ms step_avg:99.28ms
step:1674/1770 train_time:165204ms step_avg:99.28ms
step:1675/1770 train_time:165308ms step_avg:99.28ms
step:1676/1770 train_time:165413ms step_avg:99.29ms
step:1677/1770 train_time:165521ms step_avg:99.29ms
step:1678/1770 train_time:165623ms step_avg:99.29ms
step:1679/1770 train_time:165726ms step_avg:99.30ms
step:1680/1770 train_time:165829ms step_avg:99.30ms
step:1681/1770 train_time:165934ms step_avg:99.30ms
step:1682/1770 train_time:166039ms step_avg:99.31ms
step:1683/1770 train_time:166142ms step_avg:99.31ms
step:1684/1770 train_time:166246ms step_avg:99.31ms
step:1685/1770 train_time:166348ms step_avg:99.31ms
step:1686/1770 train_time:166454ms step_avg:99.32ms
step:1687/1770 train_time:166560ms step_avg:99.32ms
step:1688/1770 train_time:166663ms step_avg:99.32ms
step:1689/1770 train_time:166766ms step_avg:99.32ms
step:1690/1770 train_time:166870ms step_avg:99.33ms
step:1691/1770 train_time:166973ms step_avg:99.33ms
step:1692/1770 train_time:167076ms step_avg:99.33ms
step:1693/1770 train_time:167181ms step_avg:99.33ms
step:1694/1770 train_time:167284ms step_avg:99.34ms
step:1695/1770 train_time:167388ms step_avg:99.34ms
step:1696/1770 train_time:167493ms step_avg:99.34ms
step:1697/1770 train_time:167598ms step_avg:99.35ms
step:1698/1770 train_time:167702ms step_avg:99.35ms
step:1699/1770 train_time:167804ms step_avg:99.35ms
step:1700/1770 train_time:167907ms step_avg:99.35ms
step:1701/1770 train_time:168010ms step_avg:99.36ms
step:1702/1770 train_time:168114ms step_avg:99.36ms
step:1703/1770 train_time:168217ms step_avg:99.36ms
step:1704/1770 train_time:168321ms step_avg:99.36ms
step:1705/1770 train_time:168426ms step_avg:99.37ms
step:1706/1770 train_time:168529ms step_avg:99.37ms
step:1707/1770 train_time:168633ms step_avg:99.37ms
step:1708/1770 train_time:168737ms step_avg:99.37ms
step:1709/1770 train_time:168842ms step_avg:99.38ms
step:1710/1770 train_time:168949ms step_avg:99.38ms
step:1711/1770 train_time:169055ms step_avg:99.39ms
step:1712/1770 train_time:169159ms step_avg:99.39ms
step:1713/1770 train_time:169263ms step_avg:99.39ms
step:1714/1770 train_time:169367ms step_avg:99.39ms
step:1715/1770 train_time:169470ms step_avg:99.40ms
step:1716/1770 train_time:169575ms step_avg:99.40ms
step:1717/1770 train_time:169678ms step_avg:99.40ms
step:1718/1770 train_time:169783ms step_avg:99.40ms
step:1719/1770 train_time:169888ms step_avg:99.41ms
step:1720/1770 train_time:169994ms step_avg:99.41ms
step:1721/1770 train_time:170098ms step_avg:99.41ms
step:1722/1770 train_time:170205ms step_avg:99.42ms
step:1723/1770 train_time:170310ms step_avg:99.42ms
step:1724/1770 train_time:170417ms step_avg:99.43ms
step:1725/1770 train_time:170523ms step_avg:99.43ms
step:1726/1770 train_time:170629ms step_avg:99.43ms
step:1727/1770 train_time:170732ms step_avg:99.44ms
step:1728/1770 train_time:170837ms step_avg:99.44ms
step:1729/1770 train_time:170942ms step_avg:99.44ms
step:1730/1770 train_time:171047ms step_avg:99.45ms
step:1731/1770 train_time:171153ms step_avg:99.45ms
step:1732/1770 train_time:171257ms step_avg:99.45ms
step:1733/1770 train_time:171363ms step_avg:99.46ms
step:1734/1770 train_time:171467ms step_avg:99.46ms
step:1735/1770 train_time:171571ms step_avg:99.46ms
step:1736/1770 train_time:171675ms step_avg:99.46ms
step:1737/1770 train_time:171780ms step_avg:99.47ms
step:1738/1770 train_time:171883ms step_avg:99.47ms
step:1739/1770 train_time:171988ms step_avg:99.47ms
step:1740/1770 train_time:172091ms step_avg:99.47ms
step:1741/1770 train_time:172198ms step_avg:99.48ms
step:1742/1770 train_time:172304ms step_avg:99.48ms
step:1743/1770 train_time:172410ms step_avg:99.49ms
step:1744/1770 train_time:172514ms step_avg:99.49ms
step:1745/1770 train_time:172617ms step_avg:99.49ms
step:1746/1770 train_time:172724ms step_avg:99.50ms
step:1747/1770 train_time:172826ms step_avg:99.50ms
step:1748/1770 train_time:172933ms step_avg:99.50ms
step:1749/1770 train_time:173038ms step_avg:99.50ms
step:1750/1770 train_time:173141ms step_avg:99.51ms
step:1750/1770 val_loss:3.2823 train_time:173244ms step_avg:99.57ms
step:1751/1770 train_time:173265ms step_avg:99.52ms
step:1752/1770 train_time:173358ms step_avg:99.52ms
step:1753/1770 train_time:173462ms step_avg:99.52ms
step:1754/1770 train_time:173567ms step_avg:99.52ms
step:1755/1770 train_time:173671ms step_avg:99.52ms
step:1756/1770 train_time:173775ms step_avg:99.53ms
step:1757/1770 train_time:173879ms step_avg:99.53ms
step:1758/1770 train_time:173983ms step_avg:99.53ms
step:1759/1770 train_time:174087ms step_avg:99.54ms
step:1760/1770 train_time:174191ms step_avg:99.54ms
step:1761/1770 train_time:174298ms step_avg:99.54ms
step:1762/1770 train_time:174406ms step_avg:99.55ms
step:1763/1770 train_time:174508ms step_avg:99.55ms
step:1764/1770 train_time:174614ms step_avg:99.55ms
step:1765/1770 train_time:174718ms step_avg:99.55ms
step:1766/1770 train_time:174827ms step_avg:99.56ms
step:1767/1770 train_time:174934ms step_avg:99.56ms
step:1768/1770 train_time:175034ms step_avg:99.56ms
step:1769/1770 train_time:175137ms step_avg:99.57ms
step:1770/1770 train_time:175241ms step_avg:99.57ms
step:1770/1770 val_loss:3.2793 train_time:175346ms step_avg:99.63ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
