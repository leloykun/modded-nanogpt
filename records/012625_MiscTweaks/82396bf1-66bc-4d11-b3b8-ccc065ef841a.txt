import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sat Jan 25 22:37:55 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24247ms step_avg:nanms
step:2/1770 train_time:24722ms step_avg:nanms
step:3/1770 train_time:24818ms step_avg:nanms
step:4/1770 train_time:24910ms step_avg:nanms
step:5/1770 train_time:25004ms step_avg:nanms
step:6/1770 train_time:25097ms step_avg:nanms
step:7/1770 train_time:25191ms step_avg:nanms
step:8/1770 train_time:25284ms step_avg:nanms
step:9/1770 train_time:25378ms step_avg:nanms
step:10/1770 train_time:25471ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:93.84ms
step:14/1770 train_time:377ms step_avg:94.33ms
step:15/1770 train_time:472ms step_avg:94.30ms
step:16/1770 train_time:565ms step_avg:94.20ms
step:17/1770 train_time:659ms step_avg:94.17ms
step:18/1770 train_time:753ms step_avg:94.12ms
step:19/1770 train_time:847ms step_avg:94.06ms
step:20/1770 train_time:940ms step_avg:94.01ms
step:21/1770 train_time:1034ms step_avg:93.98ms
step:22/1770 train_time:1128ms step_avg:93.96ms
step:23/1770 train_time:1221ms step_avg:93.94ms
step:24/1770 train_time:1316ms step_avg:94.01ms
step:25/1770 train_time:1411ms step_avg:94.04ms
step:26/1770 train_time:1505ms step_avg:94.05ms
step:27/1770 train_time:1599ms step_avg:94.07ms
step:28/1770 train_time:1693ms step_avg:94.08ms
step:29/1770 train_time:1788ms step_avg:94.08ms
step:30/1770 train_time:1882ms step_avg:94.10ms
step:31/1770 train_time:1976ms step_avg:94.08ms
step:32/1770 train_time:2070ms step_avg:94.11ms
step:33/1770 train_time:2164ms step_avg:94.09ms
step:34/1770 train_time:2258ms step_avg:94.09ms
step:35/1770 train_time:2352ms step_avg:94.09ms
step:36/1770 train_time:2446ms step_avg:94.07ms
step:37/1770 train_time:2540ms step_avg:94.08ms
step:38/1770 train_time:2634ms step_avg:94.08ms
step:39/1770 train_time:2728ms step_avg:94.08ms
step:40/1770 train_time:2822ms step_avg:94.08ms
step:41/1770 train_time:2916ms step_avg:94.07ms
step:42/1770 train_time:3010ms step_avg:94.07ms
step:43/1770 train_time:3104ms step_avg:94.06ms
step:44/1770 train_time:3198ms step_avg:94.06ms
step:45/1770 train_time:3292ms step_avg:94.05ms
step:46/1770 train_time:3386ms step_avg:94.05ms
step:47/1770 train_time:3480ms step_avg:94.05ms
step:48/1770 train_time:3574ms step_avg:94.05ms
step:49/1770 train_time:3668ms step_avg:94.06ms
step:50/1770 train_time:3762ms step_avg:94.05ms
step:51/1770 train_time:3856ms step_avg:94.05ms
step:52/1770 train_time:3951ms step_avg:94.06ms
step:53/1770 train_time:4044ms step_avg:94.04ms
step:54/1770 train_time:4138ms step_avg:94.04ms
step:55/1770 train_time:4231ms step_avg:94.03ms
step:56/1770 train_time:4325ms step_avg:94.02ms
step:57/1770 train_time:4419ms step_avg:94.02ms
step:58/1770 train_time:4513ms step_avg:94.02ms
step:59/1770 train_time:4607ms step_avg:94.01ms
step:60/1770 train_time:4700ms step_avg:94.01ms
step:61/1770 train_time:4795ms step_avg:94.01ms
step:62/1770 train_time:4889ms step_avg:94.02ms
step:63/1770 train_time:4983ms step_avg:94.02ms
step:64/1770 train_time:5077ms step_avg:94.01ms
step:65/1770 train_time:5171ms step_avg:94.01ms
step:66/1770 train_time:5264ms step_avg:94.00ms
step:67/1770 train_time:5358ms step_avg:94.00ms
step:68/1770 train_time:5452ms step_avg:93.99ms
step:69/1770 train_time:5545ms step_avg:93.99ms
step:70/1770 train_time:5640ms step_avg:93.99ms
step:71/1770 train_time:5733ms step_avg:93.99ms
step:72/1770 train_time:5827ms step_avg:93.99ms
step:73/1770 train_time:5922ms step_avg:94.00ms
step:74/1770 train_time:6016ms step_avg:94.00ms
step:75/1770 train_time:6110ms step_avg:93.99ms
step:76/1770 train_time:6204ms step_avg:93.99ms
step:77/1770 train_time:6297ms step_avg:93.99ms
step:78/1770 train_time:6391ms step_avg:93.99ms
step:79/1770 train_time:6485ms step_avg:93.99ms
step:80/1770 train_time:6579ms step_avg:93.99ms
step:81/1770 train_time:6673ms step_avg:93.98ms
step:82/1770 train_time:6767ms step_avg:93.98ms
step:83/1770 train_time:6861ms step_avg:93.99ms
step:84/1770 train_time:6956ms step_avg:94.00ms
step:85/1770 train_time:7050ms step_avg:94.00ms
step:86/1770 train_time:7144ms step_avg:94.00ms
step:87/1770 train_time:7237ms step_avg:93.99ms
step:88/1770 train_time:7331ms step_avg:93.99ms
step:89/1770 train_time:7425ms step_avg:93.99ms
step:90/1770 train_time:7519ms step_avg:93.98ms
step:91/1770 train_time:7612ms step_avg:93.98ms
step:92/1770 train_time:7706ms step_avg:93.97ms
step:93/1770 train_time:7800ms step_avg:93.97ms
step:94/1770 train_time:7894ms step_avg:93.97ms
step:95/1770 train_time:7987ms step_avg:93.97ms
step:96/1770 train_time:8082ms step_avg:93.97ms
step:97/1770 train_time:8176ms step_avg:93.98ms
step:98/1770 train_time:8270ms step_avg:93.98ms
step:99/1770 train_time:8364ms step_avg:93.97ms
step:100/1770 train_time:8457ms step_avg:93.97ms
step:101/1770 train_time:8551ms step_avg:93.96ms
step:102/1770 train_time:8644ms step_avg:93.96ms
step:103/1770 train_time:8738ms step_avg:93.95ms
step:104/1770 train_time:8832ms step_avg:93.96ms
step:105/1770 train_time:8926ms step_avg:93.96ms
step:106/1770 train_time:9020ms step_avg:93.95ms
step:107/1770 train_time:9114ms step_avg:93.96ms
step:108/1770 train_time:9209ms step_avg:93.97ms
step:109/1770 train_time:9303ms step_avg:93.97ms
step:110/1770 train_time:9397ms step_avg:93.97ms
step:111/1770 train_time:9492ms step_avg:93.98ms
step:112/1770 train_time:9585ms step_avg:93.98ms
step:113/1770 train_time:9679ms step_avg:93.97ms
step:114/1770 train_time:9773ms step_avg:93.97ms
step:115/1770 train_time:9867ms step_avg:93.97ms
step:116/1770 train_time:9961ms step_avg:93.97ms
step:117/1770 train_time:10055ms step_avg:93.97ms
step:118/1770 train_time:10149ms step_avg:93.97ms
step:119/1770 train_time:10243ms step_avg:93.97ms
step:120/1770 train_time:10337ms step_avg:93.97ms
step:121/1770 train_time:10431ms step_avg:93.98ms
step:122/1770 train_time:10525ms step_avg:93.97ms
step:123/1770 train_time:10619ms step_avg:93.97ms
step:124/1770 train_time:10713ms step_avg:93.97ms
step:125/1770 train_time:10806ms step_avg:93.97ms
step:125/1770 val_loss:4.6555 train_time:10899ms step_avg:94.77ms
step:126/1770 train_time:10923ms step_avg:94.17ms
step:127/1770 train_time:10997ms step_avg:93.99ms
step:128/1770 train_time:11090ms step_avg:93.99ms
step:129/1770 train_time:11188ms step_avg:94.02ms
step:130/1770 train_time:11283ms step_avg:94.03ms
step:131/1770 train_time:11377ms step_avg:94.02ms
step:132/1770 train_time:11471ms step_avg:94.02ms
step:133/1770 train_time:11565ms step_avg:94.02ms
step:134/1770 train_time:11659ms step_avg:94.02ms
step:135/1770 train_time:11753ms step_avg:94.02ms
step:136/1770 train_time:11847ms step_avg:94.02ms
step:137/1770 train_time:11942ms step_avg:94.03ms
step:138/1770 train_time:12037ms step_avg:94.04ms
step:139/1770 train_time:12132ms step_avg:94.04ms
step:140/1770 train_time:12227ms step_avg:94.05ms
step:141/1770 train_time:12321ms step_avg:94.06ms
step:142/1770 train_time:12416ms step_avg:94.06ms
step:143/1770 train_time:12511ms step_avg:94.07ms
step:144/1770 train_time:12605ms step_avg:94.07ms
step:145/1770 train_time:12699ms step_avg:94.07ms
step:146/1770 train_time:12794ms step_avg:94.07ms
step:147/1770 train_time:12888ms step_avg:94.07ms
step:148/1770 train_time:12982ms step_avg:94.07ms
step:149/1770 train_time:13077ms step_avg:94.08ms
step:150/1770 train_time:13171ms step_avg:94.08ms
step:151/1770 train_time:13266ms step_avg:94.08ms
step:152/1770 train_time:13360ms step_avg:94.09ms
step:153/1770 train_time:13455ms step_avg:94.09ms
step:154/1770 train_time:13550ms step_avg:94.09ms
step:155/1770 train_time:13644ms step_avg:94.09ms
step:156/1770 train_time:13738ms step_avg:94.09ms
step:157/1770 train_time:13832ms step_avg:94.10ms
step:158/1770 train_time:13927ms step_avg:94.10ms
step:159/1770 train_time:14022ms step_avg:94.10ms
step:160/1770 train_time:14117ms step_avg:94.11ms
step:161/1770 train_time:14212ms step_avg:94.12ms
step:162/1770 train_time:14307ms step_avg:94.12ms
step:163/1770 train_time:14401ms step_avg:94.13ms
step:164/1770 train_time:14496ms step_avg:94.13ms
step:165/1770 train_time:14591ms step_avg:94.14ms
step:166/1770 train_time:14686ms step_avg:94.14ms
step:167/1770 train_time:14781ms step_avg:94.14ms
step:168/1770 train_time:14875ms step_avg:94.14ms
step:169/1770 train_time:14970ms step_avg:94.15ms
step:170/1770 train_time:15064ms step_avg:94.15ms
step:171/1770 train_time:15160ms step_avg:94.16ms
step:172/1770 train_time:15254ms step_avg:94.16ms
step:173/1770 train_time:15349ms step_avg:94.17ms
step:174/1770 train_time:15443ms step_avg:94.16ms
step:175/1770 train_time:15537ms step_avg:94.17ms
step:176/1770 train_time:15632ms step_avg:94.17ms
step:177/1770 train_time:15727ms step_avg:94.17ms
step:178/1770 train_time:15821ms step_avg:94.18ms
step:179/1770 train_time:15916ms step_avg:94.18ms
step:180/1770 train_time:16011ms step_avg:94.18ms
step:181/1770 train_time:16105ms step_avg:94.18ms
step:182/1770 train_time:16200ms step_avg:94.19ms
step:183/1770 train_time:16294ms step_avg:94.19ms
step:184/1770 train_time:16389ms step_avg:94.19ms
step:185/1770 train_time:16483ms step_avg:94.19ms
step:186/1770 train_time:16578ms step_avg:94.19ms
step:187/1770 train_time:16673ms step_avg:94.20ms
step:188/1770 train_time:16767ms step_avg:94.20ms
step:189/1770 train_time:16862ms step_avg:94.20ms
step:190/1770 train_time:16956ms step_avg:94.20ms
step:191/1770 train_time:17051ms step_avg:94.20ms
step:192/1770 train_time:17145ms step_avg:94.21ms
step:193/1770 train_time:17241ms step_avg:94.21ms
step:194/1770 train_time:17335ms step_avg:94.21ms
step:195/1770 train_time:17430ms step_avg:94.22ms
step:196/1770 train_time:17524ms step_avg:94.22ms
step:197/1770 train_time:17619ms step_avg:94.22ms
step:198/1770 train_time:17713ms step_avg:94.22ms
step:199/1770 train_time:17808ms step_avg:94.22ms
step:200/1770 train_time:17902ms step_avg:94.22ms
step:201/1770 train_time:17998ms step_avg:94.23ms
step:202/1770 train_time:18094ms step_avg:94.24ms
step:203/1770 train_time:18188ms step_avg:94.24ms
step:204/1770 train_time:18283ms step_avg:94.24ms
step:205/1770 train_time:18378ms step_avg:94.25ms
step:206/1770 train_time:18472ms step_avg:94.25ms
step:207/1770 train_time:18566ms step_avg:94.25ms
step:208/1770 train_time:18661ms step_avg:94.25ms
step:209/1770 train_time:18756ms step_avg:94.25ms
step:210/1770 train_time:18851ms step_avg:94.25ms
step:211/1770 train_time:18945ms step_avg:94.25ms
step:212/1770 train_time:19039ms step_avg:94.25ms
step:213/1770 train_time:19134ms step_avg:94.26ms
step:214/1770 train_time:19229ms step_avg:94.26ms
step:215/1770 train_time:19323ms step_avg:94.26ms
step:216/1770 train_time:19418ms step_avg:94.26ms
step:217/1770 train_time:19513ms step_avg:94.26ms
step:218/1770 train_time:19608ms step_avg:94.27ms
step:219/1770 train_time:19702ms step_avg:94.27ms
step:220/1770 train_time:19796ms step_avg:94.27ms
step:221/1770 train_time:19891ms step_avg:94.27ms
step:222/1770 train_time:19985ms step_avg:94.27ms
step:223/1770 train_time:20080ms step_avg:94.27ms
step:224/1770 train_time:20175ms step_avg:94.28ms
step:225/1770 train_time:20271ms step_avg:94.28ms
step:226/1770 train_time:20365ms step_avg:94.28ms
step:227/1770 train_time:20460ms step_avg:94.28ms
step:228/1770 train_time:20555ms step_avg:94.29ms
step:229/1770 train_time:20650ms step_avg:94.29ms
step:230/1770 train_time:20744ms step_avg:94.29ms
step:231/1770 train_time:20838ms step_avg:94.29ms
step:232/1770 train_time:20933ms step_avg:94.29ms
step:233/1770 train_time:21027ms step_avg:94.29ms
step:234/1770 train_time:21121ms step_avg:94.29ms
step:235/1770 train_time:21216ms step_avg:94.29ms
step:236/1770 train_time:21311ms step_avg:94.30ms
step:237/1770 train_time:21405ms step_avg:94.29ms
step:238/1770 train_time:21500ms step_avg:94.30ms
step:239/1770 train_time:21594ms step_avg:94.30ms
step:240/1770 train_time:21689ms step_avg:94.30ms
step:241/1770 train_time:21783ms step_avg:94.30ms
step:242/1770 train_time:21878ms step_avg:94.30ms
step:243/1770 train_time:21973ms step_avg:94.30ms
step:244/1770 train_time:22068ms step_avg:94.31ms
step:245/1770 train_time:22162ms step_avg:94.30ms
step:246/1770 train_time:22256ms step_avg:94.30ms
step:247/1770 train_time:22350ms step_avg:94.30ms
step:248/1770 train_time:22445ms step_avg:94.31ms
step:249/1770 train_time:22539ms step_avg:94.31ms
step:250/1770 train_time:22634ms step_avg:94.31ms
step:250/1770 val_loss:4.1125 train_time:22727ms step_avg:94.70ms
step:251/1770 train_time:22749ms step_avg:94.39ms
step:252/1770 train_time:22831ms step_avg:94.34ms
step:253/1770 train_time:22927ms step_avg:94.35ms
step:254/1770 train_time:23022ms step_avg:94.35ms
step:255/1770 train_time:23117ms step_avg:94.36ms
step:256/1770 train_time:23211ms step_avg:94.35ms
step:257/1770 train_time:23305ms step_avg:94.35ms
step:258/1770 train_time:23399ms step_avg:94.35ms
step:259/1770 train_time:23493ms step_avg:94.35ms
step:260/1770 train_time:23588ms step_avg:94.35ms
step:261/1770 train_time:23682ms step_avg:94.35ms
step:262/1770 train_time:23777ms step_avg:94.35ms
step:263/1770 train_time:23873ms step_avg:94.36ms
step:264/1770 train_time:23968ms step_avg:94.36ms
step:265/1770 train_time:24063ms step_avg:94.36ms
step:266/1770 train_time:24158ms step_avg:94.37ms
step:267/1770 train_time:24254ms step_avg:94.37ms
step:268/1770 train_time:24349ms step_avg:94.37ms
step:269/1770 train_time:24444ms step_avg:94.38ms
step:270/1770 train_time:24538ms step_avg:94.38ms
step:271/1770 train_time:24633ms step_avg:94.38ms
step:272/1770 train_time:24728ms step_avg:94.38ms
step:273/1770 train_time:24824ms step_avg:94.39ms
step:274/1770 train_time:24919ms step_avg:94.39ms
step:275/1770 train_time:25014ms step_avg:94.39ms
step:276/1770 train_time:25109ms step_avg:94.40ms
step:277/1770 train_time:25205ms step_avg:94.40ms
step:278/1770 train_time:25300ms step_avg:94.40ms
step:279/1770 train_time:25395ms step_avg:94.40ms
step:280/1770 train_time:25490ms step_avg:94.41ms
step:281/1770 train_time:25585ms step_avg:94.41ms
step:282/1770 train_time:25680ms step_avg:94.41ms
step:283/1770 train_time:25774ms step_avg:94.41ms
step:284/1770 train_time:25869ms step_avg:94.41ms
step:285/1770 train_time:25965ms step_avg:94.42ms
step:286/1770 train_time:26060ms step_avg:94.42ms
step:287/1770 train_time:26155ms step_avg:94.42ms
step:288/1770 train_time:26250ms step_avg:94.42ms
step:289/1770 train_time:26345ms step_avg:94.43ms
step:290/1770 train_time:26441ms step_avg:94.43ms
step:291/1770 train_time:26535ms step_avg:94.43ms
step:292/1770 train_time:26631ms step_avg:94.44ms
step:293/1770 train_time:26726ms step_avg:94.44ms
step:294/1770 train_time:26822ms step_avg:94.44ms
step:295/1770 train_time:26917ms step_avg:94.45ms
step:296/1770 train_time:27012ms step_avg:94.45ms
step:297/1770 train_time:27107ms step_avg:94.45ms
step:298/1770 train_time:27203ms step_avg:94.45ms
step:299/1770 train_time:27297ms step_avg:94.45ms
step:300/1770 train_time:27392ms step_avg:94.46ms
step:301/1770 train_time:27488ms step_avg:94.46ms
step:302/1770 train_time:27583ms step_avg:94.46ms
step:303/1770 train_time:27677ms step_avg:94.46ms
step:304/1770 train_time:27773ms step_avg:94.47ms
step:305/1770 train_time:27867ms step_avg:94.47ms
step:306/1770 train_time:27963ms step_avg:94.47ms
step:307/1770 train_time:28057ms step_avg:94.47ms
step:308/1770 train_time:28152ms step_avg:94.47ms
step:309/1770 train_time:28248ms step_avg:94.47ms
step:310/1770 train_time:28343ms step_avg:94.48ms
step:311/1770 train_time:28438ms step_avg:94.48ms
step:312/1770 train_time:28533ms step_avg:94.48ms
step:313/1770 train_time:28628ms step_avg:94.48ms
step:314/1770 train_time:28724ms step_avg:94.49ms
step:315/1770 train_time:28818ms step_avg:94.49ms
step:316/1770 train_time:28913ms step_avg:94.49ms
step:317/1770 train_time:29008ms step_avg:94.49ms
step:318/1770 train_time:29103ms step_avg:94.49ms
step:319/1770 train_time:29199ms step_avg:94.49ms
step:320/1770 train_time:29294ms step_avg:94.50ms
step:321/1770 train_time:29389ms step_avg:94.50ms
step:322/1770 train_time:29484ms step_avg:94.50ms
step:323/1770 train_time:29579ms step_avg:94.50ms
step:324/1770 train_time:29675ms step_avg:94.51ms
step:325/1770 train_time:29770ms step_avg:94.51ms
step:326/1770 train_time:29865ms step_avg:94.51ms
step:327/1770 train_time:29960ms step_avg:94.51ms
step:328/1770 train_time:30055ms step_avg:94.51ms
step:329/1770 train_time:30149ms step_avg:94.51ms
step:330/1770 train_time:30245ms step_avg:94.51ms
step:331/1770 train_time:30340ms step_avg:94.52ms
step:332/1770 train_time:30434ms step_avg:94.52ms
step:333/1770 train_time:30529ms step_avg:94.52ms
step:334/1770 train_time:30625ms step_avg:94.52ms
step:335/1770 train_time:30720ms step_avg:94.52ms
step:336/1770 train_time:30815ms step_avg:94.52ms
step:337/1770 train_time:30910ms step_avg:94.53ms
step:338/1770 train_time:31005ms step_avg:94.53ms
step:339/1770 train_time:31100ms step_avg:94.53ms
step:340/1770 train_time:31195ms step_avg:94.53ms
step:341/1770 train_time:31290ms step_avg:94.53ms
step:342/1770 train_time:31386ms step_avg:94.53ms
step:343/1770 train_time:31480ms step_avg:94.54ms
step:344/1770 train_time:31575ms step_avg:94.54ms
step:345/1770 train_time:31670ms step_avg:94.54ms
step:346/1770 train_time:31765ms step_avg:94.54ms
step:347/1770 train_time:31860ms step_avg:94.54ms
step:348/1770 train_time:31956ms step_avg:94.54ms
step:349/1770 train_time:32051ms step_avg:94.55ms
step:350/1770 train_time:32146ms step_avg:94.55ms
step:351/1770 train_time:32241ms step_avg:94.55ms
step:352/1770 train_time:32336ms step_avg:94.55ms
step:353/1770 train_time:32432ms step_avg:94.55ms
step:354/1770 train_time:32527ms step_avg:94.55ms
step:355/1770 train_time:32622ms step_avg:94.56ms
step:356/1770 train_time:32717ms step_avg:94.56ms
step:357/1770 train_time:32811ms step_avg:94.56ms
step:358/1770 train_time:32906ms step_avg:94.56ms
step:359/1770 train_time:33002ms step_avg:94.56ms
step:360/1770 train_time:33097ms step_avg:94.56ms
step:361/1770 train_time:33192ms step_avg:94.56ms
step:362/1770 train_time:33287ms step_avg:94.56ms
step:363/1770 train_time:33382ms step_avg:94.57ms
step:364/1770 train_time:33477ms step_avg:94.57ms
step:365/1770 train_time:33573ms step_avg:94.57ms
step:366/1770 train_time:33668ms step_avg:94.57ms
step:367/1770 train_time:33762ms step_avg:94.57ms
step:368/1770 train_time:33857ms step_avg:94.57ms
step:369/1770 train_time:33952ms step_avg:94.57ms
step:370/1770 train_time:34047ms step_avg:94.57ms
step:371/1770 train_time:34142ms step_avg:94.58ms
step:372/1770 train_time:34236ms step_avg:94.58ms
step:373/1770 train_time:34331ms step_avg:94.58ms
step:374/1770 train_time:34426ms step_avg:94.58ms
step:375/1770 train_time:34521ms step_avg:94.58ms
step:375/1770 val_loss:3.9051 train_time:34615ms step_avg:94.84ms
step:376/1770 train_time:34636ms step_avg:94.64ms
step:377/1770 train_time:34723ms step_avg:94.61ms
step:378/1770 train_time:34820ms step_avg:94.62ms
step:379/1770 train_time:34916ms step_avg:94.62ms
step:380/1770 train_time:35010ms step_avg:94.62ms
step:381/1770 train_time:35105ms step_avg:94.62ms
step:382/1770 train_time:35200ms step_avg:94.62ms
step:383/1770 train_time:35294ms step_avg:94.62ms
step:384/1770 train_time:35389ms step_avg:94.62ms
step:385/1770 train_time:35483ms step_avg:94.62ms
step:386/1770 train_time:35578ms step_avg:94.62ms
step:387/1770 train_time:35673ms step_avg:94.62ms
step:388/1770 train_time:35769ms step_avg:94.63ms
step:389/1770 train_time:35865ms step_avg:94.63ms
step:390/1770 train_time:35960ms step_avg:94.63ms
step:391/1770 train_time:36055ms step_avg:94.63ms
step:392/1770 train_time:36149ms step_avg:94.63ms
step:393/1770 train_time:36245ms step_avg:94.63ms
step:394/1770 train_time:36339ms step_avg:94.63ms
step:395/1770 train_time:36434ms step_avg:94.63ms
step:396/1770 train_time:36531ms step_avg:94.64ms
step:397/1770 train_time:36627ms step_avg:94.64ms
step:398/1770 train_time:36725ms step_avg:94.65ms
step:399/1770 train_time:36822ms step_avg:94.66ms
step:400/1770 train_time:36919ms step_avg:94.66ms
step:401/1770 train_time:37016ms step_avg:94.67ms
step:402/1770 train_time:37113ms step_avg:94.68ms
step:403/1770 train_time:37209ms step_avg:94.68ms
step:404/1770 train_time:37307ms step_avg:94.69ms
step:405/1770 train_time:37403ms step_avg:94.69ms
step:406/1770 train_time:37500ms step_avg:94.70ms
step:407/1770 train_time:37597ms step_avg:94.70ms
step:408/1770 train_time:37693ms step_avg:94.71ms
step:409/1770 train_time:37790ms step_avg:94.71ms
step:410/1770 train_time:37887ms step_avg:94.72ms
step:411/1770 train_time:37985ms step_avg:94.73ms
step:412/1770 train_time:38083ms step_avg:94.73ms
step:413/1770 train_time:38180ms step_avg:94.74ms
step:414/1770 train_time:38278ms step_avg:94.75ms
step:415/1770 train_time:38374ms step_avg:94.75ms
step:416/1770 train_time:38471ms step_avg:94.76ms
step:417/1770 train_time:38568ms step_avg:94.76ms
step:418/1770 train_time:38665ms step_avg:94.77ms
step:419/1770 train_time:38761ms step_avg:94.77ms
step:420/1770 train_time:38860ms step_avg:94.78ms
step:421/1770 train_time:38957ms step_avg:94.78ms
step:422/1770 train_time:39054ms step_avg:94.79ms
step:423/1770 train_time:39151ms step_avg:94.80ms
step:424/1770 train_time:39248ms step_avg:94.80ms
step:425/1770 train_time:39345ms step_avg:94.81ms
step:426/1770 train_time:39441ms step_avg:94.81ms
step:427/1770 train_time:39538ms step_avg:94.82ms
step:428/1770 train_time:39635ms step_avg:94.82ms
step:429/1770 train_time:39732ms step_avg:94.83ms
step:430/1770 train_time:39829ms step_avg:94.83ms
step:431/1770 train_time:39926ms step_avg:94.84ms
step:432/1770 train_time:40023ms step_avg:94.84ms
step:433/1770 train_time:40120ms step_avg:94.85ms
step:434/1770 train_time:40218ms step_avg:94.85ms
step:435/1770 train_time:40314ms step_avg:94.86ms
step:436/1770 train_time:40411ms step_avg:94.86ms
step:437/1770 train_time:40508ms step_avg:94.87ms
step:438/1770 train_time:40604ms step_avg:94.87ms
step:439/1770 train_time:40701ms step_avg:94.87ms
step:440/1770 train_time:40799ms step_avg:94.88ms
step:441/1770 train_time:40896ms step_avg:94.89ms
step:442/1770 train_time:40993ms step_avg:94.89ms
step:443/1770 train_time:41090ms step_avg:94.90ms
step:444/1770 train_time:41187ms step_avg:94.90ms
step:445/1770 train_time:41284ms step_avg:94.91ms
step:446/1770 train_time:41381ms step_avg:94.91ms
step:447/1770 train_time:41478ms step_avg:94.92ms
step:448/1770 train_time:41575ms step_avg:94.92ms
step:449/1770 train_time:41672ms step_avg:94.92ms
step:450/1770 train_time:41769ms step_avg:94.93ms
step:451/1770 train_time:41866ms step_avg:94.93ms
step:452/1770 train_time:41963ms step_avg:94.94ms
step:453/1770 train_time:42060ms step_avg:94.94ms
step:454/1770 train_time:42157ms step_avg:94.95ms
step:455/1770 train_time:42253ms step_avg:94.95ms
step:456/1770 train_time:42350ms step_avg:94.96ms
step:457/1770 train_time:42447ms step_avg:94.96ms
step:458/1770 train_time:42544ms step_avg:94.96ms
step:459/1770 train_time:42641ms step_avg:94.97ms
step:460/1770 train_time:42739ms step_avg:94.97ms
step:461/1770 train_time:42836ms step_avg:94.98ms
step:462/1770 train_time:42933ms step_avg:94.98ms
step:463/1770 train_time:43030ms step_avg:94.99ms
step:464/1770 train_time:43127ms step_avg:94.99ms
step:465/1770 train_time:43224ms step_avg:95.00ms
step:466/1770 train_time:43321ms step_avg:95.00ms
step:467/1770 train_time:43418ms step_avg:95.01ms
step:468/1770 train_time:43515ms step_avg:95.01ms
step:469/1770 train_time:43612ms step_avg:95.01ms
step:470/1770 train_time:43709ms step_avg:95.02ms
step:471/1770 train_time:43806ms step_avg:95.02ms
step:472/1770 train_time:43904ms step_avg:95.03ms
step:473/1770 train_time:44001ms step_avg:95.03ms
step:474/1770 train_time:44098ms step_avg:95.04ms
step:475/1770 train_time:44194ms step_avg:95.04ms
step:476/1770 train_time:44291ms step_avg:95.05ms
step:477/1770 train_time:44388ms step_avg:95.05ms
step:478/1770 train_time:44485ms step_avg:95.05ms
step:479/1770 train_time:44583ms step_avg:95.06ms
step:480/1770 train_time:44680ms step_avg:95.06ms
step:481/1770 train_time:44777ms step_avg:95.07ms
step:482/1770 train_time:44874ms step_avg:95.07ms
step:483/1770 train_time:44971ms step_avg:95.08ms
step:484/1770 train_time:45068ms step_avg:95.08ms
step:485/1770 train_time:45166ms step_avg:95.09ms
step:486/1770 train_time:45263ms step_avg:95.09ms
step:487/1770 train_time:45360ms step_avg:95.09ms
step:488/1770 train_time:45456ms step_avg:95.10ms
step:489/1770 train_time:45553ms step_avg:95.10ms
step:490/1770 train_time:45651ms step_avg:95.11ms
step:491/1770 train_time:45748ms step_avg:95.11ms
step:492/1770 train_time:45845ms step_avg:95.11ms
step:493/1770 train_time:45943ms step_avg:95.12ms
step:494/1770 train_time:46040ms step_avg:95.12ms
step:495/1770 train_time:46137ms step_avg:95.13ms
step:496/1770 train_time:46234ms step_avg:95.13ms
step:497/1770 train_time:46331ms step_avg:95.14ms
step:498/1770 train_time:46428ms step_avg:95.14ms
step:499/1770 train_time:46525ms step_avg:95.14ms
step:500/1770 train_time:46622ms step_avg:95.15ms
step:500/1770 val_loss:3.7535 train_time:46717ms step_avg:95.34ms
step:501/1770 train_time:46741ms step_avg:95.20ms
step:502/1770 train_time:46824ms step_avg:95.17ms
step:503/1770 train_time:46923ms step_avg:95.18ms
step:504/1770 train_time:47020ms step_avg:95.18ms
step:505/1770 train_time:47118ms step_avg:95.19ms
step:506/1770 train_time:47214ms step_avg:95.19ms
step:507/1770 train_time:47311ms step_avg:95.19ms
step:508/1770 train_time:47407ms step_avg:95.19ms
step:509/1770 train_time:47504ms step_avg:95.20ms
step:510/1770 train_time:47600ms step_avg:95.20ms
step:511/1770 train_time:47698ms step_avg:95.20ms
step:512/1770 train_time:47795ms step_avg:95.21ms
step:513/1770 train_time:47893ms step_avg:95.22ms
step:514/1770 train_time:47991ms step_avg:95.22ms
step:515/1770 train_time:48088ms step_avg:95.22ms
step:516/1770 train_time:48185ms step_avg:95.23ms
step:517/1770 train_time:48281ms step_avg:95.23ms
step:518/1770 train_time:48378ms step_avg:95.23ms
step:519/1770 train_time:48475ms step_avg:95.24ms
step:520/1770 train_time:48572ms step_avg:95.24ms
step:521/1770 train_time:48669ms step_avg:95.24ms
step:522/1770 train_time:48765ms step_avg:95.24ms
step:523/1770 train_time:48863ms step_avg:95.25ms
step:524/1770 train_time:48960ms step_avg:95.25ms
step:525/1770 train_time:49058ms step_avg:95.26ms
step:526/1770 train_time:49155ms step_avg:95.26ms
step:527/1770 train_time:49253ms step_avg:95.27ms
step:528/1770 train_time:49350ms step_avg:95.27ms
step:529/1770 train_time:49447ms step_avg:95.27ms
step:530/1770 train_time:49544ms step_avg:95.28ms
step:531/1770 train_time:49641ms step_avg:95.28ms
step:532/1770 train_time:49738ms step_avg:95.28ms
step:533/1770 train_time:49836ms step_avg:95.29ms
step:534/1770 train_time:49933ms step_avg:95.29ms
step:535/1770 train_time:50031ms step_avg:95.30ms
step:536/1770 train_time:50128ms step_avg:95.30ms
step:537/1770 train_time:50225ms step_avg:95.30ms
step:538/1770 train_time:50322ms step_avg:95.31ms
step:539/1770 train_time:50420ms step_avg:95.31ms
step:540/1770 train_time:50518ms step_avg:95.32ms
step:541/1770 train_time:50616ms step_avg:95.32ms
step:542/1770 train_time:50713ms step_avg:95.32ms
step:543/1770 train_time:50810ms step_avg:95.33ms
step:544/1770 train_time:50907ms step_avg:95.33ms
step:545/1770 train_time:51005ms step_avg:95.34ms
step:546/1770 train_time:51102ms step_avg:95.34ms
step:547/1770 train_time:51200ms step_avg:95.35ms
step:548/1770 train_time:51298ms step_avg:95.35ms
step:549/1770 train_time:51396ms step_avg:95.35ms
step:550/1770 train_time:51493ms step_avg:95.36ms
step:551/1770 train_time:51590ms step_avg:95.36ms
step:552/1770 train_time:51687ms step_avg:95.36ms
step:553/1770 train_time:51784ms step_avg:95.37ms
step:554/1770 train_time:51881ms step_avg:95.37ms
step:555/1770 train_time:51979ms step_avg:95.37ms
step:556/1770 train_time:52076ms step_avg:95.38ms
step:557/1770 train_time:52174ms step_avg:95.38ms
step:558/1770 train_time:52271ms step_avg:95.39ms
step:559/1770 train_time:52368ms step_avg:95.39ms
step:560/1770 train_time:52465ms step_avg:95.39ms
step:561/1770 train_time:52563ms step_avg:95.40ms
step:562/1770 train_time:52661ms step_avg:95.40ms
step:563/1770 train_time:52758ms step_avg:95.40ms
step:564/1770 train_time:52856ms step_avg:95.41ms
step:565/1770 train_time:52953ms step_avg:95.41ms
step:566/1770 train_time:53050ms step_avg:95.41ms
step:567/1770 train_time:53147ms step_avg:95.42ms
step:568/1770 train_time:53244ms step_avg:95.42ms
step:569/1770 train_time:53342ms step_avg:95.42ms
step:570/1770 train_time:53439ms step_avg:95.43ms
step:571/1770 train_time:53537ms step_avg:95.43ms
step:572/1770 train_time:53634ms step_avg:95.43ms
step:573/1770 train_time:53731ms step_avg:95.44ms
step:574/1770 train_time:53828ms step_avg:95.44ms
step:575/1770 train_time:53925ms step_avg:95.44ms
step:576/1770 train_time:54022ms step_avg:95.45ms
step:577/1770 train_time:54119ms step_avg:95.45ms
step:578/1770 train_time:54217ms step_avg:95.45ms
step:579/1770 train_time:54315ms step_avg:95.46ms
step:580/1770 train_time:54412ms step_avg:95.46ms
step:581/1770 train_time:54510ms step_avg:95.46ms
step:582/1770 train_time:54607ms step_avg:95.47ms
step:583/1770 train_time:54704ms step_avg:95.47ms
step:584/1770 train_time:54802ms step_avg:95.47ms
step:585/1770 train_time:54899ms step_avg:95.48ms
step:586/1770 train_time:54997ms step_avg:95.48ms
step:587/1770 train_time:55094ms step_avg:95.48ms
step:588/1770 train_time:55191ms step_avg:95.49ms
step:589/1770 train_time:55288ms step_avg:95.49ms
step:590/1770 train_time:55385ms step_avg:95.49ms
step:591/1770 train_time:55482ms step_avg:95.49ms
step:592/1770 train_time:55580ms step_avg:95.50ms
step:593/1770 train_time:55679ms step_avg:95.50ms
step:594/1770 train_time:55776ms step_avg:95.51ms
step:595/1770 train_time:55873ms step_avg:95.51ms
step:596/1770 train_time:55970ms step_avg:95.51ms
step:597/1770 train_time:56067ms step_avg:95.51ms
step:598/1770 train_time:56164ms step_avg:95.52ms
step:599/1770 train_time:56261ms step_avg:95.52ms
step:600/1770 train_time:56359ms step_avg:95.52ms
step:601/1770 train_time:56457ms step_avg:95.53ms
step:602/1770 train_time:56555ms step_avg:95.53ms
step:603/1770 train_time:56652ms step_avg:95.53ms
step:604/1770 train_time:56749ms step_avg:95.54ms
step:605/1770 train_time:56846ms step_avg:95.54ms
step:606/1770 train_time:56944ms step_avg:95.54ms
step:607/1770 train_time:57042ms step_avg:95.55ms
step:608/1770 train_time:57139ms step_avg:95.55ms
step:609/1770 train_time:57236ms step_avg:95.55ms
step:610/1770 train_time:57333ms step_avg:95.56ms
step:611/1770 train_time:57430ms step_avg:95.56ms
step:612/1770 train_time:57528ms step_avg:95.56ms
step:613/1770 train_time:57625ms step_avg:95.56ms
step:614/1770 train_time:57722ms step_avg:95.57ms
step:615/1770 train_time:57819ms step_avg:95.57ms
step:616/1770 train_time:57918ms step_avg:95.57ms
step:617/1770 train_time:58016ms step_avg:95.58ms
step:618/1770 train_time:58113ms step_avg:95.58ms
step:619/1770 train_time:58210ms step_avg:95.58ms
step:620/1770 train_time:58307ms step_avg:95.59ms
step:621/1770 train_time:58404ms step_avg:95.59ms
step:622/1770 train_time:58501ms step_avg:95.59ms
step:623/1770 train_time:58599ms step_avg:95.59ms
step:624/1770 train_time:58697ms step_avg:95.60ms
step:625/1770 train_time:58794ms step_avg:95.60ms
step:625/1770 val_loss:3.6658 train_time:58889ms step_avg:95.76ms
step:626/1770 train_time:58911ms step_avg:95.63ms
step:627/1770 train_time:58997ms step_avg:95.62ms
step:628/1770 train_time:59097ms step_avg:95.63ms
step:629/1770 train_time:59194ms step_avg:95.63ms
step:630/1770 train_time:59291ms step_avg:95.63ms
step:631/1770 train_time:59388ms step_avg:95.63ms
step:632/1770 train_time:59485ms step_avg:95.64ms
step:633/1770 train_time:59582ms step_avg:95.64ms
step:634/1770 train_time:59679ms step_avg:95.64ms
step:635/1770 train_time:59775ms step_avg:95.64ms
step:636/1770 train_time:59872ms step_avg:95.64ms
step:637/1770 train_time:59970ms step_avg:95.65ms
step:638/1770 train_time:60067ms step_avg:95.65ms
step:639/1770 train_time:60165ms step_avg:95.65ms
step:640/1770 train_time:60263ms step_avg:95.66ms
step:641/1770 train_time:60362ms step_avg:95.66ms
step:642/1770 train_time:60459ms step_avg:95.66ms
step:643/1770 train_time:60557ms step_avg:95.67ms
step:644/1770 train_time:60654ms step_avg:95.67ms
step:645/1770 train_time:60751ms step_avg:95.67ms
step:646/1770 train_time:60848ms step_avg:95.67ms
step:647/1770 train_time:60945ms step_avg:95.67ms
step:648/1770 train_time:61042ms step_avg:95.68ms
step:649/1770 train_time:61140ms step_avg:95.68ms
step:650/1770 train_time:61237ms step_avg:95.68ms
step:651/1770 train_time:61335ms step_avg:95.69ms
step:652/1770 train_time:61433ms step_avg:95.69ms
step:653/1770 train_time:61530ms step_avg:95.69ms
step:654/1770 train_time:61627ms step_avg:95.69ms
step:655/1770 train_time:61725ms step_avg:95.70ms
step:656/1770 train_time:61822ms step_avg:95.70ms
step:657/1770 train_time:61919ms step_avg:95.70ms
step:658/1770 train_time:62018ms step_avg:95.71ms
step:659/1770 train_time:62117ms step_avg:95.71ms
step:660/1770 train_time:62216ms step_avg:95.72ms
step:661/1770 train_time:62316ms step_avg:95.72ms
step:662/1770 train_time:62415ms step_avg:95.73ms
step:663/1770 train_time:62514ms step_avg:95.73ms
step:664/1770 train_time:62614ms step_avg:95.74ms
step:665/1770 train_time:62714ms step_avg:95.75ms
step:666/1770 train_time:62814ms step_avg:95.75ms
step:667/1770 train_time:62914ms step_avg:95.76ms
step:668/1770 train_time:63013ms step_avg:95.76ms
step:669/1770 train_time:63112ms step_avg:95.77ms
step:670/1770 train_time:63211ms step_avg:95.77ms
step:671/1770 train_time:63311ms step_avg:95.78ms
step:672/1770 train_time:63410ms step_avg:95.78ms
step:673/1770 train_time:63508ms step_avg:95.79ms
step:674/1770 train_time:63606ms step_avg:95.79ms
step:675/1770 train_time:63706ms step_avg:95.80ms
step:676/1770 train_time:63804ms step_avg:95.80ms
step:677/1770 train_time:63904ms step_avg:95.81ms
step:678/1770 train_time:64003ms step_avg:95.81ms
step:679/1770 train_time:64103ms step_avg:95.82ms
step:680/1770 train_time:64203ms step_avg:95.82ms
step:681/1770 train_time:64302ms step_avg:95.83ms
step:682/1770 train_time:64402ms step_avg:95.84ms
step:683/1770 train_time:64502ms step_avg:95.84ms
step:684/1770 train_time:64601ms step_avg:95.85ms
step:685/1770 train_time:64700ms step_avg:95.85ms
step:686/1770 train_time:64800ms step_avg:95.86ms
step:687/1770 train_time:64899ms step_avg:95.86ms
step:688/1770 train_time:64998ms step_avg:95.87ms
step:689/1770 train_time:65097ms step_avg:95.87ms
step:690/1770 train_time:65196ms step_avg:95.88ms
step:691/1770 train_time:65295ms step_avg:95.88ms
step:692/1770 train_time:65394ms step_avg:95.89ms
step:693/1770 train_time:65494ms step_avg:95.89ms
step:694/1770 train_time:65594ms step_avg:95.90ms
step:695/1770 train_time:65694ms step_avg:95.90ms
step:696/1770 train_time:65793ms step_avg:95.91ms
step:697/1770 train_time:65893ms step_avg:95.91ms
step:698/1770 train_time:65992ms step_avg:95.92ms
step:699/1770 train_time:66091ms step_avg:95.92ms
step:700/1770 train_time:66190ms step_avg:95.93ms
step:701/1770 train_time:66288ms step_avg:95.93ms
step:702/1770 train_time:66387ms step_avg:95.94ms
step:703/1770 train_time:66486ms step_avg:95.94ms
step:704/1770 train_time:66584ms step_avg:95.94ms
step:705/1770 train_time:66683ms step_avg:95.95ms
step:706/1770 train_time:66782ms step_avg:95.95ms
step:707/1770 train_time:66882ms step_avg:95.96ms
step:708/1770 train_time:66982ms step_avg:95.96ms
step:709/1770 train_time:67082ms step_avg:95.97ms
step:710/1770 train_time:67181ms step_avg:95.97ms
step:711/1770 train_time:67281ms step_avg:95.98ms
step:712/1770 train_time:67380ms step_avg:95.98ms
step:713/1770 train_time:67479ms step_avg:95.99ms
step:714/1770 train_time:67579ms step_avg:95.99ms
step:715/1770 train_time:67677ms step_avg:96.00ms
step:716/1770 train_time:67777ms step_avg:96.00ms
step:717/1770 train_time:67877ms step_avg:96.01ms
step:718/1770 train_time:67976ms step_avg:96.01ms
step:719/1770 train_time:68076ms step_avg:96.02ms
step:720/1770 train_time:68176ms step_avg:96.02ms
step:721/1770 train_time:68276ms step_avg:96.03ms
step:722/1770 train_time:68376ms step_avg:96.03ms
step:723/1770 train_time:68476ms step_avg:96.04ms
step:724/1770 train_time:68575ms step_avg:96.04ms
step:725/1770 train_time:68673ms step_avg:96.05ms
step:726/1770 train_time:68772ms step_avg:96.05ms
step:727/1770 train_time:68871ms step_avg:96.05ms
step:728/1770 train_time:68970ms step_avg:96.06ms
step:729/1770 train_time:69069ms step_avg:96.06ms
step:730/1770 train_time:69168ms step_avg:96.07ms
step:731/1770 train_time:69266ms step_avg:96.07ms
step:732/1770 train_time:69365ms step_avg:96.07ms
step:733/1770 train_time:69464ms step_avg:96.08ms
step:734/1770 train_time:69563ms step_avg:96.08ms
step:735/1770 train_time:69662ms step_avg:96.09ms
step:736/1770 train_time:69761ms step_avg:96.09ms
step:737/1770 train_time:69860ms step_avg:96.09ms
step:738/1770 train_time:69960ms step_avg:96.10ms
step:739/1770 train_time:70059ms step_avg:96.10ms
step:740/1770 train_time:70158ms step_avg:96.11ms
step:741/1770 train_time:70257ms step_avg:96.11ms
step:742/1770 train_time:70356ms step_avg:96.11ms
step:743/1770 train_time:70455ms step_avg:96.12ms
step:744/1770 train_time:70555ms step_avg:96.12ms
step:745/1770 train_time:70654ms step_avg:96.13ms
step:746/1770 train_time:70753ms step_avg:96.13ms
step:747/1770 train_time:70853ms step_avg:96.14ms
step:748/1770 train_time:70952ms step_avg:96.14ms
step:749/1770 train_time:71051ms step_avg:96.14ms
step:750/1770 train_time:71150ms step_avg:96.15ms
step:750/1770 val_loss:3.6017 train_time:71248ms step_avg:96.28ms
step:751/1770 train_time:71269ms step_avg:96.18ms
step:752/1770 train_time:71358ms step_avg:96.17ms
step:753/1770 train_time:71458ms step_avg:96.18ms
step:754/1770 train_time:71557ms step_avg:96.18ms
step:755/1770 train_time:71656ms step_avg:96.18ms
step:756/1770 train_time:71755ms step_avg:96.19ms
step:757/1770 train_time:71853ms step_avg:96.19ms
step:758/1770 train_time:71951ms step_avg:96.19ms
step:759/1770 train_time:72050ms step_avg:96.19ms
step:760/1770 train_time:72148ms step_avg:96.20ms
step:761/1770 train_time:72247ms step_avg:96.20ms
step:762/1770 train_time:72346ms step_avg:96.21ms
step:763/1770 train_time:72446ms step_avg:96.21ms
step:764/1770 train_time:72546ms step_avg:96.21ms
step:765/1770 train_time:72646ms step_avg:96.22ms
step:766/1770 train_time:72746ms step_avg:96.22ms
step:767/1770 train_time:72845ms step_avg:96.23ms
step:768/1770 train_time:72943ms step_avg:96.23ms
step:769/1770 train_time:73043ms step_avg:96.24ms
step:770/1770 train_time:73142ms step_avg:96.24ms
step:771/1770 train_time:73241ms step_avg:96.24ms
step:772/1770 train_time:73341ms step_avg:96.25ms
step:773/1770 train_time:73440ms step_avg:96.25ms
step:774/1770 train_time:73540ms step_avg:96.26ms
step:775/1770 train_time:73640ms step_avg:96.26ms
step:776/1770 train_time:73740ms step_avg:96.27ms
step:777/1770 train_time:73840ms step_avg:96.27ms
step:778/1770 train_time:73938ms step_avg:96.27ms
step:779/1770 train_time:74037ms step_avg:96.28ms
step:780/1770 train_time:74136ms step_avg:96.28ms
step:781/1770 train_time:74235ms step_avg:96.28ms
step:782/1770 train_time:74335ms step_avg:96.29ms
step:783/1770 train_time:74434ms step_avg:96.29ms
step:784/1770 train_time:74533ms step_avg:96.30ms
step:785/1770 train_time:74632ms step_avg:96.30ms
step:786/1770 train_time:74731ms step_avg:96.30ms
step:787/1770 train_time:74830ms step_avg:96.31ms
step:788/1770 train_time:74929ms step_avg:96.31ms
step:789/1770 train_time:75028ms step_avg:96.31ms
step:790/1770 train_time:75127ms step_avg:96.32ms
step:791/1770 train_time:75227ms step_avg:96.32ms
step:792/1770 train_time:75327ms step_avg:96.33ms
step:793/1770 train_time:75427ms step_avg:96.33ms
step:794/1770 train_time:75527ms step_avg:96.34ms
step:795/1770 train_time:75627ms step_avg:96.34ms
step:796/1770 train_time:75728ms step_avg:96.35ms
step:797/1770 train_time:75827ms step_avg:96.35ms
step:798/1770 train_time:75926ms step_avg:96.35ms
step:799/1770 train_time:76025ms step_avg:96.36ms
step:800/1770 train_time:76124ms step_avg:96.36ms
step:801/1770 train_time:76224ms step_avg:96.36ms
step:802/1770 train_time:76324ms step_avg:96.37ms
step:803/1770 train_time:76424ms step_avg:96.37ms
step:804/1770 train_time:76524ms step_avg:96.38ms
step:805/1770 train_time:76623ms step_avg:96.38ms
step:806/1770 train_time:76724ms step_avg:96.39ms
step:807/1770 train_time:76825ms step_avg:96.39ms
step:808/1770 train_time:76924ms step_avg:96.40ms
step:809/1770 train_time:77025ms step_avg:96.40ms
step:810/1770 train_time:77124ms step_avg:96.41ms
step:811/1770 train_time:77224ms step_avg:96.41ms
step:812/1770 train_time:77323ms step_avg:96.41ms
step:813/1770 train_time:77423ms step_avg:96.42ms
step:814/1770 train_time:77522ms step_avg:96.42ms
step:815/1770 train_time:77622ms step_avg:96.42ms
step:816/1770 train_time:77721ms step_avg:96.43ms
step:817/1770 train_time:77820ms step_avg:96.43ms
step:818/1770 train_time:77920ms step_avg:96.44ms
step:819/1770 train_time:78020ms step_avg:96.44ms
step:820/1770 train_time:78119ms step_avg:96.44ms
step:821/1770 train_time:78219ms step_avg:96.45ms
step:822/1770 train_time:78319ms step_avg:96.45ms
step:823/1770 train_time:78419ms step_avg:96.46ms
step:824/1770 train_time:78518ms step_avg:96.46ms
step:825/1770 train_time:78618ms step_avg:96.46ms
step:826/1770 train_time:78717ms step_avg:96.47ms
step:827/1770 train_time:78817ms step_avg:96.47ms
step:828/1770 train_time:78917ms step_avg:96.48ms
step:829/1770 train_time:79016ms step_avg:96.48ms
step:830/1770 train_time:79115ms step_avg:96.48ms
step:831/1770 train_time:79214ms step_avg:96.48ms
step:832/1770 train_time:79313ms step_avg:96.49ms
step:833/1770 train_time:79412ms step_avg:96.49ms
step:834/1770 train_time:79511ms step_avg:96.49ms
step:835/1770 train_time:79610ms step_avg:96.50ms
step:836/1770 train_time:79710ms step_avg:96.50ms
step:837/1770 train_time:79809ms step_avg:96.50ms
step:838/1770 train_time:79908ms step_avg:96.51ms
step:839/1770 train_time:80008ms step_avg:96.51ms
step:840/1770 train_time:80108ms step_avg:96.52ms
step:841/1770 train_time:80208ms step_avg:96.52ms
step:842/1770 train_time:80308ms step_avg:96.52ms
step:843/1770 train_time:80408ms step_avg:96.53ms
step:844/1770 train_time:80507ms step_avg:96.53ms
step:845/1770 train_time:80606ms step_avg:96.53ms
step:846/1770 train_time:80706ms step_avg:96.54ms
step:847/1770 train_time:80806ms step_avg:96.54ms
step:848/1770 train_time:80906ms step_avg:96.55ms
step:849/1770 train_time:81006ms step_avg:96.55ms
step:850/1770 train_time:81106ms step_avg:96.55ms
step:851/1770 train_time:81205ms step_avg:96.56ms
step:852/1770 train_time:81305ms step_avg:96.56ms
step:853/1770 train_time:81405ms step_avg:96.57ms
step:854/1770 train_time:81505ms step_avg:96.57ms
step:855/1770 train_time:81605ms step_avg:96.57ms
step:856/1770 train_time:81704ms step_avg:96.58ms
step:857/1770 train_time:81804ms step_avg:96.58ms
step:858/1770 train_time:81904ms step_avg:96.59ms
step:859/1770 train_time:82004ms step_avg:96.59ms
step:860/1770 train_time:82104ms step_avg:96.59ms
step:861/1770 train_time:82204ms step_avg:96.60ms
step:862/1770 train_time:82304ms step_avg:96.60ms
step:863/1770 train_time:82403ms step_avg:96.60ms
step:864/1770 train_time:82503ms step_avg:96.61ms
step:865/1770 train_time:82603ms step_avg:96.61ms
step:866/1770 train_time:82702ms step_avg:96.61ms
step:867/1770 train_time:82802ms step_avg:96.62ms
step:868/1770 train_time:82902ms step_avg:96.62ms
step:869/1770 train_time:83001ms step_avg:96.63ms
step:870/1770 train_time:83101ms step_avg:96.63ms
step:871/1770 train_time:83200ms step_avg:96.63ms
step:872/1770 train_time:83299ms step_avg:96.64ms
step:873/1770 train_time:83399ms step_avg:96.64ms
step:874/1770 train_time:83499ms step_avg:96.64ms
step:875/1770 train_time:83599ms step_avg:96.65ms
step:875/1770 val_loss:3.5529 train_time:83697ms step_avg:96.76ms
step:876/1770 train_time:83718ms step_avg:96.67ms
step:877/1770 train_time:83809ms step_avg:96.67ms
step:878/1770 train_time:83909ms step_avg:96.67ms
step:879/1770 train_time:84009ms step_avg:96.67ms
step:880/1770 train_time:84108ms step_avg:96.68ms
step:881/1770 train_time:84207ms step_avg:96.68ms
step:882/1770 train_time:84305ms step_avg:96.68ms
step:883/1770 train_time:84403ms step_avg:96.68ms
step:884/1770 train_time:84502ms step_avg:96.68ms
step:885/1770 train_time:84602ms step_avg:96.69ms
step:886/1770 train_time:84701ms step_avg:96.69ms
step:887/1770 train_time:84801ms step_avg:96.69ms
step:888/1770 train_time:84902ms step_avg:96.70ms
step:889/1770 train_time:85002ms step_avg:96.70ms
step:890/1770 train_time:85102ms step_avg:96.71ms
step:891/1770 train_time:85202ms step_avg:96.71ms
step:892/1770 train_time:85301ms step_avg:96.71ms
step:893/1770 train_time:85401ms step_avg:96.72ms
step:894/1770 train_time:85500ms step_avg:96.72ms
step:895/1770 train_time:85599ms step_avg:96.72ms
step:896/1770 train_time:85698ms step_avg:96.73ms
step:897/1770 train_time:85798ms step_avg:96.73ms
step:898/1770 train_time:85898ms step_avg:96.73ms
step:899/1770 train_time:85997ms step_avg:96.73ms
step:900/1770 train_time:86097ms step_avg:96.74ms
step:901/1770 train_time:86197ms step_avg:96.74ms
step:902/1770 train_time:86298ms step_avg:96.75ms
step:903/1770 train_time:86398ms step_avg:96.75ms
step:904/1770 train_time:86497ms step_avg:96.75ms
step:905/1770 train_time:86597ms step_avg:96.76ms
step:906/1770 train_time:86696ms step_avg:96.76ms
step:907/1770 train_time:86795ms step_avg:96.76ms
step:908/1770 train_time:86895ms step_avg:96.77ms
step:909/1770 train_time:86994ms step_avg:96.77ms
step:910/1770 train_time:87093ms step_avg:96.77ms
step:911/1770 train_time:87193ms step_avg:96.77ms
step:912/1770 train_time:87293ms step_avg:96.78ms
step:913/1770 train_time:87392ms step_avg:96.78ms
step:914/1770 train_time:87492ms step_avg:96.78ms
step:915/1770 train_time:87592ms step_avg:96.79ms
step:916/1770 train_time:87690ms step_avg:96.79ms
step:917/1770 train_time:87790ms step_avg:96.79ms
step:918/1770 train_time:87889ms step_avg:96.79ms
step:919/1770 train_time:87988ms step_avg:96.80ms
step:920/1770 train_time:88089ms step_avg:96.80ms
step:921/1770 train_time:88190ms step_avg:96.81ms
step:922/1770 train_time:88291ms step_avg:96.81ms
step:923/1770 train_time:88391ms step_avg:96.81ms
step:924/1770 train_time:88492ms step_avg:96.82ms
step:925/1770 train_time:88592ms step_avg:96.82ms
step:926/1770 train_time:88693ms step_avg:96.83ms
step:927/1770 train_time:88793ms step_avg:96.83ms
step:928/1770 train_time:88894ms step_avg:96.83ms
step:929/1770 train_time:88994ms step_avg:96.84ms
step:930/1770 train_time:89094ms step_avg:96.84ms
step:931/1770 train_time:89195ms step_avg:96.85ms
step:932/1770 train_time:89297ms step_avg:96.85ms
step:933/1770 train_time:89397ms step_avg:96.85ms
step:934/1770 train_time:89499ms step_avg:96.86ms
step:935/1770 train_time:89600ms step_avg:96.86ms
step:936/1770 train_time:89701ms step_avg:96.87ms
step:937/1770 train_time:89801ms step_avg:96.87ms
step:938/1770 train_time:89902ms step_avg:96.88ms
step:939/1770 train_time:90003ms step_avg:96.88ms
step:940/1770 train_time:90104ms step_avg:96.89ms
step:941/1770 train_time:90205ms step_avg:96.89ms
step:942/1770 train_time:90307ms step_avg:96.90ms
step:943/1770 train_time:90408ms step_avg:96.90ms
step:944/1770 train_time:90508ms step_avg:96.90ms
step:945/1770 train_time:90609ms step_avg:96.91ms
step:946/1770 train_time:90710ms step_avg:96.91ms
step:947/1770 train_time:90810ms step_avg:96.92ms
step:948/1770 train_time:90911ms step_avg:96.92ms
step:949/1770 train_time:91011ms step_avg:96.92ms
step:950/1770 train_time:91113ms step_avg:96.93ms
step:951/1770 train_time:91213ms step_avg:96.93ms
step:952/1770 train_time:91314ms step_avg:96.94ms
step:953/1770 train_time:91415ms step_avg:96.94ms
step:954/1770 train_time:91515ms step_avg:96.94ms
step:955/1770 train_time:91617ms step_avg:96.95ms
step:956/1770 train_time:91718ms step_avg:96.95ms
step:957/1770 train_time:91819ms step_avg:96.96ms
step:958/1770 train_time:91921ms step_avg:96.96ms
step:959/1770 train_time:92022ms step_avg:96.97ms
step:960/1770 train_time:92122ms step_avg:96.97ms
step:961/1770 train_time:92223ms step_avg:96.97ms
step:962/1770 train_time:92324ms step_avg:96.98ms
step:963/1770 train_time:92429ms step_avg:96.99ms
step:964/1770 train_time:92527ms step_avg:96.99ms
step:965/1770 train_time:92629ms step_avg:96.99ms
step:966/1770 train_time:92730ms step_avg:97.00ms
step:967/1770 train_time:92831ms step_avg:97.00ms
step:968/1770 train_time:92931ms step_avg:97.01ms
step:969/1770 train_time:93031ms step_avg:97.01ms
step:970/1770 train_time:93132ms step_avg:97.01ms
step:971/1770 train_time:93232ms step_avg:97.02ms
step:972/1770 train_time:93332ms step_avg:97.02ms
step:973/1770 train_time:93433ms step_avg:97.02ms
step:974/1770 train_time:93535ms step_avg:97.03ms
step:975/1770 train_time:93636ms step_avg:97.03ms
step:976/1770 train_time:93737ms step_avg:97.04ms
step:977/1770 train_time:93838ms step_avg:97.04ms
step:978/1770 train_time:93941ms step_avg:97.05ms
step:979/1770 train_time:94042ms step_avg:97.05ms
step:980/1770 train_time:94143ms step_avg:97.05ms
step:981/1770 train_time:94243ms step_avg:97.06ms
step:982/1770 train_time:94346ms step_avg:97.06ms
step:983/1770 train_time:94447ms step_avg:97.07ms
step:984/1770 train_time:94548ms step_avg:97.07ms
step:985/1770 train_time:94650ms step_avg:97.08ms
step:986/1770 train_time:94750ms step_avg:97.08ms
step:987/1770 train_time:94850ms step_avg:97.08ms
step:988/1770 train_time:94950ms step_avg:97.09ms
step:989/1770 train_time:95052ms step_avg:97.09ms
step:990/1770 train_time:95153ms step_avg:97.09ms
step:991/1770 train_time:95253ms step_avg:97.10ms
step:992/1770 train_time:95354ms step_avg:97.10ms
step:993/1770 train_time:95454ms step_avg:97.11ms
step:994/1770 train_time:95556ms step_avg:97.11ms
step:995/1770 train_time:95657ms step_avg:97.11ms
step:996/1770 train_time:95757ms step_avg:97.12ms
step:997/1770 train_time:95859ms step_avg:97.12ms
step:998/1770 train_time:95960ms step_avg:97.13ms
step:999/1770 train_time:96061ms step_avg:97.13ms
step:1000/1770 train_time:96162ms step_avg:97.13ms
step:1000/1770 val_loss:3.5137 train_time:96261ms step_avg:97.23ms
step:1001/1770 train_time:96284ms step_avg:97.16ms
step:1002/1770 train_time:96371ms step_avg:97.15ms
step:1003/1770 train_time:96474ms step_avg:97.15ms
step:1004/1770 train_time:96575ms step_avg:97.16ms
step:1005/1770 train_time:96675ms step_avg:97.16ms
step:1006/1770 train_time:96775ms step_avg:97.16ms
step:1007/1770 train_time:96875ms step_avg:97.17ms
step:1008/1770 train_time:96975ms step_avg:97.17ms
step:1009/1770 train_time:97076ms step_avg:97.17ms
step:1010/1770 train_time:97176ms step_avg:97.18ms
step:1011/1770 train_time:97279ms step_avg:97.18ms
step:1012/1770 train_time:97381ms step_avg:97.19ms
step:1013/1770 train_time:97482ms step_avg:97.19ms
step:1014/1770 train_time:97583ms step_avg:97.19ms
step:1015/1770 train_time:97684ms step_avg:97.20ms
step:1016/1770 train_time:97784ms step_avg:97.20ms
step:1017/1770 train_time:97885ms step_avg:97.21ms
step:1018/1770 train_time:97986ms step_avg:97.21ms
step:1019/1770 train_time:98087ms step_avg:97.21ms
step:1020/1770 train_time:98189ms step_avg:97.22ms
step:1021/1770 train_time:98291ms step_avg:97.22ms
step:1022/1770 train_time:98393ms step_avg:97.23ms
step:1023/1770 train_time:98494ms step_avg:97.23ms
step:1024/1770 train_time:98597ms step_avg:97.24ms
step:1025/1770 train_time:98698ms step_avg:97.24ms
step:1026/1770 train_time:98799ms step_avg:97.24ms
step:1027/1770 train_time:98899ms step_avg:97.25ms
step:1028/1770 train_time:99000ms step_avg:97.25ms
step:1029/1770 train_time:99100ms step_avg:97.25ms
step:1030/1770 train_time:99200ms step_avg:97.26ms
step:1031/1770 train_time:99301ms step_avg:97.26ms
step:1032/1770 train_time:99401ms step_avg:97.26ms
step:1033/1770 train_time:99502ms step_avg:97.26ms
step:1034/1770 train_time:99603ms step_avg:97.27ms
step:1035/1770 train_time:99705ms step_avg:97.27ms
step:1036/1770 train_time:99805ms step_avg:97.28ms
step:1037/1770 train_time:99907ms step_avg:97.28ms
step:1038/1770 train_time:100009ms step_avg:97.28ms
step:1039/1770 train_time:100111ms step_avg:97.29ms
step:1040/1770 train_time:100211ms step_avg:97.29ms
step:1041/1770 train_time:100312ms step_avg:97.30ms
step:1042/1770 train_time:100414ms step_avg:97.30ms
step:1043/1770 train_time:100516ms step_avg:97.30ms
step:1044/1770 train_time:100617ms step_avg:97.31ms
step:1045/1770 train_time:100718ms step_avg:97.31ms
step:1046/1770 train_time:100820ms step_avg:97.32ms
step:1047/1770 train_time:100920ms step_avg:97.32ms
step:1048/1770 train_time:101020ms step_avg:97.32ms
step:1049/1770 train_time:101120ms step_avg:97.32ms
step:1050/1770 train_time:101220ms step_avg:97.33ms
step:1051/1770 train_time:101321ms step_avg:97.33ms
step:1052/1770 train_time:101421ms step_avg:97.33ms
step:1053/1770 train_time:101522ms step_avg:97.34ms
step:1054/1770 train_time:101623ms step_avg:97.34ms
step:1055/1770 train_time:101724ms step_avg:97.34ms
step:1056/1770 train_time:101824ms step_avg:97.35ms
step:1057/1770 train_time:101925ms step_avg:97.35ms
step:1058/1770 train_time:102028ms step_avg:97.36ms
step:1059/1770 train_time:102130ms step_avg:97.36ms
step:1060/1770 train_time:102232ms step_avg:97.36ms
step:1061/1770 train_time:102333ms step_avg:97.37ms
step:1062/1770 train_time:102435ms step_avg:97.37ms
step:1063/1770 train_time:102537ms step_avg:97.38ms
step:1064/1770 train_time:102638ms step_avg:97.38ms
step:1065/1770 train_time:102739ms step_avg:97.38ms
step:1066/1770 train_time:102840ms step_avg:97.39ms
step:1067/1770 train_time:102941ms step_avg:97.39ms
step:1068/1770 train_time:103042ms step_avg:97.39ms
step:1069/1770 train_time:103142ms step_avg:97.40ms
step:1070/1770 train_time:103243ms step_avg:97.40ms
step:1071/1770 train_time:103345ms step_avg:97.40ms
step:1072/1770 train_time:103446ms step_avg:97.41ms
step:1073/1770 train_time:103546ms step_avg:97.41ms
step:1074/1770 train_time:103648ms step_avg:97.41ms
step:1075/1770 train_time:103750ms step_avg:97.42ms
step:1076/1770 train_time:103851ms step_avg:97.42ms
step:1077/1770 train_time:103953ms step_avg:97.43ms
step:1078/1770 train_time:104054ms step_avg:97.43ms
step:1079/1770 train_time:104156ms step_avg:97.43ms
step:1080/1770 train_time:104257ms step_avg:97.44ms
step:1081/1770 train_time:104358ms step_avg:97.44ms
step:1082/1770 train_time:104460ms step_avg:97.44ms
step:1083/1770 train_time:104560ms step_avg:97.45ms
step:1084/1770 train_time:104661ms step_avg:97.45ms
step:1085/1770 train_time:104762ms step_avg:97.45ms
step:1086/1770 train_time:104863ms step_avg:97.46ms
step:1087/1770 train_time:104963ms step_avg:97.46ms
step:1088/1770 train_time:105064ms step_avg:97.46ms
step:1089/1770 train_time:105166ms step_avg:97.47ms
step:1090/1770 train_time:105268ms step_avg:97.47ms
step:1091/1770 train_time:105369ms step_avg:97.47ms
step:1092/1770 train_time:105470ms step_avg:97.48ms
step:1093/1770 train_time:105572ms step_avg:97.48ms
step:1094/1770 train_time:105673ms step_avg:97.48ms
step:1095/1770 train_time:105774ms step_avg:97.49ms
step:1096/1770 train_time:105876ms step_avg:97.49ms
step:1097/1770 train_time:105977ms step_avg:97.50ms
step:1098/1770 train_time:106078ms step_avg:97.50ms
step:1099/1770 train_time:106179ms step_avg:97.50ms
step:1100/1770 train_time:106280ms step_avg:97.50ms
step:1101/1770 train_time:106381ms step_avg:97.51ms
step:1102/1770 train_time:106482ms step_avg:97.51ms
step:1103/1770 train_time:106584ms step_avg:97.51ms
step:1104/1770 train_time:106686ms step_avg:97.52ms
step:1105/1770 train_time:106787ms step_avg:97.52ms
step:1106/1770 train_time:106888ms step_avg:97.53ms
step:1107/1770 train_time:106989ms step_avg:97.53ms
step:1108/1770 train_time:107090ms step_avg:97.53ms
step:1109/1770 train_time:107191ms step_avg:97.54ms
step:1110/1770 train_time:107293ms step_avg:97.54ms
step:1111/1770 train_time:107395ms step_avg:97.54ms
step:1112/1770 train_time:107497ms step_avg:97.55ms
step:1113/1770 train_time:107598ms step_avg:97.55ms
step:1114/1770 train_time:107699ms step_avg:97.55ms
step:1115/1770 train_time:107800ms step_avg:97.56ms
step:1116/1770 train_time:107900ms step_avg:97.56ms
step:1117/1770 train_time:108001ms step_avg:97.56ms
step:1118/1770 train_time:108101ms step_avg:97.56ms
step:1119/1770 train_time:108202ms step_avg:97.57ms
step:1120/1770 train_time:108303ms step_avg:97.57ms
step:1121/1770 train_time:108403ms step_avg:97.57ms
step:1122/1770 train_time:108504ms step_avg:97.58ms
step:1123/1770 train_time:108606ms step_avg:97.58ms
step:1124/1770 train_time:108707ms step_avg:97.58ms
step:1125/1770 train_time:108809ms step_avg:97.59ms
step:1125/1770 val_loss:3.4714 train_time:108909ms step_avg:97.68ms
step:1126/1770 train_time:108930ms step_avg:97.61ms
step:1127/1770 train_time:109020ms step_avg:97.60ms
step:1128/1770 train_time:109122ms step_avg:97.60ms
step:1129/1770 train_time:109223ms step_avg:97.61ms
step:1130/1770 train_time:109323ms step_avg:97.61ms
step:1131/1770 train_time:109424ms step_avg:97.61ms
step:1132/1770 train_time:109525ms step_avg:97.62ms
step:1133/1770 train_time:109625ms step_avg:97.62ms
step:1134/1770 train_time:109725ms step_avg:97.62ms
step:1135/1770 train_time:109825ms step_avg:97.62ms
step:1136/1770 train_time:109926ms step_avg:97.63ms
step:1137/1770 train_time:110029ms step_avg:97.63ms
step:1138/1770 train_time:110129ms step_avg:97.63ms
step:1139/1770 train_time:110230ms step_avg:97.64ms
step:1140/1770 train_time:110331ms step_avg:97.64ms
step:1141/1770 train_time:110432ms step_avg:97.64ms
step:1142/1770 train_time:110534ms step_avg:97.64ms
step:1143/1770 train_time:110635ms step_avg:97.65ms
step:1144/1770 train_time:110737ms step_avg:97.65ms
step:1145/1770 train_time:110837ms step_avg:97.65ms
step:1146/1770 train_time:110939ms step_avg:97.66ms
step:1147/1770 train_time:111040ms step_avg:97.66ms
step:1148/1770 train_time:111142ms step_avg:97.66ms
step:1149/1770 train_time:111244ms step_avg:97.67ms
step:1150/1770 train_time:111345ms step_avg:97.67ms
step:1151/1770 train_time:111445ms step_avg:97.67ms
step:1152/1770 train_time:111546ms step_avg:97.68ms
step:1153/1770 train_time:111646ms step_avg:97.68ms
step:1154/1770 train_time:111748ms step_avg:97.68ms
step:1155/1770 train_time:111849ms step_avg:97.68ms
step:1156/1770 train_time:111950ms step_avg:97.69ms
step:1157/1770 train_time:112052ms step_avg:97.69ms
step:1158/1770 train_time:112153ms step_avg:97.69ms
step:1159/1770 train_time:112255ms step_avg:97.70ms
step:1160/1770 train_time:112357ms step_avg:97.70ms
step:1161/1770 train_time:112458ms step_avg:97.70ms
step:1162/1770 train_time:112559ms step_avg:97.71ms
step:1163/1770 train_time:112660ms step_avg:97.71ms
step:1164/1770 train_time:112762ms step_avg:97.71ms
step:1165/1770 train_time:112864ms step_avg:97.72ms
step:1166/1770 train_time:112966ms step_avg:97.72ms
step:1167/1770 train_time:113066ms step_avg:97.72ms
step:1168/1770 train_time:113167ms step_avg:97.73ms
step:1169/1770 train_time:113268ms step_avg:97.73ms
step:1170/1770 train_time:113369ms step_avg:97.73ms
step:1171/1770 train_time:113471ms step_avg:97.74ms
step:1172/1770 train_time:113572ms step_avg:97.74ms
step:1173/1770 train_time:113673ms step_avg:97.74ms
step:1174/1770 train_time:113775ms step_avg:97.75ms
step:1175/1770 train_time:113875ms step_avg:97.75ms
step:1176/1770 train_time:113977ms step_avg:97.75ms
step:1177/1770 train_time:114079ms step_avg:97.75ms
step:1178/1770 train_time:114181ms step_avg:97.76ms
step:1179/1770 train_time:114282ms step_avg:97.76ms
step:1180/1770 train_time:114384ms step_avg:97.76ms
step:1181/1770 train_time:114485ms step_avg:97.77ms
step:1182/1770 train_time:114586ms step_avg:97.77ms
step:1183/1770 train_time:114688ms step_avg:97.77ms
step:1184/1770 train_time:114791ms step_avg:97.78ms
step:1185/1770 train_time:114893ms step_avg:97.78ms
step:1186/1770 train_time:114996ms step_avg:97.79ms
step:1187/1770 train_time:115101ms step_avg:97.79ms
step:1188/1770 train_time:115203ms step_avg:97.80ms
step:1189/1770 train_time:115305ms step_avg:97.80ms
step:1190/1770 train_time:115407ms step_avg:97.80ms
step:1191/1770 train_time:115509ms step_avg:97.81ms
step:1192/1770 train_time:115612ms step_avg:97.81ms
step:1193/1770 train_time:115714ms step_avg:97.81ms
step:1194/1770 train_time:115816ms step_avg:97.82ms
step:1195/1770 train_time:115919ms step_avg:97.82ms
step:1196/1770 train_time:116023ms step_avg:97.83ms
step:1197/1770 train_time:116125ms step_avg:97.83ms
step:1198/1770 train_time:116227ms step_avg:97.83ms
step:1199/1770 train_time:116329ms step_avg:97.84ms
step:1200/1770 train_time:116432ms step_avg:97.84ms
step:1201/1770 train_time:116535ms step_avg:97.85ms
step:1202/1770 train_time:116636ms step_avg:97.85ms
step:1203/1770 train_time:116738ms step_avg:97.85ms
step:1204/1770 train_time:116841ms step_avg:97.86ms
step:1205/1770 train_time:116942ms step_avg:97.86ms
step:1206/1770 train_time:117046ms step_avg:97.86ms
step:1207/1770 train_time:117148ms step_avg:97.87ms
step:1208/1770 train_time:117250ms step_avg:97.87ms
step:1209/1770 train_time:117352ms step_avg:97.88ms
step:1210/1770 train_time:117455ms step_avg:97.88ms
step:1211/1770 train_time:117558ms step_avg:97.88ms
step:1212/1770 train_time:117661ms step_avg:97.89ms
step:1213/1770 train_time:117763ms step_avg:97.89ms
step:1214/1770 train_time:117865ms step_avg:97.89ms
step:1215/1770 train_time:117967ms step_avg:97.90ms
step:1216/1770 train_time:118071ms step_avg:97.90ms
step:1217/1770 train_time:118173ms step_avg:97.91ms
step:1218/1770 train_time:118275ms step_avg:97.91ms
step:1219/1770 train_time:118377ms step_avg:97.91ms
step:1220/1770 train_time:118480ms step_avg:97.92ms
step:1221/1770 train_time:118582ms step_avg:97.92ms
step:1222/1770 train_time:118685ms step_avg:97.92ms
step:1223/1770 train_time:118786ms step_avg:97.93ms
step:1224/1770 train_time:118890ms step_avg:97.93ms
step:1225/1770 train_time:118993ms step_avg:97.94ms
step:1226/1770 train_time:119095ms step_avg:97.94ms
step:1227/1770 train_time:119200ms step_avg:97.95ms
step:1228/1770 train_time:119303ms step_avg:97.95ms
step:1229/1770 train_time:119405ms step_avg:97.95ms
step:1230/1770 train_time:119507ms step_avg:97.96ms
step:1231/1770 train_time:119608ms step_avg:97.96ms
step:1232/1770 train_time:119710ms step_avg:97.96ms
step:1233/1770 train_time:119811ms step_avg:97.96ms
step:1234/1770 train_time:119913ms step_avg:97.97ms
step:1235/1770 train_time:120016ms step_avg:97.97ms
step:1236/1770 train_time:120119ms step_avg:97.98ms
step:1237/1770 train_time:120222ms step_avg:97.98ms
step:1238/1770 train_time:120325ms step_avg:97.98ms
step:1239/1770 train_time:120427ms step_avg:97.99ms
step:1240/1770 train_time:120529ms step_avg:97.99ms
step:1241/1770 train_time:120632ms step_avg:98.00ms
step:1242/1770 train_time:120734ms step_avg:98.00ms
step:1243/1770 train_time:120836ms step_avg:98.00ms
step:1244/1770 train_time:120937ms step_avg:98.00ms
step:1245/1770 train_time:121039ms step_avg:98.01ms
step:1246/1770 train_time:121142ms step_avg:98.01ms
step:1247/1770 train_time:121244ms step_avg:98.01ms
step:1248/1770 train_time:121347ms step_avg:98.02ms
step:1249/1770 train_time:121449ms step_avg:98.02ms
step:1250/1770 train_time:121550ms step_avg:98.02ms
step:1250/1770 val_loss:3.4246 train_time:121653ms step_avg:98.11ms
step:1251/1770 train_time:121674ms step_avg:98.05ms
step:1252/1770 train_time:121765ms step_avg:98.04ms
step:1253/1770 train_time:121868ms step_avg:98.04ms
step:1254/1770 train_time:121970ms step_avg:98.05ms
step:1255/1770 train_time:122075ms step_avg:98.05ms
step:1256/1770 train_time:122176ms step_avg:98.05ms
step:1257/1770 train_time:122277ms step_avg:98.06ms
step:1258/1770 train_time:122379ms step_avg:98.06ms
step:1259/1770 train_time:122481ms step_avg:98.06ms
step:1260/1770 train_time:122581ms step_avg:98.07ms
step:1261/1770 train_time:122685ms step_avg:98.07ms
step:1262/1770 train_time:122789ms step_avg:98.07ms
step:1263/1770 train_time:122890ms step_avg:98.08ms
step:1264/1770 train_time:122994ms step_avg:98.08ms
step:1265/1770 train_time:123096ms step_avg:98.08ms
step:1266/1770 train_time:123198ms step_avg:98.09ms
step:1267/1770 train_time:123300ms step_avg:98.09ms
step:1268/1770 train_time:123403ms step_avg:98.09ms
step:1269/1770 train_time:123505ms step_avg:98.10ms
step:1270/1770 train_time:123607ms step_avg:98.10ms
step:1271/1770 train_time:123709ms step_avg:98.10ms
step:1272/1770 train_time:123811ms step_avg:98.11ms
step:1273/1770 train_time:123914ms step_avg:98.11ms
step:1274/1770 train_time:124017ms step_avg:98.11ms
step:1275/1770 train_time:124118ms step_avg:98.12ms
step:1276/1770 train_time:124220ms step_avg:98.12ms
step:1277/1770 train_time:124322ms step_avg:98.12ms
step:1278/1770 train_time:124425ms step_avg:98.13ms
step:1279/1770 train_time:124528ms step_avg:98.13ms
step:1280/1770 train_time:124632ms step_avg:98.14ms
step:1281/1770 train_time:124733ms step_avg:98.14ms
step:1282/1770 train_time:124837ms step_avg:98.14ms
step:1283/1770 train_time:124940ms step_avg:98.15ms
step:1284/1770 train_time:125042ms step_avg:98.15ms
step:1285/1770 train_time:125144ms step_avg:98.15ms
step:1286/1770 train_time:125247ms step_avg:98.16ms
step:1287/1770 train_time:125350ms step_avg:98.16ms
step:1288/1770 train_time:125453ms step_avg:98.16ms
step:1289/1770 train_time:125555ms step_avg:98.17ms
step:1290/1770 train_time:125657ms step_avg:98.17ms
step:1291/1770 train_time:125759ms step_avg:98.17ms
step:1292/1770 train_time:125860ms step_avg:98.18ms
step:1293/1770 train_time:125963ms step_avg:98.18ms
step:1294/1770 train_time:126064ms step_avg:98.18ms
step:1295/1770 train_time:126167ms step_avg:98.18ms
step:1296/1770 train_time:126269ms step_avg:98.19ms
step:1297/1770 train_time:126371ms step_avg:98.19ms
step:1298/1770 train_time:126474ms step_avg:98.19ms
step:1299/1770 train_time:126577ms step_avg:98.20ms
step:1300/1770 train_time:126679ms step_avg:98.20ms
step:1301/1770 train_time:126782ms step_avg:98.20ms
step:1302/1770 train_time:126884ms step_avg:98.21ms
step:1303/1770 train_time:126985ms step_avg:98.21ms
step:1304/1770 train_time:127086ms step_avg:98.21ms
step:1305/1770 train_time:127189ms step_avg:98.22ms
step:1306/1770 train_time:127291ms step_avg:98.22ms
step:1307/1770 train_time:127394ms step_avg:98.22ms
step:1308/1770 train_time:127497ms step_avg:98.23ms
step:1309/1770 train_time:127599ms step_avg:98.23ms
step:1310/1770 train_time:127701ms step_avg:98.23ms
step:1311/1770 train_time:127802ms step_avg:98.23ms
step:1312/1770 train_time:127904ms step_avg:98.24ms
step:1313/1770 train_time:128005ms step_avg:98.24ms
step:1314/1770 train_time:128107ms step_avg:98.24ms
step:1315/1770 train_time:128208ms step_avg:98.24ms
step:1316/1770 train_time:128310ms step_avg:98.25ms
step:1317/1770 train_time:128413ms step_avg:98.25ms
step:1318/1770 train_time:128519ms step_avg:98.26ms
step:1319/1770 train_time:128622ms step_avg:98.26ms
step:1320/1770 train_time:128724ms step_avg:98.26ms
step:1321/1770 train_time:128826ms step_avg:98.27ms
step:1322/1770 train_time:128929ms step_avg:98.27ms
step:1323/1770 train_time:129031ms step_avg:98.27ms
step:1324/1770 train_time:129134ms step_avg:98.28ms
step:1325/1770 train_time:129239ms step_avg:98.28ms
step:1326/1770 train_time:129340ms step_avg:98.28ms
step:1327/1770 train_time:129445ms step_avg:98.29ms
step:1328/1770 train_time:129547ms step_avg:98.29ms
step:1329/1770 train_time:129649ms step_avg:98.29ms
step:1330/1770 train_time:129751ms step_avg:98.30ms
step:1331/1770 train_time:129853ms step_avg:98.30ms
step:1332/1770 train_time:129954ms step_avg:98.30ms
step:1333/1770 train_time:130056ms step_avg:98.30ms
step:1334/1770 train_time:130159ms step_avg:98.31ms
step:1335/1770 train_time:130261ms step_avg:98.31ms
step:1336/1770 train_time:130363ms step_avg:98.31ms
step:1337/1770 train_time:130466ms step_avg:98.32ms
step:1338/1770 train_time:130567ms step_avg:98.32ms
step:1339/1770 train_time:130669ms step_avg:98.32ms
step:1340/1770 train_time:130773ms step_avg:98.33ms
step:1341/1770 train_time:130875ms step_avg:98.33ms
step:1342/1770 train_time:130978ms step_avg:98.33ms
step:1343/1770 train_time:131081ms step_avg:98.34ms
step:1344/1770 train_time:131183ms step_avg:98.34ms
step:1345/1770 train_time:131285ms step_avg:98.34ms
step:1346/1770 train_time:131386ms step_avg:98.34ms
step:1347/1770 train_time:131488ms step_avg:98.35ms
step:1348/1770 train_time:131593ms step_avg:98.35ms
step:1349/1770 train_time:131696ms step_avg:98.35ms
step:1350/1770 train_time:131798ms step_avg:98.36ms
step:1351/1770 train_time:131901ms step_avg:98.36ms
step:1352/1770 train_time:132002ms step_avg:98.36ms
step:1353/1770 train_time:132106ms step_avg:98.37ms
step:1354/1770 train_time:132208ms step_avg:98.37ms
step:1355/1770 train_time:132310ms step_avg:98.37ms
step:1356/1770 train_time:132411ms step_avg:98.37ms
step:1357/1770 train_time:132514ms step_avg:98.38ms
step:1358/1770 train_time:132616ms step_avg:98.38ms
step:1359/1770 train_time:132719ms step_avg:98.38ms
step:1360/1770 train_time:132822ms step_avg:98.39ms
step:1361/1770 train_time:132925ms step_avg:98.39ms
step:1362/1770 train_time:133026ms step_avg:98.39ms
step:1363/1770 train_time:133129ms step_avg:98.40ms
step:1364/1770 train_time:133232ms step_avg:98.40ms
step:1365/1770 train_time:133335ms step_avg:98.40ms
step:1366/1770 train_time:133437ms step_avg:98.40ms
step:1367/1770 train_time:133539ms step_avg:98.41ms
step:1368/1770 train_time:133641ms step_avg:98.41ms
step:1369/1770 train_time:133744ms step_avg:98.41ms
step:1370/1770 train_time:133846ms step_avg:98.42ms
step:1371/1770 train_time:133948ms step_avg:98.42ms
step:1372/1770 train_time:134050ms step_avg:98.42ms
step:1373/1770 train_time:134152ms step_avg:98.42ms
step:1374/1770 train_time:134255ms step_avg:98.43ms
step:1375/1770 train_time:134358ms step_avg:98.43ms
step:1375/1770 val_loss:3.3819 train_time:134459ms step_avg:98.50ms
step:1376/1770 train_time:134480ms step_avg:98.45ms
step:1377/1770 train_time:134575ms step_avg:98.45ms
step:1378/1770 train_time:134676ms step_avg:98.45ms
step:1379/1770 train_time:134778ms step_avg:98.45ms
step:1380/1770 train_time:134880ms step_avg:98.45ms
step:1381/1770 train_time:134982ms step_avg:98.45ms
step:1382/1770 train_time:135083ms step_avg:98.46ms
step:1383/1770 train_time:135186ms step_avg:98.46ms
step:1384/1770 train_time:135289ms step_avg:98.46ms
step:1385/1770 train_time:135391ms step_avg:98.47ms
step:1386/1770 train_time:135493ms step_avg:98.47ms
step:1387/1770 train_time:135598ms step_avg:98.47ms
step:1388/1770 train_time:135700ms step_avg:98.48ms
step:1389/1770 train_time:135802ms step_avg:98.48ms
step:1390/1770 train_time:135904ms step_avg:98.48ms
step:1391/1770 train_time:136006ms step_avg:98.48ms
step:1392/1770 train_time:136109ms step_avg:98.49ms
step:1393/1770 train_time:136211ms step_avg:98.49ms
step:1394/1770 train_time:136313ms step_avg:98.49ms
step:1395/1770 train_time:136415ms step_avg:98.49ms
step:1396/1770 train_time:136519ms step_avg:98.50ms
step:1397/1770 train_time:136621ms step_avg:98.50ms
step:1398/1770 train_time:136724ms step_avg:98.50ms
step:1399/1770 train_time:136826ms step_avg:98.51ms
step:1400/1770 train_time:136929ms step_avg:98.51ms
step:1401/1770 train_time:137031ms step_avg:98.51ms
step:1402/1770 train_time:137134ms step_avg:98.52ms
step:1403/1770 train_time:137235ms step_avg:98.52ms
step:1404/1770 train_time:137338ms step_avg:98.52ms
step:1405/1770 train_time:137440ms step_avg:98.52ms
step:1406/1770 train_time:137543ms step_avg:98.53ms
step:1407/1770 train_time:137645ms step_avg:98.53ms
step:1408/1770 train_time:137747ms step_avg:98.53ms
step:1409/1770 train_time:137851ms step_avg:98.54ms
step:1410/1770 train_time:137953ms step_avg:98.54ms
step:1411/1770 train_time:138055ms step_avg:98.54ms
step:1412/1770 train_time:138157ms step_avg:98.54ms
step:1413/1770 train_time:138258ms step_avg:98.54ms
step:1414/1770 train_time:138361ms step_avg:98.55ms
step:1415/1770 train_time:138465ms step_avg:98.55ms
step:1416/1770 train_time:138569ms step_avg:98.56ms
step:1417/1770 train_time:138671ms step_avg:98.56ms
step:1418/1770 train_time:138773ms step_avg:98.56ms
step:1419/1770 train_time:138877ms step_avg:98.56ms
step:1420/1770 train_time:138980ms step_avg:98.57ms
step:1421/1770 train_time:139081ms step_avg:98.57ms
step:1422/1770 train_time:139183ms step_avg:98.57ms
step:1423/1770 train_time:139285ms step_avg:98.57ms
step:1424/1770 train_time:139389ms step_avg:98.58ms
step:1425/1770 train_time:139491ms step_avg:98.58ms
step:1426/1770 train_time:139594ms step_avg:98.58ms
step:1427/1770 train_time:139696ms step_avg:98.59ms
step:1428/1770 train_time:139800ms step_avg:98.59ms
step:1429/1770 train_time:139902ms step_avg:98.59ms
step:1430/1770 train_time:140004ms step_avg:98.59ms
step:1431/1770 train_time:140107ms step_avg:98.60ms
step:1432/1770 train_time:140209ms step_avg:98.60ms
step:1433/1770 train_time:140312ms step_avg:98.60ms
step:1434/1770 train_time:140413ms step_avg:98.60ms
step:1435/1770 train_time:140515ms step_avg:98.61ms
step:1436/1770 train_time:140620ms step_avg:98.61ms
step:1437/1770 train_time:140722ms step_avg:98.61ms
step:1438/1770 train_time:140824ms step_avg:98.62ms
step:1439/1770 train_time:140926ms step_avg:98.62ms
step:1440/1770 train_time:141028ms step_avg:98.62ms
step:1441/1770 train_time:141133ms step_avg:98.63ms
step:1442/1770 train_time:141235ms step_avg:98.63ms
step:1443/1770 train_time:141336ms step_avg:98.63ms
step:1444/1770 train_time:141439ms step_avg:98.63ms
step:1445/1770 train_time:141541ms step_avg:98.64ms
step:1446/1770 train_time:141644ms step_avg:98.64ms
step:1447/1770 train_time:141748ms step_avg:98.64ms
step:1448/1770 train_time:141851ms step_avg:98.64ms
step:1449/1770 train_time:141955ms step_avg:98.65ms
step:1450/1770 train_time:142058ms step_avg:98.65ms
step:1451/1770 train_time:142162ms step_avg:98.65ms
step:1452/1770 train_time:142265ms step_avg:98.66ms
step:1453/1770 train_time:142368ms step_avg:98.66ms
step:1454/1770 train_time:142472ms step_avg:98.67ms
step:1455/1770 train_time:142577ms step_avg:98.67ms
step:1456/1770 train_time:142682ms step_avg:98.67ms
step:1457/1770 train_time:142785ms step_avg:98.68ms
step:1458/1770 train_time:142889ms step_avg:98.68ms
step:1459/1770 train_time:142993ms step_avg:98.68ms
step:1460/1770 train_time:143097ms step_avg:98.69ms
step:1461/1770 train_time:143200ms step_avg:98.69ms
step:1462/1770 train_time:143303ms step_avg:98.69ms
step:1463/1770 train_time:143406ms step_avg:98.70ms
step:1464/1770 train_time:143512ms step_avg:98.70ms
step:1465/1770 train_time:143615ms step_avg:98.70ms
step:1466/1770 train_time:143720ms step_avg:98.71ms
step:1467/1770 train_time:143824ms step_avg:98.71ms
step:1468/1770 train_time:143928ms step_avg:98.72ms
step:1469/1770 train_time:144031ms step_avg:98.72ms
step:1470/1770 train_time:144134ms step_avg:98.72ms
step:1471/1770 train_time:144237ms step_avg:98.72ms
step:1472/1770 train_time:144341ms step_avg:98.73ms
step:1473/1770 train_time:144445ms step_avg:98.73ms
step:1474/1770 train_time:144549ms step_avg:98.74ms
step:1475/1770 train_time:144652ms step_avg:98.74ms
step:1476/1770 train_time:144755ms step_avg:98.74ms
step:1477/1770 train_time:144861ms step_avg:98.75ms
step:1478/1770 train_time:144965ms step_avg:98.75ms
step:1479/1770 train_time:145068ms step_avg:98.75ms
step:1480/1770 train_time:145171ms step_avg:98.76ms
step:1481/1770 train_time:145279ms step_avg:98.76ms
step:1482/1770 train_time:145382ms step_avg:98.76ms
step:1483/1770 train_time:145486ms step_avg:98.77ms
step:1484/1770 train_time:145589ms step_avg:98.77ms
step:1485/1770 train_time:145691ms step_avg:98.77ms
step:1486/1770 train_time:145795ms step_avg:98.78ms
step:1487/1770 train_time:145897ms step_avg:98.78ms
step:1488/1770 train_time:146001ms step_avg:98.78ms
step:1489/1770 train_time:146105ms step_avg:98.79ms
step:1490/1770 train_time:146209ms step_avg:98.79ms
step:1491/1770 train_time:146312ms step_avg:98.79ms
step:1492/1770 train_time:146415ms step_avg:98.80ms
step:1493/1770 train_time:146521ms step_avg:98.80ms
step:1494/1770 train_time:146627ms step_avg:98.81ms
step:1495/1770 train_time:146730ms step_avg:98.81ms
step:1496/1770 train_time:146832ms step_avg:98.81ms
step:1497/1770 train_time:146936ms step_avg:98.81ms
step:1498/1770 train_time:147039ms step_avg:98.82ms
step:1499/1770 train_time:147141ms step_avg:98.82ms
step:1500/1770 train_time:147244ms step_avg:98.82ms
step:1500/1770 val_loss:3.3439 train_time:147345ms step_avg:98.89ms
step:1501/1770 train_time:147367ms step_avg:98.84ms
step:1502/1770 train_time:147455ms step_avg:98.83ms
step:1503/1770 train_time:147557ms step_avg:98.83ms
step:1504/1770 train_time:147661ms step_avg:98.84ms
step:1505/1770 train_time:147766ms step_avg:98.84ms
step:1506/1770 train_time:147869ms step_avg:98.84ms
step:1507/1770 train_time:147972ms step_avg:98.85ms
step:1508/1770 train_time:148077ms step_avg:98.85ms
step:1509/1770 train_time:148179ms step_avg:98.85ms
step:1510/1770 train_time:148282ms step_avg:98.85ms
step:1511/1770 train_time:148387ms step_avg:98.86ms
step:1512/1770 train_time:148492ms step_avg:98.86ms
step:1513/1770 train_time:148597ms step_avg:98.87ms
step:1514/1770 train_time:148700ms step_avg:98.87ms
step:1515/1770 train_time:148804ms step_avg:98.87ms
step:1516/1770 train_time:148907ms step_avg:98.88ms
step:1517/1770 train_time:149011ms step_avg:98.88ms
step:1518/1770 train_time:149116ms step_avg:98.88ms
step:1519/1770 train_time:149219ms step_avg:98.89ms
step:1520/1770 train_time:149323ms step_avg:98.89ms
step:1521/1770 train_time:149426ms step_avg:98.89ms
step:1522/1770 train_time:149530ms step_avg:98.90ms
step:1523/1770 train_time:149635ms step_avg:98.90ms
step:1524/1770 train_time:149738ms step_avg:98.90ms
step:1525/1770 train_time:149841ms step_avg:98.90ms
step:1526/1770 train_time:149943ms step_avg:98.91ms
step:1527/1770 train_time:150047ms step_avg:98.91ms
step:1528/1770 train_time:150152ms step_avg:98.91ms
step:1529/1770 train_time:150254ms step_avg:98.92ms
step:1530/1770 train_time:150358ms step_avg:98.92ms
step:1531/1770 train_time:150461ms step_avg:98.92ms
step:1532/1770 train_time:150565ms step_avg:98.93ms
step:1533/1770 train_time:150669ms step_avg:98.93ms
step:1534/1770 train_time:150773ms step_avg:98.93ms
step:1535/1770 train_time:150877ms step_avg:98.94ms
step:1536/1770 train_time:150980ms step_avg:98.94ms
step:1537/1770 train_time:151084ms step_avg:98.94ms
step:1538/1770 train_time:151188ms step_avg:98.94ms
step:1539/1770 train_time:151290ms step_avg:98.95ms
step:1540/1770 train_time:151397ms step_avg:98.95ms
step:1541/1770 train_time:151500ms step_avg:98.96ms
step:1542/1770 train_time:151603ms step_avg:98.96ms
step:1543/1770 train_time:151706ms step_avg:98.96ms
step:1544/1770 train_time:151812ms step_avg:98.96ms
step:1545/1770 train_time:151915ms step_avg:98.97ms
step:1546/1770 train_time:152019ms step_avg:98.97ms
step:1547/1770 train_time:152121ms step_avg:98.97ms
step:1548/1770 train_time:152224ms step_avg:98.98ms
step:1549/1770 train_time:152328ms step_avg:98.98ms
step:1550/1770 train_time:152433ms step_avg:98.98ms
step:1551/1770 train_time:152537ms step_avg:98.99ms
step:1552/1770 train_time:152642ms step_avg:98.99ms
step:1553/1770 train_time:152744ms step_avg:98.99ms
step:1554/1770 train_time:152847ms step_avg:98.99ms
step:1555/1770 train_time:152951ms step_avg:99.00ms
step:1556/1770 train_time:153054ms step_avg:99.00ms
step:1557/1770 train_time:153158ms step_avg:99.00ms
step:1558/1770 train_time:153262ms step_avg:99.01ms
step:1559/1770 train_time:153365ms step_avg:99.01ms
step:1560/1770 train_time:153468ms step_avg:99.01ms
step:1561/1770 train_time:153573ms step_avg:99.02ms
step:1562/1770 train_time:153676ms step_avg:99.02ms
step:1563/1770 train_time:153780ms step_avg:99.02ms
step:1564/1770 train_time:153882ms step_avg:99.02ms
step:1565/1770 train_time:153985ms step_avg:99.03ms
step:1566/1770 train_time:154088ms step_avg:99.03ms
step:1567/1770 train_time:154192ms step_avg:99.03ms
step:1568/1770 train_time:154296ms step_avg:99.03ms
step:1569/1770 train_time:154402ms step_avg:99.04ms
step:1570/1770 train_time:154505ms step_avg:99.04ms
step:1571/1770 train_time:154609ms step_avg:99.04ms
step:1572/1770 train_time:154713ms step_avg:99.05ms
step:1573/1770 train_time:154818ms step_avg:99.05ms
step:1574/1770 train_time:154921ms step_avg:99.05ms
step:1575/1770 train_time:155023ms step_avg:99.06ms
step:1576/1770 train_time:155126ms step_avg:99.06ms
step:1577/1770 train_time:155231ms step_avg:99.06ms
step:1578/1770 train_time:155337ms step_avg:99.07ms
step:1579/1770 train_time:155441ms step_avg:99.07ms
step:1580/1770 train_time:155543ms step_avg:99.07ms
step:1581/1770 train_time:155649ms step_avg:99.08ms
step:1582/1770 train_time:155754ms step_avg:99.08ms
step:1583/1770 train_time:155858ms step_avg:99.08ms
step:1584/1770 train_time:155963ms step_avg:99.09ms
step:1585/1770 train_time:156066ms step_avg:99.09ms
step:1586/1770 train_time:156173ms step_avg:99.09ms
step:1587/1770 train_time:156277ms step_avg:99.10ms
step:1588/1770 train_time:156380ms step_avg:99.10ms
step:1589/1770 train_time:156485ms step_avg:99.10ms
step:1590/1770 train_time:156589ms step_avg:99.11ms
step:1591/1770 train_time:156692ms step_avg:99.11ms
step:1592/1770 train_time:156796ms step_avg:99.11ms
step:1593/1770 train_time:156899ms step_avg:99.12ms
step:1594/1770 train_time:157003ms step_avg:99.12ms
step:1595/1770 train_time:157107ms step_avg:99.12ms
step:1596/1770 train_time:157211ms step_avg:99.12ms
step:1597/1770 train_time:157313ms step_avg:99.13ms
step:1598/1770 train_time:157416ms step_avg:99.13ms
step:1599/1770 train_time:157521ms step_avg:99.13ms
step:1600/1770 train_time:157627ms step_avg:99.14ms
step:1601/1770 train_time:157733ms step_avg:99.14ms
step:1602/1770 train_time:157838ms step_avg:99.14ms
step:1603/1770 train_time:157942ms step_avg:99.15ms
step:1604/1770 train_time:158043ms step_avg:99.15ms
step:1605/1770 train_time:158146ms step_avg:99.15ms
step:1606/1770 train_time:158250ms step_avg:99.15ms
step:1607/1770 train_time:158357ms step_avg:99.16ms
step:1608/1770 train_time:158460ms step_avg:99.16ms
step:1609/1770 train_time:158564ms step_avg:99.16ms
step:1610/1770 train_time:158668ms step_avg:99.17ms
step:1611/1770 train_time:158773ms step_avg:99.17ms
step:1612/1770 train_time:158877ms step_avg:99.17ms
step:1613/1770 train_time:158981ms step_avg:99.18ms
step:1614/1770 train_time:159084ms step_avg:99.18ms
step:1615/1770 train_time:159188ms step_avg:99.18ms
step:1616/1770 train_time:159291ms step_avg:99.19ms
step:1617/1770 train_time:159397ms step_avg:99.19ms
step:1618/1770 train_time:159501ms step_avg:99.19ms
step:1619/1770 train_time:159605ms step_avg:99.20ms
step:1620/1770 train_time:159709ms step_avg:99.20ms
step:1621/1770 train_time:159812ms step_avg:99.20ms
step:1622/1770 train_time:159917ms step_avg:99.20ms
step:1623/1770 train_time:160024ms step_avg:99.21ms
step:1624/1770 train_time:160126ms step_avg:99.21ms
step:1625/1770 train_time:160230ms step_avg:99.21ms
step:1625/1770 val_loss:3.3090 train_time:160331ms step_avg:99.28ms
step:1626/1770 train_time:160353ms step_avg:99.23ms
step:1627/1770 train_time:160444ms step_avg:99.22ms
step:1628/1770 train_time:160546ms step_avg:99.22ms
step:1629/1770 train_time:160648ms step_avg:99.23ms
step:1630/1770 train_time:160751ms step_avg:99.23ms
step:1631/1770 train_time:160854ms step_avg:99.23ms
step:1632/1770 train_time:160957ms step_avg:99.23ms
step:1633/1770 train_time:161060ms step_avg:99.24ms
step:1634/1770 train_time:161163ms step_avg:99.24ms
step:1635/1770 train_time:161266ms step_avg:99.24ms
step:1636/1770 train_time:161371ms step_avg:99.24ms
step:1637/1770 train_time:161476ms step_avg:99.25ms
step:1638/1770 train_time:161580ms step_avg:99.25ms
step:1639/1770 train_time:161684ms step_avg:99.25ms
step:1640/1770 train_time:161788ms step_avg:99.26ms
step:1641/1770 train_time:161892ms step_avg:99.26ms
step:1642/1770 train_time:161995ms step_avg:99.26ms
step:1643/1770 train_time:162098ms step_avg:99.26ms
step:1644/1770 train_time:162203ms step_avg:99.27ms
step:1645/1770 train_time:162306ms step_avg:99.27ms
step:1646/1770 train_time:162410ms step_avg:99.27ms
step:1647/1770 train_time:162516ms step_avg:99.28ms
step:1648/1770 train_time:162619ms step_avg:99.28ms
step:1649/1770 train_time:162723ms step_avg:99.28ms
step:1650/1770 train_time:162826ms step_avg:99.28ms
step:1651/1770 train_time:162928ms step_avg:99.29ms
step:1652/1770 train_time:163032ms step_avg:99.29ms
step:1653/1770 train_time:163136ms step_avg:99.29ms
step:1654/1770 train_time:163243ms step_avg:99.30ms
step:1655/1770 train_time:163349ms step_avg:99.30ms
step:1656/1770 train_time:163452ms step_avg:99.30ms
step:1657/1770 train_time:163557ms step_avg:99.31ms
step:1658/1770 train_time:163662ms step_avg:99.31ms
step:1659/1770 train_time:163768ms step_avg:99.31ms
step:1660/1770 train_time:163871ms step_avg:99.32ms
step:1661/1770 train_time:163976ms step_avg:99.32ms
step:1662/1770 train_time:164081ms step_avg:99.32ms
step:1663/1770 train_time:164183ms step_avg:99.32ms
step:1664/1770 train_time:164287ms step_avg:99.33ms
step:1665/1770 train_time:164389ms step_avg:99.33ms
step:1666/1770 train_time:164493ms step_avg:99.33ms
step:1667/1770 train_time:164596ms step_avg:99.33ms
step:1668/1770 train_time:164699ms step_avg:99.34ms
step:1669/1770 train_time:164802ms step_avg:99.34ms
step:1670/1770 train_time:164905ms step_avg:99.34ms
step:1671/1770 train_time:165008ms step_avg:99.34ms
step:1672/1770 train_time:165113ms step_avg:99.35ms
step:1673/1770 train_time:165219ms step_avg:99.35ms
step:1674/1770 train_time:165321ms step_avg:99.35ms
step:1675/1770 train_time:165424ms step_avg:99.35ms
step:1676/1770 train_time:165528ms step_avg:99.36ms
step:1677/1770 train_time:165635ms step_avg:99.36ms
step:1678/1770 train_time:165738ms step_avg:99.36ms
step:1679/1770 train_time:165841ms step_avg:99.37ms
step:1680/1770 train_time:165944ms step_avg:99.37ms
step:1681/1770 train_time:166048ms step_avg:99.37ms
step:1682/1770 train_time:166153ms step_avg:99.37ms
step:1683/1770 train_time:166257ms step_avg:99.38ms
step:1684/1770 train_time:166360ms step_avg:99.38ms
step:1685/1770 train_time:166464ms step_avg:99.38ms
step:1686/1770 train_time:166569ms step_avg:99.38ms
step:1687/1770 train_time:166674ms step_avg:99.39ms
step:1688/1770 train_time:166778ms step_avg:99.39ms
step:1689/1770 train_time:166881ms step_avg:99.39ms
step:1690/1770 train_time:166985ms step_avg:99.40ms
step:1691/1770 train_time:167089ms step_avg:99.40ms
step:1692/1770 train_time:167192ms step_avg:99.40ms
step:1693/1770 train_time:167298ms step_avg:99.40ms
step:1694/1770 train_time:167401ms step_avg:99.41ms
step:1695/1770 train_time:167505ms step_avg:99.41ms
step:1696/1770 train_time:167610ms step_avg:99.41ms
step:1697/1770 train_time:167715ms step_avg:99.42ms
step:1698/1770 train_time:167819ms step_avg:99.42ms
step:1699/1770 train_time:167921ms step_avg:99.42ms
step:1700/1770 train_time:168024ms step_avg:99.42ms
step:1701/1770 train_time:168127ms step_avg:99.42ms
step:1702/1770 train_time:168232ms step_avg:99.43ms
step:1703/1770 train_time:168335ms step_avg:99.43ms
step:1704/1770 train_time:168439ms step_avg:99.43ms
step:1705/1770 train_time:168542ms step_avg:99.43ms
step:1706/1770 train_time:168645ms step_avg:99.44ms
step:1707/1770 train_time:168749ms step_avg:99.44ms
step:1708/1770 train_time:168853ms step_avg:99.44ms
step:1709/1770 train_time:168959ms step_avg:99.45ms
step:1710/1770 train_time:169067ms step_avg:99.45ms
step:1711/1770 train_time:169173ms step_avg:99.45ms
step:1712/1770 train_time:169277ms step_avg:99.46ms
step:1713/1770 train_time:169380ms step_avg:99.46ms
step:1714/1770 train_time:169485ms step_avg:99.46ms
step:1715/1770 train_time:169588ms step_avg:99.46ms
step:1716/1770 train_time:169692ms step_avg:99.47ms
step:1717/1770 train_time:169796ms step_avg:99.47ms
step:1718/1770 train_time:169901ms step_avg:99.47ms
step:1719/1770 train_time:170007ms step_avg:99.48ms
step:1720/1770 train_time:170112ms step_avg:99.48ms
step:1721/1770 train_time:170215ms step_avg:99.48ms
step:1722/1770 train_time:170322ms step_avg:99.49ms
step:1723/1770 train_time:170428ms step_avg:99.49ms
step:1724/1770 train_time:170534ms step_avg:99.49ms
step:1725/1770 train_time:170640ms step_avg:99.50ms
step:1726/1770 train_time:170746ms step_avg:99.50ms
step:1727/1770 train_time:170849ms step_avg:99.50ms
step:1728/1770 train_time:170955ms step_avg:99.51ms
step:1729/1770 train_time:171058ms step_avg:99.51ms
step:1730/1770 train_time:171164ms step_avg:99.51ms
step:1731/1770 train_time:171269ms step_avg:99.52ms
step:1732/1770 train_time:171373ms step_avg:99.52ms
step:1733/1770 train_time:171479ms step_avg:99.52ms
step:1734/1770 train_time:171583ms step_avg:99.53ms
step:1735/1770 train_time:171688ms step_avg:99.53ms
step:1736/1770 train_time:171792ms step_avg:99.53ms
step:1737/1770 train_time:171897ms step_avg:99.53ms
step:1738/1770 train_time:172002ms step_avg:99.54ms
step:1739/1770 train_time:172106ms step_avg:99.54ms
step:1740/1770 train_time:172210ms step_avg:99.54ms
step:1741/1770 train_time:172316ms step_avg:99.55ms
step:1742/1770 train_time:172423ms step_avg:99.55ms
step:1743/1770 train_time:172528ms step_avg:99.55ms
step:1744/1770 train_time:172632ms step_avg:99.56ms
step:1745/1770 train_time:172736ms step_avg:99.56ms
step:1746/1770 train_time:172843ms step_avg:99.56ms
step:1747/1770 train_time:172946ms step_avg:99.57ms
step:1748/1770 train_time:173052ms step_avg:99.57ms
step:1749/1770 train_time:173157ms step_avg:99.57ms
step:1750/1770 train_time:173261ms step_avg:99.58ms
step:1750/1770 val_loss:3.2824 train_time:173363ms step_avg:99.63ms
step:1751/1770 train_time:173384ms step_avg:99.59ms
step:1752/1770 train_time:173475ms step_avg:99.58ms
step:1753/1770 train_time:173580ms step_avg:99.59ms
step:1754/1770 train_time:173685ms step_avg:99.59ms
step:1755/1770 train_time:173788ms step_avg:99.59ms
step:1756/1770 train_time:173893ms step_avg:99.60ms
step:1757/1770 train_time:173997ms step_avg:99.60ms
step:1758/1770 train_time:174101ms step_avg:99.60ms
step:1759/1770 train_time:174205ms step_avg:99.60ms
step:1760/1770 train_time:174310ms step_avg:99.61ms
step:1761/1770 train_time:174416ms step_avg:99.61ms
step:1762/1770 train_time:174524ms step_avg:99.61ms
step:1763/1770 train_time:174627ms step_avg:99.62ms
step:1764/1770 train_time:174732ms step_avg:99.62ms
step:1765/1770 train_time:174835ms step_avg:99.62ms
step:1766/1770 train_time:174944ms step_avg:99.63ms
step:1767/1770 train_time:175048ms step_avg:99.63ms
step:1768/1770 train_time:175152ms step_avg:99.63ms
step:1769/1770 train_time:175256ms step_avg:99.63ms
step:1770/1770 train_time:175359ms step_avg:99.64ms
step:1770/1770 val_loss:3.2794 train_time:175464ms step_avg:99.70ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
