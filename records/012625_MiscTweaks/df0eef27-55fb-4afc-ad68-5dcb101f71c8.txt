import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import numpy as np

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            with torch.autocast("cuda", dtype=torch.bfloat16):
                return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, dim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads, max_seq_len)
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3*self.num_heads, -1).chunk(3, dim=-2)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, x: Tensor) -> list[Tensor | None]:
        ve = [emb(x) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_masks(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & document_bm
            full_bm  = causal_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            def build_bm(sw_num_blocks: Tensor) -> BlockMask:
                return BlockMask.from_kv_blocks(
                    torch.clamp_max(kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                    kv_indices,
                    torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                    full_kv_indices,
                    BLOCK_SIZE=BLOCK_SIZE,
                    mask_mod=document_causal,
                )
            return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        long_bm, short_bm = create_doc_swc_block_masks(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False

def train(args: Hyperparameters):
    # torchrun sets these env variables
    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])
    assert torch.cuda.is_available()
    device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
    torch.cuda.set_device(device)
    dist.init_process_group(backend="nccl", device_id=device)
    dist.barrier()
    master_process = (rank == 0) # this process will do logging, checkpointing etc.

    # begin logging
    logfile = None
    if master_process:
        run_id = uuid.uuid4()
        os.makedirs("logs", exist_ok=True)
        logfile = f"logs/{run_id}.txt"
        print(logfile)
    def print0(s, console=False):
        if master_process:
            with open(logfile, "a") as f:
                if console:
                    print(s)
                print(s, file=f)

    # begin by printing this file (the Python code)
    print0(code)
    print0("="*100)
    # log information about the hardware/software environment this is running on
    print0(f"Running Python {sys.version}")
    print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
    def nvidia_smi():
        import subprocess  # avoid top level import
        return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
    print0(nvidia_smi())
    print0("="*100)

    # load data
    batch_size = world_size * args.seq_len
    train_loader = distributed_data_generator(args.train_files, batch_size, rank, world_size)

    model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
    for param in model.parameters():
        dist.broadcast(param.detach(), 0)

    # collect the parameters to optimize
    hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim >= 2]
    embed_params = [model.embed.weight, *model.value_embeds.parameters()]
    scalar_params = [p for p in model.parameters() if p.ndim < 2]
    head_params = [model.lm_head.weight]

    # init the optimizer(s)
    adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
    # small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
    # discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
    optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True, eps=1e-10)
    optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
    optimizers = [optimizer1, optimizer2]

    # learning rate schedule: stable then decay
    def get_lr(it: int):
        t = 1 - it / args.num_iterations # time remaining in training
        assert 1 >= t >= 0
        w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
        return w * 1.0 + (1 - w) * 0.1
    schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
    @lru_cache(1)
    def sw_num_blks(window_size: int):
        return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

    model: nn.Module = torch.compile(model, dynamic=False)

    training_time_ms = 0
    # start the clock
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    # begin training
    train_steps = args.num_iterations
    if master_process:
        from collections import defaultdict
        param_name_to_grad_scale_map = defaultdict(lambda: 1e9)
        param_name_to_weight_scale_map = defaultdict(lambda: 1e9)
    for step in range(train_steps + 1):
        last_step = (step == train_steps)
        # This effectively ignores timing first 10 steps, which are slower for weird reasons.
        # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
        # steps with dummy data first, and then re-initialize the model and reset the loader.
        if step == 10:
            training_time_ms = 0
            t0 = time.perf_counter()
        timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

        # Linearly increase the block-wise sliding window size over training 128 -> 1792:
        # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
        window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
        # --------------- VALIDATION SECTION -----------------
        if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
            # stop the clock
            torch.cuda.synchronize()
            training_time_ms += 1000 * (time.perf_counter() - t0)
            model.eval()
            val_bs = world_size * args.val_seq_len
            assert args.val_tokens % val_bs == 0
            val_steps = args.val_tokens // val_bs
            val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
            val_loss = 0
            with torch.no_grad():
                for _ in range(val_steps):
                    x, y = next(val_loader)
                    val_loss += model(x, y, sw_num_blks(window_size))
            val_loss /= val_steps
            del val_loader
            dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
            print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
            model.train()
            # start the clock again
            torch.cuda.synchronize()
            t0 = time.perf_counter()

        if last_step:
            if master_process and args.save_checkpoint:
                log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
                os.makedirs(f"logs/{run_id}", exist_ok=True)
                torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
            # the last step only has the validation loop, so break to avoid training
            break

        # --------------- TRAINING SECTION BEGIN -----------------
        inputs, targets = next(train_loader)
        for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
            model(input_seq, target_seq, sw_num_blks(window_size)).backward()
        for param in model.parameters():
            dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
        if False and master_process and (step+5) % 25 == 0:
            for name, param in model.named_parameters():
                if not ("c_proj" in name or "c_fc" in name or "lm_head" in name):
                    continue
                if param.grad is not None:
                    grad_max_abs = param.grad.abs().max().item()
                    weight_max_abs = param.abs().max().item()
                    grad_scale = np.ceil(np.log2(0.8 * 40896.0 / grad_max_abs)) if grad_max_abs > 1e-9 else 1
                    weight_scale = np.ceil(np.log2(0.8 * 448.0 / weight_max_abs)) if weight_max_abs > 1e-9 else 1
                    if step > 0:
                        param_name_to_grad_scale_map[name] = min(param_name_to_grad_scale_map[name], grad_scale)
                        param_name_to_weight_scale_map[name] = min(param_name_to_weight_scale_map[name], weight_scale)
                    print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)
        # momentum warmup for Muon
        frac = min(step / 300, 1)
        for group in optimizer2.param_groups:
            group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
        # step the optimizers and schedulers
        for opt, sched in zip(optimizers, schedulers):
            opt.step()
            sched.step()
        # null the gradients
        model.zero_grad(set_to_none=True)
        # logging
        approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
        print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)
    if master_process:
        param_names = list(param_name_to_grad_scale_map.keys())
        for name in param_names:
            grad_scale = param_name_to_grad_scale_map[name]
            weight_scale = param_name_to_weight_scale_map[name]
            print0(f"{name:<40} | weight_scale={weight_scale:<3} | grad_scale={grad_scale:<3}", console=True)

    print0(
        f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
        f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
        console=True,
    )
    dist.destroy_process_group()

if __name__ == "__main__":
    args = Hyperparameters()
    train(args)

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sat Jan 25 23:13:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24192ms step_avg:nanms
step:2/1770 train_time:24704ms step_avg:nanms
step:3/1770 train_time:24799ms step_avg:nanms
step:4/1770 train_time:24892ms step_avg:nanms
step:5/1770 train_time:24985ms step_avg:nanms
step:6/1770 train_time:25079ms step_avg:nanms
step:7/1770 train_time:25172ms step_avg:nanms
step:8/1770 train_time:25266ms step_avg:nanms
step:9/1770 train_time:25360ms step_avg:nanms
step:10/1770 train_time:25453ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:93.97ms
step:14/1770 train_time:376ms step_avg:94.07ms
step:15/1770 train_time:471ms step_avg:94.16ms
step:16/1770 train_time:564ms step_avg:94.07ms
step:17/1770 train_time:659ms step_avg:94.08ms
step:18/1770 train_time:752ms step_avg:94.00ms
step:19/1770 train_time:846ms step_avg:93.97ms
step:20/1770 train_time:939ms step_avg:93.94ms
step:21/1770 train_time:1033ms step_avg:93.94ms
step:22/1770 train_time:1127ms step_avg:93.91ms
step:23/1770 train_time:1221ms step_avg:93.91ms
step:24/1770 train_time:1315ms step_avg:93.93ms
step:25/1770 train_time:1410ms step_avg:93.98ms
step:26/1770 train_time:1504ms step_avg:93.99ms
step:27/1770 train_time:1598ms step_avg:93.97ms
step:28/1770 train_time:1692ms step_avg:93.99ms
step:29/1770 train_time:1785ms step_avg:93.97ms
step:30/1770 train_time:1879ms step_avg:93.97ms
step:31/1770 train_time:1974ms step_avg:94.02ms
step:32/1770 train_time:2068ms step_avg:94.00ms
step:33/1770 train_time:2162ms step_avg:93.99ms
step:34/1770 train_time:2255ms step_avg:93.97ms
step:35/1770 train_time:2350ms step_avg:94.00ms
step:36/1770 train_time:2444ms step_avg:94.00ms
step:37/1770 train_time:2539ms step_avg:94.03ms
step:38/1770 train_time:2632ms step_avg:94.02ms
step:39/1770 train_time:2726ms step_avg:94.01ms
step:40/1770 train_time:2820ms step_avg:94.00ms
step:41/1770 train_time:2914ms step_avg:94.01ms
step:42/1770 train_time:3008ms step_avg:94.00ms
step:43/1770 train_time:3102ms step_avg:94.01ms
step:44/1770 train_time:3197ms step_avg:94.02ms
step:45/1770 train_time:3291ms step_avg:94.02ms
step:46/1770 train_time:3385ms step_avg:94.02ms
step:47/1770 train_time:3478ms step_avg:94.01ms
step:48/1770 train_time:3572ms step_avg:94.01ms
step:49/1770 train_time:3666ms step_avg:94.01ms
step:50/1770 train_time:3760ms step_avg:94.00ms
step:51/1770 train_time:3854ms step_avg:94.00ms
step:52/1770 train_time:3948ms step_avg:94.00ms
step:53/1770 train_time:4042ms step_avg:93.99ms
step:54/1770 train_time:4135ms step_avg:93.98ms
step:55/1770 train_time:4229ms step_avg:93.98ms
step:56/1770 train_time:4324ms step_avg:93.99ms
step:57/1770 train_time:4417ms step_avg:93.99ms
step:58/1770 train_time:4512ms step_avg:93.99ms
step:59/1770 train_time:4605ms step_avg:93.99ms
step:60/1770 train_time:4699ms step_avg:93.98ms
step:61/1770 train_time:4793ms step_avg:93.98ms
step:62/1770 train_time:4887ms step_avg:93.99ms
step:63/1770 train_time:4981ms step_avg:93.98ms
step:64/1770 train_time:5075ms step_avg:93.98ms
step:65/1770 train_time:5169ms step_avg:93.98ms
step:66/1770 train_time:5263ms step_avg:93.98ms
step:67/1770 train_time:5358ms step_avg:93.99ms
step:68/1770 train_time:5452ms step_avg:94.00ms
step:69/1770 train_time:5545ms step_avg:93.99ms
step:70/1770 train_time:5639ms step_avg:93.98ms
step:71/1770 train_time:5733ms step_avg:93.98ms
step:72/1770 train_time:5827ms step_avg:93.98ms
step:73/1770 train_time:5920ms step_avg:93.97ms
step:74/1770 train_time:6014ms step_avg:93.97ms
step:75/1770 train_time:6108ms step_avg:93.97ms
step:76/1770 train_time:6203ms step_avg:93.98ms
step:77/1770 train_time:6297ms step_avg:93.98ms
step:78/1770 train_time:6390ms step_avg:93.97ms
step:79/1770 train_time:6484ms step_avg:93.97ms
step:80/1770 train_time:6578ms step_avg:93.97ms
step:81/1770 train_time:6672ms step_avg:93.97ms
step:82/1770 train_time:6766ms step_avg:93.97ms
step:83/1770 train_time:6860ms step_avg:93.97ms
step:84/1770 train_time:6954ms step_avg:93.97ms
step:85/1770 train_time:7048ms step_avg:93.97ms
step:86/1770 train_time:7142ms step_avg:93.97ms
step:87/1770 train_time:7236ms step_avg:93.98ms
step:88/1770 train_time:7330ms step_avg:93.98ms
step:89/1770 train_time:7424ms step_avg:93.98ms
step:90/1770 train_time:7518ms step_avg:93.97ms
step:91/1770 train_time:7611ms step_avg:93.97ms
step:92/1770 train_time:7705ms step_avg:93.96ms
step:93/1770 train_time:7799ms step_avg:93.96ms
step:94/1770 train_time:7893ms step_avg:93.96ms
step:95/1770 train_time:7987ms step_avg:93.96ms
step:96/1770 train_time:8080ms step_avg:93.96ms
step:97/1770 train_time:8174ms step_avg:93.95ms
step:98/1770 train_time:8268ms step_avg:93.95ms
step:99/1770 train_time:8362ms step_avg:93.95ms
step:100/1770 train_time:8455ms step_avg:93.95ms
step:101/1770 train_time:8549ms step_avg:93.94ms
step:102/1770 train_time:8642ms step_avg:93.94ms
step:103/1770 train_time:8736ms step_avg:93.94ms
step:104/1770 train_time:8830ms step_avg:93.94ms
step:105/1770 train_time:8924ms step_avg:93.94ms
step:106/1770 train_time:9018ms step_avg:93.94ms
step:107/1770 train_time:9112ms step_avg:93.94ms
step:108/1770 train_time:9206ms step_avg:93.94ms
step:109/1770 train_time:9299ms step_avg:93.93ms
step:110/1770 train_time:9393ms step_avg:93.93ms
step:111/1770 train_time:9487ms step_avg:93.93ms
step:112/1770 train_time:9581ms step_avg:93.93ms
step:113/1770 train_time:9675ms step_avg:93.93ms
step:114/1770 train_time:9769ms step_avg:93.93ms
step:115/1770 train_time:9863ms step_avg:93.93ms
step:116/1770 train_time:9957ms step_avg:93.93ms
step:117/1770 train_time:10050ms step_avg:93.93ms
step:118/1770 train_time:10144ms step_avg:93.93ms
step:119/1770 train_time:10238ms step_avg:93.92ms
step:120/1770 train_time:10332ms step_avg:93.93ms
step:121/1770 train_time:10427ms step_avg:93.93ms
step:122/1770 train_time:10521ms step_avg:93.93ms
step:123/1770 train_time:10615ms step_avg:93.93ms
step:124/1770 train_time:10709ms step_avg:93.94ms
step:125/1770 train_time:10803ms step_avg:93.94ms
step:125/1770 val_loss:4.6484 train_time:10895ms step_avg:94.74ms
step:126/1770 train_time:10917ms step_avg:94.11ms
step:127/1770 train_time:10993ms step_avg:93.96ms
step:128/1770 train_time:11089ms step_avg:93.98ms
step:129/1770 train_time:11193ms step_avg:94.06ms
step:130/1770 train_time:11288ms step_avg:94.07ms
step:131/1770 train_time:11382ms step_avg:94.07ms
step:132/1770 train_time:11476ms step_avg:94.06ms
step:133/1770 train_time:11569ms step_avg:94.06ms
step:134/1770 train_time:11664ms step_avg:94.06ms
step:135/1770 train_time:11758ms step_avg:94.06ms
step:136/1770 train_time:11852ms step_avg:94.06ms
step:137/1770 train_time:11946ms step_avg:94.07ms
step:138/1770 train_time:12041ms step_avg:94.07ms
step:139/1770 train_time:12136ms step_avg:94.08ms
step:140/1770 train_time:12231ms step_avg:94.09ms
step:141/1770 train_time:12326ms step_avg:94.09ms
step:142/1770 train_time:12420ms step_avg:94.09ms
step:143/1770 train_time:12515ms step_avg:94.10ms
step:144/1770 train_time:12610ms step_avg:94.10ms
step:145/1770 train_time:12705ms step_avg:94.11ms
step:146/1770 train_time:12799ms step_avg:94.11ms
step:147/1770 train_time:12894ms step_avg:94.11ms
step:148/1770 train_time:12988ms step_avg:94.12ms
step:149/1770 train_time:13082ms step_avg:94.12ms
step:150/1770 train_time:13178ms step_avg:94.13ms
step:151/1770 train_time:13272ms step_avg:94.13ms
step:152/1770 train_time:13367ms step_avg:94.13ms
step:153/1770 train_time:13461ms step_avg:94.13ms
step:154/1770 train_time:13556ms step_avg:94.14ms
step:155/1770 train_time:13650ms step_avg:94.14ms
step:156/1770 train_time:13745ms step_avg:94.14ms
step:157/1770 train_time:13839ms step_avg:94.15ms
step:158/1770 train_time:13934ms step_avg:94.15ms
step:159/1770 train_time:14028ms step_avg:94.15ms
step:160/1770 train_time:14123ms step_avg:94.15ms
step:161/1770 train_time:14218ms step_avg:94.16ms
step:162/1770 train_time:14312ms step_avg:94.16ms
step:163/1770 train_time:14406ms step_avg:94.16ms
step:164/1770 train_time:14501ms step_avg:94.16ms
step:165/1770 train_time:14595ms step_avg:94.16ms
step:166/1770 train_time:14689ms step_avg:94.16ms
step:167/1770 train_time:14784ms step_avg:94.16ms
step:168/1770 train_time:14879ms step_avg:94.17ms
step:169/1770 train_time:14973ms step_avg:94.17ms
step:170/1770 train_time:15067ms step_avg:94.17ms
step:171/1770 train_time:15162ms step_avg:94.17ms
step:172/1770 train_time:15256ms step_avg:94.17ms
step:173/1770 train_time:15350ms step_avg:94.17ms
step:174/1770 train_time:15444ms step_avg:94.17ms
step:175/1770 train_time:15538ms step_avg:94.17ms
step:176/1770 train_time:15633ms step_avg:94.17ms
step:177/1770 train_time:15727ms step_avg:94.17ms
step:178/1770 train_time:15821ms step_avg:94.18ms
step:179/1770 train_time:15916ms step_avg:94.18ms
step:180/1770 train_time:16010ms step_avg:94.18ms
step:181/1770 train_time:16104ms step_avg:94.18ms
step:182/1770 train_time:16199ms step_avg:94.18ms
step:183/1770 train_time:16293ms step_avg:94.18ms
step:184/1770 train_time:16387ms step_avg:94.18ms
step:185/1770 train_time:16482ms step_avg:94.18ms
step:186/1770 train_time:16577ms step_avg:94.19ms
step:187/1770 train_time:16671ms step_avg:94.19ms
step:188/1770 train_time:16766ms step_avg:94.19ms
step:189/1770 train_time:16860ms step_avg:94.19ms
step:190/1770 train_time:16954ms step_avg:94.19ms
step:191/1770 train_time:17049ms step_avg:94.19ms
step:192/1770 train_time:17143ms step_avg:94.20ms
step:193/1770 train_time:17238ms step_avg:94.20ms
step:194/1770 train_time:17334ms step_avg:94.20ms
step:195/1770 train_time:17428ms step_avg:94.21ms
step:196/1770 train_time:17523ms step_avg:94.21ms
step:197/1770 train_time:17617ms step_avg:94.21ms
step:198/1770 train_time:17712ms step_avg:94.22ms
step:199/1770 train_time:17807ms step_avg:94.21ms
step:200/1770 train_time:17902ms step_avg:94.22ms
step:201/1770 train_time:17996ms step_avg:94.22ms
step:202/1770 train_time:18091ms step_avg:94.22ms
step:203/1770 train_time:18185ms step_avg:94.22ms
step:204/1770 train_time:18280ms step_avg:94.22ms
step:205/1770 train_time:18375ms step_avg:94.23ms
step:206/1770 train_time:18469ms step_avg:94.23ms
step:207/1770 train_time:18564ms step_avg:94.23ms
step:208/1770 train_time:18659ms step_avg:94.24ms
step:209/1770 train_time:18753ms step_avg:94.24ms
step:210/1770 train_time:18848ms step_avg:94.24ms
step:211/1770 train_time:18942ms step_avg:94.24ms
step:212/1770 train_time:19037ms step_avg:94.24ms
step:213/1770 train_time:19131ms step_avg:94.24ms
step:214/1770 train_time:19226ms step_avg:94.24ms
step:215/1770 train_time:19321ms step_avg:94.25ms
step:216/1770 train_time:19415ms step_avg:94.25ms
step:217/1770 train_time:19509ms step_avg:94.25ms
step:218/1770 train_time:19604ms step_avg:94.25ms
step:219/1770 train_time:19699ms step_avg:94.25ms
step:220/1770 train_time:19793ms step_avg:94.25ms
step:221/1770 train_time:19887ms step_avg:94.25ms
step:222/1770 train_time:19982ms step_avg:94.25ms
step:223/1770 train_time:20076ms step_avg:94.25ms
step:224/1770 train_time:20171ms step_avg:94.26ms
step:225/1770 train_time:20266ms step_avg:94.26ms
step:226/1770 train_time:20360ms step_avg:94.26ms
step:227/1770 train_time:20455ms step_avg:94.26ms
step:228/1770 train_time:20549ms step_avg:94.26ms
step:229/1770 train_time:20644ms step_avg:94.26ms
step:230/1770 train_time:20739ms step_avg:94.27ms
step:231/1770 train_time:20834ms step_avg:94.27ms
step:232/1770 train_time:20929ms step_avg:94.27ms
step:233/1770 train_time:21023ms step_avg:94.27ms
step:234/1770 train_time:21118ms step_avg:94.28ms
step:235/1770 train_time:21213ms step_avg:94.28ms
step:236/1770 train_time:21307ms step_avg:94.28ms
step:237/1770 train_time:21401ms step_avg:94.28ms
step:238/1770 train_time:21497ms step_avg:94.28ms
step:239/1770 train_time:21592ms step_avg:94.29ms
step:240/1770 train_time:21687ms step_avg:94.29ms
step:241/1770 train_time:21781ms step_avg:94.29ms
step:242/1770 train_time:21876ms step_avg:94.30ms
step:243/1770 train_time:21971ms step_avg:94.30ms
step:244/1770 train_time:22065ms step_avg:94.30ms
step:245/1770 train_time:22161ms step_avg:94.30ms
step:246/1770 train_time:22255ms step_avg:94.30ms
step:247/1770 train_time:22349ms step_avg:94.30ms
step:248/1770 train_time:22444ms step_avg:94.30ms
step:249/1770 train_time:22539ms step_avg:94.31ms
step:250/1770 train_time:22634ms step_avg:94.31ms
step:250/1770 val_loss:4.1165 train_time:22727ms step_avg:94.70ms
step:251/1770 train_time:22748ms step_avg:94.39ms
step:252/1770 train_time:22832ms step_avg:94.35ms
step:253/1770 train_time:22929ms step_avg:94.36ms
step:254/1770 train_time:23024ms step_avg:94.36ms
step:255/1770 train_time:23119ms step_avg:94.36ms
step:256/1770 train_time:23213ms step_avg:94.36ms
step:257/1770 train_time:23307ms step_avg:94.36ms
step:258/1770 train_time:23401ms step_avg:94.36ms
step:259/1770 train_time:23495ms step_avg:94.36ms
step:260/1770 train_time:23589ms step_avg:94.36ms
step:261/1770 train_time:23683ms step_avg:94.35ms
step:262/1770 train_time:23778ms step_avg:94.36ms
step:263/1770 train_time:23873ms step_avg:94.36ms
step:264/1770 train_time:23969ms step_avg:94.36ms
step:265/1770 train_time:24064ms step_avg:94.37ms
step:266/1770 train_time:24158ms step_avg:94.37ms
step:267/1770 train_time:24253ms step_avg:94.37ms
step:268/1770 train_time:24348ms step_avg:94.37ms
step:269/1770 train_time:24443ms step_avg:94.38ms
step:270/1770 train_time:24538ms step_avg:94.38ms
step:271/1770 train_time:24632ms step_avg:94.38ms
step:272/1770 train_time:24728ms step_avg:94.38ms
step:273/1770 train_time:24824ms step_avg:94.39ms
step:274/1770 train_time:24919ms step_avg:94.39ms
step:275/1770 train_time:25014ms step_avg:94.39ms
step:276/1770 train_time:25109ms step_avg:94.40ms
step:277/1770 train_time:25204ms step_avg:94.40ms
step:278/1770 train_time:25299ms step_avg:94.40ms
step:279/1770 train_time:25395ms step_avg:94.40ms
step:280/1770 train_time:25490ms step_avg:94.41ms
step:281/1770 train_time:25586ms step_avg:94.41ms
step:282/1770 train_time:25681ms step_avg:94.41ms
step:283/1770 train_time:25775ms step_avg:94.42ms
step:284/1770 train_time:25870ms step_avg:94.42ms
step:285/1770 train_time:25966ms step_avg:94.42ms
step:286/1770 train_time:26060ms step_avg:94.42ms
step:287/1770 train_time:26155ms step_avg:94.42ms
step:288/1770 train_time:26251ms step_avg:94.43ms
step:289/1770 train_time:26346ms step_avg:94.43ms
step:290/1770 train_time:26441ms step_avg:94.43ms
step:291/1770 train_time:26536ms step_avg:94.43ms
step:292/1770 train_time:26631ms step_avg:94.44ms
step:293/1770 train_time:26726ms step_avg:94.44ms
step:294/1770 train_time:26821ms step_avg:94.44ms
step:295/1770 train_time:26917ms step_avg:94.44ms
step:296/1770 train_time:27012ms step_avg:94.45ms
step:297/1770 train_time:27107ms step_avg:94.45ms
step:298/1770 train_time:27202ms step_avg:94.45ms
step:299/1770 train_time:27297ms step_avg:94.45ms
step:300/1770 train_time:27393ms step_avg:94.46ms
step:301/1770 train_time:27488ms step_avg:94.46ms
step:302/1770 train_time:27582ms step_avg:94.46ms
step:303/1770 train_time:27677ms step_avg:94.46ms
step:304/1770 train_time:27773ms step_avg:94.47ms
step:305/1770 train_time:27868ms step_avg:94.47ms
step:306/1770 train_time:27964ms step_avg:94.47ms
step:307/1770 train_time:28059ms step_avg:94.47ms
step:308/1770 train_time:28154ms step_avg:94.48ms
step:309/1770 train_time:28249ms step_avg:94.48ms
step:310/1770 train_time:28344ms step_avg:94.48ms
step:311/1770 train_time:28439ms step_avg:94.48ms
step:312/1770 train_time:28534ms step_avg:94.48ms
step:313/1770 train_time:28629ms step_avg:94.49ms
step:314/1770 train_time:28724ms step_avg:94.49ms
step:315/1770 train_time:28820ms step_avg:94.49ms
step:316/1770 train_time:28915ms step_avg:94.49ms
step:317/1770 train_time:29010ms step_avg:94.50ms
step:318/1770 train_time:29106ms step_avg:94.50ms
step:319/1770 train_time:29202ms step_avg:94.50ms
step:320/1770 train_time:29297ms step_avg:94.51ms
step:321/1770 train_time:29391ms step_avg:94.51ms
step:322/1770 train_time:29487ms step_avg:94.51ms
step:323/1770 train_time:29581ms step_avg:94.51ms
step:324/1770 train_time:29676ms step_avg:94.51ms
step:325/1770 train_time:29771ms step_avg:94.51ms
step:326/1770 train_time:29866ms step_avg:94.51ms
step:327/1770 train_time:29961ms step_avg:94.51ms
step:328/1770 train_time:30057ms step_avg:94.52ms
step:329/1770 train_time:30152ms step_avg:94.52ms
step:330/1770 train_time:30247ms step_avg:94.52ms
step:331/1770 train_time:30341ms step_avg:94.52ms
step:332/1770 train_time:30436ms step_avg:94.52ms
step:333/1770 train_time:30531ms step_avg:94.52ms
step:334/1770 train_time:30626ms step_avg:94.53ms
step:335/1770 train_time:30721ms step_avg:94.53ms
step:336/1770 train_time:30816ms step_avg:94.53ms
step:337/1770 train_time:30911ms step_avg:94.53ms
step:338/1770 train_time:31006ms step_avg:94.53ms
step:339/1770 train_time:31101ms step_avg:94.53ms
step:340/1770 train_time:31196ms step_avg:94.53ms
step:341/1770 train_time:31291ms step_avg:94.53ms
step:342/1770 train_time:31386ms step_avg:94.54ms
step:343/1770 train_time:31481ms step_avg:94.54ms
step:344/1770 train_time:31576ms step_avg:94.54ms
step:345/1770 train_time:31671ms step_avg:94.54ms
step:346/1770 train_time:31766ms step_avg:94.54ms
step:347/1770 train_time:31861ms step_avg:94.54ms
step:348/1770 train_time:31957ms step_avg:94.55ms
step:349/1770 train_time:32052ms step_avg:94.55ms
step:350/1770 train_time:32147ms step_avg:94.55ms
step:351/1770 train_time:32241ms step_avg:94.55ms
step:352/1770 train_time:32336ms step_avg:94.55ms
step:353/1770 train_time:32431ms step_avg:94.55ms
step:354/1770 train_time:32526ms step_avg:94.55ms
step:355/1770 train_time:32621ms step_avg:94.55ms
step:356/1770 train_time:32716ms step_avg:94.55ms
step:357/1770 train_time:32811ms step_avg:94.56ms
step:358/1770 train_time:32906ms step_avg:94.56ms
step:359/1770 train_time:33001ms step_avg:94.56ms
step:360/1770 train_time:33096ms step_avg:94.56ms
step:361/1770 train_time:33191ms step_avg:94.56ms
step:362/1770 train_time:33286ms step_avg:94.56ms
step:363/1770 train_time:33380ms step_avg:94.56ms
step:364/1770 train_time:33475ms step_avg:94.56ms
step:365/1770 train_time:33571ms step_avg:94.57ms
step:366/1770 train_time:33665ms step_avg:94.57ms
step:367/1770 train_time:33760ms step_avg:94.57ms
step:368/1770 train_time:33855ms step_avg:94.57ms
step:369/1770 train_time:33951ms step_avg:94.57ms
step:370/1770 train_time:34045ms step_avg:94.57ms
step:371/1770 train_time:34140ms step_avg:94.57ms
step:372/1770 train_time:34235ms step_avg:94.57ms
step:373/1770 train_time:34330ms step_avg:94.57ms
step:374/1770 train_time:34425ms step_avg:94.57ms
step:375/1770 train_time:34521ms step_avg:94.58ms
step:375/1770 val_loss:3.9097 train_time:34613ms step_avg:94.83ms
step:376/1770 train_time:34637ms step_avg:94.64ms
step:377/1770 train_time:34718ms step_avg:94.60ms
step:378/1770 train_time:34816ms step_avg:94.61ms
step:379/1770 train_time:34911ms step_avg:94.61ms
step:380/1770 train_time:35006ms step_avg:94.61ms
step:381/1770 train_time:35100ms step_avg:94.61ms
step:382/1770 train_time:35195ms step_avg:94.61ms
step:383/1770 train_time:35289ms step_avg:94.61ms
step:384/1770 train_time:35383ms step_avg:94.61ms
step:385/1770 train_time:35477ms step_avg:94.61ms
step:386/1770 train_time:35572ms step_avg:94.61ms
step:387/1770 train_time:35668ms step_avg:94.61ms
step:388/1770 train_time:35764ms step_avg:94.61ms
step:389/1770 train_time:35860ms step_avg:94.62ms
step:390/1770 train_time:35956ms step_avg:94.62ms
step:391/1770 train_time:36050ms step_avg:94.62ms
step:392/1770 train_time:36145ms step_avg:94.62ms
step:393/1770 train_time:36240ms step_avg:94.62ms
step:394/1770 train_time:36334ms step_avg:94.62ms
step:395/1770 train_time:36429ms step_avg:94.62ms
step:396/1770 train_time:36526ms step_avg:94.63ms
step:397/1770 train_time:36622ms step_avg:94.63ms
step:398/1770 train_time:36719ms step_avg:94.64ms
step:399/1770 train_time:36816ms step_avg:94.64ms
step:400/1770 train_time:36913ms step_avg:94.65ms
step:401/1770 train_time:37010ms step_avg:94.65ms
step:402/1770 train_time:37106ms step_avg:94.66ms
step:403/1770 train_time:37203ms step_avg:94.66ms
step:404/1770 train_time:37300ms step_avg:94.67ms
step:405/1770 train_time:37397ms step_avg:94.68ms
step:406/1770 train_time:37493ms step_avg:94.68ms
step:407/1770 train_time:37590ms step_avg:94.69ms
step:408/1770 train_time:37687ms step_avg:94.69ms
step:409/1770 train_time:37784ms step_avg:94.70ms
step:410/1770 train_time:37881ms step_avg:94.70ms
step:411/1770 train_time:37977ms step_avg:94.71ms
step:412/1770 train_time:38074ms step_avg:94.71ms
step:413/1770 train_time:38171ms step_avg:94.72ms
step:414/1770 train_time:38268ms step_avg:94.72ms
step:415/1770 train_time:38365ms step_avg:94.73ms
step:416/1770 train_time:38461ms step_avg:94.73ms
step:417/1770 train_time:38559ms step_avg:94.74ms
step:418/1770 train_time:38656ms step_avg:94.75ms
step:419/1770 train_time:38753ms step_avg:94.75ms
step:420/1770 train_time:38851ms step_avg:94.76ms
step:421/1770 train_time:38948ms step_avg:94.76ms
step:422/1770 train_time:39044ms step_avg:94.77ms
step:423/1770 train_time:39141ms step_avg:94.77ms
step:424/1770 train_time:39238ms step_avg:94.78ms
step:425/1770 train_time:39335ms step_avg:94.78ms
step:426/1770 train_time:39432ms step_avg:94.79ms
step:427/1770 train_time:39529ms step_avg:94.79ms
step:428/1770 train_time:39627ms step_avg:94.80ms
step:429/1770 train_time:39724ms step_avg:94.81ms
step:430/1770 train_time:39822ms step_avg:94.81ms
step:431/1770 train_time:39918ms step_avg:94.82ms
step:432/1770 train_time:40015ms step_avg:94.82ms
step:433/1770 train_time:40112ms step_avg:94.83ms
step:434/1770 train_time:40209ms step_avg:94.83ms
step:435/1770 train_time:40306ms step_avg:94.84ms
step:436/1770 train_time:40402ms step_avg:94.84ms
step:437/1770 train_time:40499ms step_avg:94.85ms
step:438/1770 train_time:40596ms step_avg:94.85ms
step:439/1770 train_time:40692ms step_avg:94.85ms
step:440/1770 train_time:40790ms step_avg:94.86ms
step:441/1770 train_time:40887ms step_avg:94.86ms
step:442/1770 train_time:40983ms step_avg:94.87ms
step:443/1770 train_time:41080ms step_avg:94.87ms
step:444/1770 train_time:41177ms step_avg:94.88ms
step:445/1770 train_time:41273ms step_avg:94.88ms
step:446/1770 train_time:41370ms step_avg:94.89ms
step:447/1770 train_time:41468ms step_avg:94.89ms
step:448/1770 train_time:41565ms step_avg:94.90ms
step:449/1770 train_time:41662ms step_avg:94.90ms
step:450/1770 train_time:41760ms step_avg:94.91ms
step:451/1770 train_time:41856ms step_avg:94.91ms
step:452/1770 train_time:41953ms step_avg:94.92ms
step:453/1770 train_time:42050ms step_avg:94.92ms
step:454/1770 train_time:42147ms step_avg:94.92ms
step:455/1770 train_time:42244ms step_avg:94.93ms
step:456/1770 train_time:42341ms step_avg:94.94ms
step:457/1770 train_time:42438ms step_avg:94.94ms
step:458/1770 train_time:42535ms step_avg:94.94ms
step:459/1770 train_time:42632ms step_avg:94.95ms
step:460/1770 train_time:42729ms step_avg:94.95ms
step:461/1770 train_time:42826ms step_avg:94.96ms
step:462/1770 train_time:42923ms step_avg:94.96ms
step:463/1770 train_time:43019ms step_avg:94.97ms
step:464/1770 train_time:43116ms step_avg:94.97ms
step:465/1770 train_time:43213ms step_avg:94.97ms
step:466/1770 train_time:43310ms step_avg:94.98ms
step:467/1770 train_time:43407ms step_avg:94.98ms
step:468/1770 train_time:43504ms step_avg:94.99ms
step:469/1770 train_time:43601ms step_avg:94.99ms
step:470/1770 train_time:43699ms step_avg:95.00ms
step:471/1770 train_time:43795ms step_avg:95.00ms
step:472/1770 train_time:43892ms step_avg:95.00ms
step:473/1770 train_time:43989ms step_avg:95.01ms
step:474/1770 train_time:44086ms step_avg:95.01ms
step:475/1770 train_time:44182ms step_avg:95.02ms
step:476/1770 train_time:44279ms step_avg:95.02ms
step:477/1770 train_time:44375ms step_avg:95.02ms
step:478/1770 train_time:44472ms step_avg:95.03ms
step:479/1770 train_time:44569ms step_avg:95.03ms
step:480/1770 train_time:44666ms step_avg:95.04ms
step:481/1770 train_time:44764ms step_avg:95.04ms
step:482/1770 train_time:44861ms step_avg:95.04ms
step:483/1770 train_time:44958ms step_avg:95.05ms
step:484/1770 train_time:45055ms step_avg:95.05ms
step:485/1770 train_time:45152ms step_avg:95.06ms
step:486/1770 train_time:45249ms step_avg:95.06ms
step:487/1770 train_time:45346ms step_avg:95.07ms
step:488/1770 train_time:45443ms step_avg:95.07ms
step:489/1770 train_time:45539ms step_avg:95.07ms
step:490/1770 train_time:45636ms step_avg:95.07ms
step:491/1770 train_time:45733ms step_avg:95.08ms
step:492/1770 train_time:45829ms step_avg:95.08ms
step:493/1770 train_time:45926ms step_avg:95.08ms
step:494/1770 train_time:46023ms step_avg:95.09ms
step:495/1770 train_time:46120ms step_avg:95.09ms
step:496/1770 train_time:46216ms step_avg:95.10ms
step:497/1770 train_time:46313ms step_avg:95.10ms
step:498/1770 train_time:46410ms step_avg:95.10ms
step:499/1770 train_time:46507ms step_avg:95.11ms
step:500/1770 train_time:46604ms step_avg:95.11ms
step:500/1770 val_loss:3.7584 train_time:46699ms step_avg:95.30ms
step:501/1770 train_time:46720ms step_avg:95.15ms
step:502/1770 train_time:46803ms step_avg:95.13ms
step:503/1770 train_time:46903ms step_avg:95.14ms
step:504/1770 train_time:47001ms step_avg:95.14ms
step:505/1770 train_time:47098ms step_avg:95.15ms
step:506/1770 train_time:47195ms step_avg:95.15ms
step:507/1770 train_time:47291ms step_avg:95.15ms
step:508/1770 train_time:47387ms step_avg:95.15ms
step:509/1770 train_time:47484ms step_avg:95.16ms
step:510/1770 train_time:47580ms step_avg:95.16ms
step:511/1770 train_time:47677ms step_avg:95.16ms
step:512/1770 train_time:47774ms step_avg:95.17ms
step:513/1770 train_time:47871ms step_avg:95.17ms
step:514/1770 train_time:47969ms step_avg:95.18ms
step:515/1770 train_time:48066ms step_avg:95.18ms
step:516/1770 train_time:48163ms step_avg:95.18ms
step:517/1770 train_time:48261ms step_avg:95.19ms
step:518/1770 train_time:48358ms step_avg:95.19ms
step:519/1770 train_time:48454ms step_avg:95.19ms
step:520/1770 train_time:48551ms step_avg:95.20ms
step:521/1770 train_time:48647ms step_avg:95.20ms
step:522/1770 train_time:48743ms step_avg:95.20ms
step:523/1770 train_time:48840ms step_avg:95.20ms
step:524/1770 train_time:48937ms step_avg:95.21ms
step:525/1770 train_time:49035ms step_avg:95.21ms
step:526/1770 train_time:49131ms step_avg:95.22ms
step:527/1770 train_time:49229ms step_avg:95.22ms
step:528/1770 train_time:49326ms step_avg:95.22ms
step:529/1770 train_time:49423ms step_avg:95.23ms
step:530/1770 train_time:49520ms step_avg:95.23ms
step:531/1770 train_time:49618ms step_avg:95.24ms
step:532/1770 train_time:49716ms step_avg:95.24ms
step:533/1770 train_time:49812ms step_avg:95.24ms
step:534/1770 train_time:49909ms step_avg:95.25ms
step:535/1770 train_time:50007ms step_avg:95.25ms
step:536/1770 train_time:50104ms step_avg:95.26ms
step:537/1770 train_time:50202ms step_avg:95.26ms
step:538/1770 train_time:50299ms step_avg:95.26ms
step:539/1770 train_time:50396ms step_avg:95.27ms
step:540/1770 train_time:50494ms step_avg:95.27ms
step:541/1770 train_time:50591ms step_avg:95.27ms
step:542/1770 train_time:50688ms step_avg:95.28ms
step:543/1770 train_time:50785ms step_avg:95.28ms
step:544/1770 train_time:50882ms step_avg:95.28ms
step:545/1770 train_time:50980ms step_avg:95.29ms
step:546/1770 train_time:51077ms step_avg:95.29ms
step:547/1770 train_time:51175ms step_avg:95.30ms
step:548/1770 train_time:51272ms step_avg:95.30ms
step:549/1770 train_time:51369ms step_avg:95.30ms
step:550/1770 train_time:51466ms step_avg:95.31ms
step:551/1770 train_time:51563ms step_avg:95.31ms
step:552/1770 train_time:51661ms step_avg:95.32ms
step:553/1770 train_time:51758ms step_avg:95.32ms
step:554/1770 train_time:51855ms step_avg:95.32ms
step:555/1770 train_time:51952ms step_avg:95.33ms
step:556/1770 train_time:52050ms step_avg:95.33ms
step:557/1770 train_time:52147ms step_avg:95.33ms
step:558/1770 train_time:52244ms step_avg:95.34ms
step:559/1770 train_time:52342ms step_avg:95.34ms
step:560/1770 train_time:52439ms step_avg:95.34ms
step:561/1770 train_time:52536ms step_avg:95.35ms
step:562/1770 train_time:52633ms step_avg:95.35ms
step:563/1770 train_time:52730ms step_avg:95.35ms
step:564/1770 train_time:52827ms step_avg:95.36ms
step:565/1770 train_time:52924ms step_avg:95.36ms
step:566/1770 train_time:53022ms step_avg:95.36ms
step:567/1770 train_time:53119ms step_avg:95.37ms
step:568/1770 train_time:53217ms step_avg:95.37ms
step:569/1770 train_time:53314ms step_avg:95.37ms
step:570/1770 train_time:53411ms step_avg:95.38ms
step:571/1770 train_time:53509ms step_avg:95.38ms
step:572/1770 train_time:53606ms step_avg:95.38ms
step:573/1770 train_time:53703ms step_avg:95.39ms
step:574/1770 train_time:53801ms step_avg:95.39ms
step:575/1770 train_time:53898ms step_avg:95.39ms
step:576/1770 train_time:53995ms step_avg:95.40ms
step:577/1770 train_time:54092ms step_avg:95.40ms
step:578/1770 train_time:54189ms step_avg:95.40ms
step:579/1770 train_time:54286ms step_avg:95.41ms
step:580/1770 train_time:54384ms step_avg:95.41ms
step:581/1770 train_time:54482ms step_avg:95.42ms
step:582/1770 train_time:54580ms step_avg:95.42ms
step:583/1770 train_time:54677ms step_avg:95.42ms
step:584/1770 train_time:54774ms step_avg:95.43ms
step:585/1770 train_time:54871ms step_avg:95.43ms
step:586/1770 train_time:54968ms step_avg:95.43ms
step:587/1770 train_time:55065ms step_avg:95.43ms
step:588/1770 train_time:55163ms step_avg:95.44ms
step:589/1770 train_time:55260ms step_avg:95.44ms
step:590/1770 train_time:55357ms step_avg:95.44ms
step:591/1770 train_time:55454ms step_avg:95.45ms
step:592/1770 train_time:55552ms step_avg:95.45ms
step:593/1770 train_time:55649ms step_avg:95.45ms
step:594/1770 train_time:55746ms step_avg:95.45ms
step:595/1770 train_time:55843ms step_avg:95.46ms
step:596/1770 train_time:55940ms step_avg:95.46ms
step:597/1770 train_time:56037ms step_avg:95.46ms
step:598/1770 train_time:56134ms step_avg:95.47ms
step:599/1770 train_time:56232ms step_avg:95.47ms
step:600/1770 train_time:56329ms step_avg:95.47ms
step:601/1770 train_time:56427ms step_avg:95.48ms
step:602/1770 train_time:56524ms step_avg:95.48ms
step:603/1770 train_time:56622ms step_avg:95.48ms
step:604/1770 train_time:56720ms step_avg:95.49ms
step:605/1770 train_time:56817ms step_avg:95.49ms
step:606/1770 train_time:56914ms step_avg:95.49ms
step:607/1770 train_time:57012ms step_avg:95.50ms
step:608/1770 train_time:57109ms step_avg:95.50ms
step:609/1770 train_time:57205ms step_avg:95.50ms
step:610/1770 train_time:57302ms step_avg:95.50ms
step:611/1770 train_time:57399ms step_avg:95.51ms
step:612/1770 train_time:57496ms step_avg:95.51ms
step:613/1770 train_time:57594ms step_avg:95.51ms
step:614/1770 train_time:57691ms step_avg:95.52ms
step:615/1770 train_time:57789ms step_avg:95.52ms
step:616/1770 train_time:57886ms step_avg:95.52ms
step:617/1770 train_time:57984ms step_avg:95.53ms
step:618/1770 train_time:58082ms step_avg:95.53ms
step:619/1770 train_time:58179ms step_avg:95.53ms
step:620/1770 train_time:58276ms step_avg:95.53ms
step:621/1770 train_time:58373ms step_avg:95.54ms
step:622/1770 train_time:58470ms step_avg:95.54ms
step:623/1770 train_time:58568ms step_avg:95.54ms
step:624/1770 train_time:58665ms step_avg:95.55ms
step:625/1770 train_time:58763ms step_avg:95.55ms
step:625/1770 val_loss:3.6678 train_time:58859ms step_avg:95.71ms
step:626/1770 train_time:58880ms step_avg:95.58ms
step:627/1770 train_time:58967ms step_avg:95.57ms
step:628/1770 train_time:59067ms step_avg:95.58ms
step:629/1770 train_time:59164ms step_avg:95.58ms
step:630/1770 train_time:59261ms step_avg:95.58ms
step:631/1770 train_time:59358ms step_avg:95.58ms
step:632/1770 train_time:59455ms step_avg:95.59ms
step:633/1770 train_time:59552ms step_avg:95.59ms
step:634/1770 train_time:59649ms step_avg:95.59ms
step:635/1770 train_time:59746ms step_avg:95.59ms
step:636/1770 train_time:59843ms step_avg:95.60ms
step:637/1770 train_time:59940ms step_avg:95.60ms
step:638/1770 train_time:60038ms step_avg:95.60ms
step:639/1770 train_time:60136ms step_avg:95.61ms
step:640/1770 train_time:60234ms step_avg:95.61ms
step:641/1770 train_time:60331ms step_avg:95.61ms
step:642/1770 train_time:60428ms step_avg:95.61ms
step:643/1770 train_time:60525ms step_avg:95.62ms
step:644/1770 train_time:60622ms step_avg:95.62ms
step:645/1770 train_time:60719ms step_avg:95.62ms
step:646/1770 train_time:60816ms step_avg:95.62ms
step:647/1770 train_time:60913ms step_avg:95.63ms
step:648/1770 train_time:61011ms step_avg:95.63ms
step:649/1770 train_time:61108ms step_avg:95.63ms
step:650/1770 train_time:61205ms step_avg:95.63ms
step:651/1770 train_time:61303ms step_avg:95.64ms
step:652/1770 train_time:61399ms step_avg:95.64ms
step:653/1770 train_time:61497ms step_avg:95.64ms
step:654/1770 train_time:61594ms step_avg:95.64ms
step:655/1770 train_time:61693ms step_avg:95.65ms
step:656/1770 train_time:61792ms step_avg:95.65ms
step:657/1770 train_time:61889ms step_avg:95.65ms
step:658/1770 train_time:61987ms step_avg:95.66ms
step:659/1770 train_time:62086ms step_avg:95.66ms
step:660/1770 train_time:62186ms step_avg:95.67ms
step:661/1770 train_time:62285ms step_avg:95.68ms
step:662/1770 train_time:62384ms step_avg:95.68ms
step:663/1770 train_time:62483ms step_avg:95.69ms
step:664/1770 train_time:62582ms step_avg:95.69ms
step:665/1770 train_time:62682ms step_avg:95.70ms
step:666/1770 train_time:62781ms step_avg:95.70ms
step:667/1770 train_time:62880ms step_avg:95.71ms
step:668/1770 train_time:62979ms step_avg:95.71ms
step:669/1770 train_time:63078ms step_avg:95.72ms
step:670/1770 train_time:63176ms step_avg:95.72ms
step:671/1770 train_time:63276ms step_avg:95.73ms
step:672/1770 train_time:63375ms step_avg:95.73ms
step:673/1770 train_time:63474ms step_avg:95.74ms
step:674/1770 train_time:63574ms step_avg:95.74ms
step:675/1770 train_time:63674ms step_avg:95.75ms
step:676/1770 train_time:63773ms step_avg:95.76ms
step:677/1770 train_time:63874ms step_avg:95.76ms
step:678/1770 train_time:63973ms step_avg:95.77ms
step:679/1770 train_time:64072ms step_avg:95.77ms
step:680/1770 train_time:64171ms step_avg:95.78ms
step:681/1770 train_time:64269ms step_avg:95.78ms
step:682/1770 train_time:64368ms step_avg:95.79ms
step:683/1770 train_time:64467ms step_avg:95.79ms
step:684/1770 train_time:64567ms step_avg:95.80ms
step:685/1770 train_time:64667ms step_avg:95.80ms
step:686/1770 train_time:64766ms step_avg:95.81ms
step:687/1770 train_time:64865ms step_avg:95.81ms
step:688/1770 train_time:64964ms step_avg:95.82ms
step:689/1770 train_time:65063ms step_avg:95.82ms
step:690/1770 train_time:65163ms step_avg:95.83ms
step:691/1770 train_time:65263ms step_avg:95.83ms
step:692/1770 train_time:65362ms step_avg:95.84ms
step:693/1770 train_time:65461ms step_avg:95.84ms
step:694/1770 train_time:65561ms step_avg:95.85ms
step:695/1770 train_time:65660ms step_avg:95.85ms
step:696/1770 train_time:65759ms step_avg:95.86ms
step:697/1770 train_time:65857ms step_avg:95.86ms
step:698/1770 train_time:65956ms step_avg:95.87ms
step:699/1770 train_time:66055ms step_avg:95.87ms
step:700/1770 train_time:66154ms step_avg:95.88ms
step:701/1770 train_time:66254ms step_avg:95.88ms
step:702/1770 train_time:66354ms step_avg:95.89ms
step:703/1770 train_time:66453ms step_avg:95.89ms
step:704/1770 train_time:66553ms step_avg:95.90ms
step:705/1770 train_time:66653ms step_avg:95.90ms
step:706/1770 train_time:66752ms step_avg:95.91ms
step:707/1770 train_time:66851ms step_avg:95.91ms
step:708/1770 train_time:66951ms step_avg:95.92ms
step:709/1770 train_time:67049ms step_avg:95.92ms
step:710/1770 train_time:67149ms step_avg:95.93ms
step:711/1770 train_time:67248ms step_avg:95.93ms
step:712/1770 train_time:67348ms step_avg:95.94ms
step:713/1770 train_time:67446ms step_avg:95.94ms
step:714/1770 train_time:67545ms step_avg:95.95ms
step:715/1770 train_time:67645ms step_avg:95.95ms
step:716/1770 train_time:67744ms step_avg:95.95ms
step:717/1770 train_time:67844ms step_avg:95.96ms
step:718/1770 train_time:67944ms step_avg:95.97ms
step:719/1770 train_time:68043ms step_avg:95.97ms
step:720/1770 train_time:68143ms step_avg:95.98ms
step:721/1770 train_time:68242ms step_avg:95.98ms
step:722/1770 train_time:68340ms step_avg:95.98ms
step:723/1770 train_time:68439ms step_avg:95.99ms
step:724/1770 train_time:68538ms step_avg:95.99ms
step:725/1770 train_time:68637ms step_avg:96.00ms
step:726/1770 train_time:68735ms step_avg:96.00ms
step:727/1770 train_time:68835ms step_avg:96.00ms
step:728/1770 train_time:68935ms step_avg:96.01ms
step:729/1770 train_time:69035ms step_avg:96.01ms
step:730/1770 train_time:69134ms step_avg:96.02ms
step:731/1770 train_time:69233ms step_avg:96.02ms
step:732/1770 train_time:69333ms step_avg:96.03ms
step:733/1770 train_time:69432ms step_avg:96.03ms
step:734/1770 train_time:69531ms step_avg:96.04ms
step:735/1770 train_time:69629ms step_avg:96.04ms
step:736/1770 train_time:69728ms step_avg:96.04ms
step:737/1770 train_time:69828ms step_avg:96.05ms
step:738/1770 train_time:69927ms step_avg:96.05ms
step:739/1770 train_time:70026ms step_avg:96.06ms
step:740/1770 train_time:70125ms step_avg:96.06ms
step:741/1770 train_time:70224ms step_avg:96.07ms
step:742/1770 train_time:70323ms step_avg:96.07ms
step:743/1770 train_time:70423ms step_avg:96.07ms
step:744/1770 train_time:70522ms step_avg:96.08ms
step:745/1770 train_time:70621ms step_avg:96.08ms
step:746/1770 train_time:70720ms step_avg:96.09ms
step:747/1770 train_time:70818ms step_avg:96.09ms
step:748/1770 train_time:70917ms step_avg:96.09ms
step:749/1770 train_time:71016ms step_avg:96.10ms
step:750/1770 train_time:71115ms step_avg:96.10ms
step:750/1770 val_loss:3.6019 train_time:71214ms step_avg:96.23ms
step:751/1770 train_time:71235ms step_avg:96.13ms
step:752/1770 train_time:71324ms step_avg:96.12ms
step:753/1770 train_time:71424ms step_avg:96.13ms
step:754/1770 train_time:71523ms step_avg:96.13ms
step:755/1770 train_time:71621ms step_avg:96.14ms
step:756/1770 train_time:71720ms step_avg:96.14ms
step:757/1770 train_time:71818ms step_avg:96.14ms
step:758/1770 train_time:71917ms step_avg:96.15ms
step:759/1770 train_time:72016ms step_avg:96.15ms
step:760/1770 train_time:72114ms step_avg:96.15ms
step:761/1770 train_time:72213ms step_avg:96.16ms
step:762/1770 train_time:72313ms step_avg:96.16ms
step:763/1770 train_time:72414ms step_avg:96.17ms
step:764/1770 train_time:72513ms step_avg:96.17ms
step:765/1770 train_time:72612ms step_avg:96.18ms
step:766/1770 train_time:72711ms step_avg:96.18ms
step:767/1770 train_time:72811ms step_avg:96.18ms
step:768/1770 train_time:72910ms step_avg:96.19ms
step:769/1770 train_time:73009ms step_avg:96.19ms
step:770/1770 train_time:73107ms step_avg:96.19ms
step:771/1770 train_time:73207ms step_avg:96.20ms
step:772/1770 train_time:73305ms step_avg:96.20ms
step:773/1770 train_time:73404ms step_avg:96.20ms
step:774/1770 train_time:73503ms step_avg:96.21ms
step:775/1770 train_time:73602ms step_avg:96.21ms
step:776/1770 train_time:73702ms step_avg:96.22ms
step:777/1770 train_time:73801ms step_avg:96.22ms
step:778/1770 train_time:73900ms step_avg:96.22ms
step:779/1770 train_time:73999ms step_avg:96.23ms
step:780/1770 train_time:74097ms step_avg:96.23ms
step:781/1770 train_time:74197ms step_avg:96.23ms
step:782/1770 train_time:74296ms step_avg:96.24ms
step:783/1770 train_time:74397ms step_avg:96.24ms
step:784/1770 train_time:74496ms step_avg:96.25ms
step:785/1770 train_time:74596ms step_avg:96.25ms
step:786/1770 train_time:74696ms step_avg:96.26ms
step:787/1770 train_time:74798ms step_avg:96.27ms
step:788/1770 train_time:74898ms step_avg:96.27ms
step:789/1770 train_time:74998ms step_avg:96.27ms
step:790/1770 train_time:75097ms step_avg:96.28ms
step:791/1770 train_time:75197ms step_avg:96.28ms
step:792/1770 train_time:75295ms step_avg:96.29ms
step:793/1770 train_time:75395ms step_avg:96.29ms
step:794/1770 train_time:75495ms step_avg:96.29ms
step:795/1770 train_time:75594ms step_avg:96.30ms
step:796/1770 train_time:75694ms step_avg:96.30ms
step:797/1770 train_time:75795ms step_avg:96.31ms
step:798/1770 train_time:75895ms step_avg:96.31ms
step:799/1770 train_time:75996ms step_avg:96.32ms
step:800/1770 train_time:76096ms step_avg:96.32ms
step:801/1770 train_time:76195ms step_avg:96.33ms
step:802/1770 train_time:76294ms step_avg:96.33ms
step:803/1770 train_time:76394ms step_avg:96.34ms
step:804/1770 train_time:76494ms step_avg:96.34ms
step:805/1770 train_time:76593ms step_avg:96.34ms
step:806/1770 train_time:76692ms step_avg:96.35ms
step:807/1770 train_time:76792ms step_avg:96.35ms
step:808/1770 train_time:76891ms step_avg:96.36ms
step:809/1770 train_time:76992ms step_avg:96.36ms
step:810/1770 train_time:77091ms step_avg:96.36ms
step:811/1770 train_time:77191ms step_avg:96.37ms
step:812/1770 train_time:77290ms step_avg:96.37ms
step:813/1770 train_time:77389ms step_avg:96.38ms
step:814/1770 train_time:77489ms step_avg:96.38ms
step:815/1770 train_time:77588ms step_avg:96.38ms
step:816/1770 train_time:77686ms step_avg:96.38ms
step:817/1770 train_time:77786ms step_avg:96.39ms
step:818/1770 train_time:77886ms step_avg:96.39ms
step:819/1770 train_time:77986ms step_avg:96.40ms
step:820/1770 train_time:78085ms step_avg:96.40ms
step:821/1770 train_time:78185ms step_avg:96.41ms
step:822/1770 train_time:78284ms step_avg:96.41ms
step:823/1770 train_time:78384ms step_avg:96.41ms
step:824/1770 train_time:78483ms step_avg:96.42ms
step:825/1770 train_time:78583ms step_avg:96.42ms
step:826/1770 train_time:78683ms step_avg:96.42ms
step:827/1770 train_time:78782ms step_avg:96.43ms
step:828/1770 train_time:78882ms step_avg:96.43ms
step:829/1770 train_time:78982ms step_avg:96.44ms
step:830/1770 train_time:79080ms step_avg:96.44ms
step:831/1770 train_time:79180ms step_avg:96.44ms
step:832/1770 train_time:79279ms step_avg:96.45ms
step:833/1770 train_time:79378ms step_avg:96.45ms
step:834/1770 train_time:79477ms step_avg:96.45ms
step:835/1770 train_time:79576ms step_avg:96.46ms
step:836/1770 train_time:79676ms step_avg:96.46ms
step:837/1770 train_time:79776ms step_avg:96.46ms
step:838/1770 train_time:79876ms step_avg:96.47ms
step:839/1770 train_time:79976ms step_avg:96.47ms
step:840/1770 train_time:80076ms step_avg:96.48ms
step:841/1770 train_time:80177ms step_avg:96.48ms
step:842/1770 train_time:80276ms step_avg:96.49ms
step:843/1770 train_time:80375ms step_avg:96.49ms
step:844/1770 train_time:80475ms step_avg:96.49ms
step:845/1770 train_time:80574ms step_avg:96.50ms
step:846/1770 train_time:80674ms step_avg:96.50ms
step:847/1770 train_time:80774ms step_avg:96.50ms
step:848/1770 train_time:80873ms step_avg:96.51ms
step:849/1770 train_time:80973ms step_avg:96.51ms
step:850/1770 train_time:81073ms step_avg:96.52ms
step:851/1770 train_time:81173ms step_avg:96.52ms
step:852/1770 train_time:81273ms step_avg:96.52ms
step:853/1770 train_time:81373ms step_avg:96.53ms
step:854/1770 train_time:81472ms step_avg:96.53ms
step:855/1770 train_time:81572ms step_avg:96.53ms
step:856/1770 train_time:81671ms step_avg:96.54ms
step:857/1770 train_time:81770ms step_avg:96.54ms
step:858/1770 train_time:81869ms step_avg:96.54ms
step:859/1770 train_time:81969ms step_avg:96.55ms
step:860/1770 train_time:82068ms step_avg:96.55ms
step:861/1770 train_time:82167ms step_avg:96.55ms
step:862/1770 train_time:82266ms step_avg:96.56ms
step:863/1770 train_time:82365ms step_avg:96.56ms
step:864/1770 train_time:82464ms step_avg:96.56ms
step:865/1770 train_time:82564ms step_avg:96.57ms
step:866/1770 train_time:82664ms step_avg:96.57ms
step:867/1770 train_time:82764ms step_avg:96.57ms
step:868/1770 train_time:82864ms step_avg:96.58ms
step:869/1770 train_time:82963ms step_avg:96.58ms
step:870/1770 train_time:83063ms step_avg:96.58ms
step:871/1770 train_time:83162ms step_avg:96.59ms
step:872/1770 train_time:83262ms step_avg:96.59ms
step:873/1770 train_time:83362ms step_avg:96.60ms
step:874/1770 train_time:83461ms step_avg:96.60ms
step:875/1770 train_time:83561ms step_avg:96.60ms
step:875/1770 val_loss:3.5506 train_time:83658ms step_avg:96.71ms
step:876/1770 train_time:83679ms step_avg:96.63ms
step:877/1770 train_time:83768ms step_avg:96.62ms
step:878/1770 train_time:83870ms step_avg:96.62ms
step:879/1770 train_time:83970ms step_avg:96.63ms
step:880/1770 train_time:84068ms step_avg:96.63ms
step:881/1770 train_time:84167ms step_avg:96.63ms
step:882/1770 train_time:84265ms step_avg:96.63ms
step:883/1770 train_time:84365ms step_avg:96.64ms
step:884/1770 train_time:84463ms step_avg:96.64ms
step:885/1770 train_time:84562ms step_avg:96.64ms
step:886/1770 train_time:84661ms step_avg:96.65ms
step:887/1770 train_time:84762ms step_avg:96.65ms
step:888/1770 train_time:84862ms step_avg:96.65ms
step:889/1770 train_time:84962ms step_avg:96.66ms
step:890/1770 train_time:85062ms step_avg:96.66ms
step:891/1770 train_time:85162ms step_avg:96.67ms
step:892/1770 train_time:85262ms step_avg:96.67ms
step:893/1770 train_time:85361ms step_avg:96.67ms
step:894/1770 train_time:85459ms step_avg:96.67ms
step:895/1770 train_time:85558ms step_avg:96.68ms
step:896/1770 train_time:85657ms step_avg:96.68ms
step:897/1770 train_time:85757ms step_avg:96.68ms
step:898/1770 train_time:85857ms step_avg:96.69ms
step:899/1770 train_time:85957ms step_avg:96.69ms
step:900/1770 train_time:86057ms step_avg:96.69ms
step:901/1770 train_time:86157ms step_avg:96.70ms
step:902/1770 train_time:86256ms step_avg:96.70ms
step:903/1770 train_time:86356ms step_avg:96.70ms
step:904/1770 train_time:86455ms step_avg:96.71ms
step:905/1770 train_time:86554ms step_avg:96.71ms
step:906/1770 train_time:86653ms step_avg:96.71ms
step:907/1770 train_time:86753ms step_avg:96.71ms
step:908/1770 train_time:86854ms step_avg:96.72ms
step:909/1770 train_time:86954ms step_avg:96.72ms
step:910/1770 train_time:87054ms step_avg:96.73ms
step:911/1770 train_time:87155ms step_avg:96.73ms
step:912/1770 train_time:87255ms step_avg:96.73ms
step:913/1770 train_time:87354ms step_avg:96.74ms
step:914/1770 train_time:87454ms step_avg:96.74ms
step:915/1770 train_time:87553ms step_avg:96.74ms
step:916/1770 train_time:87653ms step_avg:96.75ms
step:917/1770 train_time:87751ms step_avg:96.75ms
step:918/1770 train_time:87850ms step_avg:96.75ms
step:919/1770 train_time:87950ms step_avg:96.75ms
step:920/1770 train_time:88052ms step_avg:96.76ms
step:921/1770 train_time:88153ms step_avg:96.76ms
step:922/1770 train_time:88255ms step_avg:96.77ms
step:923/1770 train_time:88356ms step_avg:96.78ms
step:924/1770 train_time:88457ms step_avg:96.78ms
step:925/1770 train_time:88558ms step_avg:96.78ms
step:926/1770 train_time:88658ms step_avg:96.79ms
step:927/1770 train_time:88759ms step_avg:96.79ms
step:928/1770 train_time:88860ms step_avg:96.80ms
step:929/1770 train_time:88960ms step_avg:96.80ms
step:930/1770 train_time:89061ms step_avg:96.81ms
step:931/1770 train_time:89162ms step_avg:96.81ms
step:932/1770 train_time:89263ms step_avg:96.81ms
step:933/1770 train_time:89364ms step_avg:96.82ms
step:934/1770 train_time:89465ms step_avg:96.82ms
step:935/1770 train_time:89566ms step_avg:96.83ms
step:936/1770 train_time:89667ms step_avg:96.83ms
step:937/1770 train_time:89768ms step_avg:96.84ms
step:938/1770 train_time:89870ms step_avg:96.84ms
step:939/1770 train_time:89970ms step_avg:96.85ms
step:940/1770 train_time:90071ms step_avg:96.85ms
step:941/1770 train_time:90171ms step_avg:96.85ms
step:942/1770 train_time:90272ms step_avg:96.86ms
step:943/1770 train_time:90375ms step_avg:96.86ms
step:944/1770 train_time:90476ms step_avg:96.87ms
step:945/1770 train_time:90578ms step_avg:96.87ms
step:946/1770 train_time:90679ms step_avg:96.88ms
step:947/1770 train_time:90780ms step_avg:96.88ms
step:948/1770 train_time:90880ms step_avg:96.89ms
step:949/1770 train_time:90981ms step_avg:96.89ms
step:950/1770 train_time:91082ms step_avg:96.90ms
step:951/1770 train_time:91183ms step_avg:96.90ms
step:952/1770 train_time:91284ms step_avg:96.90ms
step:953/1770 train_time:91386ms step_avg:96.91ms
step:954/1770 train_time:91487ms step_avg:96.91ms
step:955/1770 train_time:91588ms step_avg:96.92ms
step:956/1770 train_time:91690ms step_avg:96.92ms
step:957/1770 train_time:91790ms step_avg:96.93ms
step:958/1770 train_time:91891ms step_avg:96.93ms
step:959/1770 train_time:91991ms step_avg:96.93ms
step:960/1770 train_time:92091ms step_avg:96.94ms
step:961/1770 train_time:92192ms step_avg:96.94ms
step:962/1770 train_time:92294ms step_avg:96.95ms
step:963/1770 train_time:92395ms step_avg:96.95ms
step:964/1770 train_time:92496ms step_avg:96.96ms
step:965/1770 train_time:92598ms step_avg:96.96ms
step:966/1770 train_time:92698ms step_avg:96.96ms
step:967/1770 train_time:92799ms step_avg:96.97ms
step:968/1770 train_time:92899ms step_avg:96.97ms
step:969/1770 train_time:92999ms step_avg:96.98ms
step:970/1770 train_time:93099ms step_avg:96.98ms
step:971/1770 train_time:93200ms step_avg:96.98ms
step:972/1770 train_time:93301ms step_avg:96.99ms
step:973/1770 train_time:93402ms step_avg:96.99ms
step:974/1770 train_time:93503ms step_avg:96.99ms
step:975/1770 train_time:93604ms step_avg:97.00ms
step:976/1770 train_time:93705ms step_avg:97.00ms
step:977/1770 train_time:93806ms step_avg:97.01ms
step:978/1770 train_time:93907ms step_avg:97.01ms
step:979/1770 train_time:94007ms step_avg:97.01ms
step:980/1770 train_time:94108ms step_avg:97.02ms
step:981/1770 train_time:94208ms step_avg:97.02ms
step:982/1770 train_time:94308ms step_avg:97.03ms
step:983/1770 train_time:94409ms step_avg:97.03ms
step:984/1770 train_time:94510ms step_avg:97.03ms
step:985/1770 train_time:94610ms step_avg:97.04ms
step:986/1770 train_time:94712ms step_avg:97.04ms
step:987/1770 train_time:94813ms step_avg:97.04ms
step:988/1770 train_time:94914ms step_avg:97.05ms
step:989/1770 train_time:95017ms step_avg:97.05ms
step:990/1770 train_time:95118ms step_avg:97.06ms
step:991/1770 train_time:95220ms step_avg:97.06ms
step:992/1770 train_time:95320ms step_avg:97.07ms
step:993/1770 train_time:95421ms step_avg:97.07ms
step:994/1770 train_time:95522ms step_avg:97.08ms
step:995/1770 train_time:95622ms step_avg:97.08ms
step:996/1770 train_time:95723ms step_avg:97.08ms
step:997/1770 train_time:95824ms step_avg:97.09ms
step:998/1770 train_time:95925ms step_avg:97.09ms
step:999/1770 train_time:96027ms step_avg:97.10ms
step:1000/1770 train_time:96129ms step_avg:97.10ms
step:1000/1770 val_loss:3.5119 train_time:96228ms step_avg:97.20ms
step:1001/1770 train_time:96250ms step_avg:97.12ms
step:1002/1770 train_time:96340ms step_avg:97.12ms
step:1003/1770 train_time:96442ms step_avg:97.12ms
step:1004/1770 train_time:96543ms step_avg:97.13ms
step:1005/1770 train_time:96642ms step_avg:97.13ms
step:1006/1770 train_time:96743ms step_avg:97.13ms
step:1007/1770 train_time:96842ms step_avg:97.13ms
step:1008/1770 train_time:96942ms step_avg:97.14ms
step:1009/1770 train_time:97042ms step_avg:97.14ms
step:1010/1770 train_time:97142ms step_avg:97.14ms
step:1011/1770 train_time:97245ms step_avg:97.15ms
step:1012/1770 train_time:97347ms step_avg:97.15ms
step:1013/1770 train_time:97449ms step_avg:97.16ms
step:1014/1770 train_time:97551ms step_avg:97.16ms
step:1015/1770 train_time:97651ms step_avg:97.16ms
step:1016/1770 train_time:97751ms step_avg:97.17ms
step:1017/1770 train_time:97851ms step_avg:97.17ms
step:1018/1770 train_time:97950ms step_avg:97.17ms
step:1019/1770 train_time:98051ms step_avg:97.18ms
step:1020/1770 train_time:98152ms step_avg:97.18ms
step:1021/1770 train_time:98254ms step_avg:97.18ms
step:1022/1770 train_time:98354ms step_avg:97.19ms
step:1023/1770 train_time:98455ms step_avg:97.19ms
step:1024/1770 train_time:98557ms step_avg:97.20ms
step:1025/1770 train_time:98659ms step_avg:97.20ms
step:1026/1770 train_time:98761ms step_avg:97.21ms
step:1027/1770 train_time:98862ms step_avg:97.21ms
step:1028/1770 train_time:98963ms step_avg:97.21ms
step:1029/1770 train_time:99063ms step_avg:97.22ms
step:1030/1770 train_time:99163ms step_avg:97.22ms
step:1031/1770 train_time:99263ms step_avg:97.22ms
step:1032/1770 train_time:99363ms step_avg:97.22ms
step:1033/1770 train_time:99464ms step_avg:97.23ms
step:1034/1770 train_time:99564ms step_avg:97.23ms
step:1035/1770 train_time:99665ms step_avg:97.23ms
step:1036/1770 train_time:99767ms step_avg:97.24ms
step:1037/1770 train_time:99867ms step_avg:97.24ms
step:1038/1770 train_time:99967ms step_avg:97.24ms
step:1039/1770 train_time:100068ms step_avg:97.25ms
step:1040/1770 train_time:100168ms step_avg:97.25ms
step:1041/1770 train_time:100269ms step_avg:97.25ms
step:1042/1770 train_time:100371ms step_avg:97.26ms
step:1043/1770 train_time:100471ms step_avg:97.26ms
step:1044/1770 train_time:100572ms step_avg:97.26ms
step:1045/1770 train_time:100673ms step_avg:97.27ms
step:1046/1770 train_time:100774ms step_avg:97.27ms
step:1047/1770 train_time:100874ms step_avg:97.28ms
step:1048/1770 train_time:100975ms step_avg:97.28ms
step:1049/1770 train_time:101075ms step_avg:97.28ms
step:1050/1770 train_time:101177ms step_avg:97.29ms
step:1051/1770 train_time:101279ms step_avg:97.29ms
step:1052/1770 train_time:101380ms step_avg:97.29ms
step:1053/1770 train_time:101482ms step_avg:97.30ms
step:1054/1770 train_time:101582ms step_avg:97.30ms
step:1055/1770 train_time:101683ms step_avg:97.30ms
step:1056/1770 train_time:101783ms step_avg:97.31ms
step:1057/1770 train_time:101883ms step_avg:97.31ms
step:1058/1770 train_time:101984ms step_avg:97.31ms
step:1059/1770 train_time:102084ms step_avg:97.32ms
step:1060/1770 train_time:102186ms step_avg:97.32ms
step:1061/1770 train_time:102289ms step_avg:97.32ms
step:1062/1770 train_time:102391ms step_avg:97.33ms
step:1063/1770 train_time:102493ms step_avg:97.33ms
step:1064/1770 train_time:102594ms step_avg:97.34ms
step:1065/1770 train_time:102694ms step_avg:97.34ms
step:1066/1770 train_time:102794ms step_avg:97.34ms
step:1067/1770 train_time:102894ms step_avg:97.35ms
step:1068/1770 train_time:102997ms step_avg:97.35ms
step:1069/1770 train_time:103098ms step_avg:97.35ms
step:1070/1770 train_time:103201ms step_avg:97.36ms
step:1071/1770 train_time:103302ms step_avg:97.36ms
step:1072/1770 train_time:103403ms step_avg:97.37ms
step:1073/1770 train_time:103503ms step_avg:97.37ms
step:1074/1770 train_time:103604ms step_avg:97.37ms
step:1075/1770 train_time:103705ms step_avg:97.38ms
step:1076/1770 train_time:103806ms step_avg:97.38ms
step:1077/1770 train_time:103907ms step_avg:97.38ms
step:1078/1770 train_time:104009ms step_avg:97.39ms
step:1079/1770 train_time:104110ms step_avg:97.39ms
step:1080/1770 train_time:104211ms step_avg:97.39ms
step:1081/1770 train_time:104312ms step_avg:97.40ms
step:1082/1770 train_time:104413ms step_avg:97.40ms
step:1083/1770 train_time:104514ms step_avg:97.40ms
step:1084/1770 train_time:104615ms step_avg:97.41ms
step:1085/1770 train_time:104716ms step_avg:97.41ms
step:1086/1770 train_time:104818ms step_avg:97.41ms
step:1087/1770 train_time:104920ms step_avg:97.42ms
step:1088/1770 train_time:105021ms step_avg:97.42ms
step:1089/1770 train_time:105122ms step_avg:97.43ms
step:1090/1770 train_time:105223ms step_avg:97.43ms
step:1091/1770 train_time:105323ms step_avg:97.43ms
step:1092/1770 train_time:105423ms step_avg:97.43ms
step:1093/1770 train_time:105523ms step_avg:97.44ms
step:1094/1770 train_time:105625ms step_avg:97.44ms
step:1095/1770 train_time:105726ms step_avg:97.44ms
step:1096/1770 train_time:105827ms step_avg:97.45ms
step:1097/1770 train_time:105929ms step_avg:97.45ms
step:1098/1770 train_time:106030ms step_avg:97.45ms
step:1099/1770 train_time:106131ms step_avg:97.46ms
step:1100/1770 train_time:106232ms step_avg:97.46ms
step:1101/1770 train_time:106333ms step_avg:97.46ms
step:1102/1770 train_time:106434ms step_avg:97.47ms
step:1103/1770 train_time:106535ms step_avg:97.47ms
step:1104/1770 train_time:106636ms step_avg:97.47ms
step:1105/1770 train_time:106737ms step_avg:97.48ms
step:1106/1770 train_time:106838ms step_avg:97.48ms
step:1107/1770 train_time:106940ms step_avg:97.48ms
step:1108/1770 train_time:107041ms step_avg:97.49ms
step:1109/1770 train_time:107143ms step_avg:97.49ms
step:1110/1770 train_time:107244ms step_avg:97.49ms
step:1111/1770 train_time:107345ms step_avg:97.50ms
step:1112/1770 train_time:107446ms step_avg:97.50ms
step:1113/1770 train_time:107547ms step_avg:97.50ms
step:1114/1770 train_time:107649ms step_avg:97.51ms
step:1115/1770 train_time:107750ms step_avg:97.51ms
step:1116/1770 train_time:107850ms step_avg:97.51ms
step:1117/1770 train_time:107951ms step_avg:97.52ms
step:1118/1770 train_time:108052ms step_avg:97.52ms
step:1119/1770 train_time:108153ms step_avg:97.52ms
step:1120/1770 train_time:108254ms step_avg:97.53ms
step:1121/1770 train_time:108355ms step_avg:97.53ms
step:1122/1770 train_time:108456ms step_avg:97.53ms
step:1123/1770 train_time:108557ms step_avg:97.54ms
step:1124/1770 train_time:108659ms step_avg:97.54ms
step:1125/1770 train_time:108761ms step_avg:97.54ms
step:1125/1770 val_loss:3.4729 train_time:108861ms step_avg:97.63ms
step:1126/1770 train_time:108882ms step_avg:97.56ms
step:1127/1770 train_time:108975ms step_avg:97.56ms
step:1128/1770 train_time:109076ms step_avg:97.56ms
step:1129/1770 train_time:109177ms step_avg:97.57ms
step:1130/1770 train_time:109277ms step_avg:97.57ms
step:1131/1770 train_time:109378ms step_avg:97.57ms
step:1132/1770 train_time:109478ms step_avg:97.57ms
step:1133/1770 train_time:109578ms step_avg:97.58ms
step:1134/1770 train_time:109679ms step_avg:97.58ms
step:1135/1770 train_time:109779ms step_avg:97.58ms
step:1136/1770 train_time:109881ms step_avg:97.59ms
step:1137/1770 train_time:109984ms step_avg:97.59ms
step:1138/1770 train_time:110086ms step_avg:97.59ms
step:1139/1770 train_time:110186ms step_avg:97.60ms
step:1140/1770 train_time:110288ms step_avg:97.60ms
step:1141/1770 train_time:110389ms step_avg:97.60ms
step:1142/1770 train_time:110489ms step_avg:97.61ms
step:1143/1770 train_time:110589ms step_avg:97.61ms
step:1144/1770 train_time:110690ms step_avg:97.61ms
step:1145/1770 train_time:110790ms step_avg:97.61ms
step:1146/1770 train_time:110891ms step_avg:97.62ms
step:1147/1770 train_time:110993ms step_avg:97.62ms
step:1148/1770 train_time:111093ms step_avg:97.62ms
step:1149/1770 train_time:111195ms step_avg:97.62ms
step:1150/1770 train_time:111296ms step_avg:97.63ms
step:1151/1770 train_time:111398ms step_avg:97.63ms
step:1152/1770 train_time:111500ms step_avg:97.64ms
step:1153/1770 train_time:111600ms step_avg:97.64ms
step:1154/1770 train_time:111700ms step_avg:97.64ms
step:1155/1770 train_time:111801ms step_avg:97.64ms
step:1156/1770 train_time:111902ms step_avg:97.65ms
step:1157/1770 train_time:112004ms step_avg:97.65ms
step:1158/1770 train_time:112108ms step_avg:97.65ms
step:1159/1770 train_time:112209ms step_avg:97.66ms
step:1160/1770 train_time:112310ms step_avg:97.66ms
step:1161/1770 train_time:112410ms step_avg:97.66ms
step:1162/1770 train_time:112511ms step_avg:97.67ms
step:1163/1770 train_time:112612ms step_avg:97.67ms
step:1164/1770 train_time:112712ms step_avg:97.67ms
step:1165/1770 train_time:112814ms step_avg:97.67ms
step:1166/1770 train_time:112916ms step_avg:97.68ms
step:1167/1770 train_time:113017ms step_avg:97.68ms
step:1168/1770 train_time:113118ms step_avg:97.68ms
step:1169/1770 train_time:113219ms step_avg:97.69ms
step:1170/1770 train_time:113320ms step_avg:97.69ms
step:1171/1770 train_time:113422ms step_avg:97.69ms
step:1172/1770 train_time:113523ms step_avg:97.70ms
step:1173/1770 train_time:113624ms step_avg:97.70ms
step:1174/1770 train_time:113726ms step_avg:97.70ms
step:1175/1770 train_time:113828ms step_avg:97.71ms
step:1176/1770 train_time:113930ms step_avg:97.71ms
step:1177/1770 train_time:114030ms step_avg:97.71ms
step:1178/1770 train_time:114131ms step_avg:97.71ms
step:1179/1770 train_time:114232ms step_avg:97.72ms
step:1180/1770 train_time:114333ms step_avg:97.72ms
step:1181/1770 train_time:114434ms step_avg:97.72ms
step:1182/1770 train_time:114535ms step_avg:97.73ms
step:1183/1770 train_time:114638ms step_avg:97.73ms
step:1184/1770 train_time:114741ms step_avg:97.73ms
step:1185/1770 train_time:114842ms step_avg:97.74ms
step:1186/1770 train_time:114945ms step_avg:97.74ms
step:1187/1770 train_time:115051ms step_avg:97.75ms
step:1188/1770 train_time:115152ms step_avg:97.75ms
step:1189/1770 train_time:115254ms step_avg:97.76ms
step:1190/1770 train_time:115355ms step_avg:97.76ms
step:1191/1770 train_time:115457ms step_avg:97.76ms
step:1192/1770 train_time:115559ms step_avg:97.77ms
step:1193/1770 train_time:115661ms step_avg:97.77ms
step:1194/1770 train_time:115763ms step_avg:97.77ms
step:1195/1770 train_time:115866ms step_avg:97.78ms
step:1196/1770 train_time:115969ms step_avg:97.78ms
step:1197/1770 train_time:116071ms step_avg:97.79ms
step:1198/1770 train_time:116173ms step_avg:97.79ms
step:1199/1770 train_time:116275ms step_avg:97.79ms
step:1200/1770 train_time:116378ms step_avg:97.80ms
step:1201/1770 train_time:116480ms step_avg:97.80ms
step:1202/1770 train_time:116582ms step_avg:97.80ms
step:1203/1770 train_time:116683ms step_avg:97.81ms
step:1204/1770 train_time:116786ms step_avg:97.81ms
step:1205/1770 train_time:116887ms step_avg:97.81ms
step:1206/1770 train_time:116990ms step_avg:97.82ms
step:1207/1770 train_time:117092ms step_avg:97.82ms
step:1208/1770 train_time:117194ms step_avg:97.82ms
step:1209/1770 train_time:117296ms step_avg:97.83ms
step:1210/1770 train_time:117399ms step_avg:97.83ms
step:1211/1770 train_time:117501ms step_avg:97.84ms
step:1212/1770 train_time:117605ms step_avg:97.84ms
step:1213/1770 train_time:117707ms step_avg:97.84ms
step:1214/1770 train_time:117808ms step_avg:97.85ms
step:1215/1770 train_time:117910ms step_avg:97.85ms
step:1216/1770 train_time:118014ms step_avg:97.86ms
step:1217/1770 train_time:118116ms step_avg:97.86ms
step:1218/1770 train_time:118217ms step_avg:97.86ms
step:1219/1770 train_time:118319ms step_avg:97.87ms
step:1220/1770 train_time:118422ms step_avg:97.87ms
step:1221/1770 train_time:118525ms step_avg:97.87ms
step:1222/1770 train_time:118628ms step_avg:97.88ms
step:1223/1770 train_time:118730ms step_avg:97.88ms
step:1224/1770 train_time:118833ms step_avg:97.89ms
step:1225/1770 train_time:118935ms step_avg:97.89ms
step:1226/1770 train_time:119037ms step_avg:97.89ms
step:1227/1770 train_time:119141ms step_avg:97.90ms
step:1228/1770 train_time:119245ms step_avg:97.90ms
step:1229/1770 train_time:119347ms step_avg:97.91ms
step:1230/1770 train_time:119448ms step_avg:97.91ms
step:1231/1770 train_time:119551ms step_avg:97.91ms
step:1232/1770 train_time:119653ms step_avg:97.92ms
step:1233/1770 train_time:119754ms step_avg:97.92ms
step:1234/1770 train_time:119856ms step_avg:97.92ms
step:1235/1770 train_time:119958ms step_avg:97.93ms
step:1236/1770 train_time:120061ms step_avg:97.93ms
step:1237/1770 train_time:120163ms step_avg:97.93ms
step:1238/1770 train_time:120266ms step_avg:97.94ms
step:1239/1770 train_time:120368ms step_avg:97.94ms
step:1240/1770 train_time:120470ms step_avg:97.94ms
step:1241/1770 train_time:120572ms step_avg:97.95ms
step:1242/1770 train_time:120673ms step_avg:97.95ms
step:1243/1770 train_time:120776ms step_avg:97.95ms
step:1244/1770 train_time:120877ms step_avg:97.96ms
step:1245/1770 train_time:120980ms step_avg:97.96ms
step:1246/1770 train_time:121082ms step_avg:97.96ms
step:1247/1770 train_time:121185ms step_avg:97.97ms
step:1248/1770 train_time:121287ms step_avg:97.97ms
step:1249/1770 train_time:121389ms step_avg:97.97ms
step:1250/1770 train_time:121491ms step_avg:97.98ms
step:1250/1770 val_loss:3.4234 train_time:121593ms step_avg:98.06ms
step:1251/1770 train_time:121614ms step_avg:98.00ms
step:1252/1770 train_time:121704ms step_avg:97.99ms
step:1253/1770 train_time:121806ms step_avg:97.99ms
step:1254/1770 train_time:121908ms step_avg:98.00ms
step:1255/1770 train_time:122012ms step_avg:98.00ms
step:1256/1770 train_time:122114ms step_avg:98.00ms
step:1257/1770 train_time:122215ms step_avg:98.01ms
step:1258/1770 train_time:122318ms step_avg:98.01ms
step:1259/1770 train_time:122420ms step_avg:98.01ms
step:1260/1770 train_time:122520ms step_avg:98.02ms
step:1261/1770 train_time:122625ms step_avg:98.02ms
step:1262/1770 train_time:122729ms step_avg:98.03ms
step:1263/1770 train_time:122831ms step_avg:98.03ms
step:1264/1770 train_time:122935ms step_avg:98.03ms
step:1265/1770 train_time:123037ms step_avg:98.04ms
step:1266/1770 train_time:123140ms step_avg:98.04ms
step:1267/1770 train_time:123242ms step_avg:98.04ms
step:1268/1770 train_time:123344ms step_avg:98.05ms
step:1269/1770 train_time:123445ms step_avg:98.05ms
step:1270/1770 train_time:123549ms step_avg:98.05ms
step:1271/1770 train_time:123650ms step_avg:98.06ms
step:1272/1770 train_time:123752ms step_avg:98.06ms
step:1273/1770 train_time:123854ms step_avg:98.06ms
step:1274/1770 train_time:123957ms step_avg:98.07ms
step:1275/1770 train_time:124059ms step_avg:98.07ms
step:1276/1770 train_time:124162ms step_avg:98.07ms
step:1277/1770 train_time:124264ms step_avg:98.08ms
step:1278/1770 train_time:124366ms step_avg:98.08ms
step:1279/1770 train_time:124469ms step_avg:98.08ms
step:1280/1770 train_time:124572ms step_avg:98.09ms
step:1281/1770 train_time:124674ms step_avg:98.09ms
step:1282/1770 train_time:124777ms step_avg:98.10ms
step:1283/1770 train_time:124880ms step_avg:98.10ms
step:1284/1770 train_time:124983ms step_avg:98.10ms
step:1285/1770 train_time:125085ms step_avg:98.11ms
step:1286/1770 train_time:125188ms step_avg:98.11ms
step:1287/1770 train_time:125292ms step_avg:98.11ms
step:1288/1770 train_time:125395ms step_avg:98.12ms
step:1289/1770 train_time:125497ms step_avg:98.12ms
step:1290/1770 train_time:125599ms step_avg:98.12ms
step:1291/1770 train_time:125701ms step_avg:98.13ms
step:1292/1770 train_time:125802ms step_avg:98.13ms
step:1293/1770 train_time:125904ms step_avg:98.13ms
step:1294/1770 train_time:126006ms step_avg:98.14ms
step:1295/1770 train_time:126109ms step_avg:98.14ms
step:1296/1770 train_time:126211ms step_avg:98.14ms
step:1297/1770 train_time:126312ms step_avg:98.14ms
step:1298/1770 train_time:126414ms step_avg:98.15ms
step:1299/1770 train_time:126517ms step_avg:98.15ms
step:1300/1770 train_time:126620ms step_avg:98.15ms
step:1301/1770 train_time:126722ms step_avg:98.16ms
step:1302/1770 train_time:126824ms step_avg:98.16ms
step:1303/1770 train_time:126925ms step_avg:98.16ms
step:1304/1770 train_time:127026ms step_avg:98.17ms
step:1305/1770 train_time:127128ms step_avg:98.17ms
step:1306/1770 train_time:127230ms step_avg:98.17ms
step:1307/1770 train_time:127331ms step_avg:98.17ms
step:1308/1770 train_time:127434ms step_avg:98.18ms
step:1309/1770 train_time:127537ms step_avg:98.18ms
step:1310/1770 train_time:127639ms step_avg:98.18ms
step:1311/1770 train_time:127742ms step_avg:98.19ms
step:1312/1770 train_time:127845ms step_avg:98.19ms
step:1313/1770 train_time:127946ms step_avg:98.19ms
step:1314/1770 train_time:128048ms step_avg:98.20ms
step:1315/1770 train_time:128150ms step_avg:98.20ms
step:1316/1770 train_time:128252ms step_avg:98.20ms
step:1317/1770 train_time:128354ms step_avg:98.20ms
step:1318/1770 train_time:128459ms step_avg:98.21ms
step:1319/1770 train_time:128562ms step_avg:98.21ms
step:1320/1770 train_time:128663ms step_avg:98.22ms
step:1321/1770 train_time:128765ms step_avg:98.22ms
step:1322/1770 train_time:128868ms step_avg:98.22ms
step:1323/1770 train_time:128971ms step_avg:98.23ms
step:1324/1770 train_time:129073ms step_avg:98.23ms
step:1325/1770 train_time:129176ms step_avg:98.23ms
step:1326/1770 train_time:129278ms step_avg:98.24ms
step:1327/1770 train_time:129384ms step_avg:98.24ms
step:1328/1770 train_time:129485ms step_avg:98.24ms
step:1329/1770 train_time:129586ms step_avg:98.25ms
step:1330/1770 train_time:129688ms step_avg:98.25ms
step:1331/1770 train_time:129789ms step_avg:98.25ms
step:1332/1770 train_time:129891ms step_avg:98.25ms
step:1333/1770 train_time:129993ms step_avg:98.26ms
step:1334/1770 train_time:130095ms step_avg:98.26ms
step:1335/1770 train_time:130197ms step_avg:98.26ms
step:1336/1770 train_time:130300ms step_avg:98.27ms
step:1337/1770 train_time:130402ms step_avg:98.27ms
step:1338/1770 train_time:130504ms step_avg:98.27ms
step:1339/1770 train_time:130606ms step_avg:98.27ms
step:1340/1770 train_time:130710ms step_avg:98.28ms
step:1341/1770 train_time:130812ms step_avg:98.28ms
step:1342/1770 train_time:130914ms step_avg:98.28ms
step:1343/1770 train_time:131017ms step_avg:98.29ms
step:1344/1770 train_time:131120ms step_avg:98.29ms
step:1345/1770 train_time:131222ms step_avg:98.29ms
step:1346/1770 train_time:131324ms step_avg:98.30ms
step:1347/1770 train_time:131426ms step_avg:98.30ms
step:1348/1770 train_time:131531ms step_avg:98.30ms
step:1349/1770 train_time:131633ms step_avg:98.31ms
step:1350/1770 train_time:131736ms step_avg:98.31ms
step:1351/1770 train_time:131838ms step_avg:98.31ms
step:1352/1770 train_time:131940ms step_avg:98.32ms
step:1353/1770 train_time:132043ms step_avg:98.32ms
step:1354/1770 train_time:132145ms step_avg:98.32ms
step:1355/1770 train_time:132247ms step_avg:98.32ms
step:1356/1770 train_time:132350ms step_avg:98.33ms
step:1357/1770 train_time:132452ms step_avg:98.33ms
step:1358/1770 train_time:132555ms step_avg:98.33ms
step:1359/1770 train_time:132657ms step_avg:98.34ms
step:1360/1770 train_time:132760ms step_avg:98.34ms
step:1361/1770 train_time:132863ms step_avg:98.34ms
step:1362/1770 train_time:132965ms step_avg:98.35ms
step:1363/1770 train_time:133068ms step_avg:98.35ms
step:1364/1770 train_time:133170ms step_avg:98.35ms
step:1365/1770 train_time:133272ms step_avg:98.36ms
step:1366/1770 train_time:133373ms step_avg:98.36ms
step:1367/1770 train_time:133476ms step_avg:98.36ms
step:1368/1770 train_time:133578ms step_avg:98.36ms
step:1369/1770 train_time:133681ms step_avg:98.37ms
step:1370/1770 train_time:133783ms step_avg:98.37ms
step:1371/1770 train_time:133885ms step_avg:98.37ms
step:1372/1770 train_time:133986ms step_avg:98.37ms
step:1373/1770 train_time:134088ms step_avg:98.38ms
step:1374/1770 train_time:134192ms step_avg:98.38ms
step:1375/1770 train_time:134294ms step_avg:98.38ms
step:1375/1770 val_loss:3.3796 train_time:134394ms step_avg:98.46ms
step:1376/1770 train_time:134415ms step_avg:98.40ms
step:1377/1770 train_time:134507ms step_avg:98.40ms
step:1378/1770 train_time:134609ms step_avg:98.40ms
step:1379/1770 train_time:134712ms step_avg:98.40ms
step:1380/1770 train_time:134814ms step_avg:98.40ms
step:1381/1770 train_time:134916ms step_avg:98.41ms
step:1382/1770 train_time:135017ms step_avg:98.41ms
step:1383/1770 train_time:135120ms step_avg:98.41ms
step:1384/1770 train_time:135222ms step_avg:98.42ms
step:1385/1770 train_time:135324ms step_avg:98.42ms
step:1386/1770 train_time:135427ms step_avg:98.42ms
step:1387/1770 train_time:135529ms step_avg:98.42ms
step:1388/1770 train_time:135631ms step_avg:98.43ms
step:1389/1770 train_time:135734ms step_avg:98.43ms
step:1390/1770 train_time:135836ms step_avg:98.43ms
step:1391/1770 train_time:135938ms step_avg:98.43ms
step:1392/1770 train_time:136039ms step_avg:98.44ms
step:1393/1770 train_time:136141ms step_avg:98.44ms
step:1394/1770 train_time:136243ms step_avg:98.44ms
step:1395/1770 train_time:136346ms step_avg:98.44ms
step:1396/1770 train_time:136450ms step_avg:98.45ms
step:1397/1770 train_time:136552ms step_avg:98.45ms
step:1398/1770 train_time:136654ms step_avg:98.45ms
step:1399/1770 train_time:136756ms step_avg:98.46ms
step:1400/1770 train_time:136859ms step_avg:98.46ms
step:1401/1770 train_time:136961ms step_avg:98.46ms
step:1402/1770 train_time:137064ms step_avg:98.47ms
step:1403/1770 train_time:137166ms step_avg:98.47ms
step:1404/1770 train_time:137268ms step_avg:98.47ms
step:1405/1770 train_time:137370ms step_avg:98.47ms
step:1406/1770 train_time:137472ms step_avg:98.48ms
step:1407/1770 train_time:137574ms step_avg:98.48ms
step:1408/1770 train_time:137677ms step_avg:98.48ms
step:1409/1770 train_time:137779ms step_avg:98.48ms
step:1410/1770 train_time:137881ms step_avg:98.49ms
step:1411/1770 train_time:137983ms step_avg:98.49ms
step:1412/1770 train_time:138085ms step_avg:98.49ms
step:1413/1770 train_time:138187ms step_avg:98.49ms
step:1414/1770 train_time:138290ms step_avg:98.50ms
step:1415/1770 train_time:138392ms step_avg:98.50ms
step:1416/1770 train_time:138497ms step_avg:98.50ms
step:1417/1770 train_time:138599ms step_avg:98.51ms
step:1418/1770 train_time:138701ms step_avg:98.51ms
step:1419/1770 train_time:138803ms step_avg:98.51ms
step:1420/1770 train_time:138906ms step_avg:98.52ms
step:1421/1770 train_time:139009ms step_avg:98.52ms
step:1422/1770 train_time:139111ms step_avg:98.52ms
step:1423/1770 train_time:139213ms step_avg:98.52ms
step:1424/1770 train_time:139316ms step_avg:98.53ms
step:1425/1770 train_time:139417ms step_avg:98.53ms
step:1426/1770 train_time:139519ms step_avg:98.53ms
step:1427/1770 train_time:139620ms step_avg:98.53ms
step:1428/1770 train_time:139724ms step_avg:98.54ms
step:1429/1770 train_time:139826ms step_avg:98.54ms
step:1430/1770 train_time:139929ms step_avg:98.54ms
step:1431/1770 train_time:140032ms step_avg:98.54ms
step:1432/1770 train_time:140134ms step_avg:98.55ms
step:1433/1770 train_time:140236ms step_avg:98.55ms
step:1434/1770 train_time:140337ms step_avg:98.55ms
step:1435/1770 train_time:140439ms step_avg:98.55ms
step:1436/1770 train_time:140543ms step_avg:98.56ms
step:1437/1770 train_time:140646ms step_avg:98.56ms
step:1438/1770 train_time:140748ms step_avg:98.56ms
step:1439/1770 train_time:140850ms step_avg:98.57ms
step:1440/1770 train_time:140953ms step_avg:98.57ms
step:1441/1770 train_time:141057ms step_avg:98.57ms
step:1442/1770 train_time:141159ms step_avg:98.57ms
step:1443/1770 train_time:141261ms step_avg:98.58ms
step:1444/1770 train_time:141363ms step_avg:98.58ms
step:1445/1770 train_time:141466ms step_avg:98.58ms
step:1446/1770 train_time:141569ms step_avg:98.59ms
step:1447/1770 train_time:141673ms step_avg:98.59ms
step:1448/1770 train_time:141776ms step_avg:98.59ms
step:1449/1770 train_time:141881ms step_avg:98.60ms
step:1450/1770 train_time:141983ms step_avg:98.60ms
step:1451/1770 train_time:142087ms step_avg:98.60ms
step:1452/1770 train_time:142190ms step_avg:98.61ms
step:1453/1770 train_time:142293ms step_avg:98.61ms
step:1454/1770 train_time:142397ms step_avg:98.61ms
step:1455/1770 train_time:142501ms step_avg:98.62ms
step:1456/1770 train_time:142606ms step_avg:98.62ms
step:1457/1770 train_time:142709ms step_avg:98.62ms
step:1458/1770 train_time:142814ms step_avg:98.63ms
step:1459/1770 train_time:142918ms step_avg:98.63ms
step:1460/1770 train_time:143022ms step_avg:98.64ms
step:1461/1770 train_time:143126ms step_avg:98.64ms
step:1462/1770 train_time:143229ms step_avg:98.64ms
step:1463/1770 train_time:143333ms step_avg:98.65ms
step:1464/1770 train_time:143437ms step_avg:98.65ms
step:1465/1770 train_time:143540ms step_avg:98.65ms
step:1466/1770 train_time:143645ms step_avg:98.66ms
step:1467/1770 train_time:143749ms step_avg:98.66ms
step:1468/1770 train_time:143853ms step_avg:98.66ms
step:1469/1770 train_time:143956ms step_avg:98.67ms
step:1470/1770 train_time:144058ms step_avg:98.67ms
step:1471/1770 train_time:144161ms step_avg:98.67ms
step:1472/1770 train_time:144265ms step_avg:98.68ms
step:1473/1770 train_time:144369ms step_avg:98.68ms
step:1474/1770 train_time:144475ms step_avg:98.68ms
step:1475/1770 train_time:144577ms step_avg:98.69ms
step:1476/1770 train_time:144680ms step_avg:98.69ms
step:1477/1770 train_time:144785ms step_avg:98.69ms
step:1478/1770 train_time:144889ms step_avg:98.70ms
step:1479/1770 train_time:144992ms step_avg:98.70ms
step:1480/1770 train_time:145096ms step_avg:98.70ms
step:1481/1770 train_time:145203ms step_avg:98.71ms
step:1482/1770 train_time:145306ms step_avg:98.71ms
step:1483/1770 train_time:145409ms step_avg:98.72ms
step:1484/1770 train_time:145512ms step_avg:98.72ms
step:1485/1770 train_time:145616ms step_avg:98.72ms
step:1486/1770 train_time:145719ms step_avg:98.73ms
step:1487/1770 train_time:145822ms step_avg:98.73ms
step:1488/1770 train_time:145925ms step_avg:98.73ms
step:1489/1770 train_time:146030ms step_avg:98.74ms
step:1490/1770 train_time:146134ms step_avg:98.74ms
step:1491/1770 train_time:146236ms step_avg:98.74ms
step:1492/1770 train_time:146340ms step_avg:98.74ms
step:1493/1770 train_time:146446ms step_avg:98.75ms
step:1494/1770 train_time:146553ms step_avg:98.76ms
step:1495/1770 train_time:146656ms step_avg:98.76ms
step:1496/1770 train_time:146759ms step_avg:98.76ms
step:1497/1770 train_time:146862ms step_avg:98.76ms
step:1498/1770 train_time:146966ms step_avg:98.77ms
step:1499/1770 train_time:147068ms step_avg:98.77ms
step:1500/1770 train_time:147171ms step_avg:98.77ms
step:1500/1770 val_loss:3.3426 train_time:147273ms step_avg:98.84ms
step:1501/1770 train_time:147294ms step_avg:98.79ms
step:1502/1770 train_time:147388ms step_avg:98.79ms
step:1503/1770 train_time:147491ms step_avg:98.79ms
step:1504/1770 train_time:147594ms step_avg:98.79ms
step:1505/1770 train_time:147700ms step_avg:98.80ms
step:1506/1770 train_time:147804ms step_avg:98.80ms
step:1507/1770 train_time:147907ms step_avg:98.80ms
step:1508/1770 train_time:148011ms step_avg:98.81ms
step:1509/1770 train_time:148114ms step_avg:98.81ms
step:1510/1770 train_time:148217ms step_avg:98.81ms
step:1511/1770 train_time:148323ms step_avg:98.82ms
step:1512/1770 train_time:148427ms step_avg:98.82ms
step:1513/1770 train_time:148531ms step_avg:98.82ms
step:1514/1770 train_time:148635ms step_avg:98.83ms
step:1515/1770 train_time:148739ms step_avg:98.83ms
step:1516/1770 train_time:148842ms step_avg:98.83ms
step:1517/1770 train_time:148945ms step_avg:98.84ms
step:1518/1770 train_time:149050ms step_avg:98.84ms
step:1519/1770 train_time:149152ms step_avg:98.84ms
step:1520/1770 train_time:149256ms step_avg:98.84ms
step:1521/1770 train_time:149360ms step_avg:98.85ms
step:1522/1770 train_time:149464ms step_avg:98.85ms
step:1523/1770 train_time:149569ms step_avg:98.86ms
step:1524/1770 train_time:149672ms step_avg:98.86ms
step:1525/1770 train_time:149774ms step_avg:98.86ms
step:1526/1770 train_time:149877ms step_avg:98.86ms
step:1527/1770 train_time:149982ms step_avg:98.87ms
step:1528/1770 train_time:150087ms step_avg:98.87ms
step:1529/1770 train_time:150190ms step_avg:98.87ms
step:1530/1770 train_time:150293ms step_avg:98.88ms
step:1531/1770 train_time:150396ms step_avg:98.88ms
step:1532/1770 train_time:150502ms step_avg:98.88ms
step:1533/1770 train_time:150606ms step_avg:98.89ms
step:1534/1770 train_time:150710ms step_avg:98.89ms
step:1535/1770 train_time:150812ms step_avg:98.89ms
step:1536/1770 train_time:150915ms step_avg:98.90ms
step:1537/1770 train_time:151020ms step_avg:98.90ms
step:1538/1770 train_time:151123ms step_avg:98.90ms
step:1539/1770 train_time:151226ms step_avg:98.91ms
step:1540/1770 train_time:151334ms step_avg:98.91ms
step:1541/1770 train_time:151439ms step_avg:98.91ms
step:1542/1770 train_time:151543ms step_avg:98.92ms
step:1543/1770 train_time:151645ms step_avg:98.92ms
step:1544/1770 train_time:151751ms step_avg:98.93ms
step:1545/1770 train_time:151854ms step_avg:98.93ms
step:1546/1770 train_time:151957ms step_avg:98.93ms
step:1547/1770 train_time:152060ms step_avg:98.93ms
step:1548/1770 train_time:152164ms step_avg:98.94ms
step:1549/1770 train_time:152269ms step_avg:98.94ms
step:1550/1770 train_time:152372ms step_avg:98.94ms
step:1551/1770 train_time:152476ms step_avg:98.95ms
step:1552/1770 train_time:152581ms step_avg:98.95ms
step:1553/1770 train_time:152684ms step_avg:98.95ms
step:1554/1770 train_time:152786ms step_avg:98.95ms
step:1555/1770 train_time:152892ms step_avg:98.96ms
step:1556/1770 train_time:152994ms step_avg:98.96ms
step:1557/1770 train_time:153097ms step_avg:98.96ms
step:1558/1770 train_time:153200ms step_avg:98.97ms
step:1559/1770 train_time:153303ms step_avg:98.97ms
step:1560/1770 train_time:153405ms step_avg:98.97ms
step:1561/1770 train_time:153511ms step_avg:98.98ms
step:1562/1770 train_time:153614ms step_avg:98.98ms
step:1563/1770 train_time:153718ms step_avg:98.98ms
step:1564/1770 train_time:153821ms step_avg:98.98ms
step:1565/1770 train_time:153924ms step_avg:98.99ms
step:1566/1770 train_time:154027ms step_avg:98.99ms
step:1567/1770 train_time:154131ms step_avg:98.99ms
step:1568/1770 train_time:154233ms step_avg:98.99ms
step:1569/1770 train_time:154340ms step_avg:99.00ms
step:1570/1770 train_time:154443ms step_avg:99.00ms
step:1571/1770 train_time:154547ms step_avg:99.00ms
step:1572/1770 train_time:154650ms step_avg:99.01ms
step:1573/1770 train_time:154755ms step_avg:99.01ms
step:1574/1770 train_time:154859ms step_avg:99.01ms
step:1575/1770 train_time:154960ms step_avg:99.02ms
step:1576/1770 train_time:155064ms step_avg:99.02ms
step:1577/1770 train_time:155169ms step_avg:99.02ms
step:1578/1770 train_time:155274ms step_avg:99.03ms
step:1579/1770 train_time:155378ms step_avg:99.03ms
step:1580/1770 train_time:155482ms step_avg:99.03ms
step:1581/1770 train_time:155587ms step_avg:99.04ms
step:1582/1770 train_time:155692ms step_avg:99.04ms
step:1583/1770 train_time:155795ms step_avg:99.04ms
step:1584/1770 train_time:155900ms step_avg:99.05ms
step:1585/1770 train_time:156003ms step_avg:99.05ms
step:1586/1770 train_time:156111ms step_avg:99.06ms
step:1587/1770 train_time:156215ms step_avg:99.06ms
step:1588/1770 train_time:156319ms step_avg:99.06ms
step:1589/1770 train_time:156424ms step_avg:99.07ms
step:1590/1770 train_time:156527ms step_avg:99.07ms
step:1591/1770 train_time:156630ms step_avg:99.07ms
step:1592/1770 train_time:156734ms step_avg:99.07ms
step:1593/1770 train_time:156837ms step_avg:99.08ms
step:1594/1770 train_time:156940ms step_avg:99.08ms
step:1595/1770 train_time:157042ms step_avg:99.08ms
step:1596/1770 train_time:157148ms step_avg:99.08ms
step:1597/1770 train_time:157250ms step_avg:99.09ms
step:1598/1770 train_time:157353ms step_avg:99.09ms
step:1599/1770 train_time:157458ms step_avg:99.09ms
step:1600/1770 train_time:157563ms step_avg:99.10ms
step:1601/1770 train_time:157667ms step_avg:99.10ms
step:1602/1770 train_time:157772ms step_avg:99.10ms
step:1603/1770 train_time:157875ms step_avg:99.11ms
step:1604/1770 train_time:157977ms step_avg:99.11ms
step:1605/1770 train_time:158080ms step_avg:99.11ms
step:1606/1770 train_time:158185ms step_avg:99.11ms
step:1607/1770 train_time:158291ms step_avg:99.12ms
step:1608/1770 train_time:158394ms step_avg:99.12ms
step:1609/1770 train_time:158498ms step_avg:99.12ms
step:1610/1770 train_time:158602ms step_avg:99.13ms
step:1611/1770 train_time:158707ms step_avg:99.13ms
step:1612/1770 train_time:158811ms step_avg:99.13ms
step:1613/1770 train_time:158914ms step_avg:99.14ms
step:1614/1770 train_time:159017ms step_avg:99.14ms
step:1615/1770 train_time:159120ms step_avg:99.14ms
step:1616/1770 train_time:159223ms step_avg:99.14ms
step:1617/1770 train_time:159328ms step_avg:99.15ms
step:1618/1770 train_time:159433ms step_avg:99.15ms
step:1619/1770 train_time:159537ms step_avg:99.15ms
step:1620/1770 train_time:159641ms step_avg:99.16ms
step:1621/1770 train_time:159745ms step_avg:99.16ms
step:1622/1770 train_time:159850ms step_avg:99.16ms
step:1623/1770 train_time:159956ms step_avg:99.17ms
step:1624/1770 train_time:160059ms step_avg:99.17ms
step:1625/1770 train_time:160161ms step_avg:99.17ms
step:1625/1770 val_loss:3.3078 train_time:160263ms step_avg:99.23ms
step:1626/1770 train_time:160284ms step_avg:99.19ms
step:1627/1770 train_time:160374ms step_avg:99.18ms
step:1628/1770 train_time:160477ms step_avg:99.18ms
step:1629/1770 train_time:160580ms step_avg:99.18ms
step:1630/1770 train_time:160684ms step_avg:99.19ms
step:1631/1770 train_time:160786ms step_avg:99.19ms
step:1632/1770 train_time:160890ms step_avg:99.19ms
step:1633/1770 train_time:160993ms step_avg:99.19ms
step:1634/1770 train_time:161096ms step_avg:99.20ms
step:1635/1770 train_time:161199ms step_avg:99.20ms
step:1636/1770 train_time:161303ms step_avg:99.20ms
step:1637/1770 train_time:161408ms step_avg:99.21ms
step:1638/1770 train_time:161511ms step_avg:99.21ms
step:1639/1770 train_time:161615ms step_avg:99.21ms
step:1640/1770 train_time:161721ms step_avg:99.22ms
step:1641/1770 train_time:161824ms step_avg:99.22ms
step:1642/1770 train_time:161926ms step_avg:99.22ms
step:1643/1770 train_time:162029ms step_avg:99.22ms
step:1644/1770 train_time:162133ms step_avg:99.22ms
step:1645/1770 train_time:162237ms step_avg:99.23ms
step:1646/1770 train_time:162342ms step_avg:99.23ms
step:1647/1770 train_time:162446ms step_avg:99.23ms
step:1648/1770 train_time:162549ms step_avg:99.24ms
step:1649/1770 train_time:162653ms step_avg:99.24ms
step:1650/1770 train_time:162757ms step_avg:99.24ms
step:1651/1770 train_time:162861ms step_avg:99.24ms
step:1652/1770 train_time:162964ms step_avg:99.25ms
step:1653/1770 train_time:163067ms step_avg:99.25ms
step:1654/1770 train_time:163173ms step_avg:99.25ms
step:1655/1770 train_time:163279ms step_avg:99.26ms
step:1656/1770 train_time:163382ms step_avg:99.26ms
step:1657/1770 train_time:163488ms step_avg:99.26ms
step:1658/1770 train_time:163592ms step_avg:99.27ms
step:1659/1770 train_time:163697ms step_avg:99.27ms
step:1660/1770 train_time:163802ms step_avg:99.27ms
step:1661/1770 train_time:163906ms step_avg:99.28ms
step:1662/1770 train_time:164010ms step_avg:99.28ms
step:1663/1770 train_time:164113ms step_avg:99.28ms
step:1664/1770 train_time:164215ms step_avg:99.28ms
step:1665/1770 train_time:164318ms step_avg:99.29ms
step:1666/1770 train_time:164422ms step_avg:99.29ms
step:1667/1770 train_time:164526ms step_avg:99.29ms
step:1668/1770 train_time:164629ms step_avg:99.29ms
step:1669/1770 train_time:164731ms step_avg:99.30ms
step:1670/1770 train_time:164835ms step_avg:99.30ms
step:1671/1770 train_time:164939ms step_avg:99.30ms
step:1672/1770 train_time:165042ms step_avg:99.30ms
step:1673/1770 train_time:165147ms step_avg:99.31ms
step:1674/1770 train_time:165250ms step_avg:99.31ms
step:1675/1770 train_time:165352ms step_avg:99.31ms
step:1676/1770 train_time:165456ms step_avg:99.31ms
step:1677/1770 train_time:165564ms step_avg:99.32ms
step:1678/1770 train_time:165667ms step_avg:99.32ms
step:1679/1770 train_time:165770ms step_avg:99.32ms
step:1680/1770 train_time:165873ms step_avg:99.33ms
step:1681/1770 train_time:165978ms step_avg:99.33ms
step:1682/1770 train_time:166084ms step_avg:99.33ms
step:1683/1770 train_time:166186ms step_avg:99.33ms
step:1684/1770 train_time:166289ms step_avg:99.34ms
step:1685/1770 train_time:166392ms step_avg:99.34ms
step:1686/1770 train_time:166496ms step_avg:99.34ms
step:1687/1770 train_time:166601ms step_avg:99.34ms
step:1688/1770 train_time:166705ms step_avg:99.35ms
step:1689/1770 train_time:166808ms step_avg:99.35ms
step:1690/1770 train_time:166911ms step_avg:99.35ms
step:1691/1770 train_time:167015ms step_avg:99.35ms
step:1692/1770 train_time:167118ms step_avg:99.36ms
step:1693/1770 train_time:167223ms step_avg:99.36ms
step:1694/1770 train_time:167326ms step_avg:99.36ms
step:1695/1770 train_time:167430ms step_avg:99.36ms
step:1696/1770 train_time:167535ms step_avg:99.37ms
step:1697/1770 train_time:167640ms step_avg:99.37ms
step:1698/1770 train_time:167744ms step_avg:99.37ms
step:1699/1770 train_time:167846ms step_avg:99.38ms
step:1700/1770 train_time:167949ms step_avg:99.38ms
step:1701/1770 train_time:168052ms step_avg:99.38ms
step:1702/1770 train_time:168158ms step_avg:99.38ms
step:1703/1770 train_time:168260ms step_avg:99.39ms
step:1704/1770 train_time:168364ms step_avg:99.39ms
step:1705/1770 train_time:168467ms step_avg:99.39ms
step:1706/1770 train_time:168570ms step_avg:99.39ms
step:1707/1770 train_time:168674ms step_avg:99.40ms
step:1708/1770 train_time:168779ms step_avg:99.40ms
step:1709/1770 train_time:168884ms step_avg:99.40ms
step:1710/1770 train_time:168991ms step_avg:99.41ms
step:1711/1770 train_time:169097ms step_avg:99.41ms
step:1712/1770 train_time:169201ms step_avg:99.41ms
step:1713/1770 train_time:169304ms step_avg:99.42ms
step:1714/1770 train_time:169408ms step_avg:99.42ms
step:1715/1770 train_time:169511ms step_avg:99.42ms
step:1716/1770 train_time:169615ms step_avg:99.42ms
step:1717/1770 train_time:169720ms step_avg:99.43ms
step:1718/1770 train_time:169825ms step_avg:99.43ms
step:1719/1770 train_time:169930ms step_avg:99.43ms
step:1720/1770 train_time:170036ms step_avg:99.44ms
step:1721/1770 train_time:170139ms step_avg:99.44ms
step:1722/1770 train_time:170247ms step_avg:99.44ms
step:1723/1770 train_time:170352ms step_avg:99.45ms
step:1724/1770 train_time:170459ms step_avg:99.45ms
step:1725/1770 train_time:170564ms step_avg:99.45ms
step:1726/1770 train_time:170670ms step_avg:99.46ms
step:1727/1770 train_time:170774ms step_avg:99.46ms
step:1728/1770 train_time:170880ms step_avg:99.46ms
step:1729/1770 train_time:170983ms step_avg:99.47ms
step:1730/1770 train_time:171088ms step_avg:99.47ms
step:1731/1770 train_time:171194ms step_avg:99.47ms
step:1732/1770 train_time:171298ms step_avg:99.48ms
step:1733/1770 train_time:171403ms step_avg:99.48ms
step:1734/1770 train_time:171506ms step_avg:99.48ms
step:1735/1770 train_time:171611ms step_avg:99.48ms
step:1736/1770 train_time:171715ms step_avg:99.49ms
step:1737/1770 train_time:171819ms step_avg:99.49ms
step:1738/1770 train_time:171924ms step_avg:99.49ms
step:1739/1770 train_time:172027ms step_avg:99.50ms
step:1740/1770 train_time:172130ms step_avg:99.50ms
step:1741/1770 train_time:172237ms step_avg:99.50ms
step:1742/1770 train_time:172344ms step_avg:99.51ms
step:1743/1770 train_time:172449ms step_avg:99.51ms
step:1744/1770 train_time:172553ms step_avg:99.51ms
step:1745/1770 train_time:172657ms step_avg:99.51ms
step:1746/1770 train_time:172764ms step_avg:99.52ms
step:1747/1770 train_time:172866ms step_avg:99.52ms
step:1748/1770 train_time:172973ms step_avg:99.52ms
step:1749/1770 train_time:173078ms step_avg:99.53ms
step:1750/1770 train_time:173181ms step_avg:99.53ms
step:1750/1770 val_loss:3.2807 train_time:173283ms step_avg:99.59ms
step:1751/1770 train_time:173305ms step_avg:99.54ms
step:1752/1770 train_time:173397ms step_avg:99.54ms
step:1753/1770 train_time:173501ms step_avg:99.54ms
step:1754/1770 train_time:173606ms step_avg:99.54ms
step:1755/1770 train_time:173709ms step_avg:99.55ms
step:1756/1770 train_time:173814ms step_avg:99.55ms
step:1757/1770 train_time:173918ms step_avg:99.55ms
step:1758/1770 train_time:174021ms step_avg:99.55ms
step:1759/1770 train_time:174126ms step_avg:99.56ms
step:1760/1770 train_time:174230ms step_avg:99.56ms
step:1761/1770 train_time:174337ms step_avg:99.56ms
step:1762/1770 train_time:174445ms step_avg:99.57ms
step:1763/1770 train_time:174548ms step_avg:99.57ms
step:1764/1770 train_time:174652ms step_avg:99.57ms
step:1765/1770 train_time:174757ms step_avg:99.58ms
step:1766/1770 train_time:174866ms step_avg:99.58ms
step:1767/1770 train_time:174969ms step_avg:99.58ms
step:1768/1770 train_time:175073ms step_avg:99.59ms
step:1769/1770 train_time:175177ms step_avg:99.59ms
step:1770/1770 train_time:175280ms step_avg:99.59ms
step:1770/1770 val_loss:3.2775 train_time:175385ms step_avg:99.65ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
