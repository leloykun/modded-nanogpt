import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 16:46:13 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24244ms step_avg:nanms
step:2/1770 train_time:24747ms step_avg:nanms
step:3/1770 train_time:24841ms step_avg:nanms
step:4/1770 train_time:24934ms step_avg:nanms
step:5/1770 train_time:25027ms step_avg:nanms
step:6/1770 train_time:25121ms step_avg:nanms
step:7/1770 train_time:25215ms step_avg:nanms
step:8/1770 train_time:25308ms step_avg:nanms
step:9/1770 train_time:25402ms step_avg:nanms
step:10/1770 train_time:25496ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:93.97ms
step:14/1770 train_time:376ms step_avg:94.10ms
step:15/1770 train_time:470ms step_avg:94.04ms
step:16/1770 train_time:564ms step_avg:94.04ms
step:17/1770 train_time:658ms step_avg:93.98ms
step:18/1770 train_time:752ms step_avg:93.95ms
step:19/1770 train_time:846ms step_avg:94.01ms
step:20/1770 train_time:940ms step_avg:93.99ms
step:21/1770 train_time:1034ms step_avg:93.98ms
step:22/1770 train_time:1129ms step_avg:94.10ms
step:23/1770 train_time:1222ms step_avg:94.03ms
step:24/1770 train_time:1316ms step_avg:94.00ms
step:25/1770 train_time:1410ms step_avg:94.01ms
step:26/1770 train_time:1505ms step_avg:94.04ms
step:27/1770 train_time:1599ms step_avg:94.04ms
step:28/1770 train_time:1692ms step_avg:94.03ms
step:29/1770 train_time:1787ms step_avg:94.05ms
step:30/1770 train_time:1881ms step_avg:94.05ms
step:31/1770 train_time:1975ms step_avg:94.04ms
step:32/1770 train_time:2068ms step_avg:94.02ms
step:33/1770 train_time:2162ms step_avg:94.02ms
step:34/1770 train_time:2256ms step_avg:94.01ms
step:35/1770 train_time:2350ms step_avg:94.01ms
step:36/1770 train_time:2444ms step_avg:94.01ms
step:37/1770 train_time:2539ms step_avg:94.02ms
step:38/1770 train_time:2633ms step_avg:94.02ms
step:39/1770 train_time:2727ms step_avg:94.02ms
step:40/1770 train_time:2821ms step_avg:94.02ms
step:41/1770 train_time:2915ms step_avg:94.02ms
step:42/1770 train_time:3009ms step_avg:94.02ms
step:43/1770 train_time:3103ms step_avg:94.03ms
step:44/1770 train_time:3197ms step_avg:94.02ms
step:45/1770 train_time:3292ms step_avg:94.05ms
step:46/1770 train_time:3385ms step_avg:94.02ms
step:47/1770 train_time:3479ms step_avg:94.04ms
step:48/1770 train_time:3573ms step_avg:94.03ms
step:49/1770 train_time:3667ms step_avg:94.03ms
step:50/1770 train_time:3761ms step_avg:94.03ms
step:51/1770 train_time:3855ms step_avg:94.03ms
step:52/1770 train_time:3949ms step_avg:94.03ms
step:53/1770 train_time:4043ms step_avg:94.03ms
step:54/1770 train_time:4137ms step_avg:94.03ms
step:55/1770 train_time:4231ms step_avg:94.01ms
step:56/1770 train_time:4325ms step_avg:94.01ms
step:57/1770 train_time:4418ms step_avg:94.01ms
step:58/1770 train_time:4512ms step_avg:94.00ms
step:59/1770 train_time:4606ms step_avg:94.01ms
step:60/1770 train_time:4700ms step_avg:94.01ms
step:61/1770 train_time:4794ms step_avg:94.01ms
step:62/1770 train_time:4889ms step_avg:94.01ms
step:63/1770 train_time:4984ms step_avg:94.03ms
step:64/1770 train_time:5077ms step_avg:94.01ms
step:65/1770 train_time:5170ms step_avg:94.00ms
step:66/1770 train_time:5264ms step_avg:94.00ms
step:67/1770 train_time:5358ms step_avg:94.00ms
step:68/1770 train_time:5452ms step_avg:94.00ms
step:69/1770 train_time:5546ms step_avg:94.00ms
step:70/1770 train_time:5640ms step_avg:94.01ms
step:71/1770 train_time:5734ms step_avg:94.01ms
step:72/1770 train_time:5828ms step_avg:94.00ms
step:73/1770 train_time:5922ms step_avg:94.00ms
step:74/1770 train_time:6016ms step_avg:94.00ms
step:75/1770 train_time:6110ms step_avg:94.00ms
step:76/1770 train_time:6204ms step_avg:94.00ms
step:77/1770 train_time:6297ms step_avg:93.99ms
step:78/1770 train_time:6391ms step_avg:93.99ms
step:79/1770 train_time:6485ms step_avg:93.99ms
step:80/1770 train_time:6579ms step_avg:93.99ms
step:81/1770 train_time:6674ms step_avg:94.00ms
step:82/1770 train_time:6767ms step_avg:93.98ms
step:83/1770 train_time:6861ms step_avg:93.98ms
step:84/1770 train_time:6955ms step_avg:93.99ms
step:85/1770 train_time:7049ms step_avg:93.99ms
step:86/1770 train_time:7143ms step_avg:93.99ms
step:87/1770 train_time:7237ms step_avg:93.98ms
step:88/1770 train_time:7330ms step_avg:93.98ms
step:89/1770 train_time:7424ms step_avg:93.97ms
step:90/1770 train_time:7517ms step_avg:93.97ms
step:91/1770 train_time:7611ms step_avg:93.97ms
step:92/1770 train_time:7705ms step_avg:93.96ms
step:93/1770 train_time:7798ms step_avg:93.96ms
step:94/1770 train_time:7892ms step_avg:93.95ms
step:95/1770 train_time:7987ms step_avg:93.96ms
step:96/1770 train_time:8081ms step_avg:93.97ms
step:97/1770 train_time:8175ms step_avg:93.97ms
step:98/1770 train_time:8269ms step_avg:93.97ms
step:99/1770 train_time:8363ms step_avg:93.97ms
step:100/1770 train_time:8457ms step_avg:93.97ms
step:101/1770 train_time:8551ms step_avg:93.97ms
step:102/1770 train_time:8644ms step_avg:93.96ms
step:103/1770 train_time:8738ms step_avg:93.96ms
step:104/1770 train_time:8832ms step_avg:93.96ms
step:105/1770 train_time:8927ms step_avg:93.96ms
step:106/1770 train_time:9020ms step_avg:93.96ms
step:107/1770 train_time:9114ms step_avg:93.96ms
step:108/1770 train_time:9208ms step_avg:93.96ms
step:109/1770 train_time:9302ms step_avg:93.96ms
step:110/1770 train_time:9396ms step_avg:93.96ms
step:111/1770 train_time:9491ms step_avg:93.97ms
step:112/1770 train_time:9585ms step_avg:93.97ms
step:113/1770 train_time:9679ms step_avg:93.97ms
step:114/1770 train_time:9772ms step_avg:93.97ms
step:115/1770 train_time:9866ms step_avg:93.97ms
step:116/1770 train_time:9960ms step_avg:93.97ms
step:117/1770 train_time:10054ms step_avg:93.96ms
step:118/1770 train_time:10148ms step_avg:93.96ms
step:119/1770 train_time:10242ms step_avg:93.96ms
step:120/1770 train_time:10336ms step_avg:93.96ms
step:121/1770 train_time:10430ms step_avg:93.96ms
step:122/1770 train_time:10524ms step_avg:93.96ms
step:123/1770 train_time:10617ms step_avg:93.96ms
step:124/1770 train_time:10711ms step_avg:93.95ms
step:125/1770 train_time:10805ms step_avg:93.96ms
step:125/1770 val_loss:4.6436 train_time:10896ms step_avg:94.75ms
step:126/1770 train_time:10920ms step_avg:94.14ms
step:127/1770 train_time:10996ms step_avg:93.99ms
step:128/1770 train_time:11094ms step_avg:94.02ms
step:129/1770 train_time:11190ms step_avg:94.04ms
step:130/1770 train_time:11284ms step_avg:94.03ms
step:131/1770 train_time:11377ms step_avg:94.03ms
step:132/1770 train_time:11471ms step_avg:94.03ms
step:133/1770 train_time:11565ms step_avg:94.02ms
step:134/1770 train_time:11659ms step_avg:94.02ms
step:135/1770 train_time:11753ms step_avg:94.03ms
step:136/1770 train_time:11848ms step_avg:94.03ms
step:137/1770 train_time:11942ms step_avg:94.03ms
step:138/1770 train_time:12036ms step_avg:94.03ms
step:139/1770 train_time:12132ms step_avg:94.04ms
step:140/1770 train_time:12227ms step_avg:94.05ms
step:141/1770 train_time:12322ms step_avg:94.06ms
step:142/1770 train_time:12416ms step_avg:94.06ms
step:143/1770 train_time:12510ms step_avg:94.06ms
step:144/1770 train_time:12605ms step_avg:94.06ms
step:145/1770 train_time:12699ms step_avg:94.07ms
step:146/1770 train_time:12793ms step_avg:94.07ms
step:147/1770 train_time:12888ms step_avg:94.07ms
step:148/1770 train_time:12982ms step_avg:94.07ms
step:149/1770 train_time:13077ms step_avg:94.08ms
step:150/1770 train_time:13173ms step_avg:94.09ms
step:151/1770 train_time:13267ms step_avg:94.10ms
step:152/1770 train_time:13362ms step_avg:94.10ms
step:153/1770 train_time:13456ms step_avg:94.10ms
step:154/1770 train_time:13551ms step_avg:94.10ms
step:155/1770 train_time:13645ms step_avg:94.11ms
step:156/1770 train_time:13740ms step_avg:94.11ms
step:157/1770 train_time:13834ms step_avg:94.11ms
step:158/1770 train_time:13930ms step_avg:94.12ms
step:159/1770 train_time:14024ms step_avg:94.12ms
step:160/1770 train_time:14118ms step_avg:94.12ms
step:161/1770 train_time:14213ms step_avg:94.12ms
step:162/1770 train_time:14307ms step_avg:94.13ms
step:163/1770 train_time:14402ms step_avg:94.13ms
step:164/1770 train_time:14496ms step_avg:94.13ms
step:165/1770 train_time:14591ms step_avg:94.14ms
step:166/1770 train_time:14686ms step_avg:94.14ms
step:167/1770 train_time:14779ms step_avg:94.14ms
step:168/1770 train_time:14874ms step_avg:94.14ms
step:169/1770 train_time:14969ms step_avg:94.14ms
step:170/1770 train_time:15064ms step_avg:94.15ms
step:171/1770 train_time:15158ms step_avg:94.15ms
step:172/1770 train_time:15255ms step_avg:94.16ms
step:173/1770 train_time:15350ms step_avg:94.17ms
step:174/1770 train_time:15444ms step_avg:94.17ms
step:175/1770 train_time:15539ms step_avg:94.17ms
step:176/1770 train_time:15633ms step_avg:94.18ms
step:177/1770 train_time:15727ms step_avg:94.18ms
step:178/1770 train_time:15822ms step_avg:94.18ms
step:179/1770 train_time:15916ms step_avg:94.18ms
step:180/1770 train_time:16011ms step_avg:94.18ms
step:181/1770 train_time:16105ms step_avg:94.18ms
step:182/1770 train_time:16199ms step_avg:94.18ms
step:183/1770 train_time:16294ms step_avg:94.18ms
step:184/1770 train_time:16389ms step_avg:94.19ms
step:185/1770 train_time:16484ms step_avg:94.19ms
step:186/1770 train_time:16578ms step_avg:94.19ms
step:187/1770 train_time:16672ms step_avg:94.19ms
step:188/1770 train_time:16767ms step_avg:94.19ms
step:189/1770 train_time:16861ms step_avg:94.20ms
step:190/1770 train_time:16956ms step_avg:94.20ms
step:191/1770 train_time:17050ms step_avg:94.20ms
step:192/1770 train_time:17144ms step_avg:94.20ms
step:193/1770 train_time:17238ms step_avg:94.20ms
step:194/1770 train_time:17333ms step_avg:94.20ms
step:195/1770 train_time:17428ms step_avg:94.21ms
step:196/1770 train_time:17523ms step_avg:94.21ms
step:197/1770 train_time:17617ms step_avg:94.21ms
step:198/1770 train_time:17712ms step_avg:94.21ms
step:199/1770 train_time:17807ms step_avg:94.22ms
step:200/1770 train_time:17901ms step_avg:94.22ms
step:201/1770 train_time:17995ms step_avg:94.22ms
step:202/1770 train_time:18090ms step_avg:94.22ms
step:203/1770 train_time:18185ms step_avg:94.22ms
step:204/1770 train_time:18279ms step_avg:94.22ms
step:205/1770 train_time:18374ms step_avg:94.22ms
step:206/1770 train_time:18469ms step_avg:94.23ms
step:207/1770 train_time:18563ms step_avg:94.23ms
step:208/1770 train_time:18658ms step_avg:94.23ms
step:209/1770 train_time:18754ms step_avg:94.24ms
step:210/1770 train_time:18849ms step_avg:94.24ms
step:211/1770 train_time:18943ms step_avg:94.24ms
step:212/1770 train_time:19038ms step_avg:94.25ms
step:213/1770 train_time:19132ms step_avg:94.25ms
step:214/1770 train_time:19227ms step_avg:94.25ms
step:215/1770 train_time:19321ms step_avg:94.25ms
step:216/1770 train_time:19416ms step_avg:94.25ms
step:217/1770 train_time:19511ms step_avg:94.26ms
step:218/1770 train_time:19606ms step_avg:94.26ms
step:219/1770 train_time:19700ms step_avg:94.26ms
step:220/1770 train_time:19795ms step_avg:94.26ms
step:221/1770 train_time:19889ms step_avg:94.26ms
step:222/1770 train_time:19984ms step_avg:94.26ms
step:223/1770 train_time:20078ms step_avg:94.26ms
step:224/1770 train_time:20173ms step_avg:94.26ms
step:225/1770 train_time:20267ms step_avg:94.27ms
step:226/1770 train_time:20363ms step_avg:94.27ms
step:227/1770 train_time:20457ms step_avg:94.27ms
step:228/1770 train_time:20552ms step_avg:94.28ms
step:229/1770 train_time:20647ms step_avg:94.28ms
step:230/1770 train_time:20741ms step_avg:94.28ms
step:231/1770 train_time:20836ms step_avg:94.28ms
step:232/1770 train_time:20931ms step_avg:94.28ms
step:233/1770 train_time:21025ms step_avg:94.28ms
step:234/1770 train_time:21120ms step_avg:94.28ms
step:235/1770 train_time:21214ms step_avg:94.28ms
step:236/1770 train_time:21310ms step_avg:94.29ms
step:237/1770 train_time:21404ms step_avg:94.29ms
step:238/1770 train_time:21498ms step_avg:94.29ms
step:239/1770 train_time:21593ms step_avg:94.29ms
step:240/1770 train_time:21688ms step_avg:94.30ms
step:241/1770 train_time:21782ms step_avg:94.30ms
step:242/1770 train_time:21877ms step_avg:94.30ms
step:243/1770 train_time:21972ms step_avg:94.30ms
step:244/1770 train_time:22067ms step_avg:94.30ms
step:245/1770 train_time:22160ms step_avg:94.30ms
step:246/1770 train_time:22255ms step_avg:94.30ms
step:247/1770 train_time:22350ms step_avg:94.30ms
step:248/1770 train_time:22444ms step_avg:94.30ms
step:249/1770 train_time:22538ms step_avg:94.30ms
step:250/1770 train_time:22633ms step_avg:94.30ms
step:250/1770 val_loss:4.1125 train_time:22726ms step_avg:94.69ms
step:251/1770 train_time:22748ms step_avg:94.39ms
step:252/1770 train_time:22832ms step_avg:94.35ms
step:253/1770 train_time:22928ms step_avg:94.35ms
step:254/1770 train_time:23022ms step_avg:94.35ms
step:255/1770 train_time:23116ms step_avg:94.35ms
step:256/1770 train_time:23210ms step_avg:94.35ms
step:257/1770 train_time:23304ms step_avg:94.35ms
step:258/1770 train_time:23398ms step_avg:94.35ms
step:259/1770 train_time:23492ms step_avg:94.35ms
step:260/1770 train_time:23586ms step_avg:94.35ms
step:261/1770 train_time:23681ms step_avg:94.35ms
step:262/1770 train_time:23776ms step_avg:94.35ms
step:263/1770 train_time:23871ms step_avg:94.35ms
step:264/1770 train_time:23966ms step_avg:94.35ms
step:265/1770 train_time:24061ms step_avg:94.36ms
step:266/1770 train_time:24156ms step_avg:94.36ms
step:267/1770 train_time:24251ms step_avg:94.36ms
step:268/1770 train_time:24346ms step_avg:94.36ms
step:269/1770 train_time:24441ms step_avg:94.37ms
step:270/1770 train_time:24536ms step_avg:94.37ms
step:271/1770 train_time:24631ms step_avg:94.37ms
step:272/1770 train_time:24726ms step_avg:94.37ms
step:273/1770 train_time:24822ms step_avg:94.38ms
step:274/1770 train_time:24917ms step_avg:94.38ms
step:275/1770 train_time:25012ms step_avg:94.39ms
step:276/1770 train_time:25107ms step_avg:94.39ms
step:277/1770 train_time:25202ms step_avg:94.39ms
step:278/1770 train_time:25297ms step_avg:94.39ms
step:279/1770 train_time:25392ms step_avg:94.39ms
step:280/1770 train_time:25487ms step_avg:94.40ms
step:281/1770 train_time:25582ms step_avg:94.40ms
step:282/1770 train_time:25677ms step_avg:94.40ms
step:283/1770 train_time:25772ms step_avg:94.40ms
step:284/1770 train_time:25867ms step_avg:94.40ms
step:285/1770 train_time:25962ms step_avg:94.41ms
step:286/1770 train_time:26057ms step_avg:94.41ms
step:287/1770 train_time:26151ms step_avg:94.41ms
step:288/1770 train_time:26246ms step_avg:94.41ms
step:289/1770 train_time:26341ms step_avg:94.41ms
step:290/1770 train_time:26437ms step_avg:94.42ms
step:291/1770 train_time:26532ms step_avg:94.42ms
step:292/1770 train_time:26627ms step_avg:94.42ms
step:293/1770 train_time:26722ms step_avg:94.43ms
step:294/1770 train_time:26818ms step_avg:94.43ms
step:295/1770 train_time:26913ms step_avg:94.43ms
step:296/1770 train_time:27007ms step_avg:94.43ms
step:297/1770 train_time:27104ms step_avg:94.44ms
step:298/1770 train_time:27199ms step_avg:94.44ms
step:299/1770 train_time:27293ms step_avg:94.44ms
step:300/1770 train_time:27388ms step_avg:94.44ms
step:301/1770 train_time:27484ms step_avg:94.45ms
step:302/1770 train_time:27579ms step_avg:94.45ms
step:303/1770 train_time:27673ms step_avg:94.45ms
step:304/1770 train_time:27768ms step_avg:94.45ms
step:305/1770 train_time:27863ms step_avg:94.45ms
step:306/1770 train_time:27959ms step_avg:94.46ms
step:307/1770 train_time:28054ms step_avg:94.46ms
step:308/1770 train_time:28149ms step_avg:94.46ms
step:309/1770 train_time:28244ms step_avg:94.46ms
step:310/1770 train_time:28339ms step_avg:94.46ms
step:311/1770 train_time:28435ms step_avg:94.47ms
step:312/1770 train_time:28529ms step_avg:94.47ms
step:313/1770 train_time:28624ms step_avg:94.47ms
step:314/1770 train_time:28720ms step_avg:94.47ms
step:315/1770 train_time:28815ms step_avg:94.48ms
step:316/1770 train_time:28910ms step_avg:94.48ms
step:317/1770 train_time:29006ms step_avg:94.48ms
step:318/1770 train_time:29102ms step_avg:94.49ms
step:319/1770 train_time:29197ms step_avg:94.49ms
step:320/1770 train_time:29291ms step_avg:94.49ms
step:321/1770 train_time:29387ms step_avg:94.49ms
step:322/1770 train_time:29482ms step_avg:94.49ms
step:323/1770 train_time:29577ms step_avg:94.49ms
step:324/1770 train_time:29671ms step_avg:94.49ms
step:325/1770 train_time:29766ms step_avg:94.50ms
step:326/1770 train_time:29862ms step_avg:94.50ms
step:327/1770 train_time:29958ms step_avg:94.50ms
step:328/1770 train_time:30053ms step_avg:94.51ms
step:329/1770 train_time:30148ms step_avg:94.51ms
step:330/1770 train_time:30244ms step_avg:94.51ms
step:331/1770 train_time:30339ms step_avg:94.51ms
step:332/1770 train_time:30434ms step_avg:94.51ms
step:333/1770 train_time:30528ms step_avg:94.51ms
step:334/1770 train_time:30623ms step_avg:94.52ms
step:335/1770 train_time:30718ms step_avg:94.52ms
step:336/1770 train_time:30813ms step_avg:94.52ms
step:337/1770 train_time:30908ms step_avg:94.52ms
step:338/1770 train_time:31003ms step_avg:94.52ms
step:339/1770 train_time:31098ms step_avg:94.52ms
step:340/1770 train_time:31193ms step_avg:94.53ms
step:341/1770 train_time:31289ms step_avg:94.53ms
step:342/1770 train_time:31384ms step_avg:94.53ms
step:343/1770 train_time:31479ms step_avg:94.53ms
step:344/1770 train_time:31574ms step_avg:94.53ms
step:345/1770 train_time:31669ms step_avg:94.54ms
step:346/1770 train_time:31764ms step_avg:94.54ms
step:347/1770 train_time:31860ms step_avg:94.54ms
step:348/1770 train_time:31955ms step_avg:94.54ms
step:349/1770 train_time:32049ms step_avg:94.54ms
step:350/1770 train_time:32144ms step_avg:94.54ms
step:351/1770 train_time:32239ms step_avg:94.54ms
step:352/1770 train_time:32334ms step_avg:94.55ms
step:353/1770 train_time:32429ms step_avg:94.55ms
step:354/1770 train_time:32525ms step_avg:94.55ms
step:355/1770 train_time:32620ms step_avg:94.55ms
step:356/1770 train_time:32715ms step_avg:94.55ms
step:357/1770 train_time:32810ms step_avg:94.55ms
step:358/1770 train_time:32905ms step_avg:94.55ms
step:359/1770 train_time:33000ms step_avg:94.56ms
step:360/1770 train_time:33095ms step_avg:94.56ms
step:361/1770 train_time:33190ms step_avg:94.56ms
step:362/1770 train_time:33285ms step_avg:94.56ms
step:363/1770 train_time:33381ms step_avg:94.56ms
step:364/1770 train_time:33475ms step_avg:94.56ms
step:365/1770 train_time:33570ms step_avg:94.56ms
step:366/1770 train_time:33666ms step_avg:94.57ms
step:367/1770 train_time:33760ms step_avg:94.57ms
step:368/1770 train_time:33856ms step_avg:94.57ms
step:369/1770 train_time:33951ms step_avg:94.57ms
step:370/1770 train_time:34046ms step_avg:94.57ms
step:371/1770 train_time:34141ms step_avg:94.57ms
step:372/1770 train_time:34236ms step_avg:94.57ms
step:373/1770 train_time:34332ms step_avg:94.58ms
step:374/1770 train_time:34427ms step_avg:94.58ms
step:375/1770 train_time:34523ms step_avg:94.58ms
step:375/1770 val_loss:3.8984 train_time:34616ms step_avg:94.84ms
step:376/1770 train_time:34640ms step_avg:94.65ms
step:377/1770 train_time:34722ms step_avg:94.61ms
step:378/1770 train_time:34819ms step_avg:94.62ms
step:379/1770 train_time:34915ms step_avg:94.62ms
step:380/1770 train_time:35010ms step_avg:94.62ms
step:381/1770 train_time:35104ms step_avg:94.62ms
step:382/1770 train_time:35199ms step_avg:94.62ms
step:383/1770 train_time:35293ms step_avg:94.62ms
step:384/1770 train_time:35388ms step_avg:94.62ms
step:385/1770 train_time:35482ms step_avg:94.62ms
step:386/1770 train_time:35577ms step_avg:94.62ms
step:387/1770 train_time:35673ms step_avg:94.62ms
step:388/1770 train_time:35769ms step_avg:94.63ms
step:389/1770 train_time:35864ms step_avg:94.63ms
step:390/1770 train_time:35960ms step_avg:94.63ms
step:391/1770 train_time:36055ms step_avg:94.63ms
step:392/1770 train_time:36150ms step_avg:94.63ms
step:393/1770 train_time:36245ms step_avg:94.63ms
step:394/1770 train_time:36340ms step_avg:94.64ms
step:395/1770 train_time:36435ms step_avg:94.64ms
step:396/1770 train_time:36532ms step_avg:94.64ms
step:397/1770 train_time:36628ms step_avg:94.65ms
step:398/1770 train_time:36725ms step_avg:94.65ms
step:399/1770 train_time:36823ms step_avg:94.66ms
step:400/1770 train_time:36920ms step_avg:94.67ms
step:401/1770 train_time:37017ms step_avg:94.67ms
step:402/1770 train_time:37115ms step_avg:94.68ms
step:403/1770 train_time:37212ms step_avg:94.69ms
step:404/1770 train_time:37308ms step_avg:94.69ms
step:405/1770 train_time:37405ms step_avg:94.70ms
step:406/1770 train_time:37502ms step_avg:94.70ms
step:407/1770 train_time:37599ms step_avg:94.71ms
step:408/1770 train_time:37696ms step_avg:94.71ms
step:409/1770 train_time:37793ms step_avg:94.72ms
step:410/1770 train_time:37890ms step_avg:94.72ms
step:411/1770 train_time:37987ms step_avg:94.73ms
step:412/1770 train_time:38085ms step_avg:94.74ms
step:413/1770 train_time:38182ms step_avg:94.74ms
step:414/1770 train_time:38280ms step_avg:94.75ms
step:415/1770 train_time:38376ms step_avg:94.76ms
step:416/1770 train_time:38473ms step_avg:94.76ms
step:417/1770 train_time:38570ms step_avg:94.77ms
step:418/1770 train_time:38668ms step_avg:94.77ms
step:419/1770 train_time:38763ms step_avg:94.78ms
step:420/1770 train_time:38861ms step_avg:94.78ms
step:421/1770 train_time:38958ms step_avg:94.79ms
step:422/1770 train_time:39056ms step_avg:94.80ms
step:423/1770 train_time:39152ms step_avg:94.80ms
step:424/1770 train_time:39249ms step_avg:94.81ms
step:425/1770 train_time:39346ms step_avg:94.81ms
step:426/1770 train_time:39444ms step_avg:94.82ms
step:427/1770 train_time:39542ms step_avg:94.83ms
step:428/1770 train_time:39640ms step_avg:94.83ms
step:429/1770 train_time:39737ms step_avg:94.84ms
step:430/1770 train_time:39833ms step_avg:94.84ms
step:431/1770 train_time:39930ms step_avg:94.85ms
step:432/1770 train_time:40027ms step_avg:94.85ms
step:433/1770 train_time:40124ms step_avg:94.86ms
step:434/1770 train_time:40221ms step_avg:94.86ms
step:435/1770 train_time:40318ms step_avg:94.87ms
step:436/1770 train_time:40415ms step_avg:94.87ms
step:437/1770 train_time:40512ms step_avg:94.88ms
step:438/1770 train_time:40609ms step_avg:94.88ms
step:439/1770 train_time:40706ms step_avg:94.89ms
step:440/1770 train_time:40803ms step_avg:94.89ms
step:441/1770 train_time:40900ms step_avg:94.90ms
step:442/1770 train_time:40998ms step_avg:94.90ms
step:443/1770 train_time:41095ms step_avg:94.91ms
step:444/1770 train_time:41191ms step_avg:94.91ms
step:445/1770 train_time:41288ms step_avg:94.91ms
step:446/1770 train_time:41385ms step_avg:94.92ms
step:447/1770 train_time:41481ms step_avg:94.92ms
step:448/1770 train_time:41578ms step_avg:94.93ms
step:449/1770 train_time:41675ms step_avg:94.93ms
step:450/1770 train_time:41772ms step_avg:94.94ms
step:451/1770 train_time:41869ms step_avg:94.94ms
step:452/1770 train_time:41965ms step_avg:94.94ms
step:453/1770 train_time:42063ms step_avg:94.95ms
step:454/1770 train_time:42160ms step_avg:94.96ms
step:455/1770 train_time:42258ms step_avg:94.96ms
step:456/1770 train_time:42355ms step_avg:94.97ms
step:457/1770 train_time:42451ms step_avg:94.97ms
step:458/1770 train_time:42548ms step_avg:94.97ms
step:459/1770 train_time:42645ms step_avg:94.98ms
step:460/1770 train_time:42742ms step_avg:94.98ms
step:461/1770 train_time:42839ms step_avg:94.99ms
step:462/1770 train_time:42936ms step_avg:94.99ms
step:463/1770 train_time:43033ms step_avg:94.99ms
step:464/1770 train_time:43129ms step_avg:95.00ms
step:465/1770 train_time:43227ms step_avg:95.00ms
step:466/1770 train_time:43324ms step_avg:95.01ms
step:467/1770 train_time:43421ms step_avg:95.01ms
step:468/1770 train_time:43518ms step_avg:95.02ms
step:469/1770 train_time:43615ms step_avg:95.02ms
step:470/1770 train_time:43712ms step_avg:95.03ms
step:471/1770 train_time:43808ms step_avg:95.03ms
step:472/1770 train_time:43906ms step_avg:95.03ms
step:473/1770 train_time:44003ms step_avg:95.04ms
step:474/1770 train_time:44101ms step_avg:95.05ms
step:475/1770 train_time:44198ms step_avg:95.05ms
step:476/1770 train_time:44295ms step_avg:95.05ms
step:477/1770 train_time:44392ms step_avg:95.06ms
step:478/1770 train_time:44489ms step_avg:95.06ms
step:479/1770 train_time:44586ms step_avg:95.07ms
step:480/1770 train_time:44684ms step_avg:95.07ms
step:481/1770 train_time:44781ms step_avg:95.08ms
step:482/1770 train_time:44878ms step_avg:95.08ms
step:483/1770 train_time:44974ms step_avg:95.08ms
step:484/1770 train_time:45071ms step_avg:95.09ms
step:485/1770 train_time:45168ms step_avg:95.09ms
step:486/1770 train_time:45265ms step_avg:95.09ms
step:487/1770 train_time:45362ms step_avg:95.10ms
step:488/1770 train_time:45460ms step_avg:95.11ms
step:489/1770 train_time:45558ms step_avg:95.11ms
step:490/1770 train_time:45654ms step_avg:95.11ms
step:491/1770 train_time:45751ms step_avg:95.12ms
step:492/1770 train_time:45847ms step_avg:95.12ms
step:493/1770 train_time:45944ms step_avg:95.12ms
step:494/1770 train_time:46041ms step_avg:95.13ms
step:495/1770 train_time:46138ms step_avg:95.13ms
step:496/1770 train_time:46235ms step_avg:95.13ms
step:497/1770 train_time:46332ms step_avg:95.14ms
step:498/1770 train_time:46429ms step_avg:95.14ms
step:499/1770 train_time:46526ms step_avg:95.15ms
step:500/1770 train_time:46623ms step_avg:95.15ms
step:500/1770 val_loss:3.7487 train_time:46719ms step_avg:95.34ms
step:501/1770 train_time:46740ms step_avg:95.19ms
step:502/1770 train_time:46827ms step_avg:95.18ms
step:503/1770 train_time:46926ms step_avg:95.19ms
step:504/1770 train_time:47023ms step_avg:95.19ms
step:505/1770 train_time:47120ms step_avg:95.19ms
step:506/1770 train_time:47216ms step_avg:95.19ms
step:507/1770 train_time:47313ms step_avg:95.20ms
step:508/1770 train_time:47409ms step_avg:95.20ms
step:509/1770 train_time:47507ms step_avg:95.20ms
step:510/1770 train_time:47603ms step_avg:95.21ms
step:511/1770 train_time:47699ms step_avg:95.21ms
step:512/1770 train_time:47796ms step_avg:95.21ms
step:513/1770 train_time:47894ms step_avg:95.22ms
step:514/1770 train_time:47991ms step_avg:95.22ms
step:515/1770 train_time:48089ms step_avg:95.23ms
step:516/1770 train_time:48187ms step_avg:95.23ms
step:517/1770 train_time:48285ms step_avg:95.24ms
step:518/1770 train_time:48381ms step_avg:95.24ms
step:519/1770 train_time:48478ms step_avg:95.24ms
step:520/1770 train_time:48574ms step_avg:95.24ms
step:521/1770 train_time:48671ms step_avg:95.25ms
step:522/1770 train_time:48768ms step_avg:95.25ms
step:523/1770 train_time:48865ms step_avg:95.25ms
step:524/1770 train_time:48962ms step_avg:95.26ms
step:525/1770 train_time:49059ms step_avg:95.26ms
step:526/1770 train_time:49156ms step_avg:95.26ms
step:527/1770 train_time:49253ms step_avg:95.27ms
step:528/1770 train_time:49351ms step_avg:95.27ms
step:529/1770 train_time:49449ms step_avg:95.28ms
step:530/1770 train_time:49546ms step_avg:95.28ms
step:531/1770 train_time:49643ms step_avg:95.28ms
step:532/1770 train_time:49740ms step_avg:95.29ms
step:533/1770 train_time:49837ms step_avg:95.29ms
step:534/1770 train_time:49934ms step_avg:95.29ms
step:535/1770 train_time:50032ms step_avg:95.30ms
step:536/1770 train_time:50130ms step_avg:95.30ms
step:537/1770 train_time:50228ms step_avg:95.31ms
step:538/1770 train_time:50325ms step_avg:95.31ms
step:539/1770 train_time:50423ms step_avg:95.32ms
step:540/1770 train_time:50520ms step_avg:95.32ms
step:541/1770 train_time:50617ms step_avg:95.32ms
step:542/1770 train_time:50714ms step_avg:95.33ms
step:543/1770 train_time:50812ms step_avg:95.33ms
step:544/1770 train_time:50910ms step_avg:95.34ms
step:545/1770 train_time:51008ms step_avg:95.34ms
step:546/1770 train_time:51105ms step_avg:95.35ms
step:547/1770 train_time:51202ms step_avg:95.35ms
step:548/1770 train_time:51299ms step_avg:95.35ms
step:549/1770 train_time:51396ms step_avg:95.35ms
step:550/1770 train_time:51494ms step_avg:95.36ms
step:551/1770 train_time:51591ms step_avg:95.36ms
step:552/1770 train_time:51688ms step_avg:95.36ms
step:553/1770 train_time:51785ms step_avg:95.37ms
step:554/1770 train_time:51882ms step_avg:95.37ms
step:555/1770 train_time:51979ms step_avg:95.37ms
step:556/1770 train_time:52076ms step_avg:95.38ms
step:557/1770 train_time:52174ms step_avg:95.38ms
step:558/1770 train_time:52271ms step_avg:95.39ms
step:559/1770 train_time:52369ms step_avg:95.39ms
step:560/1770 train_time:52467ms step_avg:95.39ms
step:561/1770 train_time:52566ms step_avg:95.40ms
step:562/1770 train_time:52661ms step_avg:95.40ms
step:563/1770 train_time:52758ms step_avg:95.40ms
step:564/1770 train_time:52855ms step_avg:95.41ms
step:565/1770 train_time:52952ms step_avg:95.41ms
step:566/1770 train_time:53050ms step_avg:95.41ms
step:567/1770 train_time:53148ms step_avg:95.42ms
step:568/1770 train_time:53245ms step_avg:95.42ms
step:569/1770 train_time:53342ms step_avg:95.42ms
step:570/1770 train_time:53439ms step_avg:95.43ms
step:571/1770 train_time:53536ms step_avg:95.43ms
step:572/1770 train_time:53634ms step_avg:95.43ms
step:573/1770 train_time:53732ms step_avg:95.44ms
step:574/1770 train_time:53829ms step_avg:95.44ms
step:575/1770 train_time:53927ms step_avg:95.45ms
step:576/1770 train_time:54024ms step_avg:95.45ms
step:577/1770 train_time:54121ms step_avg:95.45ms
step:578/1770 train_time:54218ms step_avg:95.45ms
step:579/1770 train_time:54315ms step_avg:95.46ms
step:580/1770 train_time:54413ms step_avg:95.46ms
step:581/1770 train_time:54510ms step_avg:95.46ms
step:582/1770 train_time:54607ms step_avg:95.47ms
step:583/1770 train_time:54705ms step_avg:95.47ms
step:584/1770 train_time:54802ms step_avg:95.47ms
step:585/1770 train_time:54899ms step_avg:95.48ms
step:586/1770 train_time:54996ms step_avg:95.48ms
step:587/1770 train_time:55094ms step_avg:95.48ms
step:588/1770 train_time:55191ms step_avg:95.49ms
step:589/1770 train_time:55288ms step_avg:95.49ms
step:590/1770 train_time:55386ms step_avg:95.49ms
step:591/1770 train_time:55484ms step_avg:95.50ms
step:592/1770 train_time:55581ms step_avg:95.50ms
step:593/1770 train_time:55678ms step_avg:95.50ms
step:594/1770 train_time:55775ms step_avg:95.51ms
step:595/1770 train_time:55873ms step_avg:95.51ms
step:596/1770 train_time:55970ms step_avg:95.51ms
step:597/1770 train_time:56068ms step_avg:95.52ms
step:598/1770 train_time:56165ms step_avg:95.52ms
step:599/1770 train_time:56262ms step_avg:95.52ms
step:600/1770 train_time:56359ms step_avg:95.52ms
step:601/1770 train_time:56457ms step_avg:95.53ms
step:602/1770 train_time:56554ms step_avg:95.53ms
step:603/1770 train_time:56651ms step_avg:95.53ms
step:604/1770 train_time:56749ms step_avg:95.54ms
step:605/1770 train_time:56847ms step_avg:95.54ms
step:606/1770 train_time:56945ms step_avg:95.55ms
step:607/1770 train_time:57042ms step_avg:95.55ms
step:608/1770 train_time:57139ms step_avg:95.55ms
step:609/1770 train_time:57237ms step_avg:95.55ms
step:610/1770 train_time:57334ms step_avg:95.56ms
step:611/1770 train_time:57432ms step_avg:95.56ms
step:612/1770 train_time:57529ms step_avg:95.56ms
step:613/1770 train_time:57627ms step_avg:95.57ms
step:614/1770 train_time:57724ms step_avg:95.57ms
step:615/1770 train_time:57822ms step_avg:95.57ms
step:616/1770 train_time:57919ms step_avg:95.58ms
step:617/1770 train_time:58016ms step_avg:95.58ms
step:618/1770 train_time:58112ms step_avg:95.58ms
step:619/1770 train_time:58210ms step_avg:95.58ms
step:620/1770 train_time:58308ms step_avg:95.59ms
step:621/1770 train_time:58405ms step_avg:95.59ms
step:622/1770 train_time:58502ms step_avg:95.59ms
step:623/1770 train_time:58599ms step_avg:95.59ms
step:624/1770 train_time:58696ms step_avg:95.60ms
step:625/1770 train_time:58793ms step_avg:95.60ms
step:625/1770 val_loss:3.6629 train_time:58890ms step_avg:95.76ms
step:626/1770 train_time:58913ms step_avg:95.64ms
step:627/1770 train_time:58995ms step_avg:95.62ms
step:628/1770 train_time:59093ms step_avg:95.62ms
step:629/1770 train_time:59191ms step_avg:95.62ms
step:630/1770 train_time:59288ms step_avg:95.63ms
step:631/1770 train_time:59384ms step_avg:95.63ms
step:632/1770 train_time:59481ms step_avg:95.63ms
step:633/1770 train_time:59577ms step_avg:95.63ms
step:634/1770 train_time:59675ms step_avg:95.63ms
step:635/1770 train_time:59772ms step_avg:95.63ms
step:636/1770 train_time:59869ms step_avg:95.64ms
step:637/1770 train_time:59967ms step_avg:95.64ms
step:638/1770 train_time:60064ms step_avg:95.64ms
step:639/1770 train_time:60162ms step_avg:95.65ms
step:640/1770 train_time:60260ms step_avg:95.65ms
step:641/1770 train_time:60357ms step_avg:95.65ms
step:642/1770 train_time:60455ms step_avg:95.66ms
step:643/1770 train_time:60552ms step_avg:95.66ms
step:644/1770 train_time:60649ms step_avg:95.66ms
step:645/1770 train_time:60747ms step_avg:95.66ms
step:646/1770 train_time:60844ms step_avg:95.67ms
step:647/1770 train_time:60941ms step_avg:95.67ms
step:648/1770 train_time:61038ms step_avg:95.67ms
step:649/1770 train_time:61136ms step_avg:95.67ms
step:650/1770 train_time:61235ms step_avg:95.68ms
step:651/1770 train_time:61333ms step_avg:95.68ms
step:652/1770 train_time:61430ms step_avg:95.69ms
step:653/1770 train_time:61527ms step_avg:95.69ms
step:654/1770 train_time:61624ms step_avg:95.69ms
step:655/1770 train_time:61721ms step_avg:95.69ms
step:656/1770 train_time:61819ms step_avg:95.69ms
step:657/1770 train_time:61917ms step_avg:95.70ms
step:658/1770 train_time:62016ms step_avg:95.70ms
step:659/1770 train_time:62116ms step_avg:95.71ms
step:660/1770 train_time:62216ms step_avg:95.72ms
step:661/1770 train_time:62315ms step_avg:95.72ms
step:662/1770 train_time:62415ms step_avg:95.73ms
step:663/1770 train_time:62514ms step_avg:95.73ms
step:664/1770 train_time:62614ms step_avg:95.74ms
step:665/1770 train_time:62713ms step_avg:95.75ms
step:666/1770 train_time:62812ms step_avg:95.75ms
step:667/1770 train_time:62911ms step_avg:95.75ms
step:668/1770 train_time:63010ms step_avg:95.76ms
step:669/1770 train_time:63108ms step_avg:95.76ms
step:670/1770 train_time:63207ms step_avg:95.77ms
step:671/1770 train_time:63305ms step_avg:95.77ms
step:672/1770 train_time:63404ms step_avg:95.78ms
step:673/1770 train_time:63502ms step_avg:95.78ms
step:674/1770 train_time:63601ms step_avg:95.79ms
step:675/1770 train_time:63701ms step_avg:95.79ms
step:676/1770 train_time:63800ms step_avg:95.80ms
step:677/1770 train_time:63899ms step_avg:95.80ms
step:678/1770 train_time:63999ms step_avg:95.81ms
step:679/1770 train_time:64099ms step_avg:95.81ms
step:680/1770 train_time:64198ms step_avg:95.82ms
step:681/1770 train_time:64298ms step_avg:95.82ms
step:682/1770 train_time:64397ms step_avg:95.83ms
step:683/1770 train_time:64496ms step_avg:95.83ms
step:684/1770 train_time:64595ms step_avg:95.84ms
step:685/1770 train_time:64694ms step_avg:95.84ms
step:686/1770 train_time:64794ms step_avg:95.85ms
step:687/1770 train_time:64894ms step_avg:95.85ms
step:688/1770 train_time:64994ms step_avg:95.86ms
step:689/1770 train_time:65093ms step_avg:95.87ms
step:690/1770 train_time:65193ms step_avg:95.87ms
step:691/1770 train_time:65292ms step_avg:95.88ms
step:692/1770 train_time:65392ms step_avg:95.88ms
step:693/1770 train_time:65491ms step_avg:95.89ms
step:694/1770 train_time:65590ms step_avg:95.89ms
step:695/1770 train_time:65689ms step_avg:95.90ms
step:696/1770 train_time:65788ms step_avg:95.90ms
step:697/1770 train_time:65887ms step_avg:95.91ms
step:698/1770 train_time:65987ms step_avg:95.91ms
step:699/1770 train_time:66086ms step_avg:95.92ms
step:700/1770 train_time:66184ms step_avg:95.92ms
step:701/1770 train_time:66283ms step_avg:95.92ms
step:702/1770 train_time:66383ms step_avg:95.93ms
step:703/1770 train_time:66482ms step_avg:95.93ms
step:704/1770 train_time:66582ms step_avg:95.94ms
step:705/1770 train_time:66682ms step_avg:95.94ms
step:706/1770 train_time:66781ms step_avg:95.95ms
step:707/1770 train_time:66879ms step_avg:95.95ms
step:708/1770 train_time:66979ms step_avg:95.96ms
step:709/1770 train_time:67077ms step_avg:95.96ms
step:710/1770 train_time:67177ms step_avg:95.97ms
step:711/1770 train_time:67277ms step_avg:95.97ms
step:712/1770 train_time:67377ms step_avg:95.98ms
step:713/1770 train_time:67476ms step_avg:95.98ms
step:714/1770 train_time:67576ms step_avg:95.99ms
step:715/1770 train_time:67676ms step_avg:95.99ms
step:716/1770 train_time:67775ms step_avg:96.00ms
step:717/1770 train_time:67874ms step_avg:96.00ms
step:718/1770 train_time:67974ms step_avg:96.01ms
step:719/1770 train_time:68074ms step_avg:96.01ms
step:720/1770 train_time:68173ms step_avg:96.02ms
step:721/1770 train_time:68273ms step_avg:96.02ms
step:722/1770 train_time:68372ms step_avg:96.03ms
step:723/1770 train_time:68470ms step_avg:96.03ms
step:724/1770 train_time:68569ms step_avg:96.03ms
step:725/1770 train_time:68668ms step_avg:96.04ms
step:726/1770 train_time:68767ms step_avg:96.04ms
step:727/1770 train_time:68866ms step_avg:96.05ms
step:728/1770 train_time:68964ms step_avg:96.05ms
step:729/1770 train_time:69063ms step_avg:96.05ms
step:730/1770 train_time:69161ms step_avg:96.06ms
step:731/1770 train_time:69260ms step_avg:96.06ms
step:732/1770 train_time:69360ms step_avg:96.07ms
step:733/1770 train_time:69460ms step_avg:96.07ms
step:734/1770 train_time:69560ms step_avg:96.08ms
step:735/1770 train_time:69659ms step_avg:96.08ms
step:736/1770 train_time:69757ms step_avg:96.08ms
step:737/1770 train_time:69857ms step_avg:96.09ms
step:738/1770 train_time:69956ms step_avg:96.09ms
step:739/1770 train_time:70055ms step_avg:96.10ms
step:740/1770 train_time:70154ms step_avg:96.10ms
step:741/1770 train_time:70255ms step_avg:96.11ms
step:742/1770 train_time:70355ms step_avg:96.11ms
step:743/1770 train_time:70455ms step_avg:96.12ms
step:744/1770 train_time:70554ms step_avg:96.12ms
step:745/1770 train_time:70653ms step_avg:96.13ms
step:746/1770 train_time:70752ms step_avg:96.13ms
step:747/1770 train_time:70851ms step_avg:96.13ms
step:748/1770 train_time:70950ms step_avg:96.14ms
step:749/1770 train_time:71049ms step_avg:96.14ms
step:750/1770 train_time:71148ms step_avg:96.15ms
step:750/1770 val_loss:3.5973 train_time:71245ms step_avg:96.28ms
step:751/1770 train_time:71266ms step_avg:96.18ms
step:752/1770 train_time:71350ms step_avg:96.16ms
step:753/1770 train_time:71450ms step_avg:96.16ms
step:754/1770 train_time:71549ms step_avg:96.17ms
step:755/1770 train_time:71648ms step_avg:96.17ms
step:756/1770 train_time:71747ms step_avg:96.18ms
step:757/1770 train_time:71845ms step_avg:96.18ms
step:758/1770 train_time:71944ms step_avg:96.18ms
step:759/1770 train_time:72043ms step_avg:96.19ms
step:760/1770 train_time:72142ms step_avg:96.19ms
step:761/1770 train_time:72241ms step_avg:96.19ms
step:762/1770 train_time:72342ms step_avg:96.20ms
step:763/1770 train_time:72443ms step_avg:96.21ms
step:764/1770 train_time:72543ms step_avg:96.21ms
step:765/1770 train_time:72643ms step_avg:96.22ms
step:766/1770 train_time:72742ms step_avg:96.22ms
step:767/1770 train_time:72843ms step_avg:96.23ms
step:768/1770 train_time:72942ms step_avg:96.23ms
step:769/1770 train_time:73042ms step_avg:96.23ms
step:770/1770 train_time:73141ms step_avg:96.24ms
step:771/1770 train_time:73240ms step_avg:96.24ms
step:772/1770 train_time:73340ms step_avg:96.25ms
step:773/1770 train_time:73439ms step_avg:96.25ms
step:774/1770 train_time:73539ms step_avg:96.25ms
step:775/1770 train_time:73639ms step_avg:96.26ms
step:776/1770 train_time:73738ms step_avg:96.26ms
step:777/1770 train_time:73836ms step_avg:96.27ms
step:778/1770 train_time:73936ms step_avg:96.27ms
step:779/1770 train_time:74034ms step_avg:96.27ms
step:780/1770 train_time:74133ms step_avg:96.28ms
step:781/1770 train_time:74231ms step_avg:96.28ms
step:782/1770 train_time:74330ms step_avg:96.28ms
step:783/1770 train_time:74429ms step_avg:96.29ms
step:784/1770 train_time:74528ms step_avg:96.29ms
step:785/1770 train_time:74628ms step_avg:96.29ms
step:786/1770 train_time:74728ms step_avg:96.30ms
step:787/1770 train_time:74828ms step_avg:96.30ms
step:788/1770 train_time:74926ms step_avg:96.31ms
step:789/1770 train_time:75026ms step_avg:96.31ms
step:790/1770 train_time:75126ms step_avg:96.32ms
step:791/1770 train_time:75226ms step_avg:96.32ms
step:792/1770 train_time:75325ms step_avg:96.32ms
step:793/1770 train_time:75424ms step_avg:96.33ms
step:794/1770 train_time:75524ms step_avg:96.33ms
step:795/1770 train_time:75623ms step_avg:96.34ms
step:796/1770 train_time:75723ms step_avg:96.34ms
step:797/1770 train_time:75823ms step_avg:96.34ms
step:798/1770 train_time:75923ms step_avg:96.35ms
step:799/1770 train_time:76023ms step_avg:96.35ms
step:800/1770 train_time:76122ms step_avg:96.36ms
step:801/1770 train_time:76222ms step_avg:96.36ms
step:802/1770 train_time:76322ms step_avg:96.37ms
step:803/1770 train_time:76422ms step_avg:96.37ms
step:804/1770 train_time:76521ms step_avg:96.37ms
step:805/1770 train_time:76622ms step_avg:96.38ms
step:806/1770 train_time:76722ms step_avg:96.38ms
step:807/1770 train_time:76821ms step_avg:96.39ms
step:808/1770 train_time:76921ms step_avg:96.39ms
step:809/1770 train_time:77021ms step_avg:96.40ms
step:810/1770 train_time:77121ms step_avg:96.40ms
step:811/1770 train_time:77220ms step_avg:96.40ms
step:812/1770 train_time:77319ms step_avg:96.41ms
step:813/1770 train_time:77419ms step_avg:96.41ms
step:814/1770 train_time:77518ms step_avg:96.41ms
step:815/1770 train_time:77617ms step_avg:96.42ms
step:816/1770 train_time:77716ms step_avg:96.42ms
step:817/1770 train_time:77815ms step_avg:96.43ms
step:818/1770 train_time:77914ms step_avg:96.43ms
step:819/1770 train_time:78014ms step_avg:96.43ms
step:820/1770 train_time:78113ms step_avg:96.44ms
step:821/1770 train_time:78212ms step_avg:96.44ms
step:822/1770 train_time:78311ms step_avg:96.44ms
step:823/1770 train_time:78411ms step_avg:96.45ms
step:824/1770 train_time:78510ms step_avg:96.45ms
step:825/1770 train_time:78611ms step_avg:96.45ms
step:826/1770 train_time:78710ms step_avg:96.46ms
step:827/1770 train_time:78810ms step_avg:96.46ms
step:828/1770 train_time:78909ms step_avg:96.47ms
step:829/1770 train_time:79009ms step_avg:96.47ms
step:830/1770 train_time:79108ms step_avg:96.47ms
step:831/1770 train_time:79207ms step_avg:96.48ms
step:832/1770 train_time:79306ms step_avg:96.48ms
step:833/1770 train_time:79405ms step_avg:96.48ms
step:834/1770 train_time:79505ms step_avg:96.49ms
step:835/1770 train_time:79605ms step_avg:96.49ms
step:836/1770 train_time:79705ms step_avg:96.49ms
step:837/1770 train_time:79805ms step_avg:96.50ms
step:838/1770 train_time:79905ms step_avg:96.50ms
step:839/1770 train_time:80005ms step_avg:96.51ms
step:840/1770 train_time:80105ms step_avg:96.51ms
step:841/1770 train_time:80205ms step_avg:96.52ms
step:842/1770 train_time:80305ms step_avg:96.52ms
step:843/1770 train_time:80404ms step_avg:96.52ms
step:844/1770 train_time:80504ms step_avg:96.53ms
step:845/1770 train_time:80603ms step_avg:96.53ms
step:846/1770 train_time:80703ms step_avg:96.53ms
step:847/1770 train_time:80802ms step_avg:96.54ms
step:848/1770 train_time:80902ms step_avg:96.54ms
step:849/1770 train_time:81001ms step_avg:96.55ms
step:850/1770 train_time:81101ms step_avg:96.55ms
step:851/1770 train_time:81201ms step_avg:96.55ms
step:852/1770 train_time:81300ms step_avg:96.56ms
step:853/1770 train_time:81399ms step_avg:96.56ms
step:854/1770 train_time:81498ms step_avg:96.56ms
step:855/1770 train_time:81597ms step_avg:96.57ms
step:856/1770 train_time:81697ms step_avg:96.57ms
step:857/1770 train_time:81796ms step_avg:96.57ms
step:858/1770 train_time:81896ms step_avg:96.58ms
step:859/1770 train_time:81995ms step_avg:96.58ms
step:860/1770 train_time:82095ms step_avg:96.58ms
step:861/1770 train_time:82193ms step_avg:96.58ms
step:862/1770 train_time:82292ms step_avg:96.59ms
step:863/1770 train_time:82392ms step_avg:96.59ms
step:864/1770 train_time:82492ms step_avg:96.59ms
step:865/1770 train_time:82592ms step_avg:96.60ms
step:866/1770 train_time:82693ms step_avg:96.60ms
step:867/1770 train_time:82792ms step_avg:96.61ms
step:868/1770 train_time:82891ms step_avg:96.61ms
step:869/1770 train_time:82990ms step_avg:96.61ms
step:870/1770 train_time:83090ms step_avg:96.62ms
step:871/1770 train_time:83189ms step_avg:96.62ms
step:872/1770 train_time:83289ms step_avg:96.62ms
step:873/1770 train_time:83389ms step_avg:96.63ms
step:874/1770 train_time:83488ms step_avg:96.63ms
step:875/1770 train_time:83588ms step_avg:96.63ms
step:875/1770 val_loss:3.5496 train_time:83686ms step_avg:96.75ms
step:876/1770 train_time:83707ms step_avg:96.66ms
step:877/1770 train_time:83790ms step_avg:96.64ms
step:878/1770 train_time:83891ms step_avg:96.65ms
step:879/1770 train_time:83991ms step_avg:96.65ms
step:880/1770 train_time:84089ms step_avg:96.65ms
step:881/1770 train_time:84188ms step_avg:96.66ms
step:882/1770 train_time:84288ms step_avg:96.66ms
step:883/1770 train_time:84387ms step_avg:96.66ms
step:884/1770 train_time:84486ms step_avg:96.67ms
step:885/1770 train_time:84585ms step_avg:96.67ms
step:886/1770 train_time:84686ms step_avg:96.67ms
step:887/1770 train_time:84788ms step_avg:96.68ms
step:888/1770 train_time:84889ms step_avg:96.68ms
step:889/1770 train_time:84989ms step_avg:96.69ms
step:890/1770 train_time:85088ms step_avg:96.69ms
step:891/1770 train_time:85187ms step_avg:96.69ms
step:892/1770 train_time:85287ms step_avg:96.70ms
step:893/1770 train_time:85386ms step_avg:96.70ms
step:894/1770 train_time:85486ms step_avg:96.70ms
step:895/1770 train_time:85586ms step_avg:96.71ms
step:896/1770 train_time:85686ms step_avg:96.71ms
step:897/1770 train_time:85786ms step_avg:96.72ms
step:898/1770 train_time:85887ms step_avg:96.72ms
step:899/1770 train_time:85987ms step_avg:96.72ms
step:900/1770 train_time:86087ms step_avg:96.73ms
step:901/1770 train_time:86187ms step_avg:96.73ms
step:902/1770 train_time:86286ms step_avg:96.73ms
step:903/1770 train_time:86386ms step_avg:96.74ms
step:904/1770 train_time:86485ms step_avg:96.74ms
step:905/1770 train_time:86585ms step_avg:96.74ms
step:906/1770 train_time:86685ms step_avg:96.75ms
step:907/1770 train_time:86785ms step_avg:96.75ms
step:908/1770 train_time:86885ms step_avg:96.75ms
step:909/1770 train_time:86985ms step_avg:96.76ms
step:910/1770 train_time:87086ms step_avg:96.76ms
step:911/1770 train_time:87186ms step_avg:96.77ms
step:912/1770 train_time:87286ms step_avg:96.77ms
step:913/1770 train_time:87386ms step_avg:96.77ms
step:914/1770 train_time:87485ms step_avg:96.78ms
step:915/1770 train_time:87585ms step_avg:96.78ms
step:916/1770 train_time:87684ms step_avg:96.78ms
step:917/1770 train_time:87783ms step_avg:96.78ms
step:918/1770 train_time:87883ms step_avg:96.79ms
step:919/1770 train_time:87982ms step_avg:96.79ms
step:920/1770 train_time:88083ms step_avg:96.79ms
step:921/1770 train_time:88184ms step_avg:96.80ms
step:922/1770 train_time:88285ms step_avg:96.80ms
step:923/1770 train_time:88386ms step_avg:96.81ms
step:924/1770 train_time:88487ms step_avg:96.81ms
step:925/1770 train_time:88588ms step_avg:96.82ms
step:926/1770 train_time:88688ms step_avg:96.82ms
step:927/1770 train_time:88790ms step_avg:96.83ms
step:928/1770 train_time:88891ms step_avg:96.83ms
step:929/1770 train_time:88992ms step_avg:96.84ms
step:930/1770 train_time:89093ms step_avg:96.84ms
step:931/1770 train_time:89194ms step_avg:96.85ms
step:932/1770 train_time:89294ms step_avg:96.85ms
step:933/1770 train_time:89395ms step_avg:96.85ms
step:934/1770 train_time:89495ms step_avg:96.86ms
step:935/1770 train_time:89595ms step_avg:96.86ms
step:936/1770 train_time:89696ms step_avg:96.86ms
step:937/1770 train_time:89797ms step_avg:96.87ms
step:938/1770 train_time:89899ms step_avg:96.87ms
step:939/1770 train_time:89999ms step_avg:96.88ms
step:940/1770 train_time:90100ms step_avg:96.88ms
step:941/1770 train_time:90200ms step_avg:96.89ms
step:942/1770 train_time:90301ms step_avg:96.89ms
step:943/1770 train_time:90401ms step_avg:96.89ms
step:944/1770 train_time:90502ms step_avg:96.90ms
step:945/1770 train_time:90602ms step_avg:96.90ms
step:946/1770 train_time:90704ms step_avg:96.91ms
step:947/1770 train_time:90805ms step_avg:96.91ms
step:948/1770 train_time:90907ms step_avg:96.92ms
step:949/1770 train_time:91007ms step_avg:96.92ms
step:950/1770 train_time:91108ms step_avg:96.92ms
step:951/1770 train_time:91210ms step_avg:96.93ms
step:952/1770 train_time:91311ms step_avg:96.93ms
step:953/1770 train_time:91412ms step_avg:96.94ms
step:954/1770 train_time:91513ms step_avg:96.94ms
step:955/1770 train_time:91613ms step_avg:96.95ms
step:956/1770 train_time:91714ms step_avg:96.95ms
step:957/1770 train_time:91815ms step_avg:96.95ms
step:958/1770 train_time:91915ms step_avg:96.96ms
step:959/1770 train_time:92017ms step_avg:96.96ms
step:960/1770 train_time:92117ms step_avg:96.97ms
step:961/1770 train_time:92218ms step_avg:96.97ms
step:962/1770 train_time:92320ms step_avg:96.97ms
step:963/1770 train_time:92420ms step_avg:96.98ms
step:964/1770 train_time:92521ms step_avg:96.98ms
step:965/1770 train_time:92621ms step_avg:96.99ms
step:966/1770 train_time:92722ms step_avg:96.99ms
step:967/1770 train_time:92822ms step_avg:96.99ms
step:968/1770 train_time:92922ms step_avg:97.00ms
step:969/1770 train_time:93023ms step_avg:97.00ms
step:970/1770 train_time:93125ms step_avg:97.00ms
step:971/1770 train_time:93226ms step_avg:97.01ms
step:972/1770 train_time:93327ms step_avg:97.01ms
step:973/1770 train_time:93428ms step_avg:97.02ms
step:974/1770 train_time:93529ms step_avg:97.02ms
step:975/1770 train_time:93631ms step_avg:97.03ms
step:976/1770 train_time:93732ms step_avg:97.03ms
step:977/1770 train_time:93834ms step_avg:97.04ms
step:978/1770 train_time:93935ms step_avg:97.04ms
step:979/1770 train_time:94035ms step_avg:97.04ms
step:980/1770 train_time:94136ms step_avg:97.05ms
step:981/1770 train_time:94237ms step_avg:97.05ms
step:982/1770 train_time:94338ms step_avg:97.06ms
step:983/1770 train_time:94439ms step_avg:97.06ms
step:984/1770 train_time:94540ms step_avg:97.06ms
step:985/1770 train_time:94640ms step_avg:97.07ms
step:986/1770 train_time:94741ms step_avg:97.07ms
step:987/1770 train_time:94841ms step_avg:97.07ms
step:988/1770 train_time:94941ms step_avg:97.08ms
step:989/1770 train_time:95043ms step_avg:97.08ms
step:990/1770 train_time:95144ms step_avg:97.09ms
step:991/1770 train_time:95245ms step_avg:97.09ms
step:992/1770 train_time:95346ms step_avg:97.09ms
step:993/1770 train_time:95448ms step_avg:97.10ms
step:994/1770 train_time:95549ms step_avg:97.10ms
step:995/1770 train_time:95650ms step_avg:97.11ms
step:996/1770 train_time:95752ms step_avg:97.11ms
step:997/1770 train_time:95853ms step_avg:97.12ms
step:998/1770 train_time:95954ms step_avg:97.12ms
step:999/1770 train_time:96055ms step_avg:97.12ms
step:1000/1770 train_time:96156ms step_avg:97.13ms
step:1000/1770 val_loss:3.5109 train_time:96254ms step_avg:97.23ms
step:1001/1770 train_time:96275ms step_avg:97.15ms
step:1002/1770 train_time:96362ms step_avg:97.14ms
step:1003/1770 train_time:96464ms step_avg:97.14ms
step:1004/1770 train_time:96564ms step_avg:97.15ms
step:1005/1770 train_time:96664ms step_avg:97.15ms
step:1006/1770 train_time:96764ms step_avg:97.15ms
step:1007/1770 train_time:96864ms step_avg:97.16ms
step:1008/1770 train_time:96964ms step_avg:97.16ms
step:1009/1770 train_time:97064ms step_avg:97.16ms
step:1010/1770 train_time:97165ms step_avg:97.16ms
step:1011/1770 train_time:97268ms step_avg:97.17ms
step:1012/1770 train_time:97369ms step_avg:97.18ms
step:1013/1770 train_time:97471ms step_avg:97.18ms
step:1014/1770 train_time:97571ms step_avg:97.18ms
step:1015/1770 train_time:97672ms step_avg:97.19ms
step:1016/1770 train_time:97772ms step_avg:97.19ms
step:1017/1770 train_time:97872ms step_avg:97.19ms
step:1018/1770 train_time:97973ms step_avg:97.20ms
step:1019/1770 train_time:98074ms step_avg:97.20ms
step:1020/1770 train_time:98175ms step_avg:97.20ms
step:1021/1770 train_time:98277ms step_avg:97.21ms
step:1022/1770 train_time:98377ms step_avg:97.21ms
step:1023/1770 train_time:98478ms step_avg:97.21ms
step:1024/1770 train_time:98581ms step_avg:97.22ms
step:1025/1770 train_time:98682ms step_avg:97.22ms
step:1026/1770 train_time:98783ms step_avg:97.23ms
step:1027/1770 train_time:98884ms step_avg:97.23ms
step:1028/1770 train_time:98986ms step_avg:97.24ms
step:1029/1770 train_time:99086ms step_avg:97.24ms
step:1030/1770 train_time:99187ms step_avg:97.24ms
step:1031/1770 train_time:99288ms step_avg:97.25ms
step:1032/1770 train_time:99388ms step_avg:97.25ms
step:1033/1770 train_time:99489ms step_avg:97.25ms
step:1034/1770 train_time:99590ms step_avg:97.26ms
step:1035/1770 train_time:99692ms step_avg:97.26ms
step:1036/1770 train_time:99793ms step_avg:97.26ms
step:1037/1770 train_time:99894ms step_avg:97.27ms
step:1038/1770 train_time:99995ms step_avg:97.27ms
step:1039/1770 train_time:100096ms step_avg:97.27ms
step:1040/1770 train_time:100196ms step_avg:97.28ms
step:1041/1770 train_time:100297ms step_avg:97.28ms
step:1042/1770 train_time:100398ms step_avg:97.28ms
step:1043/1770 train_time:100499ms step_avg:97.29ms
step:1044/1770 train_time:100601ms step_avg:97.29ms
step:1045/1770 train_time:100703ms step_avg:97.30ms
step:1046/1770 train_time:100804ms step_avg:97.30ms
step:1047/1770 train_time:100905ms step_avg:97.30ms
step:1048/1770 train_time:101007ms step_avg:97.31ms
step:1049/1770 train_time:101107ms step_avg:97.31ms
step:1050/1770 train_time:101208ms step_avg:97.31ms
step:1051/1770 train_time:101310ms step_avg:97.32ms
step:1052/1770 train_time:101410ms step_avg:97.32ms
step:1053/1770 train_time:101510ms step_avg:97.33ms
step:1054/1770 train_time:101611ms step_avg:97.33ms
step:1055/1770 train_time:101712ms step_avg:97.33ms
step:1056/1770 train_time:101812ms step_avg:97.33ms
step:1057/1770 train_time:101913ms step_avg:97.34ms
step:1058/1770 train_time:102014ms step_avg:97.34ms
step:1059/1770 train_time:102115ms step_avg:97.35ms
step:1060/1770 train_time:102216ms step_avg:97.35ms
step:1061/1770 train_time:102318ms step_avg:97.35ms
step:1062/1770 train_time:102419ms step_avg:97.36ms
step:1063/1770 train_time:102521ms step_avg:97.36ms
step:1064/1770 train_time:102623ms step_avg:97.37ms
step:1065/1770 train_time:102725ms step_avg:97.37ms
step:1066/1770 train_time:102826ms step_avg:97.37ms
step:1067/1770 train_time:102928ms step_avg:97.38ms
step:1068/1770 train_time:103031ms step_avg:97.38ms
step:1069/1770 train_time:103131ms step_avg:97.39ms
step:1070/1770 train_time:103232ms step_avg:97.39ms
step:1071/1770 train_time:103333ms step_avg:97.39ms
step:1072/1770 train_time:103434ms step_avg:97.40ms
step:1073/1770 train_time:103535ms step_avg:97.40ms
step:1074/1770 train_time:103636ms step_avg:97.40ms
step:1075/1770 train_time:103737ms step_avg:97.41ms
step:1076/1770 train_time:103839ms step_avg:97.41ms
step:1077/1770 train_time:103941ms step_avg:97.41ms
step:1078/1770 train_time:104043ms step_avg:97.42ms
step:1079/1770 train_time:104144ms step_avg:97.42ms
step:1080/1770 train_time:104245ms step_avg:97.43ms
step:1081/1770 train_time:104347ms step_avg:97.43ms
step:1082/1770 train_time:104448ms step_avg:97.43ms
step:1083/1770 train_time:104549ms step_avg:97.44ms
step:1084/1770 train_time:104649ms step_avg:97.44ms
step:1085/1770 train_time:104750ms step_avg:97.44ms
step:1086/1770 train_time:104851ms step_avg:97.44ms
step:1087/1770 train_time:104953ms step_avg:97.45ms
step:1088/1770 train_time:105053ms step_avg:97.45ms
step:1089/1770 train_time:105154ms step_avg:97.45ms
step:1090/1770 train_time:105255ms step_avg:97.46ms
step:1091/1770 train_time:105356ms step_avg:97.46ms
step:1092/1770 train_time:105456ms step_avg:97.46ms
step:1093/1770 train_time:105558ms step_avg:97.47ms
step:1094/1770 train_time:105660ms step_avg:97.47ms
step:1095/1770 train_time:105761ms step_avg:97.48ms
step:1096/1770 train_time:105864ms step_avg:97.48ms
step:1097/1770 train_time:105965ms step_avg:97.48ms
step:1098/1770 train_time:106065ms step_avg:97.49ms
step:1099/1770 train_time:106166ms step_avg:97.49ms
step:1100/1770 train_time:106267ms step_avg:97.49ms
step:1101/1770 train_time:106368ms step_avg:97.50ms
step:1102/1770 train_time:106469ms step_avg:97.50ms
step:1103/1770 train_time:106570ms step_avg:97.50ms
step:1104/1770 train_time:106672ms step_avg:97.51ms
step:1105/1770 train_time:106773ms step_avg:97.51ms
step:1106/1770 train_time:106874ms step_avg:97.51ms
step:1107/1770 train_time:106975ms step_avg:97.52ms
step:1108/1770 train_time:107076ms step_avg:97.52ms
step:1109/1770 train_time:107176ms step_avg:97.52ms
step:1110/1770 train_time:107277ms step_avg:97.52ms
step:1111/1770 train_time:107378ms step_avg:97.53ms
step:1112/1770 train_time:107480ms step_avg:97.53ms
step:1113/1770 train_time:107581ms step_avg:97.54ms
step:1114/1770 train_time:107683ms step_avg:97.54ms
step:1115/1770 train_time:107785ms step_avg:97.54ms
step:1116/1770 train_time:107887ms step_avg:97.55ms
step:1117/1770 train_time:107987ms step_avg:97.55ms
step:1118/1770 train_time:108088ms step_avg:97.55ms
step:1119/1770 train_time:108189ms step_avg:97.56ms
step:1120/1770 train_time:108290ms step_avg:97.56ms
step:1121/1770 train_time:108391ms step_avg:97.56ms
step:1122/1770 train_time:108492ms step_avg:97.56ms
step:1123/1770 train_time:108592ms step_avg:97.57ms
step:1124/1770 train_time:108694ms step_avg:97.57ms
step:1125/1770 train_time:108795ms step_avg:97.57ms
step:1125/1770 val_loss:3.4694 train_time:108894ms step_avg:97.66ms
step:1126/1770 train_time:108915ms step_avg:97.59ms
step:1127/1770 train_time:109005ms step_avg:97.59ms
step:1128/1770 train_time:109106ms step_avg:97.59ms
step:1129/1770 train_time:109206ms step_avg:97.59ms
step:1130/1770 train_time:109306ms step_avg:97.60ms
step:1131/1770 train_time:109407ms step_avg:97.60ms
step:1132/1770 train_time:109508ms step_avg:97.60ms
step:1133/1770 train_time:109608ms step_avg:97.60ms
step:1134/1770 train_time:109709ms step_avg:97.61ms
step:1135/1770 train_time:109809ms step_avg:97.61ms
step:1136/1770 train_time:109913ms step_avg:97.61ms
step:1137/1770 train_time:110016ms step_avg:97.62ms
step:1138/1770 train_time:110117ms step_avg:97.62ms
step:1139/1770 train_time:110217ms step_avg:97.62ms
step:1140/1770 train_time:110318ms step_avg:97.63ms
step:1141/1770 train_time:110419ms step_avg:97.63ms
step:1142/1770 train_time:110519ms step_avg:97.63ms
step:1143/1770 train_time:110620ms step_avg:97.63ms
step:1144/1770 train_time:110721ms step_avg:97.64ms
step:1145/1770 train_time:110822ms step_avg:97.64ms
step:1146/1770 train_time:110924ms step_avg:97.64ms
step:1147/1770 train_time:111025ms step_avg:97.65ms
step:1148/1770 train_time:111125ms step_avg:97.65ms
step:1149/1770 train_time:111227ms step_avg:97.65ms
step:1150/1770 train_time:111327ms step_avg:97.66ms
step:1151/1770 train_time:111430ms step_avg:97.66ms
step:1152/1770 train_time:111531ms step_avg:97.66ms
step:1153/1770 train_time:111633ms step_avg:97.67ms
step:1154/1770 train_time:111735ms step_avg:97.67ms
step:1155/1770 train_time:111837ms step_avg:97.67ms
step:1156/1770 train_time:111938ms step_avg:97.68ms
step:1157/1770 train_time:112040ms step_avg:97.68ms
step:1158/1770 train_time:112142ms step_avg:97.68ms
step:1159/1770 train_time:112243ms step_avg:97.69ms
step:1160/1770 train_time:112343ms step_avg:97.69ms
step:1161/1770 train_time:112444ms step_avg:97.69ms
step:1162/1770 train_time:112545ms step_avg:97.70ms
step:1163/1770 train_time:112646ms step_avg:97.70ms
step:1164/1770 train_time:112748ms step_avg:97.70ms
step:1165/1770 train_time:112849ms step_avg:97.70ms
step:1166/1770 train_time:112950ms step_avg:97.71ms
step:1167/1770 train_time:113052ms step_avg:97.71ms
step:1168/1770 train_time:113153ms step_avg:97.71ms
step:1169/1770 train_time:113254ms step_avg:97.72ms
step:1170/1770 train_time:113355ms step_avg:97.72ms
step:1171/1770 train_time:113457ms step_avg:97.72ms
step:1172/1770 train_time:113558ms step_avg:97.73ms
step:1173/1770 train_time:113660ms step_avg:97.73ms
step:1174/1770 train_time:113762ms step_avg:97.73ms
step:1175/1770 train_time:113862ms step_avg:97.74ms
step:1176/1770 train_time:113963ms step_avg:97.74ms
step:1177/1770 train_time:114064ms step_avg:97.74ms
step:1178/1770 train_time:114165ms step_avg:97.74ms
step:1179/1770 train_time:114265ms step_avg:97.75ms
step:1180/1770 train_time:114366ms step_avg:97.75ms
step:1181/1770 train_time:114466ms step_avg:97.75ms
step:1182/1770 train_time:114569ms step_avg:97.75ms
step:1183/1770 train_time:114671ms step_avg:97.76ms
step:1184/1770 train_time:114774ms step_avg:97.76ms
step:1185/1770 train_time:114876ms step_avg:97.77ms
step:1186/1770 train_time:114979ms step_avg:97.77ms
step:1187/1770 train_time:115083ms step_avg:97.78ms
step:1188/1770 train_time:115184ms step_avg:97.78ms
step:1189/1770 train_time:115286ms step_avg:97.78ms
step:1190/1770 train_time:115387ms step_avg:97.79ms
step:1191/1770 train_time:115490ms step_avg:97.79ms
step:1192/1770 train_time:115593ms step_avg:97.79ms
step:1193/1770 train_time:115696ms step_avg:97.80ms
step:1194/1770 train_time:115798ms step_avg:97.80ms
step:1195/1770 train_time:115900ms step_avg:97.81ms
step:1196/1770 train_time:116003ms step_avg:97.81ms
step:1197/1770 train_time:116105ms step_avg:97.81ms
step:1198/1770 train_time:116207ms step_avg:97.82ms
step:1199/1770 train_time:116308ms step_avg:97.82ms
step:1200/1770 train_time:116410ms step_avg:97.82ms
step:1201/1770 train_time:116513ms step_avg:97.83ms
step:1202/1770 train_time:116614ms step_avg:97.83ms
step:1203/1770 train_time:116716ms step_avg:97.83ms
step:1204/1770 train_time:116819ms step_avg:97.84ms
step:1205/1770 train_time:116921ms step_avg:97.84ms
step:1206/1770 train_time:117023ms step_avg:97.85ms
step:1207/1770 train_time:117125ms step_avg:97.85ms
step:1208/1770 train_time:117227ms step_avg:97.85ms
step:1209/1770 train_time:117329ms step_avg:97.86ms
step:1210/1770 train_time:117431ms step_avg:97.86ms
step:1211/1770 train_time:117533ms step_avg:97.86ms
step:1212/1770 train_time:117637ms step_avg:97.87ms
step:1213/1770 train_time:117739ms step_avg:97.87ms
step:1214/1770 train_time:117841ms step_avg:97.87ms
step:1215/1770 train_time:117943ms step_avg:97.88ms
step:1216/1770 train_time:118048ms step_avg:97.88ms
step:1217/1770 train_time:118150ms step_avg:97.89ms
step:1218/1770 train_time:118251ms step_avg:97.89ms
step:1219/1770 train_time:118354ms step_avg:97.89ms
step:1220/1770 train_time:118456ms step_avg:97.90ms
step:1221/1770 train_time:118557ms step_avg:97.90ms
step:1222/1770 train_time:118660ms step_avg:97.90ms
step:1223/1770 train_time:118761ms step_avg:97.91ms
step:1224/1770 train_time:118864ms step_avg:97.91ms
step:1225/1770 train_time:118967ms step_avg:97.92ms
step:1226/1770 train_time:119069ms step_avg:97.92ms
step:1227/1770 train_time:119174ms step_avg:97.92ms
step:1228/1770 train_time:119278ms step_avg:97.93ms
step:1229/1770 train_time:119379ms step_avg:97.93ms
step:1230/1770 train_time:119481ms step_avg:97.94ms
step:1231/1770 train_time:119584ms step_avg:97.94ms
step:1232/1770 train_time:119686ms step_avg:97.94ms
step:1233/1770 train_time:119788ms step_avg:97.95ms
step:1234/1770 train_time:119889ms step_avg:97.95ms
step:1235/1770 train_time:119992ms step_avg:97.95ms
step:1236/1770 train_time:120094ms step_avg:97.96ms
step:1237/1770 train_time:120196ms step_avg:97.96ms
step:1238/1770 train_time:120299ms step_avg:97.96ms
step:1239/1770 train_time:120401ms step_avg:97.97ms
step:1240/1770 train_time:120503ms step_avg:97.97ms
step:1241/1770 train_time:120606ms step_avg:97.97ms
step:1242/1770 train_time:120707ms step_avg:97.98ms
step:1243/1770 train_time:120809ms step_avg:97.98ms
step:1244/1770 train_time:120910ms step_avg:97.98ms
step:1245/1770 train_time:121013ms step_avg:97.99ms
step:1246/1770 train_time:121115ms step_avg:97.99ms
step:1247/1770 train_time:121217ms step_avg:97.99ms
step:1248/1770 train_time:121319ms step_avg:98.00ms
step:1249/1770 train_time:121422ms step_avg:98.00ms
step:1250/1770 train_time:121523ms step_avg:98.00ms
step:1250/1770 val_loss:3.4226 train_time:121625ms step_avg:98.08ms
step:1251/1770 train_time:121646ms step_avg:98.02ms
step:1252/1770 train_time:121734ms step_avg:98.01ms
step:1253/1770 train_time:121837ms step_avg:98.02ms
step:1254/1770 train_time:121938ms step_avg:98.02ms
step:1255/1770 train_time:122042ms step_avg:98.03ms
step:1256/1770 train_time:122143ms step_avg:98.03ms
step:1257/1770 train_time:122244ms step_avg:98.03ms
step:1258/1770 train_time:122346ms step_avg:98.03ms
step:1259/1770 train_time:122449ms step_avg:98.04ms
step:1260/1770 train_time:122550ms step_avg:98.04ms
step:1261/1770 train_time:122653ms step_avg:98.04ms
step:1262/1770 train_time:122756ms step_avg:98.05ms
step:1263/1770 train_time:122857ms step_avg:98.05ms
step:1264/1770 train_time:122961ms step_avg:98.05ms
step:1265/1770 train_time:123062ms step_avg:98.06ms
step:1266/1770 train_time:123165ms step_avg:98.06ms
step:1267/1770 train_time:123267ms step_avg:98.06ms
step:1268/1770 train_time:123369ms step_avg:98.07ms
step:1269/1770 train_time:123472ms step_avg:98.07ms
step:1270/1770 train_time:123574ms step_avg:98.07ms
step:1271/1770 train_time:123677ms step_avg:98.08ms
step:1272/1770 train_time:123779ms step_avg:98.08ms
step:1273/1770 train_time:123882ms step_avg:98.09ms
step:1274/1770 train_time:123985ms step_avg:98.09ms
step:1275/1770 train_time:124087ms step_avg:98.09ms
step:1276/1770 train_time:124190ms step_avg:98.10ms
step:1277/1770 train_time:124292ms step_avg:98.10ms
step:1278/1770 train_time:124395ms step_avg:98.10ms
step:1279/1770 train_time:124499ms step_avg:98.11ms
step:1280/1770 train_time:124600ms step_avg:98.11ms
step:1281/1770 train_time:124702ms step_avg:98.11ms
step:1282/1770 train_time:124806ms step_avg:98.12ms
step:1283/1770 train_time:124908ms step_avg:98.12ms
step:1284/1770 train_time:125010ms step_avg:98.12ms
step:1285/1770 train_time:125113ms step_avg:98.13ms
step:1286/1770 train_time:125217ms step_avg:98.13ms
step:1287/1770 train_time:125321ms step_avg:98.14ms
step:1288/1770 train_time:125424ms step_avg:98.14ms
step:1289/1770 train_time:125526ms step_avg:98.14ms
step:1290/1770 train_time:125627ms step_avg:98.15ms
step:1291/1770 train_time:125729ms step_avg:98.15ms
step:1292/1770 train_time:125832ms step_avg:98.15ms
step:1293/1770 train_time:125934ms step_avg:98.16ms
step:1294/1770 train_time:126036ms step_avg:98.16ms
step:1295/1770 train_time:126138ms step_avg:98.16ms
step:1296/1770 train_time:126241ms step_avg:98.17ms
step:1297/1770 train_time:126342ms step_avg:98.17ms
step:1298/1770 train_time:126444ms step_avg:98.17ms
step:1299/1770 train_time:126546ms step_avg:98.17ms
step:1300/1770 train_time:126647ms step_avg:98.18ms
step:1301/1770 train_time:126750ms step_avg:98.18ms
step:1302/1770 train_time:126852ms step_avg:98.18ms
step:1303/1770 train_time:126954ms step_avg:98.19ms
step:1304/1770 train_time:127056ms step_avg:98.19ms
step:1305/1770 train_time:127159ms step_avg:98.19ms
step:1306/1770 train_time:127261ms step_avg:98.20ms
step:1307/1770 train_time:127364ms step_avg:98.20ms
step:1308/1770 train_time:127466ms step_avg:98.20ms
step:1309/1770 train_time:127568ms step_avg:98.20ms
step:1310/1770 train_time:127670ms step_avg:98.21ms
step:1311/1770 train_time:127771ms step_avg:98.21ms
step:1312/1770 train_time:127873ms step_avg:98.21ms
step:1313/1770 train_time:127975ms step_avg:98.22ms
step:1314/1770 train_time:128078ms step_avg:98.22ms
step:1315/1770 train_time:128180ms step_avg:98.22ms
step:1316/1770 train_time:128282ms step_avg:98.23ms
step:1317/1770 train_time:128385ms step_avg:98.23ms
step:1318/1770 train_time:128490ms step_avg:98.23ms
step:1319/1770 train_time:128593ms step_avg:98.24ms
step:1320/1770 train_time:128694ms step_avg:98.24ms
step:1321/1770 train_time:128797ms step_avg:98.24ms
step:1322/1770 train_time:128898ms step_avg:98.25ms
step:1323/1770 train_time:129001ms step_avg:98.25ms
step:1324/1770 train_time:129104ms step_avg:98.25ms
step:1325/1770 train_time:129207ms step_avg:98.26ms
step:1326/1770 train_time:129309ms step_avg:98.26ms
step:1327/1770 train_time:129414ms step_avg:98.26ms
step:1328/1770 train_time:129516ms step_avg:98.27ms
step:1329/1770 train_time:129619ms step_avg:98.27ms
step:1330/1770 train_time:129720ms step_avg:98.27ms
step:1331/1770 train_time:129822ms step_avg:98.28ms
step:1332/1770 train_time:129924ms step_avg:98.28ms
step:1333/1770 train_time:130025ms step_avg:98.28ms
step:1334/1770 train_time:130128ms step_avg:98.28ms
step:1335/1770 train_time:130229ms step_avg:98.29ms
step:1336/1770 train_time:130330ms step_avg:98.29ms
step:1337/1770 train_time:130433ms step_avg:98.29ms
step:1338/1770 train_time:130535ms step_avg:98.29ms
step:1339/1770 train_time:130639ms step_avg:98.30ms
step:1340/1770 train_time:130742ms step_avg:98.30ms
step:1341/1770 train_time:130844ms step_avg:98.30ms
step:1342/1770 train_time:130947ms step_avg:98.31ms
step:1343/1770 train_time:131049ms step_avg:98.31ms
step:1344/1770 train_time:131152ms step_avg:98.32ms
step:1345/1770 train_time:131254ms step_avg:98.32ms
step:1346/1770 train_time:131356ms step_avg:98.32ms
step:1347/1770 train_time:131458ms step_avg:98.32ms
step:1348/1770 train_time:131562ms step_avg:98.33ms
step:1349/1770 train_time:131664ms step_avg:98.33ms
step:1350/1770 train_time:131766ms step_avg:98.33ms
step:1351/1770 train_time:131868ms step_avg:98.34ms
step:1352/1770 train_time:131971ms step_avg:98.34ms
step:1353/1770 train_time:132074ms step_avg:98.34ms
step:1354/1770 train_time:132176ms step_avg:98.35ms
step:1355/1770 train_time:132278ms step_avg:98.35ms
step:1356/1770 train_time:132379ms step_avg:98.35ms
step:1357/1770 train_time:132481ms step_avg:98.35ms
step:1358/1770 train_time:132584ms step_avg:98.36ms
step:1359/1770 train_time:132685ms step_avg:98.36ms
step:1360/1770 train_time:132788ms step_avg:98.36ms
step:1361/1770 train_time:132891ms step_avg:98.36ms
step:1362/1770 train_time:132993ms step_avg:98.37ms
step:1363/1770 train_time:133096ms step_avg:98.37ms
step:1364/1770 train_time:133200ms step_avg:98.38ms
step:1365/1770 train_time:133301ms step_avg:98.38ms
step:1366/1770 train_time:133403ms step_avg:98.38ms
step:1367/1770 train_time:133506ms step_avg:98.38ms
step:1368/1770 train_time:133608ms step_avg:98.39ms
step:1369/1770 train_time:133711ms step_avg:98.39ms
step:1370/1770 train_time:133814ms step_avg:98.39ms
step:1371/1770 train_time:133916ms step_avg:98.40ms
step:1372/1770 train_time:134017ms step_avg:98.40ms
step:1373/1770 train_time:134120ms step_avg:98.40ms
step:1374/1770 train_time:134223ms step_avg:98.40ms
step:1375/1770 train_time:134326ms step_avg:98.41ms
step:1375/1770 val_loss:3.3793 train_time:134427ms step_avg:98.48ms
step:1376/1770 train_time:134449ms step_avg:98.43ms
step:1377/1770 train_time:134539ms step_avg:98.42ms
step:1378/1770 train_time:134641ms step_avg:98.42ms
step:1379/1770 train_time:134742ms step_avg:98.42ms
step:1380/1770 train_time:134844ms step_avg:98.43ms
step:1381/1770 train_time:134946ms step_avg:98.43ms
step:1382/1770 train_time:135047ms step_avg:98.43ms
step:1383/1770 train_time:135149ms step_avg:98.43ms
step:1384/1770 train_time:135251ms step_avg:98.44ms
step:1385/1770 train_time:135353ms step_avg:98.44ms
step:1386/1770 train_time:135457ms step_avg:98.44ms
step:1387/1770 train_time:135561ms step_avg:98.45ms
step:1388/1770 train_time:135663ms step_avg:98.45ms
step:1389/1770 train_time:135765ms step_avg:98.45ms
step:1390/1770 train_time:135867ms step_avg:98.45ms
step:1391/1770 train_time:135969ms step_avg:98.46ms
step:1392/1770 train_time:136071ms step_avg:98.46ms
step:1393/1770 train_time:136174ms step_avg:98.46ms
step:1394/1770 train_time:136276ms step_avg:98.47ms
step:1395/1770 train_time:136379ms step_avg:98.47ms
step:1396/1770 train_time:136483ms step_avg:98.47ms
step:1397/1770 train_time:136585ms step_avg:98.47ms
step:1398/1770 train_time:136688ms step_avg:98.48ms
step:1399/1770 train_time:136791ms step_avg:98.48ms
step:1400/1770 train_time:136894ms step_avg:98.48ms
step:1401/1770 train_time:136996ms step_avg:98.49ms
step:1402/1770 train_time:137099ms step_avg:98.49ms
step:1403/1770 train_time:137200ms step_avg:98.49ms
step:1404/1770 train_time:137303ms step_avg:98.50ms
step:1405/1770 train_time:137404ms step_avg:98.50ms
step:1406/1770 train_time:137506ms step_avg:98.50ms
step:1407/1770 train_time:137608ms step_avg:98.50ms
step:1408/1770 train_time:137711ms step_avg:98.51ms
step:1409/1770 train_time:137813ms step_avg:98.51ms
step:1410/1770 train_time:137916ms step_avg:98.51ms
step:1411/1770 train_time:138018ms step_avg:98.51ms
step:1412/1770 train_time:138120ms step_avg:98.52ms
step:1413/1770 train_time:138223ms step_avg:98.52ms
step:1414/1770 train_time:138325ms step_avg:98.52ms
step:1415/1770 train_time:138427ms step_avg:98.52ms
step:1416/1770 train_time:138530ms step_avg:98.53ms
step:1417/1770 train_time:138632ms step_avg:98.53ms
step:1418/1770 train_time:138733ms step_avg:98.53ms
step:1419/1770 train_time:138836ms step_avg:98.53ms
step:1420/1770 train_time:138938ms step_avg:98.54ms
step:1421/1770 train_time:139040ms step_avg:98.54ms
step:1422/1770 train_time:139142ms step_avg:98.54ms
step:1423/1770 train_time:139244ms step_avg:98.54ms
step:1424/1770 train_time:139347ms step_avg:98.55ms
step:1425/1770 train_time:139449ms step_avg:98.55ms
step:1426/1770 train_time:139551ms step_avg:98.55ms
step:1427/1770 train_time:139653ms step_avg:98.56ms
step:1428/1770 train_time:139757ms step_avg:98.56ms
step:1429/1770 train_time:139860ms step_avg:98.56ms
step:1430/1770 train_time:139961ms step_avg:98.56ms
step:1431/1770 train_time:140065ms step_avg:98.57ms
step:1432/1770 train_time:140167ms step_avg:98.57ms
step:1433/1770 train_time:140268ms step_avg:98.57ms
step:1434/1770 train_time:140370ms step_avg:98.57ms
step:1435/1770 train_time:140472ms step_avg:98.58ms
step:1436/1770 train_time:140575ms step_avg:98.58ms
step:1437/1770 train_time:140677ms step_avg:98.58ms
step:1438/1770 train_time:140779ms step_avg:98.58ms
step:1439/1770 train_time:140882ms step_avg:98.59ms
step:1440/1770 train_time:140984ms step_avg:98.59ms
step:1441/1770 train_time:141089ms step_avg:98.59ms
step:1442/1770 train_time:141191ms step_avg:98.60ms
step:1443/1770 train_time:141293ms step_avg:98.60ms
step:1444/1770 train_time:141396ms step_avg:98.60ms
step:1445/1770 train_time:141499ms step_avg:98.61ms
step:1446/1770 train_time:141601ms step_avg:98.61ms
step:1447/1770 train_time:141705ms step_avg:98.61ms
step:1448/1770 train_time:141809ms step_avg:98.62ms
step:1449/1770 train_time:141913ms step_avg:98.62ms
step:1450/1770 train_time:142017ms step_avg:98.62ms
step:1451/1770 train_time:142120ms step_avg:98.63ms
step:1452/1770 train_time:142225ms step_avg:98.63ms
step:1453/1770 train_time:142327ms step_avg:98.63ms
step:1454/1770 train_time:142430ms step_avg:98.64ms
step:1455/1770 train_time:142535ms step_avg:98.64ms
step:1456/1770 train_time:142640ms step_avg:98.64ms
step:1457/1770 train_time:142743ms step_avg:98.65ms
step:1458/1770 train_time:142847ms step_avg:98.65ms
step:1459/1770 train_time:142952ms step_avg:98.66ms
step:1460/1770 train_time:143056ms step_avg:98.66ms
step:1461/1770 train_time:143160ms step_avg:98.66ms
step:1462/1770 train_time:143263ms step_avg:98.67ms
step:1463/1770 train_time:143366ms step_avg:98.67ms
step:1464/1770 train_time:143472ms step_avg:98.67ms
step:1465/1770 train_time:143575ms step_avg:98.68ms
step:1466/1770 train_time:143679ms step_avg:98.68ms
step:1467/1770 train_time:143783ms step_avg:98.68ms
step:1468/1770 train_time:143887ms step_avg:98.69ms
step:1469/1770 train_time:143990ms step_avg:98.69ms
step:1470/1770 train_time:144093ms step_avg:98.69ms
step:1471/1770 train_time:144197ms step_avg:98.70ms
step:1472/1770 train_time:144300ms step_avg:98.70ms
step:1473/1770 train_time:144403ms step_avg:98.70ms
step:1474/1770 train_time:144508ms step_avg:98.71ms
step:1475/1770 train_time:144610ms step_avg:98.71ms
step:1476/1770 train_time:144714ms step_avg:98.71ms
step:1477/1770 train_time:144819ms step_avg:98.72ms
step:1478/1770 train_time:144924ms step_avg:98.72ms
step:1479/1770 train_time:145027ms step_avg:98.72ms
step:1480/1770 train_time:145130ms step_avg:98.73ms
step:1481/1770 train_time:145237ms step_avg:98.73ms
step:1482/1770 train_time:145340ms step_avg:98.74ms
step:1483/1770 train_time:145443ms step_avg:98.74ms
step:1484/1770 train_time:145547ms step_avg:98.74ms
step:1485/1770 train_time:145649ms step_avg:98.75ms
step:1486/1770 train_time:145752ms step_avg:98.75ms
step:1487/1770 train_time:145857ms step_avg:98.75ms
step:1488/1770 train_time:145962ms step_avg:98.76ms
step:1489/1770 train_time:146066ms step_avg:98.76ms
step:1490/1770 train_time:146170ms step_avg:98.76ms
step:1491/1770 train_time:146273ms step_avg:98.77ms
step:1492/1770 train_time:146377ms step_avg:98.77ms
step:1493/1770 train_time:146483ms step_avg:98.77ms
step:1494/1770 train_time:146589ms step_avg:98.78ms
step:1495/1770 train_time:146691ms step_avg:98.78ms
step:1496/1770 train_time:146794ms step_avg:98.78ms
step:1497/1770 train_time:146898ms step_avg:98.79ms
step:1498/1770 train_time:147002ms step_avg:98.79ms
step:1499/1770 train_time:147104ms step_avg:98.79ms
step:1500/1770 train_time:147207ms step_avg:98.80ms
step:1500/1770 val_loss:3.3408 train_time:147308ms step_avg:98.86ms
step:1501/1770 train_time:147331ms step_avg:98.81ms
step:1502/1770 train_time:147419ms step_avg:98.81ms
step:1503/1770 train_time:147521ms step_avg:98.81ms
step:1504/1770 train_time:147625ms step_avg:98.81ms
step:1505/1770 train_time:147730ms step_avg:98.82ms
step:1506/1770 train_time:147833ms step_avg:98.82ms
step:1507/1770 train_time:147937ms step_avg:98.82ms
step:1508/1770 train_time:148041ms step_avg:98.83ms
step:1509/1770 train_time:148144ms step_avg:98.83ms
step:1510/1770 train_time:148246ms step_avg:98.83ms
step:1511/1770 train_time:148352ms step_avg:98.84ms
step:1512/1770 train_time:148457ms step_avg:98.84ms
step:1513/1770 train_time:148560ms step_avg:98.84ms
step:1514/1770 train_time:148663ms step_avg:98.85ms
step:1515/1770 train_time:148767ms step_avg:98.85ms
step:1516/1770 train_time:148871ms step_avg:98.85ms
step:1517/1770 train_time:148975ms step_avg:98.86ms
step:1518/1770 train_time:149080ms step_avg:98.86ms
step:1519/1770 train_time:149182ms step_avg:98.86ms
step:1520/1770 train_time:149286ms step_avg:98.87ms
step:1521/1770 train_time:149390ms step_avg:98.87ms
step:1522/1770 train_time:149494ms step_avg:98.87ms
step:1523/1770 train_time:149598ms step_avg:98.88ms
step:1524/1770 train_time:149702ms step_avg:98.88ms
step:1525/1770 train_time:149805ms step_avg:98.88ms
step:1526/1770 train_time:149909ms step_avg:98.88ms
step:1527/1770 train_time:150013ms step_avg:98.89ms
step:1528/1770 train_time:150119ms step_avg:98.89ms
step:1529/1770 train_time:150222ms step_avg:98.90ms
step:1530/1770 train_time:150324ms step_avg:98.90ms
step:1531/1770 train_time:150427ms step_avg:98.90ms
step:1532/1770 train_time:150532ms step_avg:98.90ms
step:1533/1770 train_time:150637ms step_avg:98.91ms
step:1534/1770 train_time:150740ms step_avg:98.91ms
step:1535/1770 train_time:150843ms step_avg:98.91ms
step:1536/1770 train_time:150946ms step_avg:98.92ms
step:1537/1770 train_time:151049ms step_avg:98.92ms
step:1538/1770 train_time:151154ms step_avg:98.92ms
step:1539/1770 train_time:151257ms step_avg:98.93ms
step:1540/1770 train_time:151365ms step_avg:98.93ms
step:1541/1770 train_time:151470ms step_avg:98.94ms
step:1542/1770 train_time:151575ms step_avg:98.94ms
step:1543/1770 train_time:151677ms step_avg:98.94ms
step:1544/1770 train_time:151783ms step_avg:98.95ms
step:1545/1770 train_time:151886ms step_avg:98.95ms
step:1546/1770 train_time:151989ms step_avg:98.95ms
step:1547/1770 train_time:152093ms step_avg:98.95ms
step:1548/1770 train_time:152197ms step_avg:98.96ms
step:1549/1770 train_time:152300ms step_avg:98.96ms
step:1550/1770 train_time:152406ms step_avg:98.96ms
step:1551/1770 train_time:152509ms step_avg:98.97ms
step:1552/1770 train_time:152614ms step_avg:98.97ms
step:1553/1770 train_time:152717ms step_avg:98.97ms
step:1554/1770 train_time:152820ms step_avg:98.98ms
step:1555/1770 train_time:152923ms step_avg:98.98ms
step:1556/1770 train_time:153026ms step_avg:98.98ms
step:1557/1770 train_time:153130ms step_avg:98.99ms
step:1558/1770 train_time:153234ms step_avg:98.99ms
step:1559/1770 train_time:153337ms step_avg:98.99ms
step:1560/1770 train_time:153440ms step_avg:98.99ms
step:1561/1770 train_time:153545ms step_avg:99.00ms
step:1562/1770 train_time:153649ms step_avg:99.00ms
step:1563/1770 train_time:153753ms step_avg:99.00ms
step:1564/1770 train_time:153856ms step_avg:99.01ms
step:1565/1770 train_time:153958ms step_avg:99.01ms
step:1566/1770 train_time:154061ms step_avg:99.01ms
step:1567/1770 train_time:154164ms step_avg:99.01ms
step:1568/1770 train_time:154268ms step_avg:99.02ms
step:1569/1770 train_time:154375ms step_avg:99.02ms
step:1570/1770 train_time:154478ms step_avg:99.02ms
step:1571/1770 train_time:154581ms step_avg:99.03ms
step:1572/1770 train_time:154685ms step_avg:99.03ms
step:1573/1770 train_time:154790ms step_avg:99.03ms
step:1574/1770 train_time:154893ms step_avg:99.04ms
step:1575/1770 train_time:154996ms step_avg:99.04ms
step:1576/1770 train_time:155100ms step_avg:99.04ms
step:1577/1770 train_time:155204ms step_avg:99.05ms
step:1578/1770 train_time:155309ms step_avg:99.05ms
step:1579/1770 train_time:155413ms step_avg:99.05ms
step:1580/1770 train_time:155516ms step_avg:99.05ms
step:1581/1770 train_time:155622ms step_avg:99.06ms
step:1582/1770 train_time:155727ms step_avg:99.06ms
step:1583/1770 train_time:155830ms step_avg:99.07ms
step:1584/1770 train_time:155936ms step_avg:99.07ms
step:1585/1770 train_time:156039ms step_avg:99.07ms
step:1586/1770 train_time:156146ms step_avg:99.08ms
step:1587/1770 train_time:156250ms step_avg:99.08ms
step:1588/1770 train_time:156353ms step_avg:99.08ms
step:1589/1770 train_time:156459ms step_avg:99.09ms
step:1590/1770 train_time:156561ms step_avg:99.09ms
step:1591/1770 train_time:156664ms step_avg:99.09ms
step:1592/1770 train_time:156768ms step_avg:99.09ms
step:1593/1770 train_time:156872ms step_avg:99.10ms
step:1594/1770 train_time:156975ms step_avg:99.10ms
step:1595/1770 train_time:157078ms step_avg:99.10ms
step:1596/1770 train_time:157184ms step_avg:99.11ms
step:1597/1770 train_time:157286ms step_avg:99.11ms
step:1598/1770 train_time:157390ms step_avg:99.11ms
step:1599/1770 train_time:157494ms step_avg:99.12ms
step:1600/1770 train_time:157600ms step_avg:99.12ms
step:1601/1770 train_time:157704ms step_avg:99.12ms
step:1602/1770 train_time:157808ms step_avg:99.13ms
step:1603/1770 train_time:157912ms step_avg:99.13ms
step:1604/1770 train_time:158014ms step_avg:99.13ms
step:1605/1770 train_time:158117ms step_avg:99.13ms
step:1606/1770 train_time:158221ms step_avg:99.14ms
step:1607/1770 train_time:158327ms step_avg:99.14ms
step:1608/1770 train_time:158430ms step_avg:99.14ms
step:1609/1770 train_time:158535ms step_avg:99.15ms
step:1610/1770 train_time:158639ms step_avg:99.15ms
step:1611/1770 train_time:158744ms step_avg:99.15ms
step:1612/1770 train_time:158849ms step_avg:99.16ms
step:1613/1770 train_time:158952ms step_avg:99.16ms
step:1614/1770 train_time:159055ms step_avg:99.16ms
step:1615/1770 train_time:159159ms step_avg:99.16ms
step:1616/1770 train_time:159263ms step_avg:99.17ms
step:1617/1770 train_time:159369ms step_avg:99.17ms
step:1618/1770 train_time:159473ms step_avg:99.17ms
step:1619/1770 train_time:159577ms step_avg:99.18ms
step:1620/1770 train_time:159681ms step_avg:99.18ms
step:1621/1770 train_time:159785ms step_avg:99.18ms
step:1622/1770 train_time:159889ms step_avg:99.19ms
step:1623/1770 train_time:159995ms step_avg:99.19ms
step:1624/1770 train_time:160097ms step_avg:99.19ms
step:1625/1770 train_time:160200ms step_avg:99.19ms
step:1625/1770 val_loss:3.3070 train_time:160302ms step_avg:99.26ms
step:1626/1770 train_time:160326ms step_avg:99.21ms
step:1627/1770 train_time:160411ms step_avg:99.20ms
step:1628/1770 train_time:160514ms step_avg:99.21ms
step:1629/1770 train_time:160617ms step_avg:99.21ms
step:1630/1770 train_time:160720ms step_avg:99.21ms
step:1631/1770 train_time:160823ms step_avg:99.21ms
step:1632/1770 train_time:160925ms step_avg:99.21ms
step:1633/1770 train_time:161028ms step_avg:99.22ms
step:1634/1770 train_time:161131ms step_avg:99.22ms
step:1635/1770 train_time:161234ms step_avg:99.22ms
step:1636/1770 train_time:161339ms step_avg:99.22ms
step:1637/1770 train_time:161445ms step_avg:99.23ms
step:1638/1770 train_time:161548ms step_avg:99.23ms
step:1639/1770 train_time:161651ms step_avg:99.23ms
step:1640/1770 train_time:161754ms step_avg:99.24ms
step:1641/1770 train_time:161857ms step_avg:99.24ms
step:1642/1770 train_time:161961ms step_avg:99.24ms
step:1643/1770 train_time:162063ms step_avg:99.24ms
step:1644/1770 train_time:162168ms step_avg:99.25ms
step:1645/1770 train_time:162271ms step_avg:99.25ms
step:1646/1770 train_time:162377ms step_avg:99.25ms
step:1647/1770 train_time:162482ms step_avg:99.26ms
step:1648/1770 train_time:162586ms step_avg:99.26ms
step:1649/1770 train_time:162689ms step_avg:99.26ms
step:1650/1770 train_time:162792ms step_avg:99.26ms
step:1651/1770 train_time:162895ms step_avg:99.27ms
step:1652/1770 train_time:162999ms step_avg:99.27ms
step:1653/1770 train_time:163102ms step_avg:99.27ms
step:1654/1770 train_time:163209ms step_avg:99.28ms
step:1655/1770 train_time:163316ms step_avg:99.28ms
step:1656/1770 train_time:163419ms step_avg:99.28ms
step:1657/1770 train_time:163525ms step_avg:99.29ms
step:1658/1770 train_time:163628ms step_avg:99.29ms
step:1659/1770 train_time:163732ms step_avg:99.29ms
step:1660/1770 train_time:163836ms step_avg:99.29ms
step:1661/1770 train_time:163940ms step_avg:99.30ms
step:1662/1770 train_time:164044ms step_avg:99.30ms
step:1663/1770 train_time:164147ms step_avg:99.30ms
step:1664/1770 train_time:164250ms step_avg:99.30ms
step:1665/1770 train_time:164354ms step_avg:99.31ms
step:1666/1770 train_time:164457ms step_avg:99.31ms
step:1667/1770 train_time:164560ms step_avg:99.31ms
step:1668/1770 train_time:164663ms step_avg:99.31ms
step:1669/1770 train_time:164766ms step_avg:99.32ms
step:1670/1770 train_time:164869ms step_avg:99.32ms
step:1671/1770 train_time:164973ms step_avg:99.32ms
step:1672/1770 train_time:165077ms step_avg:99.32ms
step:1673/1770 train_time:165182ms step_avg:99.33ms
step:1674/1770 train_time:165284ms step_avg:99.33ms
step:1675/1770 train_time:165386ms step_avg:99.33ms
step:1676/1770 train_time:165492ms step_avg:99.33ms
step:1677/1770 train_time:165599ms step_avg:99.34ms
step:1678/1770 train_time:165701ms step_avg:99.34ms
step:1679/1770 train_time:165806ms step_avg:99.34ms
step:1680/1770 train_time:165909ms step_avg:99.35ms
step:1681/1770 train_time:166013ms step_avg:99.35ms
step:1682/1770 train_time:166118ms step_avg:99.35ms
step:1683/1770 train_time:166221ms step_avg:99.36ms
step:1684/1770 train_time:166325ms step_avg:99.36ms
step:1685/1770 train_time:166428ms step_avg:99.36ms
step:1686/1770 train_time:166532ms step_avg:99.36ms
step:1687/1770 train_time:166637ms step_avg:99.37ms
step:1688/1770 train_time:166741ms step_avg:99.37ms
step:1689/1770 train_time:166844ms step_avg:99.37ms
step:1690/1770 train_time:166947ms step_avg:99.37ms
step:1691/1770 train_time:167050ms step_avg:99.38ms
step:1692/1770 train_time:167154ms step_avg:99.38ms
step:1693/1770 train_time:167259ms step_avg:99.38ms
step:1694/1770 train_time:167362ms step_avg:99.38ms
step:1695/1770 train_time:167465ms step_avg:99.39ms
step:1696/1770 train_time:167570ms step_avg:99.39ms
step:1697/1770 train_time:167675ms step_avg:99.39ms
step:1698/1770 train_time:167780ms step_avg:99.40ms
step:1699/1770 train_time:167882ms step_avg:99.40ms
step:1700/1770 train_time:167987ms step_avg:99.40ms
step:1701/1770 train_time:168090ms step_avg:99.40ms
step:1702/1770 train_time:168194ms step_avg:99.41ms
step:1703/1770 train_time:168297ms step_avg:99.41ms
step:1704/1770 train_time:168402ms step_avg:99.41ms
step:1705/1770 train_time:168505ms step_avg:99.41ms
step:1706/1770 train_time:168608ms step_avg:99.42ms
step:1707/1770 train_time:168712ms step_avg:99.42ms
step:1708/1770 train_time:168817ms step_avg:99.42ms
step:1709/1770 train_time:168922ms step_avg:99.42ms
step:1710/1770 train_time:169029ms step_avg:99.43ms
step:1711/1770 train_time:169135ms step_avg:99.43ms
step:1712/1770 train_time:169239ms step_avg:99.44ms
step:1713/1770 train_time:169343ms step_avg:99.44ms
step:1714/1770 train_time:169446ms step_avg:99.44ms
step:1715/1770 train_time:169549ms step_avg:99.44ms
step:1716/1770 train_time:169653ms step_avg:99.45ms
step:1717/1770 train_time:169757ms step_avg:99.45ms
step:1718/1770 train_time:169863ms step_avg:99.45ms
step:1719/1770 train_time:169968ms step_avg:99.45ms
step:1720/1770 train_time:170072ms step_avg:99.46ms
step:1721/1770 train_time:170176ms step_avg:99.46ms
step:1722/1770 train_time:170283ms step_avg:99.46ms
step:1723/1770 train_time:170388ms step_avg:99.47ms
step:1724/1770 train_time:170494ms step_avg:99.47ms
step:1725/1770 train_time:170600ms step_avg:99.48ms
step:1726/1770 train_time:170706ms step_avg:99.48ms
step:1727/1770 train_time:170810ms step_avg:99.48ms
step:1728/1770 train_time:170915ms step_avg:99.48ms
step:1729/1770 train_time:171018ms step_avg:99.49ms
step:1730/1770 train_time:171124ms step_avg:99.49ms
step:1731/1770 train_time:171230ms step_avg:99.49ms
step:1732/1770 train_time:171333ms step_avg:99.50ms
step:1733/1770 train_time:171439ms step_avg:99.50ms
step:1734/1770 train_time:171543ms step_avg:99.50ms
step:1735/1770 train_time:171648ms step_avg:99.51ms
step:1736/1770 train_time:171751ms step_avg:99.51ms
step:1737/1770 train_time:171856ms step_avg:99.51ms
step:1738/1770 train_time:171960ms step_avg:99.51ms
step:1739/1770 train_time:172064ms step_avg:99.52ms
step:1740/1770 train_time:172168ms step_avg:99.52ms
step:1741/1770 train_time:172275ms step_avg:99.52ms
step:1742/1770 train_time:172382ms step_avg:99.53ms
step:1743/1770 train_time:172486ms step_avg:99.53ms
step:1744/1770 train_time:172590ms step_avg:99.53ms
step:1745/1770 train_time:172694ms step_avg:99.54ms
step:1746/1770 train_time:172801ms step_avg:99.54ms
step:1747/1770 train_time:172904ms step_avg:99.54ms
step:1748/1770 train_time:173010ms step_avg:99.55ms
step:1749/1770 train_time:173114ms step_avg:99.55ms
step:1750/1770 train_time:173219ms step_avg:99.55ms
step:1750/1770 val_loss:3.2802 train_time:173321ms step_avg:99.61ms
step:1751/1770 train_time:173342ms step_avg:99.56ms
step:1752/1770 train_time:173432ms step_avg:99.56ms
step:1753/1770 train_time:173535ms step_avg:99.56ms
step:1754/1770 train_time:173640ms step_avg:99.56ms
step:1755/1770 train_time:173744ms step_avg:99.57ms
step:1756/1770 train_time:173848ms step_avg:99.57ms
step:1757/1770 train_time:173953ms step_avg:99.57ms
step:1758/1770 train_time:174056ms step_avg:99.57ms
step:1759/1770 train_time:174161ms step_avg:99.58ms
step:1760/1770 train_time:174266ms step_avg:99.58ms
step:1761/1770 train_time:174374ms step_avg:99.59ms
step:1762/1770 train_time:174482ms step_avg:99.59ms
step:1763/1770 train_time:174585ms step_avg:99.59ms
step:1764/1770 train_time:174689ms step_avg:99.59ms
step:1765/1770 train_time:174793ms step_avg:99.60ms
step:1766/1770 train_time:174901ms step_avg:99.60ms
step:1767/1770 train_time:175004ms step_avg:99.60ms
step:1768/1770 train_time:175108ms step_avg:99.61ms
step:1769/1770 train_time:175212ms step_avg:99.61ms
step:1770/1770 train_time:175315ms step_avg:99.61ms
step:1770/1770 val_loss:3.2771 train_time:175420ms step_avg:99.67ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
