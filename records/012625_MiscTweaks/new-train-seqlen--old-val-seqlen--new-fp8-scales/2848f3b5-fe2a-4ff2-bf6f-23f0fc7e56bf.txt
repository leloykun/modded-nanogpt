import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 14:43:22 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   30C    P0             115W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24075ms step_avg:nanms
step:2/1770 train_time:24498ms step_avg:nanms
step:3/1770 train_time:24641ms step_avg:nanms
step:4/1770 train_time:24734ms step_avg:nanms
step:5/1770 train_time:24827ms step_avg:nanms
step:6/1770 train_time:24920ms step_avg:nanms
step:7/1770 train_time:25014ms step_avg:nanms
step:8/1770 train_time:25107ms step_avg:nanms
step:9/1770 train_time:25200ms step_avg:nanms
step:10/1770 train_time:25294ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:281ms step_avg:93.70ms
step:14/1770 train_time:375ms step_avg:93.73ms
step:15/1770 train_time:470ms step_avg:94.01ms
step:16/1770 train_time:562ms step_avg:93.72ms
step:17/1770 train_time:656ms step_avg:93.73ms
step:18/1770 train_time:750ms step_avg:93.71ms
step:19/1770 train_time:844ms step_avg:93.72ms
step:20/1770 train_time:937ms step_avg:93.72ms
step:21/1770 train_time:1031ms step_avg:93.69ms
step:22/1770 train_time:1124ms step_avg:93.68ms
step:23/1770 train_time:1218ms step_avg:93.67ms
step:24/1770 train_time:1311ms step_avg:93.64ms
step:25/1770 train_time:1405ms step_avg:93.68ms
step:26/1770 train_time:1500ms step_avg:93.74ms
step:27/1770 train_time:1593ms step_avg:93.73ms
step:28/1770 train_time:1687ms step_avg:93.72ms
step:29/1770 train_time:1781ms step_avg:93.72ms
step:30/1770 train_time:1874ms step_avg:93.70ms
step:31/1770 train_time:1968ms step_avg:93.70ms
step:32/1770 train_time:2061ms step_avg:93.67ms
step:33/1770 train_time:2154ms step_avg:93.66ms
step:34/1770 train_time:2247ms step_avg:93.64ms
step:35/1770 train_time:2341ms step_avg:93.64ms
step:36/1770 train_time:2435ms step_avg:93.64ms
step:37/1770 train_time:2528ms step_avg:93.65ms
step:38/1770 train_time:2623ms step_avg:93.67ms
step:39/1770 train_time:2717ms step_avg:93.68ms
step:40/1770 train_time:2811ms step_avg:93.69ms
step:41/1770 train_time:2905ms step_avg:93.71ms
step:42/1770 train_time:2999ms step_avg:93.72ms
step:43/1770 train_time:3093ms step_avg:93.73ms
step:44/1770 train_time:3187ms step_avg:93.73ms
step:45/1770 train_time:3281ms step_avg:93.74ms
step:46/1770 train_time:3375ms step_avg:93.76ms
step:47/1770 train_time:3469ms step_avg:93.75ms
step:48/1770 train_time:3563ms step_avg:93.76ms
step:49/1770 train_time:3657ms step_avg:93.77ms
step:50/1770 train_time:3751ms step_avg:93.76ms
step:51/1770 train_time:3844ms step_avg:93.76ms
step:52/1770 train_time:3939ms step_avg:93.78ms
step:53/1770 train_time:4033ms step_avg:93.78ms
step:54/1770 train_time:4127ms step_avg:93.79ms
step:55/1770 train_time:4220ms step_avg:93.78ms
step:56/1770 train_time:4313ms step_avg:93.77ms
step:57/1770 train_time:4407ms step_avg:93.77ms
step:58/1770 train_time:4501ms step_avg:93.76ms
step:59/1770 train_time:4594ms step_avg:93.75ms
step:60/1770 train_time:4687ms step_avg:93.75ms
step:61/1770 train_time:4781ms step_avg:93.75ms
step:62/1770 train_time:4874ms step_avg:93.74ms
step:63/1770 train_time:4968ms step_avg:93.74ms
step:64/1770 train_time:5063ms step_avg:93.75ms
step:65/1770 train_time:5157ms step_avg:93.76ms
step:66/1770 train_time:5250ms step_avg:93.75ms
step:67/1770 train_time:5344ms step_avg:93.75ms
step:68/1770 train_time:5438ms step_avg:93.75ms
step:69/1770 train_time:5531ms step_avg:93.75ms
step:70/1770 train_time:5626ms step_avg:93.76ms
step:71/1770 train_time:5720ms step_avg:93.77ms
step:72/1770 train_time:5814ms step_avg:93.77ms
step:73/1770 train_time:5907ms step_avg:93.76ms
step:74/1770 train_time:6001ms step_avg:93.76ms
step:75/1770 train_time:6094ms step_avg:93.76ms
step:76/1770 train_time:6188ms step_avg:93.76ms
step:77/1770 train_time:6281ms step_avg:93.75ms
step:78/1770 train_time:6375ms step_avg:93.75ms
step:79/1770 train_time:6468ms step_avg:93.74ms
step:80/1770 train_time:6562ms step_avg:93.75ms
step:81/1770 train_time:6656ms step_avg:93.75ms
step:82/1770 train_time:6750ms step_avg:93.75ms
step:83/1770 train_time:6844ms step_avg:93.75ms
step:84/1770 train_time:6937ms step_avg:93.75ms
step:85/1770 train_time:7031ms step_avg:93.75ms
step:86/1770 train_time:7125ms step_avg:93.75ms
step:87/1770 train_time:7218ms step_avg:93.75ms
step:88/1770 train_time:7312ms step_avg:93.74ms
step:89/1770 train_time:7405ms step_avg:93.74ms
step:90/1770 train_time:7499ms step_avg:93.74ms
step:91/1770 train_time:7594ms step_avg:93.75ms
step:92/1770 train_time:7687ms step_avg:93.74ms
step:93/1770 train_time:7781ms step_avg:93.75ms
step:94/1770 train_time:7874ms step_avg:93.74ms
step:95/1770 train_time:7968ms step_avg:93.74ms
step:96/1770 train_time:8062ms step_avg:93.74ms
step:97/1770 train_time:8155ms step_avg:93.74ms
step:98/1770 train_time:8248ms step_avg:93.73ms
step:99/1770 train_time:8342ms step_avg:93.73ms
step:100/1770 train_time:8436ms step_avg:93.73ms
step:101/1770 train_time:8530ms step_avg:93.73ms
step:102/1770 train_time:8624ms step_avg:93.74ms
step:103/1770 train_time:8718ms step_avg:93.75ms
step:104/1770 train_time:8812ms step_avg:93.75ms
step:105/1770 train_time:8906ms step_avg:93.75ms
step:106/1770 train_time:9000ms step_avg:93.75ms
step:107/1770 train_time:9094ms step_avg:93.75ms
step:108/1770 train_time:9187ms step_avg:93.74ms
step:109/1770 train_time:9281ms step_avg:93.75ms
step:110/1770 train_time:9375ms step_avg:93.75ms
step:111/1770 train_time:9468ms step_avg:93.75ms
step:112/1770 train_time:9567ms step_avg:93.79ms
step:113/1770 train_time:9655ms step_avg:93.74ms
step:114/1770 train_time:9749ms step_avg:93.74ms
step:115/1770 train_time:9843ms step_avg:93.75ms
step:116/1770 train_time:9937ms step_avg:93.74ms
step:117/1770 train_time:10031ms step_avg:93.74ms
step:118/1770 train_time:10124ms step_avg:93.74ms
step:119/1770 train_time:10218ms step_avg:93.74ms
step:120/1770 train_time:10312ms step_avg:93.74ms
step:121/1770 train_time:10406ms step_avg:93.75ms
step:122/1770 train_time:10499ms step_avg:93.74ms
step:123/1770 train_time:10592ms step_avg:93.74ms
step:124/1770 train_time:10686ms step_avg:93.74ms
step:125/1770 train_time:10780ms step_avg:93.74ms
step:125/1770 val_loss:4.6521 train_time:10872ms step_avg:94.54ms
step:126/1770 train_time:10893ms step_avg:93.91ms
step:127/1770 train_time:10978ms step_avg:93.83ms
step:128/1770 train_time:11077ms step_avg:93.87ms
step:129/1770 train_time:11172ms step_avg:93.88ms
step:130/1770 train_time:11266ms step_avg:93.88ms
step:131/1770 train_time:11359ms step_avg:93.88ms
step:132/1770 train_time:11453ms step_avg:93.87ms
step:133/1770 train_time:11546ms step_avg:93.87ms
step:134/1770 train_time:11641ms step_avg:93.88ms
step:135/1770 train_time:11734ms step_avg:93.87ms
step:136/1770 train_time:11828ms step_avg:93.87ms
step:137/1770 train_time:11922ms step_avg:93.87ms
step:138/1770 train_time:12017ms step_avg:93.88ms
step:139/1770 train_time:12112ms step_avg:93.89ms
step:140/1770 train_time:12206ms step_avg:93.90ms
step:141/1770 train_time:12300ms step_avg:93.90ms
step:142/1770 train_time:12395ms step_avg:93.90ms
step:143/1770 train_time:12489ms step_avg:93.90ms
step:144/1770 train_time:12583ms step_avg:93.90ms
step:145/1770 train_time:12677ms step_avg:93.90ms
step:146/1770 train_time:12771ms step_avg:93.91ms
step:147/1770 train_time:12865ms step_avg:93.90ms
step:148/1770 train_time:12959ms step_avg:93.91ms
step:149/1770 train_time:13054ms step_avg:93.92ms
step:150/1770 train_time:13149ms step_avg:93.92ms
step:151/1770 train_time:13243ms step_avg:93.92ms
step:152/1770 train_time:13338ms step_avg:93.93ms
step:153/1770 train_time:13433ms step_avg:93.94ms
step:154/1770 train_time:13527ms step_avg:93.94ms
step:155/1770 train_time:13621ms step_avg:93.94ms
step:156/1770 train_time:13715ms step_avg:93.94ms
step:157/1770 train_time:13810ms step_avg:93.94ms
step:158/1770 train_time:13904ms step_avg:93.94ms
step:159/1770 train_time:13998ms step_avg:93.95ms
step:160/1770 train_time:14093ms step_avg:93.95ms
step:161/1770 train_time:14187ms step_avg:93.95ms
step:162/1770 train_time:14281ms step_avg:93.95ms
step:163/1770 train_time:14376ms step_avg:93.96ms
step:164/1770 train_time:14470ms step_avg:93.96ms
step:165/1770 train_time:14564ms step_avg:93.96ms
step:166/1770 train_time:14658ms step_avg:93.96ms
step:167/1770 train_time:14753ms step_avg:93.97ms
step:168/1770 train_time:14847ms step_avg:93.97ms
step:169/1770 train_time:14941ms step_avg:93.97ms
step:170/1770 train_time:15036ms step_avg:93.97ms
step:171/1770 train_time:15131ms step_avg:93.98ms
step:172/1770 train_time:15225ms step_avg:93.98ms
step:173/1770 train_time:15319ms step_avg:93.98ms
step:174/1770 train_time:15414ms step_avg:93.99ms
step:175/1770 train_time:15508ms step_avg:93.99ms
step:176/1770 train_time:15604ms step_avg:94.00ms
step:177/1770 train_time:15698ms step_avg:94.00ms
step:178/1770 train_time:15793ms step_avg:94.00ms
step:179/1770 train_time:15887ms step_avg:94.00ms
step:180/1770 train_time:15980ms step_avg:94.00ms
step:181/1770 train_time:16075ms step_avg:94.01ms
step:182/1770 train_time:16170ms step_avg:94.01ms
step:183/1770 train_time:16264ms step_avg:94.01ms
step:184/1770 train_time:16358ms step_avg:94.01ms
step:185/1770 train_time:16453ms step_avg:94.02ms
step:186/1770 train_time:16547ms step_avg:94.02ms
step:187/1770 train_time:16641ms step_avg:94.02ms
step:188/1770 train_time:16736ms step_avg:94.02ms
step:189/1770 train_time:16830ms step_avg:94.02ms
step:190/1770 train_time:16924ms step_avg:94.02ms
step:191/1770 train_time:17019ms step_avg:94.03ms
step:192/1770 train_time:17114ms step_avg:94.03ms
step:193/1770 train_time:17208ms step_avg:94.03ms
step:194/1770 train_time:17302ms step_avg:94.03ms
step:195/1770 train_time:17396ms step_avg:94.03ms
step:196/1770 train_time:17491ms step_avg:94.04ms
step:197/1770 train_time:17585ms step_avg:94.04ms
step:198/1770 train_time:17680ms step_avg:94.04ms
step:199/1770 train_time:17774ms step_avg:94.04ms
step:200/1770 train_time:17868ms step_avg:94.04ms
step:201/1770 train_time:17962ms step_avg:94.04ms
step:202/1770 train_time:18057ms step_avg:94.05ms
step:203/1770 train_time:18151ms step_avg:94.05ms
step:204/1770 train_time:18245ms step_avg:94.05ms
step:205/1770 train_time:18339ms step_avg:94.05ms
step:206/1770 train_time:18434ms step_avg:94.05ms
step:207/1770 train_time:18529ms step_avg:94.05ms
step:208/1770 train_time:18623ms step_avg:94.06ms
step:209/1770 train_time:18718ms step_avg:94.06ms
step:210/1770 train_time:18812ms step_avg:94.06ms
step:211/1770 train_time:18907ms step_avg:94.06ms
step:212/1770 train_time:19001ms step_avg:94.06ms
step:213/1770 train_time:19096ms step_avg:94.07ms
step:214/1770 train_time:19190ms step_avg:94.07ms
step:215/1770 train_time:19285ms step_avg:94.07ms
step:216/1770 train_time:19379ms step_avg:94.07ms
step:217/1770 train_time:19474ms step_avg:94.08ms
step:218/1770 train_time:19568ms step_avg:94.08ms
step:219/1770 train_time:19662ms step_avg:94.08ms
step:220/1770 train_time:19757ms step_avg:94.08ms
step:221/1770 train_time:19851ms step_avg:94.08ms
step:222/1770 train_time:19945ms step_avg:94.08ms
step:223/1770 train_time:20040ms step_avg:94.08ms
step:224/1770 train_time:20135ms step_avg:94.09ms
step:225/1770 train_time:20230ms step_avg:94.09ms
step:226/1770 train_time:20324ms step_avg:94.09ms
step:227/1770 train_time:20418ms step_avg:94.09ms
step:228/1770 train_time:20514ms step_avg:94.10ms
step:229/1770 train_time:20608ms step_avg:94.10ms
step:230/1770 train_time:20702ms step_avg:94.10ms
step:231/1770 train_time:20797ms step_avg:94.10ms
step:232/1770 train_time:20891ms step_avg:94.10ms
step:233/1770 train_time:20985ms step_avg:94.10ms
step:234/1770 train_time:21079ms step_avg:94.10ms
step:235/1770 train_time:21174ms step_avg:94.11ms
step:236/1770 train_time:21268ms step_avg:94.11ms
step:237/1770 train_time:21363ms step_avg:94.11ms
step:238/1770 train_time:21458ms step_avg:94.11ms
step:239/1770 train_time:21553ms step_avg:94.12ms
step:240/1770 train_time:21648ms step_avg:94.12ms
step:241/1770 train_time:21742ms step_avg:94.12ms
step:242/1770 train_time:21837ms step_avg:94.12ms
step:243/1770 train_time:21932ms step_avg:94.13ms
step:244/1770 train_time:22026ms step_avg:94.13ms
step:245/1770 train_time:22120ms step_avg:94.13ms
step:246/1770 train_time:22215ms step_avg:94.13ms
step:247/1770 train_time:22309ms step_avg:94.13ms
step:248/1770 train_time:22404ms step_avg:94.13ms
step:249/1770 train_time:22499ms step_avg:94.14ms
step:250/1770 train_time:22593ms step_avg:94.14ms
step:250/1770 val_loss:4.1098 train_time:22686ms step_avg:94.52ms
step:251/1770 train_time:22709ms step_avg:94.23ms
step:252/1770 train_time:22792ms step_avg:94.18ms
step:253/1770 train_time:22892ms step_avg:94.21ms
step:254/1770 train_time:22987ms step_avg:94.21ms
step:255/1770 train_time:23082ms step_avg:94.21ms
step:256/1770 train_time:23175ms step_avg:94.21ms
step:257/1770 train_time:23270ms step_avg:94.21ms
step:258/1770 train_time:23364ms step_avg:94.21ms
step:259/1770 train_time:23458ms step_avg:94.21ms
step:260/1770 train_time:23552ms step_avg:94.21ms
step:261/1770 train_time:23646ms step_avg:94.21ms
step:262/1770 train_time:23741ms step_avg:94.21ms
step:263/1770 train_time:23836ms step_avg:94.21ms
step:264/1770 train_time:23931ms step_avg:94.22ms
step:265/1770 train_time:24027ms step_avg:94.22ms
step:266/1770 train_time:24123ms step_avg:94.23ms
step:267/1770 train_time:24217ms step_avg:94.23ms
step:268/1770 train_time:24312ms step_avg:94.23ms
step:269/1770 train_time:24407ms step_avg:94.23ms
step:270/1770 train_time:24501ms step_avg:94.23ms
step:271/1770 train_time:24595ms step_avg:94.24ms
step:272/1770 train_time:24690ms step_avg:94.24ms
step:273/1770 train_time:24785ms step_avg:94.24ms
step:274/1770 train_time:24881ms step_avg:94.25ms
step:275/1770 train_time:24976ms step_avg:94.25ms
step:276/1770 train_time:25071ms step_avg:94.25ms
step:277/1770 train_time:25166ms step_avg:94.26ms
step:278/1770 train_time:25261ms step_avg:94.26ms
step:279/1770 train_time:25356ms step_avg:94.26ms
step:280/1770 train_time:25451ms step_avg:94.26ms
step:281/1770 train_time:25546ms step_avg:94.27ms
step:282/1770 train_time:25641ms step_avg:94.27ms
step:283/1770 train_time:25735ms step_avg:94.27ms
step:284/1770 train_time:25830ms step_avg:94.27ms
step:285/1770 train_time:25927ms step_avg:94.28ms
step:286/1770 train_time:26022ms step_avg:94.28ms
step:287/1770 train_time:26117ms step_avg:94.28ms
step:288/1770 train_time:26212ms step_avg:94.29ms
step:289/1770 train_time:26307ms step_avg:94.29ms
step:290/1770 train_time:26402ms step_avg:94.29ms
step:291/1770 train_time:26496ms step_avg:94.29ms
step:292/1770 train_time:26591ms step_avg:94.29ms
step:293/1770 train_time:26687ms step_avg:94.30ms
step:294/1770 train_time:26782ms step_avg:94.30ms
step:295/1770 train_time:26876ms step_avg:94.30ms
step:296/1770 train_time:26971ms step_avg:94.30ms
step:297/1770 train_time:27067ms step_avg:94.31ms
step:298/1770 train_time:27162ms step_avg:94.31ms
step:299/1770 train_time:27257ms step_avg:94.32ms
step:300/1770 train_time:27352ms step_avg:94.32ms
step:301/1770 train_time:27448ms step_avg:94.32ms
step:302/1770 train_time:27543ms step_avg:94.33ms
step:303/1770 train_time:27638ms step_avg:94.33ms
step:304/1770 train_time:27732ms step_avg:94.33ms
step:305/1770 train_time:27828ms step_avg:94.33ms
step:306/1770 train_time:27923ms step_avg:94.33ms
step:307/1770 train_time:28018ms step_avg:94.34ms
step:308/1770 train_time:28113ms step_avg:94.34ms
step:309/1770 train_time:28208ms step_avg:94.34ms
step:310/1770 train_time:28303ms step_avg:94.34ms
step:311/1770 train_time:28398ms step_avg:94.35ms
step:312/1770 train_time:28492ms step_avg:94.35ms
step:313/1770 train_time:28588ms step_avg:94.35ms
step:314/1770 train_time:28683ms step_avg:94.35ms
step:315/1770 train_time:28778ms step_avg:94.35ms
step:316/1770 train_time:28873ms step_avg:94.36ms
step:317/1770 train_time:28969ms step_avg:94.36ms
step:318/1770 train_time:29064ms step_avg:94.36ms
step:319/1770 train_time:29159ms step_avg:94.37ms
step:320/1770 train_time:29254ms step_avg:94.37ms
step:321/1770 train_time:29349ms step_avg:94.37ms
step:322/1770 train_time:29444ms step_avg:94.37ms
step:323/1770 train_time:29539ms step_avg:94.37ms
step:324/1770 train_time:29634ms step_avg:94.37ms
step:325/1770 train_time:29729ms step_avg:94.38ms
step:326/1770 train_time:29824ms step_avg:94.38ms
step:327/1770 train_time:29919ms step_avg:94.38ms
step:328/1770 train_time:30014ms step_avg:94.38ms
step:329/1770 train_time:30109ms step_avg:94.39ms
step:330/1770 train_time:30205ms step_avg:94.39ms
step:331/1770 train_time:30300ms step_avg:94.39ms
step:332/1770 train_time:30395ms step_avg:94.39ms
step:333/1770 train_time:30490ms step_avg:94.40ms
step:334/1770 train_time:30586ms step_avg:94.40ms
step:335/1770 train_time:30681ms step_avg:94.40ms
step:336/1770 train_time:30776ms step_avg:94.41ms
step:337/1770 train_time:30871ms step_avg:94.41ms
step:338/1770 train_time:30966ms step_avg:94.41ms
step:339/1770 train_time:31061ms step_avg:94.41ms
step:340/1770 train_time:31156ms step_avg:94.41ms
step:341/1770 train_time:31251ms step_avg:94.41ms
step:342/1770 train_time:31346ms step_avg:94.42ms
step:343/1770 train_time:31444ms step_avg:94.43ms
step:344/1770 train_time:31536ms step_avg:94.42ms
step:345/1770 train_time:31631ms step_avg:94.42ms
step:346/1770 train_time:31727ms step_avg:94.42ms
step:347/1770 train_time:31822ms step_avg:94.43ms
step:348/1770 train_time:31917ms step_avg:94.43ms
step:349/1770 train_time:32012ms step_avg:94.43ms
step:350/1770 train_time:32107ms step_avg:94.43ms
step:351/1770 train_time:32203ms step_avg:94.44ms
step:352/1770 train_time:32297ms step_avg:94.44ms
step:353/1770 train_time:32392ms step_avg:94.44ms
step:354/1770 train_time:32488ms step_avg:94.44ms
step:355/1770 train_time:32583ms step_avg:94.44ms
step:356/1770 train_time:32679ms step_avg:94.45ms
step:357/1770 train_time:32773ms step_avg:94.45ms
step:358/1770 train_time:32869ms step_avg:94.45ms
step:359/1770 train_time:32964ms step_avg:94.45ms
step:360/1770 train_time:33058ms step_avg:94.45ms
step:361/1770 train_time:33153ms step_avg:94.45ms
step:362/1770 train_time:33248ms step_avg:94.45ms
step:363/1770 train_time:33343ms step_avg:94.46ms
step:364/1770 train_time:33439ms step_avg:94.46ms
step:365/1770 train_time:33533ms step_avg:94.46ms
step:366/1770 train_time:33629ms step_avg:94.46ms
step:367/1770 train_time:33725ms step_avg:94.47ms
step:368/1770 train_time:33820ms step_avg:94.47ms
step:369/1770 train_time:33914ms step_avg:94.47ms
step:370/1770 train_time:34009ms step_avg:94.47ms
step:371/1770 train_time:34104ms step_avg:94.47ms
step:372/1770 train_time:34199ms step_avg:94.47ms
step:373/1770 train_time:34295ms step_avg:94.48ms
step:374/1770 train_time:34390ms step_avg:94.48ms
step:375/1770 train_time:34485ms step_avg:94.48ms
step:375/1770 val_loss:3.8980 train_time:34578ms step_avg:94.74ms
step:376/1770 train_time:34600ms step_avg:94.53ms
step:377/1770 train_time:34687ms step_avg:94.51ms
step:378/1770 train_time:34787ms step_avg:94.53ms
step:379/1770 train_time:34882ms step_avg:94.53ms
step:380/1770 train_time:34977ms step_avg:94.53ms
step:381/1770 train_time:35072ms step_avg:94.53ms
step:382/1770 train_time:35167ms step_avg:94.53ms
step:383/1770 train_time:35261ms step_avg:94.53ms
step:384/1770 train_time:35356ms step_avg:94.53ms
step:385/1770 train_time:35450ms step_avg:94.53ms
step:386/1770 train_time:35545ms step_avg:94.53ms
step:387/1770 train_time:35640ms step_avg:94.54ms
step:388/1770 train_time:35736ms step_avg:94.54ms
step:389/1770 train_time:35831ms step_avg:94.54ms
step:390/1770 train_time:35926ms step_avg:94.54ms
step:391/1770 train_time:36021ms step_avg:94.54ms
step:392/1770 train_time:36116ms step_avg:94.54ms
step:393/1770 train_time:36211ms step_avg:94.55ms
step:394/1770 train_time:36305ms step_avg:94.55ms
step:395/1770 train_time:36400ms step_avg:94.55ms
step:396/1770 train_time:36497ms step_avg:94.55ms
step:397/1770 train_time:36594ms step_avg:94.56ms
step:398/1770 train_time:36691ms step_avg:94.56ms
step:399/1770 train_time:36787ms step_avg:94.57ms
step:400/1770 train_time:36884ms step_avg:94.57ms
step:401/1770 train_time:36981ms step_avg:94.58ms
step:402/1770 train_time:37078ms step_avg:94.59ms
step:403/1770 train_time:37174ms step_avg:94.59ms
step:404/1770 train_time:37272ms step_avg:94.60ms
step:405/1770 train_time:37369ms step_avg:94.60ms
step:406/1770 train_time:37465ms step_avg:94.61ms
step:407/1770 train_time:37566ms step_avg:94.62ms
step:408/1770 train_time:37658ms step_avg:94.62ms
step:409/1770 train_time:37756ms step_avg:94.63ms
step:410/1770 train_time:37853ms step_avg:94.63ms
step:411/1770 train_time:37950ms step_avg:94.64ms
step:412/1770 train_time:38048ms step_avg:94.65ms
step:413/1770 train_time:38144ms step_avg:94.65ms
step:414/1770 train_time:38240ms step_avg:94.65ms
step:415/1770 train_time:38337ms step_avg:94.66ms
step:416/1770 train_time:38436ms step_avg:94.67ms
step:417/1770 train_time:38532ms step_avg:94.67ms
step:418/1770 train_time:38630ms step_avg:94.68ms
step:419/1770 train_time:38727ms step_avg:94.69ms
step:420/1770 train_time:38824ms step_avg:94.69ms
step:421/1770 train_time:38921ms step_avg:94.70ms
step:422/1770 train_time:39018ms step_avg:94.70ms
step:423/1770 train_time:39115ms step_avg:94.71ms
step:424/1770 train_time:39213ms step_avg:94.72ms
step:425/1770 train_time:39309ms step_avg:94.72ms
step:426/1770 train_time:39406ms step_avg:94.73ms
step:427/1770 train_time:39502ms step_avg:94.73ms
step:428/1770 train_time:39599ms step_avg:94.73ms
step:429/1770 train_time:39695ms step_avg:94.74ms
step:430/1770 train_time:39793ms step_avg:94.75ms
step:431/1770 train_time:39890ms step_avg:94.75ms
step:432/1770 train_time:39987ms step_avg:94.76ms
step:433/1770 train_time:40084ms step_avg:94.76ms
step:434/1770 train_time:40181ms step_avg:94.77ms
step:435/1770 train_time:40278ms step_avg:94.77ms
step:436/1770 train_time:40376ms step_avg:94.78ms
step:437/1770 train_time:40474ms step_avg:94.79ms
step:438/1770 train_time:40571ms step_avg:94.79ms
step:439/1770 train_time:40668ms step_avg:94.80ms
step:440/1770 train_time:40764ms step_avg:94.80ms
step:441/1770 train_time:40861ms step_avg:94.81ms
step:442/1770 train_time:40958ms step_avg:94.81ms
step:443/1770 train_time:41055ms step_avg:94.81ms
step:444/1770 train_time:41152ms step_avg:94.82ms
step:445/1770 train_time:41249ms step_avg:94.83ms
step:446/1770 train_time:41346ms step_avg:94.83ms
step:447/1770 train_time:41443ms step_avg:94.84ms
step:448/1770 train_time:41540ms step_avg:94.84ms
step:449/1770 train_time:41637ms step_avg:94.84ms
step:450/1770 train_time:41734ms step_avg:94.85ms
step:451/1770 train_time:41832ms step_avg:94.86ms
step:452/1770 train_time:41929ms step_avg:94.86ms
step:453/1770 train_time:42025ms step_avg:94.87ms
step:454/1770 train_time:42122ms step_avg:94.87ms
step:455/1770 train_time:42219ms step_avg:94.87ms
step:456/1770 train_time:42316ms step_avg:94.88ms
step:457/1770 train_time:42413ms step_avg:94.88ms
step:458/1770 train_time:42510ms step_avg:94.89ms
step:459/1770 train_time:42607ms step_avg:94.89ms
step:460/1770 train_time:42703ms step_avg:94.90ms
step:461/1770 train_time:42800ms step_avg:94.90ms
step:462/1770 train_time:42897ms step_avg:94.90ms
step:463/1770 train_time:42994ms step_avg:94.91ms
step:464/1770 train_time:43092ms step_avg:94.92ms
step:465/1770 train_time:43189ms step_avg:94.92ms
step:466/1770 train_time:43286ms step_avg:94.93ms
step:467/1770 train_time:43382ms step_avg:94.93ms
step:468/1770 train_time:43479ms step_avg:94.93ms
step:469/1770 train_time:43576ms step_avg:94.94ms
step:470/1770 train_time:43673ms step_avg:94.94ms
step:471/1770 train_time:43771ms step_avg:94.95ms
step:472/1770 train_time:43867ms step_avg:94.95ms
step:473/1770 train_time:43963ms step_avg:94.95ms
step:474/1770 train_time:44060ms step_avg:94.96ms
step:475/1770 train_time:44157ms step_avg:94.96ms
step:476/1770 train_time:44254ms step_avg:94.97ms
step:477/1770 train_time:44352ms step_avg:94.97ms
step:478/1770 train_time:44449ms step_avg:94.98ms
step:479/1770 train_time:44546ms step_avg:94.98ms
step:480/1770 train_time:44642ms step_avg:94.98ms
step:481/1770 train_time:44739ms step_avg:94.99ms
step:482/1770 train_time:44836ms step_avg:94.99ms
step:483/1770 train_time:44933ms step_avg:95.00ms
step:484/1770 train_time:45030ms step_avg:95.00ms
step:485/1770 train_time:45127ms step_avg:95.00ms
step:486/1770 train_time:45223ms step_avg:95.01ms
step:487/1770 train_time:45320ms step_avg:95.01ms
step:488/1770 train_time:45417ms step_avg:95.01ms
step:489/1770 train_time:45514ms step_avg:95.02ms
step:490/1770 train_time:45612ms step_avg:95.02ms
step:491/1770 train_time:45709ms step_avg:95.03ms
step:492/1770 train_time:45806ms step_avg:95.03ms
step:493/1770 train_time:45903ms step_avg:95.04ms
step:494/1770 train_time:45999ms step_avg:95.04ms
step:495/1770 train_time:46097ms step_avg:95.05ms
step:496/1770 train_time:46194ms step_avg:95.05ms
step:497/1770 train_time:46291ms step_avg:95.05ms
step:498/1770 train_time:46389ms step_avg:95.06ms
step:499/1770 train_time:46485ms step_avg:95.06ms
step:500/1770 train_time:46581ms step_avg:95.06ms
step:500/1770 val_loss:3.7495 train_time:46677ms step_avg:95.26ms
step:501/1770 train_time:46700ms step_avg:95.11ms
step:502/1770 train_time:46787ms step_avg:95.10ms
step:503/1770 train_time:46886ms step_avg:95.10ms
step:504/1770 train_time:46984ms step_avg:95.11ms
step:505/1770 train_time:47081ms step_avg:95.11ms
step:506/1770 train_time:47177ms step_avg:95.11ms
step:507/1770 train_time:47273ms step_avg:95.12ms
step:508/1770 train_time:47369ms step_avg:95.12ms
step:509/1770 train_time:47467ms step_avg:95.12ms
step:510/1770 train_time:47562ms step_avg:95.12ms
step:511/1770 train_time:47659ms step_avg:95.13ms
step:512/1770 train_time:47756ms step_avg:95.13ms
step:513/1770 train_time:47853ms step_avg:95.14ms
step:514/1770 train_time:47950ms step_avg:95.14ms
step:515/1770 train_time:48048ms step_avg:95.14ms
step:516/1770 train_time:48145ms step_avg:95.15ms
step:517/1770 train_time:48242ms step_avg:95.15ms
step:518/1770 train_time:48338ms step_avg:95.15ms
step:519/1770 train_time:48434ms step_avg:95.16ms
step:520/1770 train_time:48531ms step_avg:95.16ms
step:521/1770 train_time:48628ms step_avg:95.16ms
step:522/1770 train_time:48725ms step_avg:95.17ms
step:523/1770 train_time:48822ms step_avg:95.17ms
step:524/1770 train_time:48920ms step_avg:95.17ms
step:525/1770 train_time:49016ms step_avg:95.18ms
step:526/1770 train_time:49113ms step_avg:95.18ms
step:527/1770 train_time:49210ms step_avg:95.18ms
step:528/1770 train_time:49308ms step_avg:95.19ms
step:529/1770 train_time:49406ms step_avg:95.19ms
step:530/1770 train_time:49503ms step_avg:95.20ms
step:531/1770 train_time:49600ms step_avg:95.20ms
step:532/1770 train_time:49696ms step_avg:95.20ms
step:533/1770 train_time:49793ms step_avg:95.21ms
step:534/1770 train_time:49891ms step_avg:95.21ms
step:535/1770 train_time:49989ms step_avg:95.22ms
step:536/1770 train_time:50087ms step_avg:95.22ms
step:537/1770 train_time:50185ms step_avg:95.23ms
step:538/1770 train_time:50282ms step_avg:95.23ms
step:539/1770 train_time:50379ms step_avg:95.23ms
step:540/1770 train_time:50476ms step_avg:95.24ms
step:541/1770 train_time:50572ms step_avg:95.24ms
step:542/1770 train_time:50669ms step_avg:95.24ms
step:543/1770 train_time:50767ms step_avg:95.25ms
step:544/1770 train_time:50865ms step_avg:95.25ms
step:545/1770 train_time:50963ms step_avg:95.26ms
step:546/1770 train_time:51060ms step_avg:95.26ms
step:547/1770 train_time:51157ms step_avg:95.26ms
step:548/1770 train_time:51254ms step_avg:95.27ms
step:549/1770 train_time:51352ms step_avg:95.27ms
step:550/1770 train_time:51450ms step_avg:95.28ms
step:551/1770 train_time:51547ms step_avg:95.28ms
step:552/1770 train_time:51644ms step_avg:95.29ms
step:553/1770 train_time:51742ms step_avg:95.29ms
step:554/1770 train_time:51838ms step_avg:95.29ms
step:555/1770 train_time:51935ms step_avg:95.29ms
step:556/1770 train_time:52032ms step_avg:95.30ms
step:557/1770 train_time:52129ms step_avg:95.30ms
step:558/1770 train_time:52227ms step_avg:95.31ms
step:559/1770 train_time:52325ms step_avg:95.31ms
step:560/1770 train_time:52423ms step_avg:95.31ms
step:561/1770 train_time:52520ms step_avg:95.32ms
step:562/1770 train_time:52617ms step_avg:95.32ms
step:563/1770 train_time:52713ms step_avg:95.32ms
step:564/1770 train_time:52811ms step_avg:95.33ms
step:565/1770 train_time:52908ms step_avg:95.33ms
step:566/1770 train_time:53006ms step_avg:95.33ms
step:567/1770 train_time:53103ms step_avg:95.34ms
step:568/1770 train_time:53201ms step_avg:95.34ms
step:569/1770 train_time:53298ms step_avg:95.35ms
step:570/1770 train_time:53395ms step_avg:95.35ms
step:571/1770 train_time:53493ms step_avg:95.35ms
step:572/1770 train_time:53591ms step_avg:95.36ms
step:573/1770 train_time:53688ms step_avg:95.36ms
step:574/1770 train_time:53786ms step_avg:95.37ms
step:575/1770 train_time:53884ms step_avg:95.37ms
step:576/1770 train_time:53980ms step_avg:95.37ms
step:577/1770 train_time:54077ms step_avg:95.37ms
step:578/1770 train_time:54174ms step_avg:95.38ms
step:579/1770 train_time:54271ms step_avg:95.38ms
step:580/1770 train_time:54369ms step_avg:95.38ms
step:581/1770 train_time:54467ms step_avg:95.39ms
step:582/1770 train_time:54565ms step_avg:95.39ms
step:583/1770 train_time:54662ms step_avg:95.40ms
step:584/1770 train_time:54758ms step_avg:95.40ms
step:585/1770 train_time:54855ms step_avg:95.40ms
step:586/1770 train_time:54952ms step_avg:95.40ms
step:587/1770 train_time:55050ms step_avg:95.41ms
step:588/1770 train_time:55147ms step_avg:95.41ms
step:589/1770 train_time:55245ms step_avg:95.41ms
step:590/1770 train_time:55342ms step_avg:95.42ms
step:591/1770 train_time:55439ms step_avg:95.42ms
step:592/1770 train_time:55536ms step_avg:95.42ms
step:593/1770 train_time:55633ms step_avg:95.43ms
step:594/1770 train_time:55731ms step_avg:95.43ms
step:595/1770 train_time:55829ms step_avg:95.43ms
step:596/1770 train_time:55926ms step_avg:95.44ms
step:597/1770 train_time:56024ms step_avg:95.44ms
step:598/1770 train_time:56121ms step_avg:95.44ms
step:599/1770 train_time:56218ms step_avg:95.45ms
step:600/1770 train_time:56315ms step_avg:95.45ms
step:601/1770 train_time:56412ms step_avg:95.45ms
step:602/1770 train_time:56510ms step_avg:95.46ms
step:603/1770 train_time:56608ms step_avg:95.46ms
step:604/1770 train_time:56705ms step_avg:95.46ms
step:605/1770 train_time:56803ms step_avg:95.47ms
step:606/1770 train_time:56900ms step_avg:95.47ms
step:607/1770 train_time:56996ms step_avg:95.47ms
step:608/1770 train_time:57094ms step_avg:95.47ms
step:609/1770 train_time:57191ms step_avg:95.48ms
step:610/1770 train_time:57290ms step_avg:95.48ms
step:611/1770 train_time:57388ms step_avg:95.49ms
step:612/1770 train_time:57485ms step_avg:95.49ms
step:613/1770 train_time:57583ms step_avg:95.49ms
step:614/1770 train_time:57680ms step_avg:95.50ms
step:615/1770 train_time:57777ms step_avg:95.50ms
step:616/1770 train_time:57874ms step_avg:95.50ms
step:617/1770 train_time:57971ms step_avg:95.50ms
step:618/1770 train_time:58069ms step_avg:95.51ms
step:619/1770 train_time:58167ms step_avg:95.51ms
step:620/1770 train_time:58265ms step_avg:95.52ms
step:621/1770 train_time:58362ms step_avg:95.52ms
step:622/1770 train_time:58459ms step_avg:95.52ms
step:623/1770 train_time:58557ms step_avg:95.52ms
step:624/1770 train_time:58654ms step_avg:95.53ms
step:625/1770 train_time:58751ms step_avg:95.53ms
step:625/1770 val_loss:3.6603 train_time:58848ms step_avg:95.69ms
step:626/1770 train_time:58869ms step_avg:95.57ms
step:627/1770 train_time:58961ms step_avg:95.56ms
step:628/1770 train_time:59061ms step_avg:95.57ms
step:629/1770 train_time:59158ms step_avg:95.57ms
step:630/1770 train_time:59255ms step_avg:95.57ms
step:631/1770 train_time:59352ms step_avg:95.57ms
step:632/1770 train_time:59448ms step_avg:95.58ms
step:633/1770 train_time:59545ms step_avg:95.58ms
step:634/1770 train_time:59642ms step_avg:95.58ms
step:635/1770 train_time:59740ms step_avg:95.58ms
step:636/1770 train_time:59837ms step_avg:95.59ms
step:637/1770 train_time:59935ms step_avg:95.59ms
step:638/1770 train_time:60034ms step_avg:95.60ms
step:639/1770 train_time:60132ms step_avg:95.60ms
step:640/1770 train_time:60229ms step_avg:95.60ms
step:641/1770 train_time:60327ms step_avg:95.61ms
step:642/1770 train_time:60424ms step_avg:95.61ms
step:643/1770 train_time:60521ms step_avg:95.61ms
step:644/1770 train_time:60618ms step_avg:95.61ms
step:645/1770 train_time:60715ms step_avg:95.61ms
step:646/1770 train_time:60812ms step_avg:95.62ms
step:647/1770 train_time:60909ms step_avg:95.62ms
step:648/1770 train_time:61006ms step_avg:95.62ms
step:649/1770 train_time:61103ms step_avg:95.62ms
step:650/1770 train_time:61201ms step_avg:95.63ms
step:651/1770 train_time:61299ms step_avg:95.63ms
step:652/1770 train_time:61396ms step_avg:95.63ms
step:653/1770 train_time:61494ms step_avg:95.64ms
step:654/1770 train_time:61590ms step_avg:95.64ms
step:655/1770 train_time:61687ms step_avg:95.64ms
step:656/1770 train_time:61784ms step_avg:95.64ms
step:657/1770 train_time:61881ms step_avg:95.64ms
step:658/1770 train_time:61980ms step_avg:95.65ms
step:659/1770 train_time:62080ms step_avg:95.65ms
step:660/1770 train_time:62179ms step_avg:95.66ms
step:661/1770 train_time:62279ms step_avg:95.67ms
step:662/1770 train_time:62378ms step_avg:95.67ms
step:663/1770 train_time:62478ms step_avg:95.68ms
step:664/1770 train_time:62577ms step_avg:95.68ms
step:665/1770 train_time:62676ms step_avg:95.69ms
step:666/1770 train_time:62775ms step_avg:95.69ms
step:667/1770 train_time:62874ms step_avg:95.70ms
step:668/1770 train_time:62974ms step_avg:95.70ms
step:669/1770 train_time:63073ms step_avg:95.71ms
step:670/1770 train_time:63172ms step_avg:95.71ms
step:671/1770 train_time:63270ms step_avg:95.72ms
step:672/1770 train_time:63369ms step_avg:95.72ms
step:673/1770 train_time:63468ms step_avg:95.73ms
step:674/1770 train_time:63567ms step_avg:95.73ms
step:675/1770 train_time:63665ms step_avg:95.74ms
step:676/1770 train_time:63764ms step_avg:95.74ms
step:677/1770 train_time:63864ms step_avg:95.75ms
step:678/1770 train_time:63962ms step_avg:95.75ms
step:679/1770 train_time:64062ms step_avg:95.76ms
step:680/1770 train_time:64161ms step_avg:95.76ms
step:681/1770 train_time:64260ms step_avg:95.77ms
step:682/1770 train_time:64360ms step_avg:95.77ms
step:683/1770 train_time:64460ms step_avg:95.78ms
step:684/1770 train_time:64560ms step_avg:95.79ms
step:685/1770 train_time:64660ms step_avg:95.79ms
step:686/1770 train_time:64759ms step_avg:95.80ms
step:687/1770 train_time:64859ms step_avg:95.80ms
step:688/1770 train_time:64959ms step_avg:95.81ms
step:689/1770 train_time:65059ms step_avg:95.82ms
step:690/1770 train_time:65159ms step_avg:95.82ms
step:691/1770 train_time:65258ms step_avg:95.83ms
step:692/1770 train_time:65358ms step_avg:95.83ms
step:693/1770 train_time:65457ms step_avg:95.84ms
step:694/1770 train_time:65558ms step_avg:95.84ms
step:695/1770 train_time:65658ms step_avg:95.85ms
step:696/1770 train_time:65760ms step_avg:95.86ms
step:697/1770 train_time:65856ms step_avg:95.86ms
step:698/1770 train_time:65955ms step_avg:95.87ms
step:699/1770 train_time:66055ms step_avg:95.87ms
step:700/1770 train_time:66154ms step_avg:95.88ms
step:701/1770 train_time:66253ms step_avg:95.88ms
step:702/1770 train_time:66352ms step_avg:95.88ms
step:703/1770 train_time:66451ms step_avg:95.89ms
step:704/1770 train_time:66550ms step_avg:95.89ms
step:705/1770 train_time:66649ms step_avg:95.90ms
step:706/1770 train_time:66747ms step_avg:95.90ms
step:707/1770 train_time:66846ms step_avg:95.91ms
step:708/1770 train_time:66944ms step_avg:95.91ms
step:709/1770 train_time:67043ms step_avg:95.91ms
step:710/1770 train_time:67142ms step_avg:95.92ms
step:711/1770 train_time:67241ms step_avg:95.92ms
step:712/1770 train_time:67341ms step_avg:95.93ms
step:713/1770 train_time:67440ms step_avg:95.93ms
step:714/1770 train_time:67539ms step_avg:95.94ms
step:715/1770 train_time:67639ms step_avg:95.94ms
step:716/1770 train_time:67739ms step_avg:95.95ms
step:717/1770 train_time:67839ms step_avg:95.95ms
step:718/1770 train_time:67938ms step_avg:95.96ms
step:719/1770 train_time:68038ms step_avg:95.96ms
step:720/1770 train_time:68138ms step_avg:95.97ms
step:721/1770 train_time:68237ms step_avg:95.97ms
step:722/1770 train_time:68336ms step_avg:95.98ms
step:723/1770 train_time:68435ms step_avg:95.98ms
step:724/1770 train_time:68534ms step_avg:95.99ms
step:725/1770 train_time:68634ms step_avg:95.99ms
step:726/1770 train_time:68734ms step_avg:96.00ms
step:727/1770 train_time:68832ms step_avg:96.00ms
step:728/1770 train_time:68932ms step_avg:96.01ms
step:729/1770 train_time:69031ms step_avg:96.01ms
step:730/1770 train_time:69130ms step_avg:96.01ms
step:731/1770 train_time:69228ms step_avg:96.02ms
step:732/1770 train_time:69327ms step_avg:96.02ms
step:733/1770 train_time:69426ms step_avg:96.02ms
step:734/1770 train_time:69525ms step_avg:96.03ms
step:735/1770 train_time:69624ms step_avg:96.03ms
step:736/1770 train_time:69723ms step_avg:96.04ms
step:737/1770 train_time:69822ms step_avg:96.04ms
step:738/1770 train_time:69922ms step_avg:96.05ms
step:739/1770 train_time:70021ms step_avg:96.05ms
step:740/1770 train_time:70121ms step_avg:96.06ms
step:741/1770 train_time:70221ms step_avg:96.06ms
step:742/1770 train_time:70320ms step_avg:96.07ms
step:743/1770 train_time:70420ms step_avg:96.07ms
step:744/1770 train_time:70519ms step_avg:96.08ms
step:745/1770 train_time:70619ms step_avg:96.08ms
step:746/1770 train_time:70717ms step_avg:96.08ms
step:747/1770 train_time:70817ms step_avg:96.09ms
step:748/1770 train_time:70917ms step_avg:96.09ms
step:749/1770 train_time:71016ms step_avg:96.10ms
step:750/1770 train_time:71115ms step_avg:96.10ms
step:750/1770 val_loss:3.5968 train_time:71213ms step_avg:96.23ms
step:751/1770 train_time:71234ms step_avg:96.13ms
step:752/1770 train_time:71322ms step_avg:96.12ms
step:753/1770 train_time:71426ms step_avg:96.13ms
step:754/1770 train_time:71520ms step_avg:96.13ms
step:755/1770 train_time:71619ms step_avg:96.13ms
step:756/1770 train_time:71718ms step_avg:96.14ms
step:757/1770 train_time:71816ms step_avg:96.14ms
step:758/1770 train_time:71914ms step_avg:96.14ms
step:759/1770 train_time:72012ms step_avg:96.14ms
step:760/1770 train_time:72111ms step_avg:96.15ms
step:761/1770 train_time:72209ms step_avg:96.15ms
step:762/1770 train_time:72309ms step_avg:96.16ms
step:763/1770 train_time:72410ms step_avg:96.16ms
step:764/1770 train_time:72510ms step_avg:96.17ms
step:765/1770 train_time:72610ms step_avg:96.17ms
step:766/1770 train_time:72709ms step_avg:96.18ms
step:767/1770 train_time:72810ms step_avg:96.18ms
step:768/1770 train_time:72910ms step_avg:96.19ms
step:769/1770 train_time:73009ms step_avg:96.19ms
step:770/1770 train_time:73109ms step_avg:96.20ms
step:771/1770 train_time:73208ms step_avg:96.20ms
step:772/1770 train_time:73307ms step_avg:96.20ms
step:773/1770 train_time:73406ms step_avg:96.21ms
step:774/1770 train_time:73505ms step_avg:96.21ms
step:775/1770 train_time:73605ms step_avg:96.22ms
step:776/1770 train_time:73705ms step_avg:96.22ms
step:777/1770 train_time:73804ms step_avg:96.22ms
step:778/1770 train_time:73903ms step_avg:96.23ms
step:779/1770 train_time:74002ms step_avg:96.23ms
step:780/1770 train_time:74100ms step_avg:96.23ms
step:781/1770 train_time:74198ms step_avg:96.24ms
step:782/1770 train_time:74296ms step_avg:96.24ms
step:783/1770 train_time:74396ms step_avg:96.24ms
step:784/1770 train_time:74495ms step_avg:96.25ms
step:785/1770 train_time:74594ms step_avg:96.25ms
step:786/1770 train_time:74693ms step_avg:96.25ms
step:787/1770 train_time:74793ms step_avg:96.26ms
step:788/1770 train_time:74892ms step_avg:96.26ms
step:789/1770 train_time:74992ms step_avg:96.27ms
step:790/1770 train_time:75092ms step_avg:96.27ms
step:791/1770 train_time:75192ms step_avg:96.28ms
step:792/1770 train_time:75293ms step_avg:96.28ms
step:793/1770 train_time:75392ms step_avg:96.29ms
step:794/1770 train_time:75492ms step_avg:96.29ms
step:795/1770 train_time:75592ms step_avg:96.30ms
step:796/1770 train_time:75693ms step_avg:96.30ms
step:797/1770 train_time:75793ms step_avg:96.31ms
step:798/1770 train_time:75893ms step_avg:96.31ms
step:799/1770 train_time:75992ms step_avg:96.31ms
step:800/1770 train_time:76092ms step_avg:96.32ms
step:801/1770 train_time:76192ms step_avg:96.32ms
step:802/1770 train_time:76292ms step_avg:96.33ms
step:803/1770 train_time:76392ms step_avg:96.33ms
step:804/1770 train_time:76492ms step_avg:96.34ms
step:805/1770 train_time:76592ms step_avg:96.34ms
step:806/1770 train_time:76692ms step_avg:96.35ms
step:807/1770 train_time:76792ms step_avg:96.35ms
step:808/1770 train_time:76891ms step_avg:96.35ms
step:809/1770 train_time:76991ms step_avg:96.36ms
step:810/1770 train_time:77091ms step_avg:96.36ms
step:811/1770 train_time:77191ms step_avg:96.37ms
step:812/1770 train_time:77292ms step_avg:96.37ms
step:813/1770 train_time:77392ms step_avg:96.38ms
step:814/1770 train_time:77492ms step_avg:96.38ms
step:815/1770 train_time:77592ms step_avg:96.39ms
step:816/1770 train_time:77691ms step_avg:96.39ms
step:817/1770 train_time:77791ms step_avg:96.40ms
step:818/1770 train_time:77891ms step_avg:96.40ms
step:819/1770 train_time:77991ms step_avg:96.40ms
step:820/1770 train_time:78091ms step_avg:96.41ms
step:821/1770 train_time:78191ms step_avg:96.41ms
step:822/1770 train_time:78291ms step_avg:96.42ms
step:823/1770 train_time:78391ms step_avg:96.42ms
step:824/1770 train_time:78491ms step_avg:96.43ms
step:825/1770 train_time:78591ms step_avg:96.43ms
step:826/1770 train_time:78691ms step_avg:96.43ms
step:827/1770 train_time:78791ms step_avg:96.44ms
step:828/1770 train_time:78891ms step_avg:96.44ms
step:829/1770 train_time:78991ms step_avg:96.45ms
step:830/1770 train_time:79091ms step_avg:96.45ms
step:831/1770 train_time:79190ms step_avg:96.46ms
step:832/1770 train_time:79290ms step_avg:96.46ms
step:833/1770 train_time:79390ms step_avg:96.46ms
step:834/1770 train_time:79490ms step_avg:96.47ms
step:835/1770 train_time:79589ms step_avg:96.47ms
step:836/1770 train_time:79690ms step_avg:96.48ms
step:837/1770 train_time:79789ms step_avg:96.48ms
step:838/1770 train_time:79889ms step_avg:96.48ms
step:839/1770 train_time:79989ms step_avg:96.49ms
step:840/1770 train_time:80090ms step_avg:96.49ms
step:841/1770 train_time:80190ms step_avg:96.50ms
step:842/1770 train_time:80289ms step_avg:96.50ms
step:843/1770 train_time:80389ms step_avg:96.51ms
step:844/1770 train_time:80490ms step_avg:96.51ms
step:845/1770 train_time:80590ms step_avg:96.51ms
step:846/1770 train_time:80690ms step_avg:96.52ms
step:847/1770 train_time:80790ms step_avg:96.52ms
step:848/1770 train_time:80890ms step_avg:96.53ms
step:849/1770 train_time:80989ms step_avg:96.53ms
step:850/1770 train_time:81089ms step_avg:96.53ms
step:851/1770 train_time:81189ms step_avg:96.54ms
step:852/1770 train_time:81288ms step_avg:96.54ms
step:853/1770 train_time:81387ms step_avg:96.54ms
step:854/1770 train_time:81487ms step_avg:96.55ms
step:855/1770 train_time:81587ms step_avg:96.55ms
step:856/1770 train_time:81686ms step_avg:96.56ms
step:857/1770 train_time:81785ms step_avg:96.56ms
step:858/1770 train_time:81885ms step_avg:96.56ms
step:859/1770 train_time:81984ms step_avg:96.57ms
step:860/1770 train_time:82085ms step_avg:96.57ms
step:861/1770 train_time:82184ms step_avg:96.57ms
step:862/1770 train_time:82284ms step_avg:96.58ms
step:863/1770 train_time:82383ms step_avg:96.58ms
step:864/1770 train_time:82482ms step_avg:96.58ms
step:865/1770 train_time:82580ms step_avg:96.58ms
step:866/1770 train_time:82680ms step_avg:96.59ms
step:867/1770 train_time:82779ms step_avg:96.59ms
step:868/1770 train_time:82877ms step_avg:96.59ms
step:869/1770 train_time:82976ms step_avg:96.60ms
step:870/1770 train_time:83075ms step_avg:96.60ms
step:871/1770 train_time:83174ms step_avg:96.60ms
step:872/1770 train_time:83273ms step_avg:96.60ms
step:873/1770 train_time:83373ms step_avg:96.61ms
step:874/1770 train_time:83472ms step_avg:96.61ms
step:875/1770 train_time:83573ms step_avg:96.62ms
step:875/1770 val_loss:3.5482 train_time:83671ms step_avg:96.73ms
step:876/1770 train_time:83692ms step_avg:96.64ms
step:877/1770 train_time:83785ms step_avg:96.64ms
step:878/1770 train_time:83886ms step_avg:96.64ms
step:879/1770 train_time:83985ms step_avg:96.65ms
step:880/1770 train_time:84084ms step_avg:96.65ms
step:881/1770 train_time:84182ms step_avg:96.65ms
step:882/1770 train_time:84281ms step_avg:96.65ms
step:883/1770 train_time:84380ms step_avg:96.66ms
step:884/1770 train_time:84480ms step_avg:96.66ms
step:885/1770 train_time:84579ms step_avg:96.66ms
step:886/1770 train_time:84679ms step_avg:96.67ms
step:887/1770 train_time:84781ms step_avg:96.67ms
step:888/1770 train_time:84882ms step_avg:96.68ms
step:889/1770 train_time:84983ms step_avg:96.68ms
step:890/1770 train_time:85083ms step_avg:96.68ms
step:891/1770 train_time:85182ms step_avg:96.69ms
step:892/1770 train_time:85282ms step_avg:96.69ms
step:893/1770 train_time:85382ms step_avg:96.69ms
step:894/1770 train_time:85481ms step_avg:96.70ms
step:895/1770 train_time:85580ms step_avg:96.70ms
step:896/1770 train_time:85678ms step_avg:96.70ms
step:897/1770 train_time:85780ms step_avg:96.71ms
step:898/1770 train_time:85880ms step_avg:96.71ms
step:899/1770 train_time:85979ms step_avg:96.71ms
step:900/1770 train_time:86080ms step_avg:96.72ms
step:901/1770 train_time:86179ms step_avg:96.72ms
step:902/1770 train_time:86279ms step_avg:96.73ms
step:903/1770 train_time:86379ms step_avg:96.73ms
step:904/1770 train_time:86479ms step_avg:96.73ms
step:905/1770 train_time:86579ms step_avg:96.74ms
step:906/1770 train_time:86679ms step_avg:96.74ms
step:907/1770 train_time:86779ms step_avg:96.74ms
step:908/1770 train_time:86880ms step_avg:96.75ms
step:909/1770 train_time:86980ms step_avg:96.75ms
step:910/1770 train_time:87080ms step_avg:96.76ms
step:911/1770 train_time:87180ms step_avg:96.76ms
step:912/1770 train_time:87280ms step_avg:96.76ms
step:913/1770 train_time:87379ms step_avg:96.77ms
step:914/1770 train_time:87478ms step_avg:96.77ms
step:915/1770 train_time:87579ms step_avg:96.77ms
step:916/1770 train_time:87679ms step_avg:96.78ms
step:917/1770 train_time:87779ms step_avg:96.78ms
step:918/1770 train_time:87878ms step_avg:96.78ms
step:919/1770 train_time:87978ms step_avg:96.79ms
step:920/1770 train_time:88082ms step_avg:96.79ms
step:921/1770 train_time:88184ms step_avg:96.80ms
step:922/1770 train_time:88285ms step_avg:96.80ms
step:923/1770 train_time:88385ms step_avg:96.81ms
step:924/1770 train_time:88485ms step_avg:96.81ms
step:925/1770 train_time:88585ms step_avg:96.81ms
step:926/1770 train_time:88686ms step_avg:96.82ms
step:927/1770 train_time:88786ms step_avg:96.82ms
step:928/1770 train_time:88887ms step_avg:96.83ms
step:929/1770 train_time:88987ms step_avg:96.83ms
step:930/1770 train_time:89088ms step_avg:96.83ms
step:931/1770 train_time:89188ms step_avg:96.84ms
step:932/1770 train_time:89288ms step_avg:96.84ms
step:933/1770 train_time:89389ms step_avg:96.85ms
step:934/1770 train_time:89489ms step_avg:96.85ms
step:935/1770 train_time:89590ms step_avg:96.85ms
step:936/1770 train_time:89689ms step_avg:96.86ms
step:937/1770 train_time:89789ms step_avg:96.86ms
step:938/1770 train_time:89889ms step_avg:96.86ms
step:939/1770 train_time:89989ms step_avg:96.87ms
step:940/1770 train_time:90093ms step_avg:96.87ms
step:941/1770 train_time:90189ms step_avg:96.87ms
step:942/1770 train_time:90289ms step_avg:96.88ms
step:943/1770 train_time:90390ms step_avg:96.88ms
step:944/1770 train_time:90489ms step_avg:96.88ms
step:945/1770 train_time:90589ms step_avg:96.89ms
step:946/1770 train_time:90690ms step_avg:96.89ms
step:947/1770 train_time:90790ms step_avg:96.89ms
step:948/1770 train_time:90890ms step_avg:96.90ms
step:949/1770 train_time:90990ms step_avg:96.90ms
step:950/1770 train_time:91091ms step_avg:96.91ms
step:951/1770 train_time:91192ms step_avg:96.91ms
step:952/1770 train_time:91292ms step_avg:96.91ms
step:953/1770 train_time:91392ms step_avg:96.92ms
step:954/1770 train_time:91492ms step_avg:96.92ms
step:955/1770 train_time:91596ms step_avg:96.93ms
step:956/1770 train_time:91692ms step_avg:96.93ms
step:957/1770 train_time:91792ms step_avg:96.93ms
step:958/1770 train_time:91893ms step_avg:96.93ms
step:959/1770 train_time:91993ms step_avg:96.94ms
step:960/1770 train_time:92093ms step_avg:96.94ms
step:961/1770 train_time:92194ms step_avg:96.94ms
step:962/1770 train_time:92296ms step_avg:96.95ms
step:963/1770 train_time:92396ms step_avg:96.95ms
step:964/1770 train_time:92496ms step_avg:96.96ms
step:965/1770 train_time:92597ms step_avg:96.96ms
step:966/1770 train_time:92697ms step_avg:96.96ms
step:967/1770 train_time:92798ms step_avg:96.97ms
step:968/1770 train_time:92899ms step_avg:96.97ms
step:969/1770 train_time:93000ms step_avg:96.98ms
step:970/1770 train_time:93104ms step_avg:96.98ms
step:971/1770 train_time:93204ms step_avg:96.99ms
step:972/1770 train_time:93305ms step_avg:96.99ms
step:973/1770 train_time:93406ms step_avg:96.99ms
step:974/1770 train_time:93506ms step_avg:97.00ms
step:975/1770 train_time:93607ms step_avg:97.00ms
step:976/1770 train_time:93707ms step_avg:97.01ms
step:977/1770 train_time:93808ms step_avg:97.01ms
step:978/1770 train_time:93908ms step_avg:97.01ms
step:979/1770 train_time:94010ms step_avg:97.02ms
step:980/1770 train_time:94110ms step_avg:97.02ms
step:981/1770 train_time:94210ms step_avg:97.02ms
step:982/1770 train_time:94310ms step_avg:97.03ms
step:983/1770 train_time:94410ms step_avg:97.03ms
step:984/1770 train_time:94511ms step_avg:97.03ms
step:985/1770 train_time:94611ms step_avg:97.04ms
step:986/1770 train_time:94711ms step_avg:97.04ms
step:987/1770 train_time:94811ms step_avg:97.04ms
step:988/1770 train_time:94913ms step_avg:97.05ms
step:989/1770 train_time:95013ms step_avg:97.05ms
step:990/1770 train_time:95113ms step_avg:97.05ms
step:991/1770 train_time:95214ms step_avg:97.06ms
step:992/1770 train_time:95315ms step_avg:97.06ms
step:993/1770 train_time:95416ms step_avg:97.07ms
step:994/1770 train_time:95517ms step_avg:97.07ms
step:995/1770 train_time:95617ms step_avg:97.07ms
step:996/1770 train_time:95718ms step_avg:97.08ms
step:997/1770 train_time:95819ms step_avg:97.08ms
step:998/1770 train_time:95921ms step_avg:97.09ms
step:999/1770 train_time:96023ms step_avg:97.09ms
step:1000/1770 train_time:96125ms step_avg:97.10ms
step:1000/1770 val_loss:3.5088 train_time:96223ms step_avg:97.20ms
step:1001/1770 train_time:96244ms step_avg:97.12ms
step:1002/1770 train_time:96335ms step_avg:97.11ms
step:1003/1770 train_time:96437ms step_avg:97.12ms
step:1004/1770 train_time:96538ms step_avg:97.12ms
step:1005/1770 train_time:96638ms step_avg:97.12ms
step:1006/1770 train_time:96738ms step_avg:97.13ms
step:1007/1770 train_time:96838ms step_avg:97.13ms
step:1008/1770 train_time:96938ms step_avg:97.13ms
step:1009/1770 train_time:97038ms step_avg:97.14ms
step:1010/1770 train_time:97138ms step_avg:97.14ms
step:1011/1770 train_time:97240ms step_avg:97.14ms
step:1012/1770 train_time:97343ms step_avg:97.15ms
step:1013/1770 train_time:97445ms step_avg:97.15ms
step:1014/1770 train_time:97545ms step_avg:97.16ms
step:1015/1770 train_time:97645ms step_avg:97.16ms
step:1016/1770 train_time:97745ms step_avg:97.16ms
step:1017/1770 train_time:97845ms step_avg:97.17ms
step:1018/1770 train_time:97945ms step_avg:97.17ms
step:1019/1770 train_time:98045ms step_avg:97.17ms
step:1020/1770 train_time:98145ms step_avg:97.17ms
step:1021/1770 train_time:98246ms step_avg:97.18ms
step:1022/1770 train_time:98346ms step_avg:97.18ms
step:1023/1770 train_time:98447ms step_avg:97.18ms
step:1024/1770 train_time:98548ms step_avg:97.19ms
step:1025/1770 train_time:98649ms step_avg:97.19ms
step:1026/1770 train_time:98750ms step_avg:97.19ms
step:1027/1770 train_time:98851ms step_avg:97.20ms
step:1028/1770 train_time:98952ms step_avg:97.20ms
step:1029/1770 train_time:99053ms step_avg:97.21ms
step:1030/1770 train_time:99155ms step_avg:97.21ms
step:1031/1770 train_time:99256ms step_avg:97.21ms
step:1032/1770 train_time:99357ms step_avg:97.22ms
step:1033/1770 train_time:99459ms step_avg:97.22ms
step:1034/1770 train_time:99559ms step_avg:97.23ms
step:1035/1770 train_time:99660ms step_avg:97.23ms
step:1036/1770 train_time:99760ms step_avg:97.23ms
step:1037/1770 train_time:99860ms step_avg:97.23ms
step:1038/1770 train_time:99964ms step_avg:97.24ms
step:1039/1770 train_time:100062ms step_avg:97.24ms
step:1040/1770 train_time:100164ms step_avg:97.25ms
step:1041/1770 train_time:100264ms step_avg:97.25ms
step:1042/1770 train_time:100365ms step_avg:97.25ms
step:1043/1770 train_time:100465ms step_avg:97.26ms
step:1044/1770 train_time:100566ms step_avg:97.26ms
step:1045/1770 train_time:100666ms step_avg:97.26ms
step:1046/1770 train_time:100767ms step_avg:97.27ms
step:1047/1770 train_time:100867ms step_avg:97.27ms
step:1048/1770 train_time:100970ms step_avg:97.27ms
step:1049/1770 train_time:101070ms step_avg:97.28ms
step:1050/1770 train_time:101171ms step_avg:97.28ms
step:1051/1770 train_time:101272ms step_avg:97.28ms
step:1052/1770 train_time:101374ms step_avg:97.29ms
step:1053/1770 train_time:101475ms step_avg:97.29ms
step:1054/1770 train_time:101577ms step_avg:97.30ms
step:1055/1770 train_time:101678ms step_avg:97.30ms
step:1056/1770 train_time:101778ms step_avg:97.30ms
step:1057/1770 train_time:101878ms step_avg:97.30ms
step:1058/1770 train_time:101979ms step_avg:97.31ms
step:1059/1770 train_time:102080ms step_avg:97.31ms
step:1060/1770 train_time:102180ms step_avg:97.31ms
step:1061/1770 train_time:102282ms step_avg:97.32ms
step:1062/1770 train_time:102383ms step_avg:97.32ms
step:1063/1770 train_time:102485ms step_avg:97.33ms
step:1064/1770 train_time:102586ms step_avg:97.33ms
step:1065/1770 train_time:102686ms step_avg:97.33ms
step:1066/1770 train_time:102787ms step_avg:97.34ms
step:1067/1770 train_time:102887ms step_avg:97.34ms
step:1068/1770 train_time:102988ms step_avg:97.34ms
step:1069/1770 train_time:103089ms step_avg:97.35ms
step:1070/1770 train_time:103190ms step_avg:97.35ms
step:1071/1770 train_time:103293ms step_avg:97.35ms
step:1072/1770 train_time:103394ms step_avg:97.36ms
step:1073/1770 train_time:103496ms step_avg:97.36ms
step:1074/1770 train_time:103597ms step_avg:97.37ms
step:1075/1770 train_time:103699ms step_avg:97.37ms
step:1076/1770 train_time:103802ms step_avg:97.38ms
step:1077/1770 train_time:103902ms step_avg:97.38ms
step:1078/1770 train_time:104002ms step_avg:97.38ms
step:1079/1770 train_time:104102ms step_avg:97.38ms
step:1080/1770 train_time:104203ms step_avg:97.39ms
step:1081/1770 train_time:104303ms step_avg:97.39ms
step:1082/1770 train_time:104403ms step_avg:97.39ms
step:1083/1770 train_time:104504ms step_avg:97.39ms
step:1084/1770 train_time:104605ms step_avg:97.40ms
step:1085/1770 train_time:104706ms step_avg:97.40ms
step:1086/1770 train_time:104806ms step_avg:97.40ms
step:1087/1770 train_time:104907ms step_avg:97.41ms
step:1088/1770 train_time:105007ms step_avg:97.41ms
step:1089/1770 train_time:105108ms step_avg:97.41ms
step:1090/1770 train_time:105211ms step_avg:97.42ms
step:1091/1770 train_time:105312ms step_avg:97.42ms
step:1092/1770 train_time:105412ms step_avg:97.42ms
step:1093/1770 train_time:105513ms step_avg:97.43ms
step:1094/1770 train_time:105614ms step_avg:97.43ms
step:1095/1770 train_time:105716ms step_avg:97.43ms
step:1096/1770 train_time:105817ms step_avg:97.44ms
step:1097/1770 train_time:105918ms step_avg:97.44ms
step:1098/1770 train_time:106018ms step_avg:97.44ms
step:1099/1770 train_time:106119ms step_avg:97.45ms
step:1100/1770 train_time:106221ms step_avg:97.45ms
step:1101/1770 train_time:106321ms step_avg:97.45ms
step:1102/1770 train_time:106422ms step_avg:97.46ms
step:1103/1770 train_time:106523ms step_avg:97.46ms
step:1104/1770 train_time:106624ms step_avg:97.46ms
step:1105/1770 train_time:106725ms step_avg:97.47ms
step:1106/1770 train_time:106825ms step_avg:97.47ms
step:1107/1770 train_time:106926ms step_avg:97.47ms
step:1108/1770 train_time:107026ms step_avg:97.47ms
step:1109/1770 train_time:107127ms step_avg:97.48ms
step:1110/1770 train_time:107228ms step_avg:97.48ms
step:1111/1770 train_time:107331ms step_avg:97.49ms
step:1112/1770 train_time:107433ms step_avg:97.49ms
step:1113/1770 train_time:107534ms step_avg:97.49ms
step:1114/1770 train_time:107635ms step_avg:97.50ms
step:1115/1770 train_time:107737ms step_avg:97.50ms
step:1116/1770 train_time:107839ms step_avg:97.50ms
step:1117/1770 train_time:107940ms step_avg:97.51ms
step:1118/1770 train_time:108040ms step_avg:97.51ms
step:1119/1770 train_time:108141ms step_avg:97.51ms
step:1120/1770 train_time:108241ms step_avg:97.51ms
step:1121/1770 train_time:108342ms step_avg:97.52ms
step:1122/1770 train_time:108442ms step_avg:97.52ms
step:1123/1770 train_time:108542ms step_avg:97.52ms
step:1124/1770 train_time:108643ms step_avg:97.53ms
step:1125/1770 train_time:108744ms step_avg:97.53ms
step:1125/1770 val_loss:3.4708 train_time:108842ms step_avg:97.62ms
step:1126/1770 train_time:108865ms step_avg:97.55ms
step:1127/1770 train_time:108956ms step_avg:97.54ms
step:1128/1770 train_time:109059ms step_avg:97.55ms
step:1129/1770 train_time:109160ms step_avg:97.55ms
step:1130/1770 train_time:109261ms step_avg:97.55ms
step:1131/1770 train_time:109361ms step_avg:97.56ms
step:1132/1770 train_time:109462ms step_avg:97.56ms
step:1133/1770 train_time:109562ms step_avg:97.56ms
step:1134/1770 train_time:109661ms step_avg:97.56ms
step:1135/1770 train_time:109762ms step_avg:97.57ms
step:1136/1770 train_time:109863ms step_avg:97.57ms
step:1137/1770 train_time:109965ms step_avg:97.57ms
step:1138/1770 train_time:110065ms step_avg:97.58ms
step:1139/1770 train_time:110166ms step_avg:97.58ms
step:1140/1770 train_time:110268ms step_avg:97.58ms
step:1141/1770 train_time:110369ms step_avg:97.59ms
step:1142/1770 train_time:110469ms step_avg:97.59ms
step:1143/1770 train_time:110570ms step_avg:97.59ms
step:1144/1770 train_time:110671ms step_avg:97.59ms
step:1145/1770 train_time:110771ms step_avg:97.60ms
step:1146/1770 train_time:110874ms step_avg:97.60ms
step:1147/1770 train_time:110977ms step_avg:97.61ms
step:1148/1770 train_time:111079ms step_avg:97.61ms
step:1149/1770 train_time:111180ms step_avg:97.61ms
step:1150/1770 train_time:111280ms step_avg:97.61ms
step:1151/1770 train_time:111382ms step_avg:97.62ms
step:1152/1770 train_time:111483ms step_avg:97.62ms
step:1153/1770 train_time:111583ms step_avg:97.62ms
step:1154/1770 train_time:111684ms step_avg:97.63ms
step:1155/1770 train_time:111785ms step_avg:97.63ms
step:1156/1770 train_time:111885ms step_avg:97.63ms
step:1157/1770 train_time:111987ms step_avg:97.63ms
step:1158/1770 train_time:112088ms step_avg:97.64ms
step:1159/1770 train_time:112189ms step_avg:97.64ms
step:1160/1770 train_time:112290ms step_avg:97.64ms
step:1161/1770 train_time:112392ms step_avg:97.65ms
step:1162/1770 train_time:112494ms step_avg:97.65ms
step:1163/1770 train_time:112596ms step_avg:97.65ms
step:1164/1770 train_time:112698ms step_avg:97.66ms
step:1165/1770 train_time:112800ms step_avg:97.66ms
step:1166/1770 train_time:112901ms step_avg:97.67ms
step:1167/1770 train_time:113001ms step_avg:97.67ms
step:1168/1770 train_time:113102ms step_avg:97.67ms
step:1169/1770 train_time:113202ms step_avg:97.67ms
step:1170/1770 train_time:113303ms step_avg:97.67ms
step:1171/1770 train_time:113403ms step_avg:97.68ms
step:1172/1770 train_time:113504ms step_avg:97.68ms
step:1173/1770 train_time:113604ms step_avg:97.68ms
step:1174/1770 train_time:113704ms step_avg:97.68ms
step:1175/1770 train_time:113805ms step_avg:97.69ms
step:1176/1770 train_time:113907ms step_avg:97.69ms
step:1177/1770 train_time:114009ms step_avg:97.69ms
step:1178/1770 train_time:114110ms step_avg:97.70ms
step:1179/1770 train_time:114211ms step_avg:97.70ms
step:1180/1770 train_time:114317ms step_avg:97.71ms
step:1181/1770 train_time:114414ms step_avg:97.71ms
step:1182/1770 train_time:114516ms step_avg:97.71ms
step:1183/1770 train_time:114619ms step_avg:97.71ms
step:1184/1770 train_time:114722ms step_avg:97.72ms
step:1185/1770 train_time:114823ms step_avg:97.72ms
step:1186/1770 train_time:114925ms step_avg:97.73ms
step:1187/1770 train_time:115029ms step_avg:97.73ms
step:1188/1770 train_time:115131ms step_avg:97.73ms
step:1189/1770 train_time:115232ms step_avg:97.74ms
step:1190/1770 train_time:115333ms step_avg:97.74ms
step:1191/1770 train_time:115435ms step_avg:97.74ms
step:1192/1770 train_time:115538ms step_avg:97.75ms
step:1193/1770 train_time:115641ms step_avg:97.75ms
step:1194/1770 train_time:115742ms step_avg:97.76ms
step:1195/1770 train_time:115845ms step_avg:97.76ms
step:1196/1770 train_time:115948ms step_avg:97.76ms
step:1197/1770 train_time:116049ms step_avg:97.77ms
step:1198/1770 train_time:116151ms step_avg:97.77ms
step:1199/1770 train_time:116253ms step_avg:97.77ms
step:1200/1770 train_time:116355ms step_avg:97.78ms
step:1201/1770 train_time:116458ms step_avg:97.78ms
step:1202/1770 train_time:116560ms step_avg:97.79ms
step:1203/1770 train_time:116662ms step_avg:97.79ms
step:1204/1770 train_time:116765ms step_avg:97.79ms
step:1205/1770 train_time:116867ms step_avg:97.80ms
step:1206/1770 train_time:116969ms step_avg:97.80ms
step:1207/1770 train_time:117070ms step_avg:97.80ms
step:1208/1770 train_time:117171ms step_avg:97.81ms
step:1209/1770 train_time:117272ms step_avg:97.81ms
step:1210/1770 train_time:117374ms step_avg:97.81ms
step:1211/1770 train_time:117478ms step_avg:97.82ms
step:1212/1770 train_time:117581ms step_avg:97.82ms
step:1213/1770 train_time:117683ms step_avg:97.82ms
step:1214/1770 train_time:117785ms step_avg:97.83ms
step:1215/1770 train_time:117887ms step_avg:97.83ms
step:1216/1770 train_time:117991ms step_avg:97.84ms
step:1217/1770 train_time:118093ms step_avg:97.84ms
step:1218/1770 train_time:118195ms step_avg:97.84ms
step:1219/1770 train_time:118297ms step_avg:97.85ms
step:1220/1770 train_time:118400ms step_avg:97.85ms
step:1221/1770 train_time:118501ms step_avg:97.85ms
step:1222/1770 train_time:118605ms step_avg:97.86ms
step:1223/1770 train_time:118706ms step_avg:97.86ms
step:1224/1770 train_time:118809ms step_avg:97.87ms
step:1225/1770 train_time:118912ms step_avg:97.87ms
step:1226/1770 train_time:119014ms step_avg:97.87ms
step:1227/1770 train_time:119118ms step_avg:97.88ms
step:1228/1770 train_time:119221ms step_avg:97.88ms
step:1229/1770 train_time:119323ms step_avg:97.89ms
step:1230/1770 train_time:119425ms step_avg:97.89ms
step:1231/1770 train_time:119528ms step_avg:97.89ms
step:1232/1770 train_time:119628ms step_avg:97.90ms
step:1233/1770 train_time:119730ms step_avg:97.90ms
step:1234/1770 train_time:119833ms step_avg:97.90ms
step:1235/1770 train_time:119935ms step_avg:97.91ms
step:1236/1770 train_time:120037ms step_avg:97.91ms
step:1237/1770 train_time:120140ms step_avg:97.91ms
step:1238/1770 train_time:120242ms step_avg:97.92ms
step:1239/1770 train_time:120343ms step_avg:97.92ms
step:1240/1770 train_time:120445ms step_avg:97.92ms
step:1241/1770 train_time:120547ms step_avg:97.93ms
step:1242/1770 train_time:120649ms step_avg:97.93ms
step:1243/1770 train_time:120751ms step_avg:97.93ms
step:1244/1770 train_time:120854ms step_avg:97.94ms
step:1245/1770 train_time:120955ms step_avg:97.94ms
step:1246/1770 train_time:121058ms step_avg:97.94ms
step:1247/1770 train_time:121160ms step_avg:97.95ms
step:1248/1770 train_time:121262ms step_avg:97.95ms
step:1249/1770 train_time:121364ms step_avg:97.95ms
step:1250/1770 train_time:121466ms step_avg:97.96ms
step:1250/1770 val_loss:3.4228 train_time:121567ms step_avg:98.04ms
step:1251/1770 train_time:121588ms step_avg:97.98ms
step:1252/1770 train_time:121678ms step_avg:97.97ms
step:1253/1770 train_time:121781ms step_avg:97.97ms
step:1254/1770 train_time:121884ms step_avg:97.98ms
step:1255/1770 train_time:121988ms step_avg:97.98ms
step:1256/1770 train_time:122088ms step_avg:97.98ms
step:1257/1770 train_time:122190ms step_avg:97.99ms
step:1258/1770 train_time:122292ms step_avg:97.99ms
step:1259/1770 train_time:122394ms step_avg:97.99ms
step:1260/1770 train_time:122495ms step_avg:98.00ms
step:1261/1770 train_time:122598ms step_avg:98.00ms
step:1262/1770 train_time:122700ms step_avg:98.00ms
step:1263/1770 train_time:122802ms step_avg:98.01ms
step:1264/1770 train_time:122906ms step_avg:98.01ms
step:1265/1770 train_time:123008ms step_avg:98.01ms
step:1266/1770 train_time:123111ms step_avg:98.02ms
step:1267/1770 train_time:123213ms step_avg:98.02ms
step:1268/1770 train_time:123315ms step_avg:98.02ms
step:1269/1770 train_time:123417ms step_avg:98.03ms
step:1270/1770 train_time:123520ms step_avg:98.03ms
step:1271/1770 train_time:123622ms step_avg:98.03ms
step:1272/1770 train_time:123724ms step_avg:98.04ms
step:1273/1770 train_time:123827ms step_avg:98.04ms
step:1274/1770 train_time:123930ms step_avg:98.05ms
step:1275/1770 train_time:124031ms step_avg:98.05ms
step:1276/1770 train_time:124133ms step_avg:98.05ms
step:1277/1770 train_time:124235ms step_avg:98.05ms
step:1278/1770 train_time:124337ms step_avg:98.06ms
step:1279/1770 train_time:124440ms step_avg:98.06ms
step:1280/1770 train_time:124544ms step_avg:98.07ms
step:1281/1770 train_time:124645ms step_avg:98.07ms
step:1282/1770 train_time:124748ms step_avg:98.07ms
step:1283/1770 train_time:124850ms step_avg:98.08ms
step:1284/1770 train_time:124953ms step_avg:98.08ms
step:1285/1770 train_time:125054ms step_avg:98.08ms
step:1286/1770 train_time:125158ms step_avg:98.09ms
step:1287/1770 train_time:125261ms step_avg:98.09ms
step:1288/1770 train_time:125365ms step_avg:98.09ms
step:1289/1770 train_time:125467ms step_avg:98.10ms
step:1290/1770 train_time:125568ms step_avg:98.10ms
step:1291/1770 train_time:125670ms step_avg:98.10ms
step:1292/1770 train_time:125771ms step_avg:98.11ms
step:1293/1770 train_time:125874ms step_avg:98.11ms
step:1294/1770 train_time:125975ms step_avg:98.11ms
step:1295/1770 train_time:126078ms step_avg:98.11ms
step:1296/1770 train_time:126181ms step_avg:98.12ms
step:1297/1770 train_time:126282ms step_avg:98.12ms
step:1298/1770 train_time:126384ms step_avg:98.12ms
step:1299/1770 train_time:126487ms step_avg:98.13ms
step:1300/1770 train_time:126588ms step_avg:98.13ms
step:1301/1770 train_time:126691ms step_avg:98.13ms
step:1302/1770 train_time:126793ms step_avg:98.14ms
step:1303/1770 train_time:126894ms step_avg:98.14ms
step:1304/1770 train_time:126996ms step_avg:98.14ms
step:1305/1770 train_time:127098ms step_avg:98.15ms
step:1306/1770 train_time:127199ms step_avg:98.15ms
step:1307/1770 train_time:127302ms step_avg:98.15ms
step:1308/1770 train_time:127406ms step_avg:98.16ms
step:1309/1770 train_time:127509ms step_avg:98.16ms
step:1310/1770 train_time:127610ms step_avg:98.16ms
step:1311/1770 train_time:127712ms step_avg:98.16ms
step:1312/1770 train_time:127814ms step_avg:98.17ms
step:1313/1770 train_time:127916ms step_avg:98.17ms
step:1314/1770 train_time:128017ms step_avg:98.17ms
step:1315/1770 train_time:128119ms step_avg:98.18ms
step:1316/1770 train_time:128221ms step_avg:98.18ms
step:1317/1770 train_time:128325ms step_avg:98.18ms
step:1318/1770 train_time:128430ms step_avg:98.19ms
step:1319/1770 train_time:128534ms step_avg:98.19ms
step:1320/1770 train_time:128636ms step_avg:98.20ms
step:1321/1770 train_time:128739ms step_avg:98.20ms
step:1322/1770 train_time:128841ms step_avg:98.20ms
step:1323/1770 train_time:128943ms step_avg:98.21ms
step:1324/1770 train_time:129047ms step_avg:98.21ms
step:1325/1770 train_time:129150ms step_avg:98.21ms
step:1326/1770 train_time:129251ms step_avg:98.21ms
step:1327/1770 train_time:129356ms step_avg:98.22ms
step:1328/1770 train_time:129457ms step_avg:98.22ms
step:1329/1770 train_time:129560ms step_avg:98.23ms
step:1330/1770 train_time:129662ms step_avg:98.23ms
step:1331/1770 train_time:129764ms step_avg:98.23ms
step:1332/1770 train_time:129866ms step_avg:98.23ms
step:1333/1770 train_time:129968ms step_avg:98.24ms
step:1334/1770 train_time:130069ms step_avg:98.24ms
step:1335/1770 train_time:130170ms step_avg:98.24ms
step:1336/1770 train_time:130271ms step_avg:98.24ms
step:1337/1770 train_time:130373ms step_avg:98.25ms
step:1338/1770 train_time:130474ms step_avg:98.25ms
step:1339/1770 train_time:130576ms step_avg:98.25ms
step:1340/1770 train_time:130679ms step_avg:98.25ms
step:1341/1770 train_time:130781ms step_avg:98.26ms
step:1342/1770 train_time:130886ms step_avg:98.26ms
step:1343/1770 train_time:130990ms step_avg:98.27ms
step:1344/1770 train_time:131091ms step_avg:98.27ms
step:1345/1770 train_time:131193ms step_avg:98.27ms
step:1346/1770 train_time:131295ms step_avg:98.27ms
step:1347/1770 train_time:131397ms step_avg:98.28ms
step:1348/1770 train_time:131501ms step_avg:98.28ms
step:1349/1770 train_time:131604ms step_avg:98.29ms
step:1350/1770 train_time:131706ms step_avg:98.29ms
step:1351/1770 train_time:131808ms step_avg:98.29ms
step:1352/1770 train_time:131909ms step_avg:98.29ms
step:1353/1770 train_time:132012ms step_avg:98.30ms
step:1354/1770 train_time:132113ms step_avg:98.30ms
step:1355/1770 train_time:132215ms step_avg:98.30ms
step:1356/1770 train_time:132317ms step_avg:98.30ms
step:1357/1770 train_time:132419ms step_avg:98.31ms
step:1358/1770 train_time:132522ms step_avg:98.31ms
step:1359/1770 train_time:132625ms step_avg:98.31ms
step:1360/1770 train_time:132728ms step_avg:98.32ms
step:1361/1770 train_time:132830ms step_avg:98.32ms
step:1362/1770 train_time:132932ms step_avg:98.32ms
step:1363/1770 train_time:133034ms step_avg:98.33ms
step:1364/1770 train_time:133136ms step_avg:98.33ms
step:1365/1770 train_time:133238ms step_avg:98.33ms
step:1366/1770 train_time:133340ms step_avg:98.33ms
step:1367/1770 train_time:133443ms step_avg:98.34ms
step:1368/1770 train_time:133545ms step_avg:98.34ms
step:1369/1770 train_time:133649ms step_avg:98.34ms
step:1370/1770 train_time:133751ms step_avg:98.35ms
step:1371/1770 train_time:133854ms step_avg:98.35ms
step:1372/1770 train_time:133956ms step_avg:98.35ms
step:1373/1770 train_time:134058ms step_avg:98.36ms
step:1374/1770 train_time:134161ms step_avg:98.36ms
step:1375/1770 train_time:134264ms step_avg:98.36ms
step:1375/1770 val_loss:3.3784 train_time:134364ms step_avg:98.44ms
step:1376/1770 train_time:134386ms step_avg:98.38ms
step:1377/1770 train_time:134477ms step_avg:98.37ms
step:1378/1770 train_time:134579ms step_avg:98.38ms
step:1379/1770 train_time:134680ms step_avg:98.38ms
step:1380/1770 train_time:134781ms step_avg:98.38ms
step:1381/1770 train_time:134884ms step_avg:98.38ms
step:1382/1770 train_time:134986ms step_avg:98.39ms
step:1383/1770 train_time:135089ms step_avg:98.39ms
step:1384/1770 train_time:135191ms step_avg:98.39ms
step:1385/1770 train_time:135293ms step_avg:98.39ms
step:1386/1770 train_time:135396ms step_avg:98.40ms
step:1387/1770 train_time:135501ms step_avg:98.40ms
step:1388/1770 train_time:135602ms step_avg:98.41ms
step:1389/1770 train_time:135705ms step_avg:98.41ms
step:1390/1770 train_time:135808ms step_avg:98.41ms
step:1391/1770 train_time:135909ms step_avg:98.41ms
step:1392/1770 train_time:136015ms step_avg:98.42ms
step:1393/1770 train_time:136113ms step_avg:98.42ms
step:1394/1770 train_time:136214ms step_avg:98.42ms
step:1395/1770 train_time:136317ms step_avg:98.42ms
step:1396/1770 train_time:136421ms step_avg:98.43ms
step:1397/1770 train_time:136523ms step_avg:98.43ms
step:1398/1770 train_time:136625ms step_avg:98.43ms
step:1399/1770 train_time:136727ms step_avg:98.44ms
step:1400/1770 train_time:136831ms step_avg:98.44ms
step:1401/1770 train_time:136933ms step_avg:98.44ms
step:1402/1770 train_time:137035ms step_avg:98.44ms
step:1403/1770 train_time:137137ms step_avg:98.45ms
step:1404/1770 train_time:137239ms step_avg:98.45ms
step:1405/1770 train_time:137341ms step_avg:98.45ms
step:1406/1770 train_time:137442ms step_avg:98.45ms
step:1407/1770 train_time:137544ms step_avg:98.46ms
step:1408/1770 train_time:137647ms step_avg:98.46ms
step:1409/1770 train_time:137749ms step_avg:98.46ms
step:1410/1770 train_time:137852ms step_avg:98.47ms
step:1411/1770 train_time:137953ms step_avg:98.47ms
step:1412/1770 train_time:138055ms step_avg:98.47ms
step:1413/1770 train_time:138156ms step_avg:98.47ms
step:1414/1770 train_time:138259ms step_avg:98.48ms
step:1415/1770 train_time:138361ms step_avg:98.48ms
step:1416/1770 train_time:138464ms step_avg:98.48ms
step:1417/1770 train_time:138566ms step_avg:98.48ms
step:1418/1770 train_time:138669ms step_avg:98.49ms
step:1419/1770 train_time:138772ms step_avg:98.49ms
step:1420/1770 train_time:138874ms step_avg:98.49ms
step:1421/1770 train_time:138976ms step_avg:98.49ms
step:1422/1770 train_time:139078ms step_avg:98.50ms
step:1423/1770 train_time:139179ms step_avg:98.50ms
step:1424/1770 train_time:139281ms step_avg:98.50ms
step:1425/1770 train_time:139383ms step_avg:98.50ms
step:1426/1770 train_time:139485ms step_avg:98.51ms
step:1427/1770 train_time:139587ms step_avg:98.51ms
step:1428/1770 train_time:139691ms step_avg:98.51ms
step:1429/1770 train_time:139793ms step_avg:98.52ms
step:1430/1770 train_time:139894ms step_avg:98.52ms
step:1431/1770 train_time:139997ms step_avg:98.52ms
step:1432/1770 train_time:140099ms step_avg:98.52ms
step:1433/1770 train_time:140200ms step_avg:98.52ms
step:1434/1770 train_time:140301ms step_avg:98.53ms
step:1435/1770 train_time:140404ms step_avg:98.53ms
step:1436/1770 train_time:140507ms step_avg:98.53ms
step:1437/1770 train_time:140611ms step_avg:98.54ms
step:1438/1770 train_time:140713ms step_avg:98.54ms
step:1439/1770 train_time:140815ms step_avg:98.54ms
step:1440/1770 train_time:140917ms step_avg:98.54ms
step:1441/1770 train_time:141024ms step_avg:98.55ms
step:1442/1770 train_time:141123ms step_avg:98.55ms
step:1443/1770 train_time:141226ms step_avg:98.55ms
step:1444/1770 train_time:141330ms step_avg:98.56ms
step:1445/1770 train_time:141433ms step_avg:98.56ms
step:1446/1770 train_time:141535ms step_avg:98.56ms
step:1447/1770 train_time:141640ms step_avg:98.57ms
step:1448/1770 train_time:141743ms step_avg:98.57ms
step:1449/1770 train_time:141848ms step_avg:98.57ms
step:1450/1770 train_time:141951ms step_avg:98.58ms
step:1451/1770 train_time:142054ms step_avg:98.58ms
step:1452/1770 train_time:142156ms step_avg:98.58ms
step:1453/1770 train_time:142259ms step_avg:98.59ms
step:1454/1770 train_time:142362ms step_avg:98.59ms
step:1455/1770 train_time:142467ms step_avg:98.59ms
step:1456/1770 train_time:142571ms step_avg:98.60ms
step:1457/1770 train_time:142674ms step_avg:98.60ms
step:1458/1770 train_time:142778ms step_avg:98.60ms
step:1459/1770 train_time:142882ms step_avg:98.61ms
step:1460/1770 train_time:142985ms step_avg:98.61ms
step:1461/1770 train_time:143088ms step_avg:98.61ms
step:1462/1770 train_time:143191ms step_avg:98.62ms
step:1463/1770 train_time:143294ms step_avg:98.62ms
step:1464/1770 train_time:143398ms step_avg:98.62ms
step:1465/1770 train_time:143501ms step_avg:98.63ms
step:1466/1770 train_time:143605ms step_avg:98.63ms
step:1467/1770 train_time:143709ms step_avg:98.63ms
step:1468/1770 train_time:143812ms step_avg:98.64ms
step:1469/1770 train_time:143914ms step_avg:98.64ms
step:1470/1770 train_time:144017ms step_avg:98.64ms
step:1471/1770 train_time:144120ms step_avg:98.64ms
step:1472/1770 train_time:144223ms step_avg:98.65ms
step:1473/1770 train_time:144327ms step_avg:98.65ms
step:1474/1770 train_time:144432ms step_avg:98.66ms
step:1475/1770 train_time:144536ms step_avg:98.66ms
step:1476/1770 train_time:144638ms step_avg:98.66ms
step:1477/1770 train_time:144744ms step_avg:98.67ms
step:1478/1770 train_time:144848ms step_avg:98.67ms
step:1479/1770 train_time:144952ms step_avg:98.67ms
step:1480/1770 train_time:145055ms step_avg:98.68ms
step:1481/1770 train_time:145162ms step_avg:98.68ms
step:1482/1770 train_time:145264ms step_avg:98.69ms
step:1483/1770 train_time:145368ms step_avg:98.69ms
step:1484/1770 train_time:145471ms step_avg:98.69ms
step:1485/1770 train_time:145574ms step_avg:98.69ms
step:1486/1770 train_time:145676ms step_avg:98.70ms
step:1487/1770 train_time:145779ms step_avg:98.70ms
step:1488/1770 train_time:145882ms step_avg:98.70ms
step:1489/1770 train_time:145986ms step_avg:98.71ms
step:1490/1770 train_time:146091ms step_avg:98.71ms
step:1491/1770 train_time:146194ms step_avg:98.71ms
step:1492/1770 train_time:146298ms step_avg:98.72ms
step:1493/1770 train_time:146404ms step_avg:98.72ms
step:1494/1770 train_time:146510ms step_avg:98.73ms
step:1495/1770 train_time:146613ms step_avg:98.73ms
step:1496/1770 train_time:146715ms step_avg:98.73ms
step:1497/1770 train_time:146818ms step_avg:98.73ms
step:1498/1770 train_time:146920ms step_avg:98.74ms
step:1499/1770 train_time:147025ms step_avg:98.74ms
step:1500/1770 train_time:147127ms step_avg:98.74ms
step:1500/1770 val_loss:3.3408 train_time:147228ms step_avg:98.81ms
step:1501/1770 train_time:147249ms step_avg:98.76ms
step:1502/1770 train_time:147343ms step_avg:98.76ms
step:1503/1770 train_time:147447ms step_avg:98.76ms
step:1504/1770 train_time:147550ms step_avg:98.76ms
step:1505/1770 train_time:147655ms step_avg:98.77ms
step:1506/1770 train_time:147759ms step_avg:98.77ms
step:1507/1770 train_time:147862ms step_avg:98.77ms
step:1508/1770 train_time:147967ms step_avg:98.78ms
step:1509/1770 train_time:148070ms step_avg:98.78ms
step:1510/1770 train_time:148172ms step_avg:98.78ms
step:1511/1770 train_time:148277ms step_avg:98.79ms
step:1512/1770 train_time:148381ms step_avg:98.79ms
step:1513/1770 train_time:148486ms step_avg:98.79ms
step:1514/1770 train_time:148589ms step_avg:98.80ms
step:1515/1770 train_time:148691ms step_avg:98.80ms
step:1516/1770 train_time:148795ms step_avg:98.80ms
step:1517/1770 train_time:148898ms step_avg:98.80ms
step:1518/1770 train_time:149004ms step_avg:98.81ms
step:1519/1770 train_time:149107ms step_avg:98.81ms
step:1520/1770 train_time:149211ms step_avg:98.82ms
step:1521/1770 train_time:149314ms step_avg:98.82ms
step:1522/1770 train_time:149418ms step_avg:98.82ms
step:1523/1770 train_time:149522ms step_avg:98.82ms
step:1524/1770 train_time:149625ms step_avg:98.83ms
step:1525/1770 train_time:149728ms step_avg:98.83ms
step:1526/1770 train_time:149830ms step_avg:98.83ms
step:1527/1770 train_time:149934ms step_avg:98.84ms
step:1528/1770 train_time:150039ms step_avg:98.84ms
step:1529/1770 train_time:150142ms step_avg:98.84ms
step:1530/1770 train_time:150245ms step_avg:98.85ms
step:1531/1770 train_time:150348ms step_avg:98.85ms
step:1532/1770 train_time:150451ms step_avg:98.85ms
step:1533/1770 train_time:150555ms step_avg:98.85ms
step:1534/1770 train_time:150659ms step_avg:98.86ms
step:1535/1770 train_time:150762ms step_avg:98.86ms
step:1536/1770 train_time:150865ms step_avg:98.86ms
step:1537/1770 train_time:150968ms step_avg:98.87ms
step:1538/1770 train_time:151073ms step_avg:98.87ms
step:1539/1770 train_time:151176ms step_avg:98.87ms
step:1540/1770 train_time:151282ms step_avg:98.88ms
step:1541/1770 train_time:151387ms step_avg:98.88ms
step:1542/1770 train_time:151490ms step_avg:98.88ms
step:1543/1770 train_time:151592ms step_avg:98.89ms
step:1544/1770 train_time:151697ms step_avg:98.89ms
step:1545/1770 train_time:151800ms step_avg:98.89ms
step:1546/1770 train_time:151904ms step_avg:98.90ms
step:1547/1770 train_time:152008ms step_avg:98.90ms
step:1548/1770 train_time:152110ms step_avg:98.90ms
step:1549/1770 train_time:152213ms step_avg:98.90ms
step:1550/1770 train_time:152317ms step_avg:98.91ms
step:1551/1770 train_time:152420ms step_avg:98.91ms
step:1552/1770 train_time:152525ms step_avg:98.91ms
step:1553/1770 train_time:152629ms step_avg:98.92ms
step:1554/1770 train_time:152731ms step_avg:98.92ms
step:1555/1770 train_time:152835ms step_avg:98.92ms
step:1556/1770 train_time:152937ms step_avg:98.92ms
step:1557/1770 train_time:153041ms step_avg:98.93ms
step:1558/1770 train_time:153145ms step_avg:98.93ms
step:1559/1770 train_time:153248ms step_avg:98.93ms
step:1560/1770 train_time:153350ms step_avg:98.94ms
step:1561/1770 train_time:153455ms step_avg:98.94ms
step:1562/1770 train_time:153558ms step_avg:98.94ms
step:1563/1770 train_time:153662ms step_avg:98.95ms
step:1564/1770 train_time:153765ms step_avg:98.95ms
step:1565/1770 train_time:153868ms step_avg:98.95ms
step:1566/1770 train_time:153971ms step_avg:98.95ms
step:1567/1770 train_time:154074ms step_avg:98.96ms
step:1568/1770 train_time:154176ms step_avg:98.96ms
step:1569/1770 train_time:154283ms step_avg:98.96ms
step:1570/1770 train_time:154386ms step_avg:98.97ms
step:1571/1770 train_time:154489ms step_avg:98.97ms
step:1572/1770 train_time:154592ms step_avg:98.97ms
step:1573/1770 train_time:154697ms step_avg:98.97ms
step:1574/1770 train_time:154801ms step_avg:98.98ms
step:1575/1770 train_time:154903ms step_avg:98.98ms
step:1576/1770 train_time:155006ms step_avg:98.98ms
step:1577/1770 train_time:155111ms step_avg:98.99ms
step:1578/1770 train_time:155215ms step_avg:98.99ms
step:1579/1770 train_time:155317ms step_avg:98.99ms
step:1580/1770 train_time:155421ms step_avg:98.99ms
step:1581/1770 train_time:155527ms step_avg:99.00ms
step:1582/1770 train_time:155631ms step_avg:99.00ms
step:1583/1770 train_time:155734ms step_avg:99.00ms
step:1584/1770 train_time:155838ms step_avg:99.01ms
step:1585/1770 train_time:155942ms step_avg:99.01ms
step:1586/1770 train_time:156050ms step_avg:99.02ms
step:1587/1770 train_time:156154ms step_avg:99.02ms
step:1588/1770 train_time:156257ms step_avg:99.02ms
step:1589/1770 train_time:156363ms step_avg:99.03ms
step:1590/1770 train_time:156466ms step_avg:99.03ms
step:1591/1770 train_time:156569ms step_avg:99.03ms
step:1592/1770 train_time:156673ms step_avg:99.03ms
step:1593/1770 train_time:156775ms step_avg:99.04ms
step:1594/1770 train_time:156879ms step_avg:99.04ms
step:1595/1770 train_time:156982ms step_avg:99.04ms
step:1596/1770 train_time:157087ms step_avg:99.05ms
step:1597/1770 train_time:157190ms step_avg:99.05ms
step:1598/1770 train_time:157293ms step_avg:99.05ms
step:1599/1770 train_time:157398ms step_avg:99.05ms
step:1600/1770 train_time:157504ms step_avg:99.06ms
step:1601/1770 train_time:157608ms step_avg:99.06ms
step:1602/1770 train_time:157712ms step_avg:99.07ms
step:1603/1770 train_time:157815ms step_avg:99.07ms
step:1604/1770 train_time:157917ms step_avg:99.07ms
step:1605/1770 train_time:158020ms step_avg:99.07ms
step:1606/1770 train_time:158124ms step_avg:99.08ms
step:1607/1770 train_time:158231ms step_avg:99.08ms
step:1608/1770 train_time:158334ms step_avg:99.08ms
step:1609/1770 train_time:158436ms step_avg:99.08ms
step:1610/1770 train_time:158541ms step_avg:99.09ms
step:1611/1770 train_time:158647ms step_avg:99.09ms
step:1612/1770 train_time:158752ms step_avg:99.10ms
step:1613/1770 train_time:158855ms step_avg:99.10ms
step:1614/1770 train_time:158958ms step_avg:99.10ms
step:1615/1770 train_time:159062ms step_avg:99.10ms
step:1616/1770 train_time:159166ms step_avg:99.11ms
step:1617/1770 train_time:159271ms step_avg:99.11ms
step:1618/1770 train_time:159375ms step_avg:99.11ms
step:1619/1770 train_time:159478ms step_avg:99.12ms
step:1620/1770 train_time:159583ms step_avg:99.12ms
step:1621/1770 train_time:159687ms step_avg:99.12ms
step:1622/1770 train_time:159791ms step_avg:99.13ms
step:1623/1770 train_time:159896ms step_avg:99.13ms
step:1624/1770 train_time:159999ms step_avg:99.13ms
step:1625/1770 train_time:160102ms step_avg:99.13ms
step:1625/1770 val_loss:3.3067 train_time:160205ms step_avg:99.20ms
step:1626/1770 train_time:160226ms step_avg:99.15ms
step:1627/1770 train_time:160317ms step_avg:99.14ms
step:1628/1770 train_time:160421ms step_avg:99.15ms
step:1629/1770 train_time:160523ms step_avg:99.15ms
step:1630/1770 train_time:160626ms step_avg:99.15ms
step:1631/1770 train_time:160729ms step_avg:99.15ms
step:1632/1770 train_time:160832ms step_avg:99.16ms
step:1633/1770 train_time:160934ms step_avg:99.16ms
step:1634/1770 train_time:161038ms step_avg:99.16ms
step:1635/1770 train_time:161141ms step_avg:99.16ms
step:1636/1770 train_time:161245ms step_avg:99.17ms
step:1637/1770 train_time:161349ms step_avg:99.17ms
step:1638/1770 train_time:161453ms step_avg:99.17ms
step:1639/1770 train_time:161556ms step_avg:99.18ms
step:1640/1770 train_time:161662ms step_avg:99.18ms
step:1641/1770 train_time:161765ms step_avg:99.18ms
step:1642/1770 train_time:161868ms step_avg:99.18ms
step:1643/1770 train_time:161971ms step_avg:99.19ms
step:1644/1770 train_time:162076ms step_avg:99.19ms
step:1645/1770 train_time:162180ms step_avg:99.19ms
step:1646/1770 train_time:162285ms step_avg:99.20ms
step:1647/1770 train_time:162389ms step_avg:99.20ms
step:1648/1770 train_time:162491ms step_avg:99.20ms
step:1649/1770 train_time:162595ms step_avg:99.20ms
step:1650/1770 train_time:162699ms step_avg:99.21ms
step:1651/1770 train_time:162803ms step_avg:99.21ms
step:1652/1770 train_time:162907ms step_avg:99.21ms
step:1653/1770 train_time:163009ms step_avg:99.21ms
step:1654/1770 train_time:163115ms step_avg:99.22ms
step:1655/1770 train_time:163221ms step_avg:99.22ms
step:1656/1770 train_time:163324ms step_avg:99.22ms
step:1657/1770 train_time:163429ms step_avg:99.23ms
step:1658/1770 train_time:163533ms step_avg:99.23ms
step:1659/1770 train_time:163637ms step_avg:99.23ms
step:1660/1770 train_time:163741ms step_avg:99.24ms
step:1661/1770 train_time:163847ms step_avg:99.24ms
step:1662/1770 train_time:163951ms step_avg:99.24ms
step:1663/1770 train_time:164054ms step_avg:99.25ms
step:1664/1770 train_time:164156ms step_avg:99.25ms
step:1665/1770 train_time:164260ms step_avg:99.25ms
step:1666/1770 train_time:164364ms step_avg:99.25ms
step:1667/1770 train_time:164466ms step_avg:99.26ms
step:1668/1770 train_time:164568ms step_avg:99.26ms
step:1669/1770 train_time:164670ms step_avg:99.26ms
step:1670/1770 train_time:164773ms step_avg:99.26ms
step:1671/1770 train_time:164876ms step_avg:99.26ms
step:1672/1770 train_time:164982ms step_avg:99.27ms
step:1673/1770 train_time:165086ms step_avg:99.27ms
step:1674/1770 train_time:165189ms step_avg:99.27ms
step:1675/1770 train_time:165292ms step_avg:99.27ms
step:1676/1770 train_time:165396ms step_avg:99.28ms
step:1677/1770 train_time:165504ms step_avg:99.28ms
step:1678/1770 train_time:165607ms step_avg:99.28ms
step:1679/1770 train_time:165710ms step_avg:99.29ms
step:1680/1770 train_time:165813ms step_avg:99.29ms
step:1681/1770 train_time:165916ms step_avg:99.29ms
step:1682/1770 train_time:166022ms step_avg:99.30ms
step:1683/1770 train_time:166125ms step_avg:99.30ms
step:1684/1770 train_time:166228ms step_avg:99.30ms
step:1685/1770 train_time:166331ms step_avg:99.30ms
step:1686/1770 train_time:166435ms step_avg:99.31ms
step:1687/1770 train_time:166541ms step_avg:99.31ms
step:1688/1770 train_time:166644ms step_avg:99.31ms
step:1689/1770 train_time:166747ms step_avg:99.31ms
step:1690/1770 train_time:166850ms step_avg:99.32ms
step:1691/1770 train_time:166954ms step_avg:99.32ms
step:1692/1770 train_time:167057ms step_avg:99.32ms
step:1693/1770 train_time:167162ms step_avg:99.32ms
step:1694/1770 train_time:167265ms step_avg:99.33ms
step:1695/1770 train_time:167368ms step_avg:99.33ms
step:1696/1770 train_time:167474ms step_avg:99.33ms
step:1697/1770 train_time:167579ms step_avg:99.34ms
step:1698/1770 train_time:167683ms step_avg:99.34ms
step:1699/1770 train_time:167786ms step_avg:99.34ms
step:1700/1770 train_time:167889ms step_avg:99.34ms
step:1701/1770 train_time:167992ms step_avg:99.34ms
step:1702/1770 train_time:168095ms step_avg:99.35ms
step:1703/1770 train_time:168199ms step_avg:99.35ms
step:1704/1770 train_time:168304ms step_avg:99.35ms
step:1705/1770 train_time:168407ms step_avg:99.36ms
step:1706/1770 train_time:168510ms step_avg:99.36ms
step:1707/1770 train_time:168614ms step_avg:99.36ms
step:1708/1770 train_time:168718ms step_avg:99.36ms
step:1709/1770 train_time:168823ms step_avg:99.37ms
step:1710/1770 train_time:168931ms step_avg:99.37ms
step:1711/1770 train_time:169036ms step_avg:99.37ms
step:1712/1770 train_time:169141ms step_avg:99.38ms
step:1713/1770 train_time:169244ms step_avg:99.38ms
step:1714/1770 train_time:169348ms step_avg:99.38ms
step:1715/1770 train_time:169452ms step_avg:99.39ms
step:1716/1770 train_time:169556ms step_avg:99.39ms
step:1717/1770 train_time:169659ms step_avg:99.39ms
step:1718/1770 train_time:169765ms step_avg:99.39ms
step:1719/1770 train_time:169869ms step_avg:99.40ms
step:1720/1770 train_time:169974ms step_avg:99.40ms
step:1721/1770 train_time:170077ms step_avg:99.40ms
step:1722/1770 train_time:170184ms step_avg:99.41ms
step:1723/1770 train_time:170290ms step_avg:99.41ms
step:1724/1770 train_time:170397ms step_avg:99.41ms
step:1725/1770 train_time:170503ms step_avg:99.42ms
step:1726/1770 train_time:170609ms step_avg:99.42ms
step:1727/1770 train_time:170712ms step_avg:99.42ms
step:1728/1770 train_time:170818ms step_avg:99.43ms
step:1729/1770 train_time:170922ms step_avg:99.43ms
step:1730/1770 train_time:171027ms step_avg:99.43ms
step:1731/1770 train_time:171132ms step_avg:99.44ms
step:1732/1770 train_time:171236ms step_avg:99.44ms
step:1733/1770 train_time:171342ms step_avg:99.44ms
step:1734/1770 train_time:171446ms step_avg:99.45ms
step:1735/1770 train_time:171550ms step_avg:99.45ms
step:1736/1770 train_time:171654ms step_avg:99.45ms
step:1737/1770 train_time:171758ms step_avg:99.45ms
step:1738/1770 train_time:171862ms step_avg:99.46ms
step:1739/1770 train_time:171967ms step_avg:99.46ms
step:1740/1770 train_time:172070ms step_avg:99.46ms
step:1741/1770 train_time:172177ms step_avg:99.47ms
step:1742/1770 train_time:172284ms step_avg:99.47ms
step:1743/1770 train_time:172389ms step_avg:99.47ms
step:1744/1770 train_time:172493ms step_avg:99.48ms
step:1745/1770 train_time:172597ms step_avg:99.48ms
step:1746/1770 train_time:172703ms step_avg:99.48ms
step:1747/1770 train_time:172806ms step_avg:99.49ms
step:1748/1770 train_time:172912ms step_avg:99.49ms
step:1749/1770 train_time:173018ms step_avg:99.49ms
step:1750/1770 train_time:173121ms step_avg:99.49ms
step:1750/1770 val_loss:3.2801 train_time:173225ms step_avg:99.55ms
step:1751/1770 train_time:173246ms step_avg:99.51ms
step:1752/1770 train_time:173338ms step_avg:99.50ms
step:1753/1770 train_time:173442ms step_avg:99.51ms
step:1754/1770 train_time:173546ms step_avg:99.51ms
step:1755/1770 train_time:173650ms step_avg:99.51ms
step:1756/1770 train_time:173755ms step_avg:99.52ms
step:1757/1770 train_time:173859ms step_avg:99.52ms
step:1758/1770 train_time:173963ms step_avg:99.52ms
step:1759/1770 train_time:174068ms step_avg:99.52ms
step:1760/1770 train_time:174173ms step_avg:99.53ms
step:1761/1770 train_time:174279ms step_avg:99.53ms
step:1762/1770 train_time:174387ms step_avg:99.54ms
step:1763/1770 train_time:174490ms step_avg:99.54ms
step:1764/1770 train_time:174595ms step_avg:99.54ms
step:1765/1770 train_time:174699ms step_avg:99.54ms
step:1766/1770 train_time:174807ms step_avg:99.55ms
step:1767/1770 train_time:174909ms step_avg:99.55ms
step:1768/1770 train_time:175014ms step_avg:99.55ms
step:1769/1770 train_time:175116ms step_avg:99.55ms
step:1770/1770 train_time:175219ms step_avg:99.56ms
step:1770/1770 val_loss:3.2771 train_time:175323ms step_avg:99.62ms
peak memory allocated: 28840 MiB reserved: 32212 MiB
