import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 15:54:19 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   37C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23767ms step_avg:nanms
step:2/1770 train_time:24183ms step_avg:nanms
step:3/1770 train_time:24279ms step_avg:nanms
step:4/1770 train_time:24372ms step_avg:nanms
step:5/1770 train_time:24466ms step_avg:nanms
step:6/1770 train_time:24560ms step_avg:nanms
step:7/1770 train_time:24653ms step_avg:nanms
step:8/1770 train_time:24747ms step_avg:nanms
step:9/1770 train_time:24841ms step_avg:nanms
step:10/1770 train_time:24934ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:346ms step_avg:115.41ms
step:14/1770 train_time:439ms step_avg:109.81ms
step:15/1770 train_time:533ms step_avg:106.59ms
step:16/1770 train_time:627ms step_avg:104.49ms
step:17/1770 train_time:721ms step_avg:102.96ms
step:18/1770 train_time:814ms step_avg:101.77ms
step:19/1770 train_time:908ms step_avg:100.84ms
step:20/1770 train_time:1001ms step_avg:100.12ms
step:21/1770 train_time:1095ms step_avg:99.57ms
step:22/1770 train_time:1189ms step_avg:99.08ms
step:23/1770 train_time:1284ms step_avg:98.74ms
step:24/1770 train_time:1378ms step_avg:98.44ms
step:25/1770 train_time:1472ms step_avg:98.14ms
step:26/1770 train_time:1566ms step_avg:97.90ms
step:27/1770 train_time:1660ms step_avg:97.65ms
step:28/1770 train_time:1754ms step_avg:97.44ms
step:29/1770 train_time:1848ms step_avg:97.25ms
step:30/1770 train_time:1942ms step_avg:97.09ms
step:31/1770 train_time:2035ms step_avg:96.92ms
step:32/1770 train_time:2130ms step_avg:96.80ms
step:33/1770 train_time:2224ms step_avg:96.69ms
step:34/1770 train_time:2318ms step_avg:96.58ms
step:35/1770 train_time:2412ms step_avg:96.48ms
step:36/1770 train_time:2506ms step_avg:96.37ms
step:37/1770 train_time:2600ms step_avg:96.28ms
step:38/1770 train_time:2694ms step_avg:96.21ms
step:39/1770 train_time:2788ms step_avg:96.14ms
step:40/1770 train_time:2882ms step_avg:96.07ms
step:41/1770 train_time:2976ms step_avg:95.99ms
step:42/1770 train_time:3069ms step_avg:95.92ms
step:43/1770 train_time:3163ms step_avg:95.86ms
step:44/1770 train_time:3257ms step_avg:95.80ms
step:45/1770 train_time:3351ms step_avg:95.75ms
step:46/1770 train_time:3445ms step_avg:95.70ms
step:47/1770 train_time:3539ms step_avg:95.66ms
step:48/1770 train_time:3634ms step_avg:95.62ms
step:49/1770 train_time:3728ms step_avg:95.59ms
step:50/1770 train_time:3822ms step_avg:95.54ms
step:51/1770 train_time:3916ms step_avg:95.50ms
step:52/1770 train_time:4010ms step_avg:95.48ms
step:53/1770 train_time:4104ms step_avg:95.45ms
step:54/1770 train_time:4199ms step_avg:95.42ms
step:55/1770 train_time:4292ms step_avg:95.38ms
step:56/1770 train_time:4386ms step_avg:95.35ms
step:57/1770 train_time:4480ms step_avg:95.32ms
step:58/1770 train_time:4574ms step_avg:95.29ms
step:59/1770 train_time:4668ms step_avg:95.26ms
step:60/1770 train_time:4761ms step_avg:95.22ms
step:61/1770 train_time:4855ms step_avg:95.19ms
step:62/1770 train_time:4949ms step_avg:95.16ms
step:63/1770 train_time:5042ms step_avg:95.13ms
step:64/1770 train_time:5136ms step_avg:95.11ms
step:65/1770 train_time:5230ms step_avg:95.10ms
step:66/1770 train_time:5325ms step_avg:95.09ms
step:67/1770 train_time:5418ms step_avg:95.06ms
step:68/1770 train_time:5512ms step_avg:95.04ms
step:69/1770 train_time:5606ms step_avg:95.02ms
step:70/1770 train_time:5700ms step_avg:95.00ms
step:71/1770 train_time:5794ms step_avg:94.99ms
step:72/1770 train_time:5889ms step_avg:94.98ms
step:73/1770 train_time:5983ms step_avg:94.96ms
step:74/1770 train_time:6076ms step_avg:94.94ms
step:75/1770 train_time:6170ms step_avg:94.93ms
step:76/1770 train_time:6264ms step_avg:94.91ms
step:77/1770 train_time:6358ms step_avg:94.90ms
step:78/1770 train_time:6452ms step_avg:94.88ms
step:79/1770 train_time:6547ms step_avg:94.88ms
step:80/1770 train_time:6641ms step_avg:94.87ms
step:81/1770 train_time:6735ms step_avg:94.86ms
step:82/1770 train_time:6829ms step_avg:94.84ms
step:83/1770 train_time:6923ms step_avg:94.83ms
step:84/1770 train_time:7017ms step_avg:94.82ms
step:85/1770 train_time:7110ms step_avg:94.81ms
step:86/1770 train_time:7204ms step_avg:94.79ms
step:87/1770 train_time:7298ms step_avg:94.78ms
step:88/1770 train_time:7392ms step_avg:94.77ms
step:89/1770 train_time:7486ms step_avg:94.76ms
step:90/1770 train_time:7580ms step_avg:94.75ms
step:91/1770 train_time:7674ms step_avg:94.74ms
step:92/1770 train_time:7768ms step_avg:94.73ms
step:93/1770 train_time:7861ms step_avg:94.71ms
step:94/1770 train_time:7955ms step_avg:94.70ms
step:95/1770 train_time:8049ms step_avg:94.70ms
step:96/1770 train_time:8144ms step_avg:94.69ms
step:97/1770 train_time:8238ms step_avg:94.69ms
step:98/1770 train_time:8332ms step_avg:94.68ms
step:99/1770 train_time:8425ms step_avg:94.67ms
step:100/1770 train_time:8520ms step_avg:94.67ms
step:101/1770 train_time:8613ms step_avg:94.65ms
step:102/1770 train_time:8707ms step_avg:94.64ms
step:103/1770 train_time:8801ms step_avg:94.63ms
step:104/1770 train_time:8895ms step_avg:94.62ms
step:105/1770 train_time:8988ms step_avg:94.61ms
step:106/1770 train_time:9082ms step_avg:94.60ms
step:107/1770 train_time:9176ms step_avg:94.60ms
step:108/1770 train_time:9271ms step_avg:94.60ms
step:109/1770 train_time:9365ms step_avg:94.59ms
step:110/1770 train_time:9459ms step_avg:94.59ms
step:111/1770 train_time:9553ms step_avg:94.58ms
step:112/1770 train_time:9647ms step_avg:94.58ms
step:113/1770 train_time:9740ms step_avg:94.57ms
step:114/1770 train_time:9834ms step_avg:94.56ms
step:115/1770 train_time:9928ms step_avg:94.56ms
step:116/1770 train_time:10023ms step_avg:94.55ms
step:117/1770 train_time:10116ms step_avg:94.55ms
step:118/1770 train_time:10210ms step_avg:94.54ms
step:119/1770 train_time:10305ms step_avg:94.54ms
step:120/1770 train_time:10399ms step_avg:94.53ms
step:121/1770 train_time:10533ms step_avg:94.89ms
step:122/1770 train_time:10586ms step_avg:94.52ms
step:123/1770 train_time:10681ms step_avg:94.52ms
step:124/1770 train_time:10775ms step_avg:94.51ms
step:125/1770 train_time:10868ms step_avg:94.51ms
step:125/1770 val_loss:4.6516 train_time:10960ms step_avg:95.31ms
step:126/1770 train_time:10983ms step_avg:94.68ms
step:127/1770 train_time:11065ms step_avg:94.57ms
step:128/1770 train_time:11185ms step_avg:94.79ms
step:129/1770 train_time:11278ms step_avg:94.78ms
step:130/1770 train_time:11372ms step_avg:94.77ms
step:131/1770 train_time:11466ms step_avg:94.76ms
step:132/1770 train_time:11560ms step_avg:94.75ms
step:133/1770 train_time:11653ms step_avg:94.74ms
step:134/1770 train_time:11747ms step_avg:94.74ms
step:135/1770 train_time:11842ms step_avg:94.74ms
step:136/1770 train_time:11936ms step_avg:94.73ms
step:137/1770 train_time:12030ms step_avg:94.73ms
step:138/1770 train_time:12126ms step_avg:94.73ms
step:139/1770 train_time:12221ms step_avg:94.73ms
step:140/1770 train_time:12316ms step_avg:94.74ms
step:141/1770 train_time:12410ms step_avg:94.73ms
step:142/1770 train_time:12505ms step_avg:94.73ms
step:143/1770 train_time:12599ms step_avg:94.73ms
step:144/1770 train_time:12694ms step_avg:94.73ms
step:145/1770 train_time:12788ms step_avg:94.73ms
step:146/1770 train_time:12882ms step_avg:94.72ms
step:147/1770 train_time:12977ms step_avg:94.72ms
step:148/1770 train_time:13071ms step_avg:94.72ms
step:149/1770 train_time:13166ms step_avg:94.72ms
step:150/1770 train_time:13260ms step_avg:94.71ms
step:151/1770 train_time:13354ms step_avg:94.71ms
step:152/1770 train_time:13448ms step_avg:94.71ms
step:153/1770 train_time:13543ms step_avg:94.71ms
step:154/1770 train_time:13638ms step_avg:94.71ms
step:155/1770 train_time:13732ms step_avg:94.71ms
step:156/1770 train_time:13827ms step_avg:94.70ms
step:157/1770 train_time:13921ms step_avg:94.70ms
step:158/1770 train_time:14016ms step_avg:94.70ms
step:159/1770 train_time:14110ms step_avg:94.70ms
step:160/1770 train_time:14205ms step_avg:94.70ms
step:161/1770 train_time:14299ms step_avg:94.70ms
step:162/1770 train_time:14394ms step_avg:94.70ms
step:163/1770 train_time:14488ms step_avg:94.69ms
step:164/1770 train_time:14584ms step_avg:94.70ms
step:165/1770 train_time:14677ms step_avg:94.69ms
step:166/1770 train_time:14772ms step_avg:94.69ms
step:167/1770 train_time:14866ms step_avg:94.69ms
step:168/1770 train_time:14961ms step_avg:94.69ms
step:169/1770 train_time:15056ms step_avg:94.69ms
step:170/1770 train_time:15151ms step_avg:94.69ms
step:171/1770 train_time:15245ms step_avg:94.69ms
step:172/1770 train_time:15340ms step_avg:94.69ms
step:173/1770 train_time:15435ms step_avg:94.69ms
step:174/1770 train_time:15530ms step_avg:94.69ms
step:175/1770 train_time:15624ms step_avg:94.69ms
step:176/1770 train_time:15720ms step_avg:94.70ms
step:177/1770 train_time:15814ms step_avg:94.70ms
step:178/1770 train_time:15908ms step_avg:94.69ms
step:179/1770 train_time:16002ms step_avg:94.69ms
step:180/1770 train_time:16097ms step_avg:94.69ms
step:181/1770 train_time:16192ms step_avg:94.69ms
step:182/1770 train_time:16286ms step_avg:94.68ms
step:183/1770 train_time:16380ms step_avg:94.68ms
step:184/1770 train_time:16475ms step_avg:94.68ms
step:185/1770 train_time:16570ms step_avg:94.68ms
step:186/1770 train_time:16665ms step_avg:94.69ms
step:187/1770 train_time:16760ms step_avg:94.69ms
step:188/1770 train_time:16854ms step_avg:94.69ms
step:189/1770 train_time:16949ms step_avg:94.69ms
step:190/1770 train_time:17043ms step_avg:94.68ms
step:191/1770 train_time:17138ms step_avg:94.68ms
step:192/1770 train_time:17233ms step_avg:94.69ms
step:193/1770 train_time:17327ms step_avg:94.68ms
step:194/1770 train_time:17421ms step_avg:94.68ms
step:195/1770 train_time:17516ms step_avg:94.68ms
step:196/1770 train_time:17611ms step_avg:94.68ms
step:197/1770 train_time:17706ms step_avg:94.69ms
step:198/1770 train_time:17801ms step_avg:94.69ms
step:199/1770 train_time:17896ms step_avg:94.69ms
step:200/1770 train_time:17990ms step_avg:94.69ms
step:201/1770 train_time:18085ms step_avg:94.68ms
step:202/1770 train_time:18180ms step_avg:94.69ms
step:203/1770 train_time:18274ms step_avg:94.69ms
step:204/1770 train_time:18369ms step_avg:94.68ms
step:205/1770 train_time:18463ms step_avg:94.68ms
step:206/1770 train_time:18558ms step_avg:94.68ms
step:207/1770 train_time:18652ms step_avg:94.68ms
step:208/1770 train_time:18747ms step_avg:94.68ms
step:209/1770 train_time:18842ms step_avg:94.68ms
step:210/1770 train_time:18936ms step_avg:94.68ms
step:211/1770 train_time:19031ms step_avg:94.68ms
step:212/1770 train_time:19126ms step_avg:94.68ms
step:213/1770 train_time:19221ms step_avg:94.68ms
step:214/1770 train_time:19315ms step_avg:94.68ms
step:215/1770 train_time:19409ms step_avg:94.68ms
step:216/1770 train_time:19504ms step_avg:94.68ms
step:217/1770 train_time:19599ms step_avg:94.68ms
step:218/1770 train_time:19693ms step_avg:94.68ms
step:219/1770 train_time:19789ms step_avg:94.68ms
step:220/1770 train_time:19883ms step_avg:94.68ms
step:221/1770 train_time:19978ms step_avg:94.68ms
step:222/1770 train_time:20072ms step_avg:94.68ms
step:223/1770 train_time:20167ms step_avg:94.68ms
step:224/1770 train_time:20261ms step_avg:94.68ms
step:225/1770 train_time:20356ms step_avg:94.68ms
step:226/1770 train_time:20450ms step_avg:94.68ms
step:227/1770 train_time:20545ms step_avg:94.68ms
step:228/1770 train_time:20640ms step_avg:94.68ms
step:229/1770 train_time:20736ms step_avg:94.68ms
step:230/1770 train_time:20830ms step_avg:94.68ms
step:231/1770 train_time:20925ms step_avg:94.68ms
step:232/1770 train_time:21020ms step_avg:94.69ms
step:233/1770 train_time:21116ms step_avg:94.69ms
step:234/1770 train_time:21209ms step_avg:94.68ms
step:235/1770 train_time:21304ms step_avg:94.68ms
step:236/1770 train_time:21398ms step_avg:94.68ms
step:237/1770 train_time:21493ms step_avg:94.68ms
step:238/1770 train_time:21588ms step_avg:94.68ms
step:239/1770 train_time:21682ms step_avg:94.68ms
step:240/1770 train_time:21777ms step_avg:94.68ms
step:241/1770 train_time:21872ms step_avg:94.68ms
step:242/1770 train_time:21967ms step_avg:94.68ms
step:243/1770 train_time:22062ms step_avg:94.69ms
step:244/1770 train_time:22157ms step_avg:94.69ms
step:245/1770 train_time:22251ms step_avg:94.69ms
step:246/1770 train_time:22345ms step_avg:94.68ms
step:247/1770 train_time:22440ms step_avg:94.68ms
step:248/1770 train_time:22535ms step_avg:94.68ms
step:249/1770 train_time:22629ms step_avg:94.68ms
step:250/1770 train_time:22724ms step_avg:94.68ms
step:250/1770 val_loss:4.1072 train_time:22817ms step_avg:95.07ms
step:251/1770 train_time:22839ms step_avg:94.77ms
step:252/1770 train_time:22923ms step_avg:94.72ms
step:253/1770 train_time:23025ms step_avg:94.75ms
step:254/1770 train_time:23121ms step_avg:94.76ms
step:255/1770 train_time:23217ms step_avg:94.76ms
step:256/1770 train_time:23312ms step_avg:94.76ms
step:257/1770 train_time:23406ms step_avg:94.76ms
step:258/1770 train_time:23500ms step_avg:94.76ms
step:259/1770 train_time:23594ms step_avg:94.76ms
step:260/1770 train_time:23688ms step_avg:94.75ms
step:261/1770 train_time:23783ms step_avg:94.75ms
step:262/1770 train_time:23878ms step_avg:94.75ms
step:263/1770 train_time:23973ms step_avg:94.75ms
step:264/1770 train_time:24068ms step_avg:94.76ms
step:265/1770 train_time:24164ms step_avg:94.76ms
step:266/1770 train_time:24259ms step_avg:94.76ms
step:267/1770 train_time:24354ms step_avg:94.76ms
step:268/1770 train_time:24449ms step_avg:94.77ms
step:269/1770 train_time:24544ms step_avg:94.76ms
step:270/1770 train_time:24639ms step_avg:94.77ms
step:271/1770 train_time:24733ms step_avg:94.76ms
step:272/1770 train_time:24828ms step_avg:94.76ms
step:273/1770 train_time:24923ms step_avg:94.76ms
step:274/1770 train_time:25019ms step_avg:94.77ms
step:275/1770 train_time:25114ms step_avg:94.77ms
step:276/1770 train_time:25210ms step_avg:94.77ms
step:277/1770 train_time:25305ms step_avg:94.78ms
step:278/1770 train_time:25401ms step_avg:94.78ms
step:279/1770 train_time:25495ms step_avg:94.78ms
step:280/1770 train_time:25590ms step_avg:94.78ms
step:281/1770 train_time:25685ms step_avg:94.78ms
step:282/1770 train_time:25780ms step_avg:94.78ms
step:283/1770 train_time:25875ms step_avg:94.78ms
step:284/1770 train_time:25970ms step_avg:94.78ms
step:285/1770 train_time:26066ms step_avg:94.78ms
step:286/1770 train_time:26161ms step_avg:94.79ms
step:287/1770 train_time:26257ms step_avg:94.79ms
step:288/1770 train_time:26352ms step_avg:94.79ms
step:289/1770 train_time:26448ms step_avg:94.79ms
step:290/1770 train_time:26543ms step_avg:94.80ms
step:291/1770 train_time:26638ms step_avg:94.80ms
step:292/1770 train_time:26733ms step_avg:94.80ms
step:293/1770 train_time:26827ms step_avg:94.80ms
step:294/1770 train_time:26923ms step_avg:94.80ms
step:295/1770 train_time:27018ms step_avg:94.80ms
step:296/1770 train_time:27113ms step_avg:94.80ms
step:297/1770 train_time:27208ms step_avg:94.80ms
step:298/1770 train_time:27303ms step_avg:94.80ms
step:299/1770 train_time:27398ms step_avg:94.80ms
step:300/1770 train_time:27494ms step_avg:94.81ms
step:301/1770 train_time:27589ms step_avg:94.81ms
step:302/1770 train_time:27684ms step_avg:94.81ms
step:303/1770 train_time:27779ms step_avg:94.81ms
step:304/1770 train_time:27873ms step_avg:94.81ms
step:305/1770 train_time:27968ms step_avg:94.81ms
step:306/1770 train_time:28063ms step_avg:94.81ms
step:307/1770 train_time:28158ms step_avg:94.81ms
step:308/1770 train_time:28253ms step_avg:94.81ms
step:309/1770 train_time:28348ms step_avg:94.81ms
step:310/1770 train_time:28443ms step_avg:94.81ms
step:311/1770 train_time:28539ms step_avg:94.81ms
step:312/1770 train_time:28634ms step_avg:94.82ms
step:313/1770 train_time:28729ms step_avg:94.82ms
step:314/1770 train_time:28824ms step_avg:94.82ms
step:315/1770 train_time:28920ms step_avg:94.82ms
step:316/1770 train_time:29015ms step_avg:94.82ms
step:317/1770 train_time:29111ms step_avg:94.82ms
step:318/1770 train_time:29205ms step_avg:94.82ms
step:319/1770 train_time:29301ms step_avg:94.82ms
step:320/1770 train_time:29396ms step_avg:94.83ms
step:321/1770 train_time:29491ms step_avg:94.83ms
step:322/1770 train_time:29586ms step_avg:94.83ms
step:323/1770 train_time:29681ms step_avg:94.83ms
step:324/1770 train_time:29777ms step_avg:94.83ms
step:325/1770 train_time:29872ms step_avg:94.83ms
step:326/1770 train_time:29968ms step_avg:94.83ms
step:327/1770 train_time:30063ms step_avg:94.84ms
step:328/1770 train_time:30158ms step_avg:94.84ms
step:329/1770 train_time:30253ms step_avg:94.84ms
step:330/1770 train_time:30348ms step_avg:94.84ms
step:331/1770 train_time:30443ms step_avg:94.84ms
step:332/1770 train_time:30538ms step_avg:94.84ms
step:333/1770 train_time:30633ms step_avg:94.84ms
step:334/1770 train_time:30727ms step_avg:94.84ms
step:335/1770 train_time:30823ms step_avg:94.84ms
step:336/1770 train_time:30918ms step_avg:94.84ms
step:337/1770 train_time:31014ms step_avg:94.84ms
step:338/1770 train_time:31109ms step_avg:94.84ms
step:339/1770 train_time:31204ms step_avg:94.85ms
step:340/1770 train_time:31299ms step_avg:94.85ms
step:341/1770 train_time:31394ms step_avg:94.85ms
step:342/1770 train_time:31488ms step_avg:94.84ms
step:343/1770 train_time:31583ms step_avg:94.84ms
step:344/1770 train_time:31678ms step_avg:94.84ms
step:345/1770 train_time:31773ms step_avg:94.84ms
step:346/1770 train_time:31868ms step_avg:94.85ms
step:347/1770 train_time:31963ms step_avg:94.84ms
step:348/1770 train_time:32058ms step_avg:94.85ms
step:349/1770 train_time:32153ms step_avg:94.85ms
step:350/1770 train_time:32248ms step_avg:94.85ms
step:351/1770 train_time:32343ms step_avg:94.85ms
step:352/1770 train_time:32438ms step_avg:94.85ms
step:353/1770 train_time:32533ms step_avg:94.85ms
step:354/1770 train_time:32628ms step_avg:94.85ms
step:355/1770 train_time:32724ms step_avg:94.85ms
step:356/1770 train_time:32820ms step_avg:94.86ms
step:357/1770 train_time:32915ms step_avg:94.86ms
step:358/1770 train_time:33011ms step_avg:94.86ms
step:359/1770 train_time:33106ms step_avg:94.86ms
step:360/1770 train_time:33201ms step_avg:94.86ms
step:361/1770 train_time:33296ms step_avg:94.86ms
step:362/1770 train_time:33392ms step_avg:94.86ms
step:363/1770 train_time:33486ms step_avg:94.86ms
step:364/1770 train_time:33582ms step_avg:94.86ms
step:365/1770 train_time:33677ms step_avg:94.87ms
step:366/1770 train_time:33772ms step_avg:94.87ms
step:367/1770 train_time:33867ms step_avg:94.87ms
step:368/1770 train_time:33962ms step_avg:94.87ms
step:369/1770 train_time:34058ms step_avg:94.87ms
step:370/1770 train_time:34154ms step_avg:94.87ms
step:371/1770 train_time:34248ms step_avg:94.87ms
step:372/1770 train_time:34343ms step_avg:94.87ms
step:373/1770 train_time:34438ms step_avg:94.87ms
step:374/1770 train_time:34534ms step_avg:94.87ms
step:375/1770 train_time:34628ms step_avg:94.87ms
step:375/1770 val_loss:3.9049 train_time:34722ms step_avg:95.13ms
step:376/1770 train_time:34744ms step_avg:94.93ms
step:377/1770 train_time:34829ms step_avg:94.90ms
step:378/1770 train_time:34927ms step_avg:94.91ms
step:379/1770 train_time:35023ms step_avg:94.91ms
step:380/1770 train_time:35117ms step_avg:94.91ms
step:381/1770 train_time:35212ms step_avg:94.91ms
step:382/1770 train_time:35307ms step_avg:94.91ms
step:383/1770 train_time:35401ms step_avg:94.91ms
step:384/1770 train_time:35496ms step_avg:94.91ms
step:385/1770 train_time:35591ms step_avg:94.91ms
step:386/1770 train_time:35685ms step_avg:94.91ms
step:387/1770 train_time:35780ms step_avg:94.91ms
step:388/1770 train_time:35876ms step_avg:94.91ms
step:389/1770 train_time:35972ms step_avg:94.91ms
step:390/1770 train_time:36067ms step_avg:94.91ms
step:391/1770 train_time:36162ms step_avg:94.91ms
step:392/1770 train_time:36257ms step_avg:94.91ms
step:393/1770 train_time:36352ms step_avg:94.91ms
step:394/1770 train_time:36446ms step_avg:94.91ms
step:395/1770 train_time:36541ms step_avg:94.91ms
step:396/1770 train_time:36638ms step_avg:94.92ms
step:397/1770 train_time:36735ms step_avg:94.92ms
step:398/1770 train_time:36832ms step_avg:94.93ms
step:399/1770 train_time:36929ms step_avg:94.93ms
step:400/1770 train_time:37026ms step_avg:94.94ms
step:401/1770 train_time:37123ms step_avg:94.94ms
step:402/1770 train_time:37220ms step_avg:94.95ms
step:403/1770 train_time:37317ms step_avg:94.95ms
step:404/1770 train_time:37413ms step_avg:94.96ms
step:405/1770 train_time:37510ms step_avg:94.96ms
step:406/1770 train_time:37606ms step_avg:94.97ms
step:407/1770 train_time:37703ms step_avg:94.97ms
step:408/1770 train_time:37799ms step_avg:94.97ms
step:409/1770 train_time:37896ms step_avg:94.98ms
step:410/1770 train_time:37993ms step_avg:94.98ms
step:411/1770 train_time:38091ms step_avg:94.99ms
step:412/1770 train_time:38189ms step_avg:95.00ms
step:413/1770 train_time:38287ms step_avg:95.00ms
step:414/1770 train_time:38384ms step_avg:95.01ms
step:415/1770 train_time:38481ms step_avg:95.01ms
step:416/1770 train_time:38577ms step_avg:95.02ms
step:417/1770 train_time:38674ms step_avg:95.02ms
step:418/1770 train_time:38771ms step_avg:95.03ms
step:419/1770 train_time:38868ms step_avg:95.03ms
step:420/1770 train_time:38965ms step_avg:95.04ms
step:421/1770 train_time:39062ms step_avg:95.04ms
step:422/1770 train_time:39159ms step_avg:95.05ms
step:423/1770 train_time:39256ms step_avg:95.05ms
step:424/1770 train_time:39354ms step_avg:95.06ms
step:425/1770 train_time:39451ms step_avg:95.06ms
step:426/1770 train_time:39549ms step_avg:95.07ms
step:427/1770 train_time:39645ms step_avg:95.07ms
step:428/1770 train_time:39742ms step_avg:95.08ms
step:429/1770 train_time:39838ms step_avg:95.08ms
step:430/1770 train_time:39935ms step_avg:95.08ms
step:431/1770 train_time:40033ms step_avg:95.09ms
step:432/1770 train_time:40131ms step_avg:95.10ms
step:433/1770 train_time:40228ms step_avg:95.10ms
step:434/1770 train_time:40324ms step_avg:95.10ms
step:435/1770 train_time:40421ms step_avg:95.11ms
step:436/1770 train_time:40518ms step_avg:95.11ms
step:437/1770 train_time:40615ms step_avg:95.12ms
step:438/1770 train_time:40712ms step_avg:95.12ms
step:439/1770 train_time:40809ms step_avg:95.13ms
step:440/1770 train_time:40906ms step_avg:95.13ms
step:441/1770 train_time:41003ms step_avg:95.13ms
step:442/1770 train_time:41100ms step_avg:95.14ms
step:443/1770 train_time:41197ms step_avg:95.14ms
step:444/1770 train_time:41294ms step_avg:95.15ms
step:445/1770 train_time:41391ms step_avg:95.15ms
step:446/1770 train_time:41489ms step_avg:95.16ms
step:447/1770 train_time:41585ms step_avg:95.16ms
step:448/1770 train_time:41682ms step_avg:95.16ms
step:449/1770 train_time:41778ms step_avg:95.17ms
step:450/1770 train_time:41875ms step_avg:95.17ms
step:451/1770 train_time:41972ms step_avg:95.18ms
step:452/1770 train_time:42070ms step_avg:95.18ms
step:453/1770 train_time:42167ms step_avg:95.19ms
step:454/1770 train_time:42264ms step_avg:95.19ms
step:455/1770 train_time:42361ms step_avg:95.19ms
step:456/1770 train_time:42458ms step_avg:95.20ms
step:457/1770 train_time:42556ms step_avg:95.20ms
step:458/1770 train_time:42653ms step_avg:95.21ms
step:459/1770 train_time:42750ms step_avg:95.21ms
step:460/1770 train_time:42847ms step_avg:95.22ms
step:461/1770 train_time:42944ms step_avg:95.22ms
step:462/1770 train_time:43040ms step_avg:95.22ms
step:463/1770 train_time:43137ms step_avg:95.23ms
step:464/1770 train_time:43234ms step_avg:95.23ms
step:465/1770 train_time:43332ms step_avg:95.23ms
step:466/1770 train_time:43429ms step_avg:95.24ms
step:467/1770 train_time:43526ms step_avg:95.24ms
step:468/1770 train_time:43623ms step_avg:95.25ms
step:469/1770 train_time:43720ms step_avg:95.25ms
step:470/1770 train_time:43817ms step_avg:95.25ms
step:471/1770 train_time:43914ms step_avg:95.26ms
step:472/1770 train_time:44011ms step_avg:95.26ms
step:473/1770 train_time:44108ms step_avg:95.27ms
step:474/1770 train_time:44205ms step_avg:95.27ms
step:475/1770 train_time:44301ms step_avg:95.27ms
step:476/1770 train_time:44399ms step_avg:95.28ms
step:477/1770 train_time:44496ms step_avg:95.28ms
step:478/1770 train_time:44593ms step_avg:95.28ms
step:479/1770 train_time:44690ms step_avg:95.29ms
step:480/1770 train_time:44787ms step_avg:95.29ms
step:481/1770 train_time:44884ms step_avg:95.29ms
step:482/1770 train_time:44981ms step_avg:95.30ms
step:483/1770 train_time:45078ms step_avg:95.30ms
step:484/1770 train_time:45174ms step_avg:95.30ms
step:485/1770 train_time:45271ms step_avg:95.31ms
step:486/1770 train_time:45368ms step_avg:95.31ms
step:487/1770 train_time:45466ms step_avg:95.32ms
step:488/1770 train_time:45563ms step_avg:95.32ms
step:489/1770 train_time:45660ms step_avg:95.32ms
step:490/1770 train_time:45758ms step_avg:95.33ms
step:491/1770 train_time:45855ms step_avg:95.33ms
step:492/1770 train_time:45952ms step_avg:95.34ms
step:493/1770 train_time:46049ms step_avg:95.34ms
step:494/1770 train_time:46146ms step_avg:95.34ms
step:495/1770 train_time:46243ms step_avg:95.35ms
step:496/1770 train_time:46340ms step_avg:95.35ms
step:497/1770 train_time:46437ms step_avg:95.35ms
step:498/1770 train_time:46534ms step_avg:95.36ms
step:499/1770 train_time:46631ms step_avg:95.36ms
step:500/1770 train_time:46729ms step_avg:95.37ms
step:500/1770 val_loss:3.7518 train_time:46825ms step_avg:95.56ms
step:501/1770 train_time:46847ms step_avg:95.41ms
step:502/1770 train_time:46932ms step_avg:95.39ms
step:503/1770 train_time:47031ms step_avg:95.40ms
step:504/1770 train_time:47128ms step_avg:95.40ms
step:505/1770 train_time:47225ms step_avg:95.40ms
step:506/1770 train_time:47322ms step_avg:95.41ms
step:507/1770 train_time:47418ms step_avg:95.41ms
step:508/1770 train_time:47515ms step_avg:95.41ms
step:509/1770 train_time:47612ms step_avg:95.41ms
step:510/1770 train_time:47708ms step_avg:95.42ms
step:511/1770 train_time:47805ms step_avg:95.42ms
step:512/1770 train_time:47902ms step_avg:95.42ms
step:513/1770 train_time:48000ms step_avg:95.43ms
step:514/1770 train_time:48097ms step_avg:95.43ms
step:515/1770 train_time:48194ms step_avg:95.43ms
step:516/1770 train_time:48291ms step_avg:95.44ms
step:517/1770 train_time:48387ms step_avg:95.44ms
step:518/1770 train_time:48484ms step_avg:95.44ms
step:519/1770 train_time:48581ms step_avg:95.44ms
step:520/1770 train_time:48678ms step_avg:95.45ms
step:521/1770 train_time:48775ms step_avg:95.45ms
step:522/1770 train_time:48872ms step_avg:95.45ms
step:523/1770 train_time:48969ms step_avg:95.46ms
step:524/1770 train_time:49067ms step_avg:95.46ms
step:525/1770 train_time:49164ms step_avg:95.46ms
step:526/1770 train_time:49262ms step_avg:95.47ms
step:527/1770 train_time:49359ms step_avg:95.47ms
step:528/1770 train_time:49456ms step_avg:95.48ms
step:529/1770 train_time:49554ms step_avg:95.48ms
step:530/1770 train_time:49651ms step_avg:95.48ms
step:531/1770 train_time:49748ms step_avg:95.49ms
step:532/1770 train_time:49846ms step_avg:95.49ms
step:533/1770 train_time:49943ms step_avg:95.49ms
step:534/1770 train_time:50041ms step_avg:95.50ms
step:535/1770 train_time:50138ms step_avg:95.50ms
step:536/1770 train_time:50235ms step_avg:95.50ms
step:537/1770 train_time:50332ms step_avg:95.51ms
step:538/1770 train_time:50430ms step_avg:95.51ms
step:539/1770 train_time:50528ms step_avg:95.52ms
step:540/1770 train_time:50625ms step_avg:95.52ms
step:541/1770 train_time:50723ms step_avg:95.52ms
step:542/1770 train_time:50821ms step_avg:95.53ms
step:543/1770 train_time:50918ms step_avg:95.53ms
step:544/1770 train_time:51015ms step_avg:95.53ms
step:545/1770 train_time:51113ms step_avg:95.54ms
step:546/1770 train_time:51210ms step_avg:95.54ms
step:547/1770 train_time:51307ms step_avg:95.54ms
step:548/1770 train_time:51405ms step_avg:95.55ms
step:549/1770 train_time:51502ms step_avg:95.55ms
step:550/1770 train_time:51599ms step_avg:95.55ms
step:551/1770 train_time:51696ms step_avg:95.56ms
step:552/1770 train_time:51793ms step_avg:95.56ms
step:553/1770 train_time:51890ms step_avg:95.56ms
step:554/1770 train_time:51988ms step_avg:95.57ms
step:555/1770 train_time:52085ms step_avg:95.57ms
step:556/1770 train_time:52183ms step_avg:95.57ms
step:557/1770 train_time:52281ms step_avg:95.58ms
step:558/1770 train_time:52378ms step_avg:95.58ms
step:559/1770 train_time:52475ms step_avg:95.58ms
step:560/1770 train_time:52573ms step_avg:95.59ms
step:561/1770 train_time:52670ms step_avg:95.59ms
step:562/1770 train_time:52767ms step_avg:95.59ms
step:563/1770 train_time:52864ms step_avg:95.59ms
step:564/1770 train_time:52962ms step_avg:95.60ms
step:565/1770 train_time:53059ms step_avg:95.60ms
step:566/1770 train_time:53157ms step_avg:95.61ms
step:567/1770 train_time:53255ms step_avg:95.61ms
step:568/1770 train_time:53352ms step_avg:95.61ms
step:569/1770 train_time:53450ms step_avg:95.62ms
step:570/1770 train_time:53547ms step_avg:95.62ms
step:571/1770 train_time:53645ms step_avg:95.62ms
step:572/1770 train_time:53742ms step_avg:95.63ms
step:573/1770 train_time:53839ms step_avg:95.63ms
step:574/1770 train_time:53936ms step_avg:95.63ms
step:575/1770 train_time:54033ms step_avg:95.63ms
step:576/1770 train_time:54131ms step_avg:95.64ms
step:577/1770 train_time:54229ms step_avg:95.64ms
step:578/1770 train_time:54327ms step_avg:95.65ms
step:579/1770 train_time:54424ms step_avg:95.65ms
step:580/1770 train_time:54522ms step_avg:95.65ms
step:581/1770 train_time:54620ms step_avg:95.66ms
step:582/1770 train_time:54717ms step_avg:95.66ms
step:583/1770 train_time:54814ms step_avg:95.66ms
step:584/1770 train_time:54911ms step_avg:95.66ms
step:585/1770 train_time:55009ms step_avg:95.67ms
step:586/1770 train_time:55106ms step_avg:95.67ms
step:587/1770 train_time:55204ms step_avg:95.68ms
step:588/1770 train_time:55303ms step_avg:95.68ms
step:589/1770 train_time:55400ms step_avg:95.68ms
step:590/1770 train_time:55498ms step_avg:95.69ms
step:591/1770 train_time:55595ms step_avg:95.69ms
step:592/1770 train_time:55692ms step_avg:95.69ms
step:593/1770 train_time:55789ms step_avg:95.69ms
step:594/1770 train_time:55886ms step_avg:95.70ms
step:595/1770 train_time:55984ms step_avg:95.70ms
step:596/1770 train_time:56082ms step_avg:95.70ms
step:597/1770 train_time:56179ms step_avg:95.71ms
step:598/1770 train_time:56277ms step_avg:95.71ms
step:599/1770 train_time:56374ms step_avg:95.71ms
step:600/1770 train_time:56471ms step_avg:95.71ms
step:601/1770 train_time:56568ms step_avg:95.72ms
step:602/1770 train_time:56666ms step_avg:95.72ms
step:603/1770 train_time:56763ms step_avg:95.72ms
step:604/1770 train_time:56861ms step_avg:95.73ms
step:605/1770 train_time:56958ms step_avg:95.73ms
step:606/1770 train_time:57056ms step_avg:95.73ms
step:607/1770 train_time:57153ms step_avg:95.73ms
step:608/1770 train_time:57251ms step_avg:95.74ms
step:609/1770 train_time:57349ms step_avg:95.74ms
step:610/1770 train_time:57446ms step_avg:95.74ms
step:611/1770 train_time:57544ms step_avg:95.75ms
step:612/1770 train_time:57642ms step_avg:95.75ms
step:613/1770 train_time:57739ms step_avg:95.75ms
step:614/1770 train_time:57836ms step_avg:95.75ms
step:615/1770 train_time:57933ms step_avg:95.76ms
step:616/1770 train_time:58031ms step_avg:95.76ms
step:617/1770 train_time:58128ms step_avg:95.76ms
step:618/1770 train_time:58226ms step_avg:95.77ms
step:619/1770 train_time:58324ms step_avg:95.77ms
step:620/1770 train_time:58422ms step_avg:95.77ms
step:621/1770 train_time:58519ms step_avg:95.78ms
step:622/1770 train_time:58617ms step_avg:95.78ms
step:623/1770 train_time:58714ms step_avg:95.78ms
step:624/1770 train_time:58812ms step_avg:95.78ms
step:625/1770 train_time:58909ms step_avg:95.79ms
step:625/1770 val_loss:3.6653 train_time:59005ms step_avg:95.94ms
step:626/1770 train_time:59027ms step_avg:95.82ms
step:627/1770 train_time:59112ms step_avg:95.80ms
step:628/1770 train_time:59211ms step_avg:95.81ms
step:629/1770 train_time:59310ms step_avg:95.82ms
step:630/1770 train_time:59407ms step_avg:95.82ms
step:631/1770 train_time:59504ms step_avg:95.82ms
step:632/1770 train_time:59601ms step_avg:95.82ms
step:633/1770 train_time:59698ms step_avg:95.82ms
step:634/1770 train_time:59794ms step_avg:95.82ms
step:635/1770 train_time:59892ms step_avg:95.83ms
step:636/1770 train_time:59989ms step_avg:95.83ms
step:637/1770 train_time:60086ms step_avg:95.83ms
step:638/1770 train_time:60185ms step_avg:95.84ms
step:639/1770 train_time:60283ms step_avg:95.84ms
step:640/1770 train_time:60381ms step_avg:95.84ms
step:641/1770 train_time:60478ms step_avg:95.84ms
step:642/1770 train_time:60575ms step_avg:95.85ms
step:643/1770 train_time:60672ms step_avg:95.85ms
step:644/1770 train_time:60769ms step_avg:95.85ms
step:645/1770 train_time:60867ms step_avg:95.85ms
step:646/1770 train_time:60963ms step_avg:95.85ms
step:647/1770 train_time:61061ms step_avg:95.86ms
step:648/1770 train_time:61159ms step_avg:95.86ms
step:649/1770 train_time:61257ms step_avg:95.86ms
step:650/1770 train_time:61355ms step_avg:95.87ms
step:651/1770 train_time:61452ms step_avg:95.87ms
step:652/1770 train_time:61550ms step_avg:95.87ms
step:653/1770 train_time:61647ms step_avg:95.87ms
step:654/1770 train_time:61744ms step_avg:95.88ms
step:655/1770 train_time:61841ms step_avg:95.88ms
step:656/1770 train_time:61939ms step_avg:95.88ms
step:657/1770 train_time:62035ms step_avg:95.88ms
step:658/1770 train_time:62134ms step_avg:95.89ms
step:659/1770 train_time:62233ms step_avg:95.89ms
step:660/1770 train_time:62332ms step_avg:95.90ms
step:661/1770 train_time:62431ms step_avg:95.90ms
step:662/1770 train_time:62531ms step_avg:95.91ms
step:663/1770 train_time:62632ms step_avg:95.91ms
step:664/1770 train_time:62732ms step_avg:95.92ms
step:665/1770 train_time:62832ms step_avg:95.93ms
step:666/1770 train_time:62931ms step_avg:95.93ms
step:667/1770 train_time:63032ms step_avg:95.94ms
step:668/1770 train_time:63132ms step_avg:95.94ms
step:669/1770 train_time:63231ms step_avg:95.95ms
step:670/1770 train_time:63330ms step_avg:95.96ms
step:671/1770 train_time:63429ms step_avg:95.96ms
step:672/1770 train_time:63529ms step_avg:95.96ms
step:673/1770 train_time:63627ms step_avg:95.97ms
step:674/1770 train_time:63726ms step_avg:95.97ms
step:675/1770 train_time:63825ms step_avg:95.98ms
step:676/1770 train_time:63924ms step_avg:95.98ms
step:677/1770 train_time:64024ms step_avg:95.99ms
step:678/1770 train_time:64124ms step_avg:95.99ms
step:679/1770 train_time:64223ms step_avg:96.00ms
step:680/1770 train_time:64323ms step_avg:96.00ms
step:681/1770 train_time:64422ms step_avg:96.01ms
step:682/1770 train_time:64521ms step_avg:96.01ms
step:683/1770 train_time:64620ms step_avg:96.02ms
step:684/1770 train_time:64719ms step_avg:96.02ms
step:685/1770 train_time:64818ms step_avg:96.03ms
step:686/1770 train_time:64917ms step_avg:96.03ms
step:687/1770 train_time:65016ms step_avg:96.04ms
step:688/1770 train_time:65115ms step_avg:96.04ms
step:689/1770 train_time:65215ms step_avg:96.05ms
step:690/1770 train_time:65315ms step_avg:96.05ms
step:691/1770 train_time:65414ms step_avg:96.06ms
step:692/1770 train_time:65513ms step_avg:96.06ms
step:693/1770 train_time:65613ms step_avg:96.07ms
step:694/1770 train_time:65712ms step_avg:96.07ms
step:695/1770 train_time:65811ms step_avg:96.07ms
step:696/1770 train_time:65910ms step_avg:96.08ms
step:697/1770 train_time:66010ms step_avg:96.08ms
step:698/1770 train_time:66110ms step_avg:96.09ms
step:699/1770 train_time:66210ms step_avg:96.10ms
step:700/1770 train_time:66309ms step_avg:96.10ms
step:701/1770 train_time:66408ms step_avg:96.10ms
step:702/1770 train_time:66507ms step_avg:96.11ms
step:703/1770 train_time:66606ms step_avg:96.11ms
step:704/1770 train_time:66705ms step_avg:96.12ms
step:705/1770 train_time:66804ms step_avg:96.12ms
step:706/1770 train_time:66903ms step_avg:96.13ms
step:707/1770 train_time:67003ms step_avg:96.13ms
step:708/1770 train_time:67102ms step_avg:96.13ms
step:709/1770 train_time:67202ms step_avg:96.14ms
step:710/1770 train_time:67300ms step_avg:96.14ms
step:711/1770 train_time:67400ms step_avg:96.15ms
step:712/1770 train_time:67499ms step_avg:96.15ms
step:713/1770 train_time:67598ms step_avg:96.16ms
step:714/1770 train_time:67696ms step_avg:96.16ms
step:715/1770 train_time:67796ms step_avg:96.16ms
step:716/1770 train_time:67895ms step_avg:96.17ms
step:717/1770 train_time:67994ms step_avg:96.17ms
step:718/1770 train_time:68093ms step_avg:96.18ms
step:719/1770 train_time:68192ms step_avg:96.18ms
step:720/1770 train_time:68291ms step_avg:96.19ms
step:721/1770 train_time:68391ms step_avg:96.19ms
step:722/1770 train_time:68490ms step_avg:96.19ms
step:723/1770 train_time:68589ms step_avg:96.20ms
step:724/1770 train_time:68688ms step_avg:96.20ms
step:725/1770 train_time:68787ms step_avg:96.21ms
step:726/1770 train_time:68886ms step_avg:96.21ms
step:727/1770 train_time:68986ms step_avg:96.21ms
step:728/1770 train_time:69084ms step_avg:96.22ms
step:729/1770 train_time:69184ms step_avg:96.22ms
step:730/1770 train_time:69283ms step_avg:96.23ms
step:731/1770 train_time:69383ms step_avg:96.23ms
step:732/1770 train_time:69483ms step_avg:96.24ms
step:733/1770 train_time:69583ms step_avg:96.24ms
step:734/1770 train_time:69682ms step_avg:96.25ms
step:735/1770 train_time:69781ms step_avg:96.25ms
step:736/1770 train_time:69880ms step_avg:96.25ms
step:737/1770 train_time:69979ms step_avg:96.26ms
step:738/1770 train_time:70078ms step_avg:96.26ms
step:739/1770 train_time:70177ms step_avg:96.26ms
step:740/1770 train_time:70276ms step_avg:96.27ms
step:741/1770 train_time:70375ms step_avg:96.27ms
step:742/1770 train_time:70474ms step_avg:96.28ms
step:743/1770 train_time:70574ms step_avg:96.28ms
step:744/1770 train_time:70672ms step_avg:96.28ms
step:745/1770 train_time:70772ms step_avg:96.29ms
step:746/1770 train_time:70871ms step_avg:96.29ms
step:747/1770 train_time:70970ms step_avg:96.30ms
step:748/1770 train_time:71069ms step_avg:96.30ms
step:749/1770 train_time:71168ms step_avg:96.30ms
step:750/1770 train_time:71267ms step_avg:96.31ms
step:750/1770 val_loss:3.6028 train_time:71365ms step_avg:96.44ms
step:751/1770 train_time:71386ms step_avg:96.34ms
step:752/1770 train_time:71476ms step_avg:96.33ms
step:753/1770 train_time:71576ms step_avg:96.33ms
step:754/1770 train_time:71675ms step_avg:96.34ms
step:755/1770 train_time:71774ms step_avg:96.34ms
step:756/1770 train_time:71872ms step_avg:96.34ms
step:757/1770 train_time:71970ms step_avg:96.35ms
step:758/1770 train_time:72069ms step_avg:96.35ms
step:759/1770 train_time:72167ms step_avg:96.35ms
step:760/1770 train_time:72266ms step_avg:96.35ms
step:761/1770 train_time:72365ms step_avg:96.36ms
step:762/1770 train_time:72464ms step_avg:96.36ms
step:763/1770 train_time:72564ms step_avg:96.37ms
step:764/1770 train_time:72664ms step_avg:96.37ms
step:765/1770 train_time:72765ms step_avg:96.38ms
step:766/1770 train_time:72865ms step_avg:96.38ms
step:767/1770 train_time:72965ms step_avg:96.39ms
step:768/1770 train_time:73064ms step_avg:96.39ms
step:769/1770 train_time:73164ms step_avg:96.40ms
step:770/1770 train_time:73262ms step_avg:96.40ms
step:771/1770 train_time:73362ms step_avg:96.40ms
step:772/1770 train_time:73461ms step_avg:96.41ms
step:773/1770 train_time:73560ms step_avg:96.41ms
step:774/1770 train_time:73660ms step_avg:96.41ms
step:775/1770 train_time:73759ms step_avg:96.42ms
step:776/1770 train_time:73859ms step_avg:96.42ms
step:777/1770 train_time:73959ms step_avg:96.43ms
step:778/1770 train_time:74058ms step_avg:96.43ms
step:779/1770 train_time:74158ms step_avg:96.43ms
step:780/1770 train_time:74257ms step_avg:96.44ms
step:781/1770 train_time:74356ms step_avg:96.44ms
step:782/1770 train_time:74456ms step_avg:96.45ms
step:783/1770 train_time:74555ms step_avg:96.45ms
step:784/1770 train_time:74653ms step_avg:96.45ms
step:785/1770 train_time:74753ms step_avg:96.46ms
step:786/1770 train_time:74852ms step_avg:96.46ms
step:787/1770 train_time:74951ms step_avg:96.46ms
step:788/1770 train_time:75050ms step_avg:96.47ms
step:789/1770 train_time:75150ms step_avg:96.47ms
step:790/1770 train_time:75250ms step_avg:96.47ms
step:791/1770 train_time:75350ms step_avg:96.48ms
step:792/1770 train_time:75450ms step_avg:96.48ms
step:793/1770 train_time:75550ms step_avg:96.49ms
step:794/1770 train_time:75650ms step_avg:96.49ms
step:795/1770 train_time:75750ms step_avg:96.50ms
step:796/1770 train_time:75849ms step_avg:96.50ms
step:797/1770 train_time:75949ms step_avg:96.50ms
step:798/1770 train_time:76048ms step_avg:96.51ms
step:799/1770 train_time:76147ms step_avg:96.51ms
step:800/1770 train_time:76247ms step_avg:96.51ms
step:801/1770 train_time:76346ms step_avg:96.52ms
step:802/1770 train_time:76446ms step_avg:96.52ms
step:803/1770 train_time:76546ms step_avg:96.53ms
step:804/1770 train_time:76646ms step_avg:96.53ms
step:805/1770 train_time:76746ms step_avg:96.54ms
step:806/1770 train_time:76845ms step_avg:96.54ms
step:807/1770 train_time:76945ms step_avg:96.54ms
step:808/1770 train_time:77044ms step_avg:96.55ms
step:809/1770 train_time:77144ms step_avg:96.55ms
step:810/1770 train_time:77244ms step_avg:96.56ms
step:811/1770 train_time:77343ms step_avg:96.56ms
step:812/1770 train_time:77443ms step_avg:96.56ms
step:813/1770 train_time:77542ms step_avg:96.57ms
step:814/1770 train_time:77642ms step_avg:96.57ms
step:815/1770 train_time:77741ms step_avg:96.57ms
step:816/1770 train_time:77841ms step_avg:96.58ms
step:817/1770 train_time:77940ms step_avg:96.58ms
step:818/1770 train_time:78039ms step_avg:96.58ms
step:819/1770 train_time:78138ms step_avg:96.59ms
step:820/1770 train_time:78238ms step_avg:96.59ms
step:821/1770 train_time:78337ms step_avg:96.59ms
step:822/1770 train_time:78437ms step_avg:96.60ms
step:823/1770 train_time:78536ms step_avg:96.60ms
step:824/1770 train_time:78636ms step_avg:96.60ms
step:825/1770 train_time:78735ms step_avg:96.61ms
step:826/1770 train_time:78834ms step_avg:96.61ms
step:827/1770 train_time:78933ms step_avg:96.61ms
step:828/1770 train_time:79032ms step_avg:96.62ms
step:829/1770 train_time:79131ms step_avg:96.62ms
step:830/1770 train_time:79231ms step_avg:96.62ms
step:831/1770 train_time:79330ms step_avg:96.63ms
step:832/1770 train_time:79430ms step_avg:96.63ms
step:833/1770 train_time:79530ms step_avg:96.63ms
step:834/1770 train_time:79629ms step_avg:96.64ms
step:835/1770 train_time:79728ms step_avg:96.64ms
step:836/1770 train_time:79828ms step_avg:96.64ms
step:837/1770 train_time:79927ms step_avg:96.65ms
step:838/1770 train_time:80027ms step_avg:96.65ms
step:839/1770 train_time:80127ms step_avg:96.65ms
step:840/1770 train_time:80226ms step_avg:96.66ms
step:841/1770 train_time:80326ms step_avg:96.66ms
step:842/1770 train_time:80425ms step_avg:96.66ms
step:843/1770 train_time:80525ms step_avg:96.67ms
step:844/1770 train_time:80625ms step_avg:96.67ms
step:845/1770 train_time:80724ms step_avg:96.68ms
step:846/1770 train_time:80823ms step_avg:96.68ms
step:847/1770 train_time:80922ms step_avg:96.68ms
step:848/1770 train_time:81021ms step_avg:96.68ms
step:849/1770 train_time:81121ms step_avg:96.69ms
step:850/1770 train_time:81220ms step_avg:96.69ms
step:851/1770 train_time:81319ms step_avg:96.69ms
step:852/1770 train_time:81418ms step_avg:96.70ms
step:853/1770 train_time:81518ms step_avg:96.70ms
step:854/1770 train_time:81618ms step_avg:96.70ms
step:855/1770 train_time:81717ms step_avg:96.71ms
step:856/1770 train_time:81817ms step_avg:96.71ms
step:857/1770 train_time:81917ms step_avg:96.71ms
step:858/1770 train_time:82017ms step_avg:96.72ms
step:859/1770 train_time:82117ms step_avg:96.72ms
step:860/1770 train_time:82215ms step_avg:96.72ms
step:861/1770 train_time:82315ms step_avg:96.73ms
step:862/1770 train_time:82415ms step_avg:96.73ms
step:863/1770 train_time:82513ms step_avg:96.73ms
step:864/1770 train_time:82613ms step_avg:96.74ms
step:865/1770 train_time:82712ms step_avg:96.74ms
step:866/1770 train_time:82812ms step_avg:96.74ms
step:867/1770 train_time:82912ms step_avg:96.75ms
step:868/1770 train_time:83012ms step_avg:96.75ms
step:869/1770 train_time:83112ms step_avg:96.75ms
step:870/1770 train_time:83211ms step_avg:96.76ms
step:871/1770 train_time:83311ms step_avg:96.76ms
step:872/1770 train_time:83411ms step_avg:96.76ms
step:873/1770 train_time:83510ms step_avg:96.77ms
step:874/1770 train_time:83610ms step_avg:96.77ms
step:875/1770 train_time:83709ms step_avg:96.77ms
step:875/1770 val_loss:3.5518 train_time:83807ms step_avg:96.89ms
step:876/1770 train_time:83829ms step_avg:96.80ms
step:877/1770 train_time:83920ms step_avg:96.79ms
step:878/1770 train_time:84020ms step_avg:96.80ms
step:879/1770 train_time:84119ms step_avg:96.80ms
step:880/1770 train_time:84217ms step_avg:96.80ms
step:881/1770 train_time:84316ms step_avg:96.80ms
step:882/1770 train_time:84415ms step_avg:96.81ms
step:883/1770 train_time:84513ms step_avg:96.81ms
step:884/1770 train_time:84612ms step_avg:96.81ms
step:885/1770 train_time:84711ms step_avg:96.81ms
step:886/1770 train_time:84810ms step_avg:96.82ms
step:887/1770 train_time:84911ms step_avg:96.82ms
step:888/1770 train_time:85011ms step_avg:96.82ms
step:889/1770 train_time:85111ms step_avg:96.83ms
step:890/1770 train_time:85210ms step_avg:96.83ms
step:891/1770 train_time:85310ms step_avg:96.83ms
step:892/1770 train_time:85410ms step_avg:96.84ms
step:893/1770 train_time:85510ms step_avg:96.84ms
step:894/1770 train_time:85610ms step_avg:96.84ms
step:895/1770 train_time:85710ms step_avg:96.85ms
step:896/1770 train_time:85809ms step_avg:96.85ms
step:897/1770 train_time:85908ms step_avg:96.85ms
step:898/1770 train_time:86008ms step_avg:96.86ms
step:899/1770 train_time:86107ms step_avg:96.86ms
step:900/1770 train_time:86206ms step_avg:96.86ms
step:901/1770 train_time:86305ms step_avg:96.86ms
step:902/1770 train_time:86405ms step_avg:96.87ms
step:903/1770 train_time:86505ms step_avg:96.87ms
step:904/1770 train_time:86604ms step_avg:96.87ms
step:905/1770 train_time:86704ms step_avg:96.88ms
step:906/1770 train_time:86803ms step_avg:96.88ms
step:907/1770 train_time:86903ms step_avg:96.88ms
step:908/1770 train_time:87003ms step_avg:96.89ms
step:909/1770 train_time:87102ms step_avg:96.89ms
step:910/1770 train_time:87202ms step_avg:96.89ms
step:911/1770 train_time:87302ms step_avg:96.89ms
step:912/1770 train_time:87402ms step_avg:96.90ms
step:913/1770 train_time:87501ms step_avg:96.90ms
step:914/1770 train_time:87601ms step_avg:96.90ms
step:915/1770 train_time:87700ms step_avg:96.91ms
step:916/1770 train_time:87800ms step_avg:96.91ms
step:917/1770 train_time:87899ms step_avg:96.91ms
step:918/1770 train_time:87999ms step_avg:96.92ms
step:919/1770 train_time:88098ms step_avg:96.92ms
step:920/1770 train_time:88200ms step_avg:96.92ms
step:921/1770 train_time:88301ms step_avg:96.93ms
step:922/1770 train_time:88401ms step_avg:96.93ms
step:923/1770 train_time:88502ms step_avg:96.94ms
step:924/1770 train_time:88603ms step_avg:96.94ms
step:925/1770 train_time:88704ms step_avg:96.94ms
step:926/1770 train_time:88804ms step_avg:96.95ms
step:927/1770 train_time:88905ms step_avg:96.95ms
step:928/1770 train_time:89006ms step_avg:96.96ms
step:929/1770 train_time:89108ms step_avg:96.96ms
step:930/1770 train_time:89209ms step_avg:96.97ms
step:931/1770 train_time:89310ms step_avg:96.97ms
step:932/1770 train_time:89411ms step_avg:96.98ms
step:933/1770 train_time:89512ms step_avg:96.98ms
step:934/1770 train_time:89613ms step_avg:96.98ms
step:935/1770 train_time:89714ms step_avg:96.99ms
step:936/1770 train_time:89814ms step_avg:96.99ms
step:937/1770 train_time:89914ms step_avg:96.99ms
step:938/1770 train_time:90015ms step_avg:97.00ms
step:939/1770 train_time:90116ms step_avg:97.00ms
step:940/1770 train_time:90217ms step_avg:97.01ms
step:941/1770 train_time:90317ms step_avg:97.01ms
step:942/1770 train_time:90418ms step_avg:97.02ms
step:943/1770 train_time:90520ms step_avg:97.02ms
step:944/1770 train_time:90620ms step_avg:97.02ms
step:945/1770 train_time:90721ms step_avg:97.03ms
step:946/1770 train_time:90822ms step_avg:97.03ms
step:947/1770 train_time:90924ms step_avg:97.04ms
step:948/1770 train_time:91024ms step_avg:97.04ms
step:949/1770 train_time:91125ms step_avg:97.04ms
step:950/1770 train_time:91226ms step_avg:97.05ms
step:951/1770 train_time:91327ms step_avg:97.05ms
step:952/1770 train_time:91427ms step_avg:97.06ms
step:953/1770 train_time:91529ms step_avg:97.06ms
step:954/1770 train_time:91629ms step_avg:97.07ms
step:955/1770 train_time:91731ms step_avg:97.07ms
step:956/1770 train_time:91833ms step_avg:97.08ms
step:957/1770 train_time:91933ms step_avg:97.08ms
step:958/1770 train_time:92034ms step_avg:97.08ms
step:959/1770 train_time:92135ms step_avg:97.09ms
step:960/1770 train_time:92235ms step_avg:97.09ms
step:961/1770 train_time:92336ms step_avg:97.09ms
step:962/1770 train_time:92438ms step_avg:97.10ms
step:963/1770 train_time:92538ms step_avg:97.10ms
step:964/1770 train_time:92639ms step_avg:97.11ms
step:965/1770 train_time:92741ms step_avg:97.11ms
step:966/1770 train_time:92842ms step_avg:97.12ms
step:967/1770 train_time:92942ms step_avg:97.12ms
step:968/1770 train_time:93044ms step_avg:97.12ms
step:969/1770 train_time:93144ms step_avg:97.13ms
step:970/1770 train_time:93244ms step_avg:97.13ms
step:971/1770 train_time:93345ms step_avg:97.13ms
step:972/1770 train_time:93446ms step_avg:97.14ms
step:973/1770 train_time:93546ms step_avg:97.14ms
step:974/1770 train_time:93648ms step_avg:97.14ms
step:975/1770 train_time:93750ms step_avg:97.15ms
step:976/1770 train_time:93851ms step_avg:97.15ms
step:977/1770 train_time:93952ms step_avg:97.16ms
step:978/1770 train_time:94053ms step_avg:97.16ms
step:979/1770 train_time:94153ms step_avg:97.17ms
step:980/1770 train_time:94254ms step_avg:97.17ms
step:981/1770 train_time:94355ms step_avg:97.17ms
step:982/1770 train_time:94456ms step_avg:97.18ms
step:983/1770 train_time:94558ms step_avg:97.18ms
step:984/1770 train_time:94659ms step_avg:97.19ms
step:985/1770 train_time:94761ms step_avg:97.19ms
step:986/1770 train_time:94862ms step_avg:97.19ms
step:987/1770 train_time:94963ms step_avg:97.20ms
step:988/1770 train_time:95063ms step_avg:97.20ms
step:989/1770 train_time:95165ms step_avg:97.21ms
step:990/1770 train_time:95265ms step_avg:97.21ms
step:991/1770 train_time:95366ms step_avg:97.21ms
step:992/1770 train_time:95469ms step_avg:97.22ms
step:993/1770 train_time:95571ms step_avg:97.22ms
step:994/1770 train_time:95671ms step_avg:97.23ms
step:995/1770 train_time:95773ms step_avg:97.23ms
step:996/1770 train_time:95874ms step_avg:97.24ms
step:997/1770 train_time:95975ms step_avg:97.24ms
step:998/1770 train_time:96075ms step_avg:97.24ms
step:999/1770 train_time:96176ms step_avg:97.25ms
step:1000/1770 train_time:96278ms step_avg:97.25ms
step:1000/1770 val_loss:3.5148 train_time:96378ms step_avg:97.35ms
step:1001/1770 train_time:96400ms step_avg:97.28ms
step:1002/1770 train_time:96488ms step_avg:97.27ms
step:1003/1770 train_time:96591ms step_avg:97.27ms
step:1004/1770 train_time:96692ms step_avg:97.28ms
step:1005/1770 train_time:96792ms step_avg:97.28ms
step:1006/1770 train_time:96892ms step_avg:97.28ms
step:1007/1770 train_time:96992ms step_avg:97.28ms
step:1008/1770 train_time:97092ms step_avg:97.29ms
step:1009/1770 train_time:97192ms step_avg:97.29ms
step:1010/1770 train_time:97292ms step_avg:97.29ms
step:1011/1770 train_time:97394ms step_avg:97.30ms
step:1012/1770 train_time:97497ms step_avg:97.30ms
step:1013/1770 train_time:97598ms step_avg:97.31ms
step:1014/1770 train_time:97700ms step_avg:97.31ms
step:1015/1770 train_time:97800ms step_avg:97.31ms
step:1016/1770 train_time:97901ms step_avg:97.32ms
step:1017/1770 train_time:98001ms step_avg:97.32ms
step:1018/1770 train_time:98101ms step_avg:97.32ms
step:1019/1770 train_time:98202ms step_avg:97.33ms
step:1020/1770 train_time:98303ms step_avg:97.33ms
step:1021/1770 train_time:98405ms step_avg:97.33ms
step:1022/1770 train_time:98506ms step_avg:97.34ms
step:1023/1770 train_time:98607ms step_avg:97.34ms
step:1024/1770 train_time:98709ms step_avg:97.35ms
step:1025/1770 train_time:98810ms step_avg:97.35ms
step:1026/1770 train_time:98911ms step_avg:97.35ms
step:1027/1770 train_time:99011ms step_avg:97.36ms
step:1028/1770 train_time:99113ms step_avg:97.36ms
step:1029/1770 train_time:99213ms step_avg:97.36ms
step:1030/1770 train_time:99314ms step_avg:97.37ms
step:1031/1770 train_time:99414ms step_avg:97.37ms
step:1032/1770 train_time:99515ms step_avg:97.37ms
step:1033/1770 train_time:99616ms step_avg:97.38ms
step:1034/1770 train_time:99718ms step_avg:97.38ms
step:1035/1770 train_time:99820ms step_avg:97.39ms
step:1036/1770 train_time:99922ms step_avg:97.39ms
step:1037/1770 train_time:100023ms step_avg:97.39ms
step:1038/1770 train_time:100123ms step_avg:97.40ms
step:1039/1770 train_time:100224ms step_avg:97.40ms
step:1040/1770 train_time:100324ms step_avg:97.40ms
step:1041/1770 train_time:100424ms step_avg:97.40ms
step:1042/1770 train_time:100525ms step_avg:97.41ms
step:1043/1770 train_time:100625ms step_avg:97.41ms
step:1044/1770 train_time:100726ms step_avg:97.41ms
step:1045/1770 train_time:100828ms step_avg:97.42ms
step:1046/1770 train_time:100929ms step_avg:97.42ms
step:1047/1770 train_time:101031ms step_avg:97.43ms
step:1048/1770 train_time:101132ms step_avg:97.43ms
step:1049/1770 train_time:101233ms step_avg:97.43ms
step:1050/1770 train_time:101333ms step_avg:97.44ms
step:1051/1770 train_time:101435ms step_avg:97.44ms
step:1052/1770 train_time:101535ms step_avg:97.44ms
step:1053/1770 train_time:101637ms step_avg:97.45ms
step:1054/1770 train_time:101739ms step_avg:97.45ms
step:1055/1770 train_time:101841ms step_avg:97.46ms
step:1056/1770 train_time:101941ms step_avg:97.46ms
step:1057/1770 train_time:102042ms step_avg:97.46ms
step:1058/1770 train_time:102143ms step_avg:97.46ms
step:1059/1770 train_time:102244ms step_avg:97.47ms
step:1060/1770 train_time:102347ms step_avg:97.47ms
step:1061/1770 train_time:102448ms step_avg:97.48ms
step:1062/1770 train_time:102550ms step_avg:97.48ms
step:1063/1770 train_time:102653ms step_avg:97.49ms
step:1064/1770 train_time:102754ms step_avg:97.49ms
step:1065/1770 train_time:102856ms step_avg:97.49ms
step:1066/1770 train_time:102956ms step_avg:97.50ms
step:1067/1770 train_time:103057ms step_avg:97.50ms
step:1068/1770 train_time:103158ms step_avg:97.50ms
step:1069/1770 train_time:103259ms step_avg:97.51ms
step:1070/1770 train_time:103361ms step_avg:97.51ms
step:1071/1770 train_time:103462ms step_avg:97.51ms
step:1072/1770 train_time:103564ms step_avg:97.52ms
step:1073/1770 train_time:103665ms step_avg:97.52ms
step:1074/1770 train_time:103767ms step_avg:97.53ms
step:1075/1770 train_time:103868ms step_avg:97.53ms
step:1076/1770 train_time:103971ms step_avg:97.53ms
step:1077/1770 train_time:104073ms step_avg:97.54ms
step:1078/1770 train_time:104173ms step_avg:97.54ms
step:1079/1770 train_time:104274ms step_avg:97.54ms
step:1080/1770 train_time:104375ms step_avg:97.55ms
step:1081/1770 train_time:104475ms step_avg:97.55ms
step:1082/1770 train_time:104576ms step_avg:97.55ms
step:1083/1770 train_time:104677ms step_avg:97.56ms
step:1084/1770 train_time:104779ms step_avg:97.56ms
step:1085/1770 train_time:104880ms step_avg:97.56ms
step:1086/1770 train_time:104982ms step_avg:97.57ms
step:1087/1770 train_time:105083ms step_avg:97.57ms
step:1088/1770 train_time:105184ms step_avg:97.57ms
step:1089/1770 train_time:105284ms step_avg:97.58ms
step:1090/1770 train_time:105385ms step_avg:97.58ms
step:1091/1770 train_time:105486ms step_avg:97.58ms
step:1092/1770 train_time:105586ms step_avg:97.58ms
step:1093/1770 train_time:105687ms step_avg:97.59ms
step:1094/1770 train_time:105788ms step_avg:97.59ms
step:1095/1770 train_time:105889ms step_avg:97.59ms
step:1096/1770 train_time:105991ms step_avg:97.60ms
step:1097/1770 train_time:106092ms step_avg:97.60ms
step:1098/1770 train_time:106192ms step_avg:97.60ms
step:1099/1770 train_time:106293ms step_avg:97.61ms
step:1100/1770 train_time:106395ms step_avg:97.61ms
step:1101/1770 train_time:106496ms step_avg:97.61ms
step:1102/1770 train_time:106598ms step_avg:97.62ms
step:1103/1770 train_time:106700ms step_avg:97.62ms
step:1104/1770 train_time:106803ms step_avg:97.63ms
step:1105/1770 train_time:106904ms step_avg:97.63ms
step:1106/1770 train_time:107005ms step_avg:97.63ms
step:1107/1770 train_time:107106ms step_avg:97.63ms
step:1108/1770 train_time:107206ms step_avg:97.64ms
step:1109/1770 train_time:107307ms step_avg:97.64ms
step:1110/1770 train_time:107409ms step_avg:97.64ms
step:1111/1770 train_time:107510ms step_avg:97.65ms
step:1112/1770 train_time:107612ms step_avg:97.65ms
step:1113/1770 train_time:107712ms step_avg:97.65ms
step:1114/1770 train_time:107813ms step_avg:97.66ms
step:1115/1770 train_time:107915ms step_avg:97.66ms
step:1116/1770 train_time:108016ms step_avg:97.66ms
step:1117/1770 train_time:108116ms step_avg:97.67ms
step:1118/1770 train_time:108217ms step_avg:97.67ms
step:1119/1770 train_time:108319ms step_avg:97.67ms
step:1120/1770 train_time:108421ms step_avg:97.68ms
step:1121/1770 train_time:108522ms step_avg:97.68ms
step:1122/1770 train_time:108623ms step_avg:97.68ms
step:1123/1770 train_time:108723ms step_avg:97.68ms
step:1124/1770 train_time:108824ms step_avg:97.69ms
step:1125/1770 train_time:108925ms step_avg:97.69ms
step:1125/1770 val_loss:3.4732 train_time:109024ms step_avg:97.78ms
step:1126/1770 train_time:109046ms step_avg:97.71ms
step:1127/1770 train_time:109136ms step_avg:97.70ms
step:1128/1770 train_time:109238ms step_avg:97.71ms
step:1129/1770 train_time:109340ms step_avg:97.71ms
step:1130/1770 train_time:109441ms step_avg:97.72ms
step:1131/1770 train_time:109542ms step_avg:97.72ms
step:1132/1770 train_time:109643ms step_avg:97.72ms
step:1133/1770 train_time:109744ms step_avg:97.72ms
step:1134/1770 train_time:109845ms step_avg:97.73ms
step:1135/1770 train_time:109945ms step_avg:97.73ms
step:1136/1770 train_time:110047ms step_avg:97.73ms
step:1137/1770 train_time:110149ms step_avg:97.74ms
step:1138/1770 train_time:110251ms step_avg:97.74ms
step:1139/1770 train_time:110352ms step_avg:97.74ms
step:1140/1770 train_time:110454ms step_avg:97.75ms
step:1141/1770 train_time:110554ms step_avg:97.75ms
step:1142/1770 train_time:110655ms step_avg:97.75ms
step:1143/1770 train_time:110756ms step_avg:97.75ms
step:1144/1770 train_time:110857ms step_avg:97.76ms
step:1145/1770 train_time:110958ms step_avg:97.76ms
step:1146/1770 train_time:111061ms step_avg:97.76ms
step:1147/1770 train_time:111162ms step_avg:97.77ms
step:1148/1770 train_time:111264ms step_avg:97.77ms
step:1149/1770 train_time:111365ms step_avg:97.77ms
step:1150/1770 train_time:111467ms step_avg:97.78ms
step:1151/1770 train_time:111569ms step_avg:97.78ms
step:1152/1770 train_time:111670ms step_avg:97.78ms
step:1153/1770 train_time:111771ms step_avg:97.79ms
step:1154/1770 train_time:111873ms step_avg:97.79ms
step:1155/1770 train_time:111974ms step_avg:97.79ms
step:1156/1770 train_time:112074ms step_avg:97.80ms
step:1157/1770 train_time:112176ms step_avg:97.80ms
step:1158/1770 train_time:112277ms step_avg:97.80ms
step:1159/1770 train_time:112377ms step_avg:97.80ms
step:1160/1770 train_time:112479ms step_avg:97.81ms
step:1161/1770 train_time:112580ms step_avg:97.81ms
step:1162/1770 train_time:112682ms step_avg:97.81ms
step:1163/1770 train_time:112783ms step_avg:97.82ms
step:1164/1770 train_time:112884ms step_avg:97.82ms
step:1165/1770 train_time:112985ms step_avg:97.82ms
step:1166/1770 train_time:113087ms step_avg:97.83ms
step:1167/1770 train_time:113187ms step_avg:97.83ms
step:1168/1770 train_time:113288ms step_avg:97.83ms
step:1169/1770 train_time:113389ms step_avg:97.83ms
step:1170/1770 train_time:113490ms step_avg:97.84ms
step:1171/1770 train_time:113593ms step_avg:97.84ms
step:1172/1770 train_time:113694ms step_avg:97.84ms
step:1173/1770 train_time:113795ms step_avg:97.85ms
step:1174/1770 train_time:113896ms step_avg:97.85ms
step:1175/1770 train_time:113997ms step_avg:97.85ms
step:1176/1770 train_time:114098ms step_avg:97.85ms
step:1177/1770 train_time:114199ms step_avg:97.86ms
step:1178/1770 train_time:114300ms step_avg:97.86ms
step:1179/1770 train_time:114403ms step_avg:97.86ms
step:1180/1770 train_time:114504ms step_avg:97.87ms
step:1181/1770 train_time:114605ms step_avg:97.87ms
step:1182/1770 train_time:114706ms step_avg:97.87ms
step:1183/1770 train_time:114808ms step_avg:97.88ms
step:1184/1770 train_time:114912ms step_avg:97.88ms
step:1185/1770 train_time:115014ms step_avg:97.88ms
step:1186/1770 train_time:115117ms step_avg:97.89ms
step:1187/1770 train_time:115221ms step_avg:97.89ms
step:1188/1770 train_time:115323ms step_avg:97.90ms
step:1189/1770 train_time:115425ms step_avg:97.90ms
step:1190/1770 train_time:115527ms step_avg:97.90ms
step:1191/1770 train_time:115629ms step_avg:97.91ms
step:1192/1770 train_time:115732ms step_avg:97.91ms
step:1193/1770 train_time:115834ms step_avg:97.92ms
step:1194/1770 train_time:115935ms step_avg:97.92ms
step:1195/1770 train_time:116038ms step_avg:97.92ms
step:1196/1770 train_time:116141ms step_avg:97.93ms
step:1197/1770 train_time:116243ms step_avg:97.93ms
step:1198/1770 train_time:116345ms step_avg:97.93ms
step:1199/1770 train_time:116447ms step_avg:97.94ms
step:1200/1770 train_time:116549ms step_avg:97.94ms
step:1201/1770 train_time:116651ms step_avg:97.94ms
step:1202/1770 train_time:116753ms step_avg:97.95ms
step:1203/1770 train_time:116855ms step_avg:97.95ms
step:1204/1770 train_time:116957ms step_avg:97.95ms
step:1205/1770 train_time:117059ms step_avg:97.96ms
step:1206/1770 train_time:117162ms step_avg:97.96ms
step:1207/1770 train_time:117264ms step_avg:97.96ms
step:1208/1770 train_time:117366ms step_avg:97.97ms
step:1209/1770 train_time:117468ms step_avg:97.97ms
step:1210/1770 train_time:117570ms step_avg:97.97ms
step:1211/1770 train_time:117673ms step_avg:97.98ms
step:1212/1770 train_time:117778ms step_avg:97.98ms
step:1213/1770 train_time:117880ms step_avg:97.99ms
step:1214/1770 train_time:117981ms step_avg:97.99ms
step:1215/1770 train_time:118084ms step_avg:98.00ms
step:1216/1770 train_time:118188ms step_avg:98.00ms
step:1217/1770 train_time:118291ms step_avg:98.00ms
step:1218/1770 train_time:118393ms step_avg:98.01ms
step:1219/1770 train_time:118495ms step_avg:98.01ms
step:1220/1770 train_time:118597ms step_avg:98.01ms
step:1221/1770 train_time:118699ms step_avg:98.02ms
step:1222/1770 train_time:118803ms step_avg:98.02ms
step:1223/1770 train_time:118905ms step_avg:98.03ms
step:1224/1770 train_time:119008ms step_avg:98.03ms
step:1225/1770 train_time:119110ms step_avg:98.03ms
step:1226/1770 train_time:119212ms step_avg:98.04ms
step:1227/1770 train_time:119317ms step_avg:98.04ms
step:1228/1770 train_time:119420ms step_avg:98.05ms
step:1229/1770 train_time:119522ms step_avg:98.05ms
step:1230/1770 train_time:119624ms step_avg:98.05ms
step:1231/1770 train_time:119727ms step_avg:98.06ms
step:1232/1770 train_time:119829ms step_avg:98.06ms
step:1233/1770 train_time:119930ms step_avg:98.06ms
step:1234/1770 train_time:120033ms step_avg:98.07ms
step:1235/1770 train_time:120135ms step_avg:98.07ms
step:1236/1770 train_time:120238ms step_avg:98.07ms
step:1237/1770 train_time:120339ms step_avg:98.08ms
step:1238/1770 train_time:120442ms step_avg:98.08ms
step:1239/1770 train_time:120544ms step_avg:98.08ms
step:1240/1770 train_time:120646ms step_avg:98.09ms
step:1241/1770 train_time:120749ms step_avg:98.09ms
step:1242/1770 train_time:120850ms step_avg:98.09ms
step:1243/1770 train_time:120952ms step_avg:98.10ms
step:1244/1770 train_time:121054ms step_avg:98.10ms
step:1245/1770 train_time:121157ms step_avg:98.10ms
step:1246/1770 train_time:121259ms step_avg:98.11ms
step:1247/1770 train_time:121361ms step_avg:98.11ms
step:1248/1770 train_time:121463ms step_avg:98.11ms
step:1249/1770 train_time:121565ms step_avg:98.12ms
step:1250/1770 train_time:121668ms step_avg:98.12ms
step:1250/1770 val_loss:3.4268 train_time:121769ms step_avg:98.20ms
step:1251/1770 train_time:121791ms step_avg:98.14ms
step:1252/1770 train_time:121880ms step_avg:98.13ms
step:1253/1770 train_time:121983ms step_avg:98.14ms
step:1254/1770 train_time:122085ms step_avg:98.14ms
step:1255/1770 train_time:122189ms step_avg:98.14ms
step:1256/1770 train_time:122290ms step_avg:98.15ms
step:1257/1770 train_time:122392ms step_avg:98.15ms
step:1258/1770 train_time:122496ms step_avg:98.15ms
step:1259/1770 train_time:122598ms step_avg:98.16ms
step:1260/1770 train_time:122700ms step_avg:98.16ms
step:1261/1770 train_time:122803ms step_avg:98.16ms
step:1262/1770 train_time:122906ms step_avg:98.17ms
step:1263/1770 train_time:123008ms step_avg:98.17ms
step:1264/1770 train_time:123111ms step_avg:98.17ms
step:1265/1770 train_time:123213ms step_avg:98.18ms
step:1266/1770 train_time:123316ms step_avg:98.18ms
step:1267/1770 train_time:123418ms step_avg:98.18ms
step:1268/1770 train_time:123520ms step_avg:98.19ms
step:1269/1770 train_time:123622ms step_avg:98.19ms
step:1270/1770 train_time:123724ms step_avg:98.19ms
step:1271/1770 train_time:123828ms step_avg:98.20ms
step:1272/1770 train_time:123930ms step_avg:98.20ms
step:1273/1770 train_time:124033ms step_avg:98.20ms
step:1274/1770 train_time:124135ms step_avg:98.21ms
step:1275/1770 train_time:124237ms step_avg:98.21ms
step:1276/1770 train_time:124340ms step_avg:98.21ms
step:1277/1770 train_time:124442ms step_avg:98.22ms
step:1278/1770 train_time:124545ms step_avg:98.22ms
step:1279/1770 train_time:124647ms step_avg:98.22ms
step:1280/1770 train_time:124750ms step_avg:98.23ms
step:1281/1770 train_time:124852ms step_avg:98.23ms
step:1282/1770 train_time:124956ms step_avg:98.24ms
step:1283/1770 train_time:125058ms step_avg:98.24ms
step:1284/1770 train_time:125160ms step_avg:98.24ms
step:1285/1770 train_time:125263ms step_avg:98.25ms
step:1286/1770 train_time:125366ms step_avg:98.25ms
step:1287/1770 train_time:125470ms step_avg:98.25ms
step:1288/1770 train_time:125572ms step_avg:98.26ms
step:1289/1770 train_time:125674ms step_avg:98.26ms
step:1290/1770 train_time:125775ms step_avg:98.26ms
step:1291/1770 train_time:125877ms step_avg:98.26ms
step:1292/1770 train_time:125979ms step_avg:98.27ms
step:1293/1770 train_time:126081ms step_avg:98.27ms
step:1294/1770 train_time:126183ms step_avg:98.27ms
step:1295/1770 train_time:126286ms step_avg:98.28ms
step:1296/1770 train_time:126389ms step_avg:98.28ms
step:1297/1770 train_time:126491ms step_avg:98.28ms
step:1298/1770 train_time:126592ms step_avg:98.29ms
step:1299/1770 train_time:126694ms step_avg:98.29ms
step:1300/1770 train_time:126795ms step_avg:98.29ms
step:1301/1770 train_time:126897ms step_avg:98.29ms
step:1302/1770 train_time:127001ms step_avg:98.30ms
step:1303/1770 train_time:127103ms step_avg:98.30ms
step:1304/1770 train_time:127206ms step_avg:98.30ms
step:1305/1770 train_time:127309ms step_avg:98.31ms
step:1306/1770 train_time:127411ms step_avg:98.31ms
step:1307/1770 train_time:127512ms step_avg:98.31ms
step:1308/1770 train_time:127614ms step_avg:98.32ms
step:1309/1770 train_time:127717ms step_avg:98.32ms
step:1310/1770 train_time:127818ms step_avg:98.32ms
step:1311/1770 train_time:127919ms step_avg:98.32ms
step:1312/1770 train_time:128021ms step_avg:98.33ms
step:1313/1770 train_time:128122ms step_avg:98.33ms
step:1314/1770 train_time:128225ms step_avg:98.33ms
step:1315/1770 train_time:128328ms step_avg:98.34ms
step:1316/1770 train_time:128431ms step_avg:98.34ms
step:1317/1770 train_time:128533ms step_avg:98.34ms
step:1318/1770 train_time:128639ms step_avg:98.35ms
step:1319/1770 train_time:128741ms step_avg:98.35ms
step:1320/1770 train_time:128843ms step_avg:98.35ms
step:1321/1770 train_time:128946ms step_avg:98.36ms
step:1322/1770 train_time:129048ms step_avg:98.36ms
step:1323/1770 train_time:129151ms step_avg:98.36ms
step:1324/1770 train_time:129254ms step_avg:98.37ms
step:1325/1770 train_time:129358ms step_avg:98.37ms
step:1326/1770 train_time:129460ms step_avg:98.37ms
step:1327/1770 train_time:129565ms step_avg:98.38ms
step:1328/1770 train_time:129667ms step_avg:98.38ms
step:1329/1770 train_time:129769ms step_avg:98.38ms
step:1330/1770 train_time:129871ms step_avg:98.39ms
step:1331/1770 train_time:129973ms step_avg:98.39ms
step:1332/1770 train_time:130075ms step_avg:98.39ms
step:1333/1770 train_time:130178ms step_avg:98.40ms
step:1334/1770 train_time:130280ms step_avg:98.40ms
step:1335/1770 train_time:130382ms step_avg:98.40ms
step:1336/1770 train_time:130484ms step_avg:98.40ms
step:1337/1770 train_time:130586ms step_avg:98.41ms
step:1338/1770 train_time:130687ms step_avg:98.41ms
step:1339/1770 train_time:130789ms step_avg:98.41ms
step:1340/1770 train_time:130893ms step_avg:98.42ms
step:1341/1770 train_time:130994ms step_avg:98.42ms
step:1342/1770 train_time:131097ms step_avg:98.42ms
step:1343/1770 train_time:131199ms step_avg:98.42ms
step:1344/1770 train_time:131302ms step_avg:98.43ms
step:1345/1770 train_time:131403ms step_avg:98.43ms
step:1346/1770 train_time:131506ms step_avg:98.43ms
step:1347/1770 train_time:131609ms step_avg:98.44ms
step:1348/1770 train_time:131713ms step_avg:98.44ms
step:1349/1770 train_time:131816ms step_avg:98.44ms
step:1350/1770 train_time:131918ms step_avg:98.45ms
step:1351/1770 train_time:132020ms step_avg:98.45ms
step:1352/1770 train_time:132122ms step_avg:98.45ms
step:1353/1770 train_time:132224ms step_avg:98.45ms
step:1354/1770 train_time:132326ms step_avg:98.46ms
step:1355/1770 train_time:132428ms step_avg:98.46ms
step:1356/1770 train_time:132530ms step_avg:98.46ms
step:1357/1770 train_time:132633ms step_avg:98.47ms
step:1358/1770 train_time:132735ms step_avg:98.47ms
step:1359/1770 train_time:132838ms step_avg:98.47ms
step:1360/1770 train_time:132940ms step_avg:98.47ms
step:1361/1770 train_time:133043ms step_avg:98.48ms
step:1362/1770 train_time:133145ms step_avg:98.48ms
step:1363/1770 train_time:133248ms step_avg:98.48ms
step:1364/1770 train_time:133350ms step_avg:98.49ms
step:1365/1770 train_time:133452ms step_avg:98.49ms
step:1366/1770 train_time:133554ms step_avg:98.49ms
step:1367/1770 train_time:133657ms step_avg:98.49ms
step:1368/1770 train_time:133759ms step_avg:98.50ms
step:1369/1770 train_time:133861ms step_avg:98.50ms
step:1370/1770 train_time:133964ms step_avg:98.50ms
step:1371/1770 train_time:134066ms step_avg:98.51ms
step:1372/1770 train_time:134169ms step_avg:98.51ms
step:1373/1770 train_time:134271ms step_avg:98.51ms
step:1374/1770 train_time:134374ms step_avg:98.51ms
step:1375/1770 train_time:134477ms step_avg:98.52ms
step:1375/1770 val_loss:3.3825 train_time:134577ms step_avg:98.59ms
step:1376/1770 train_time:134599ms step_avg:98.53ms
step:1377/1770 train_time:134691ms step_avg:98.53ms
step:1378/1770 train_time:134793ms step_avg:98.53ms
step:1379/1770 train_time:134895ms step_avg:98.54ms
step:1380/1770 train_time:134996ms step_avg:98.54ms
step:1381/1770 train_time:135099ms step_avg:98.54ms
step:1382/1770 train_time:135200ms step_avg:98.54ms
step:1383/1770 train_time:135302ms step_avg:98.55ms
step:1384/1770 train_time:135405ms step_avg:98.55ms
step:1385/1770 train_time:135507ms step_avg:98.55ms
step:1386/1770 train_time:135610ms step_avg:98.55ms
step:1387/1770 train_time:135713ms step_avg:98.56ms
step:1388/1770 train_time:135815ms step_avg:98.56ms
step:1389/1770 train_time:135917ms step_avg:98.56ms
step:1390/1770 train_time:136019ms step_avg:98.56ms
step:1391/1770 train_time:136121ms step_avg:98.57ms
step:1392/1770 train_time:136223ms step_avg:98.57ms
step:1393/1770 train_time:136325ms step_avg:98.57ms
step:1394/1770 train_time:136427ms step_avg:98.57ms
step:1395/1770 train_time:136530ms step_avg:98.58ms
step:1396/1770 train_time:136633ms step_avg:98.58ms
step:1397/1770 train_time:136735ms step_avg:98.58ms
step:1398/1770 train_time:136838ms step_avg:98.59ms
step:1399/1770 train_time:136941ms step_avg:98.59ms
step:1400/1770 train_time:137043ms step_avg:98.59ms
step:1401/1770 train_time:137145ms step_avg:98.59ms
step:1402/1770 train_time:137248ms step_avg:98.60ms
step:1403/1770 train_time:137351ms step_avg:98.60ms
step:1404/1770 train_time:137454ms step_avg:98.60ms
step:1405/1770 train_time:137555ms step_avg:98.61ms
step:1406/1770 train_time:137657ms step_avg:98.61ms
step:1407/1770 train_time:137759ms step_avg:98.61ms
step:1408/1770 train_time:137861ms step_avg:98.61ms
step:1409/1770 train_time:137963ms step_avg:98.62ms
step:1410/1770 train_time:138066ms step_avg:98.62ms
step:1411/1770 train_time:138168ms step_avg:98.62ms
step:1412/1770 train_time:138270ms step_avg:98.62ms
step:1413/1770 train_time:138372ms step_avg:98.63ms
step:1414/1770 train_time:138476ms step_avg:98.63ms
step:1415/1770 train_time:138578ms step_avg:98.63ms
step:1416/1770 train_time:138681ms step_avg:98.64ms
step:1417/1770 train_time:138783ms step_avg:98.64ms
step:1418/1770 train_time:138886ms step_avg:98.64ms
step:1419/1770 train_time:138988ms step_avg:98.64ms
step:1420/1770 train_time:139090ms step_avg:98.65ms
step:1421/1770 train_time:139193ms step_avg:98.65ms
step:1422/1770 train_time:139295ms step_avg:98.65ms
step:1423/1770 train_time:139397ms step_avg:98.65ms
step:1424/1770 train_time:139500ms step_avg:98.66ms
step:1425/1770 train_time:139602ms step_avg:98.66ms
step:1426/1770 train_time:139704ms step_avg:98.66ms
step:1427/1770 train_time:139806ms step_avg:98.66ms
step:1428/1770 train_time:139909ms step_avg:98.67ms
step:1429/1770 train_time:140013ms step_avg:98.67ms
step:1430/1770 train_time:140115ms step_avg:98.67ms
step:1431/1770 train_time:140218ms step_avg:98.68ms
step:1432/1770 train_time:140319ms step_avg:98.68ms
step:1433/1770 train_time:140421ms step_avg:98.68ms
step:1434/1770 train_time:140522ms step_avg:98.68ms
step:1435/1770 train_time:140624ms step_avg:98.68ms
step:1436/1770 train_time:140728ms step_avg:98.69ms
step:1437/1770 train_time:140831ms step_avg:98.69ms
step:1438/1770 train_time:140933ms step_avg:98.69ms
step:1439/1770 train_time:141035ms step_avg:98.69ms
step:1440/1770 train_time:141138ms step_avg:98.70ms
step:1441/1770 train_time:141243ms step_avg:98.70ms
step:1442/1770 train_time:141344ms step_avg:98.70ms
step:1443/1770 train_time:141447ms step_avg:98.71ms
step:1444/1770 train_time:141549ms step_avg:98.71ms
step:1445/1770 train_time:141652ms step_avg:98.71ms
step:1446/1770 train_time:141755ms step_avg:98.72ms
step:1447/1770 train_time:141859ms step_avg:98.72ms
step:1448/1770 train_time:141963ms step_avg:98.72ms
step:1449/1770 train_time:142067ms step_avg:98.73ms
step:1450/1770 train_time:142169ms step_avg:98.73ms
step:1451/1770 train_time:142273ms step_avg:98.73ms
step:1452/1770 train_time:142377ms step_avg:98.74ms
step:1453/1770 train_time:142479ms step_avg:98.74ms
step:1454/1770 train_time:142583ms step_avg:98.74ms
step:1455/1770 train_time:142688ms step_avg:98.75ms
step:1456/1770 train_time:142792ms step_avg:98.75ms
step:1457/1770 train_time:142895ms step_avg:98.75ms
step:1458/1770 train_time:142999ms step_avg:98.76ms
step:1459/1770 train_time:143103ms step_avg:98.76ms
step:1460/1770 train_time:143206ms step_avg:98.76ms
step:1461/1770 train_time:143311ms step_avg:98.77ms
step:1462/1770 train_time:143413ms step_avg:98.77ms
step:1463/1770 train_time:143517ms step_avg:98.77ms
step:1464/1770 train_time:143622ms step_avg:98.78ms
step:1465/1770 train_time:143725ms step_avg:98.78ms
step:1466/1770 train_time:143829ms step_avg:98.78ms
step:1467/1770 train_time:143933ms step_avg:98.79ms
step:1468/1770 train_time:144036ms step_avg:98.79ms
step:1469/1770 train_time:144140ms step_avg:98.79ms
step:1470/1770 train_time:144242ms step_avg:98.80ms
step:1471/1770 train_time:144346ms step_avg:98.80ms
step:1472/1770 train_time:144450ms step_avg:98.80ms
step:1473/1770 train_time:144553ms step_avg:98.81ms
step:1474/1770 train_time:144658ms step_avg:98.81ms
step:1475/1770 train_time:144761ms step_avg:98.81ms
step:1476/1770 train_time:144864ms step_avg:98.82ms
step:1477/1770 train_time:144970ms step_avg:98.82ms
step:1478/1770 train_time:145074ms step_avg:98.82ms
step:1479/1770 train_time:145177ms step_avg:98.83ms
step:1480/1770 train_time:145280ms step_avg:98.83ms
step:1481/1770 train_time:145388ms step_avg:98.84ms
step:1482/1770 train_time:145490ms step_avg:98.84ms
step:1483/1770 train_time:145593ms step_avg:98.84ms
step:1484/1770 train_time:145696ms step_avg:98.84ms
step:1485/1770 train_time:145800ms step_avg:98.85ms
step:1486/1770 train_time:145903ms step_avg:98.85ms
step:1487/1770 train_time:146006ms step_avg:98.85ms
step:1488/1770 train_time:146110ms step_avg:98.86ms
step:1489/1770 train_time:146215ms step_avg:98.86ms
step:1490/1770 train_time:146319ms step_avg:98.86ms
step:1491/1770 train_time:146422ms step_avg:98.87ms
step:1492/1770 train_time:146526ms step_avg:98.87ms
step:1493/1770 train_time:146632ms step_avg:98.88ms
step:1494/1770 train_time:146739ms step_avg:98.88ms
step:1495/1770 train_time:146842ms step_avg:98.88ms
step:1496/1770 train_time:146945ms step_avg:98.89ms
step:1497/1770 train_time:147049ms step_avg:98.89ms
step:1498/1770 train_time:147152ms step_avg:98.89ms
step:1499/1770 train_time:147255ms step_avg:98.90ms
step:1500/1770 train_time:147357ms step_avg:98.90ms
step:1500/1770 val_loss:3.3446 train_time:147459ms step_avg:98.97ms
step:1501/1770 train_time:147481ms step_avg:98.91ms
step:1502/1770 train_time:147574ms step_avg:98.91ms
step:1503/1770 train_time:147677ms step_avg:98.91ms
step:1504/1770 train_time:147780ms step_avg:98.92ms
step:1505/1770 train_time:147886ms step_avg:98.92ms
step:1506/1770 train_time:147989ms step_avg:98.92ms
step:1507/1770 train_time:148093ms step_avg:98.93ms
step:1508/1770 train_time:148198ms step_avg:98.93ms
step:1509/1770 train_time:148301ms step_avg:98.93ms
step:1510/1770 train_time:148403ms step_avg:98.94ms
step:1511/1770 train_time:148510ms step_avg:98.94ms
step:1512/1770 train_time:148614ms step_avg:98.94ms
step:1513/1770 train_time:148717ms step_avg:98.95ms
step:1514/1770 train_time:148821ms step_avg:98.95ms
step:1515/1770 train_time:148924ms step_avg:98.95ms
step:1516/1770 train_time:149028ms step_avg:98.96ms
step:1517/1770 train_time:149132ms step_avg:98.96ms
step:1518/1770 train_time:149238ms step_avg:98.96ms
step:1519/1770 train_time:149339ms step_avg:98.97ms
step:1520/1770 train_time:149444ms step_avg:98.97ms
step:1521/1770 train_time:149547ms step_avg:98.97ms
step:1522/1770 train_time:149652ms step_avg:98.98ms
step:1523/1770 train_time:149755ms step_avg:98.98ms
step:1524/1770 train_time:149858ms step_avg:98.98ms
step:1525/1770 train_time:149961ms step_avg:98.98ms
step:1526/1770 train_time:150064ms step_avg:98.99ms
step:1527/1770 train_time:150168ms step_avg:98.99ms
step:1528/1770 train_time:150273ms step_avg:98.99ms
step:1529/1770 train_time:150376ms step_avg:99.00ms
step:1530/1770 train_time:150479ms step_avg:99.00ms
step:1531/1770 train_time:150582ms step_avg:99.00ms
step:1532/1770 train_time:150687ms step_avg:99.01ms
step:1533/1770 train_time:150790ms step_avg:99.01ms
step:1534/1770 train_time:150894ms step_avg:99.01ms
step:1535/1770 train_time:150997ms step_avg:99.01ms
step:1536/1770 train_time:151099ms step_avg:99.02ms
step:1537/1770 train_time:151202ms step_avg:99.02ms
step:1538/1770 train_time:151306ms step_avg:99.02ms
step:1539/1770 train_time:151409ms step_avg:99.02ms
step:1540/1770 train_time:151515ms step_avg:99.03ms
step:1541/1770 train_time:151620ms step_avg:99.03ms
step:1542/1770 train_time:151724ms step_avg:99.04ms
step:1543/1770 train_time:151826ms step_avg:99.04ms
step:1544/1770 train_time:151932ms step_avg:99.04ms
step:1545/1770 train_time:152035ms step_avg:99.05ms
step:1546/1770 train_time:152139ms step_avg:99.05ms
step:1547/1770 train_time:152242ms step_avg:99.05ms
step:1548/1770 train_time:152346ms step_avg:99.05ms
step:1549/1770 train_time:152450ms step_avg:99.06ms
step:1550/1770 train_time:152554ms step_avg:99.06ms
step:1551/1770 train_time:152657ms step_avg:99.06ms
step:1552/1770 train_time:152763ms step_avg:99.07ms
step:1553/1770 train_time:152866ms step_avg:99.07ms
step:1554/1770 train_time:152968ms step_avg:99.07ms
step:1555/1770 train_time:153072ms step_avg:99.08ms
step:1556/1770 train_time:153175ms step_avg:99.08ms
step:1557/1770 train_time:153278ms step_avg:99.08ms
step:1558/1770 train_time:153381ms step_avg:99.08ms
step:1559/1770 train_time:153485ms step_avg:99.09ms
step:1560/1770 train_time:153588ms step_avg:99.09ms
step:1561/1770 train_time:153693ms step_avg:99.09ms
step:1562/1770 train_time:153797ms step_avg:99.10ms
step:1563/1770 train_time:153900ms step_avg:99.10ms
step:1564/1770 train_time:154002ms step_avg:99.10ms
step:1565/1770 train_time:154105ms step_avg:99.10ms
step:1566/1770 train_time:154208ms step_avg:99.11ms
step:1567/1770 train_time:154311ms step_avg:99.11ms
step:1568/1770 train_time:154415ms step_avg:99.11ms
step:1569/1770 train_time:154521ms step_avg:99.12ms
step:1570/1770 train_time:154624ms step_avg:99.12ms
step:1571/1770 train_time:154727ms step_avg:99.12ms
step:1572/1770 train_time:154831ms step_avg:99.12ms
step:1573/1770 train_time:154936ms step_avg:99.13ms
step:1574/1770 train_time:155040ms step_avg:99.13ms
step:1575/1770 train_time:155141ms step_avg:99.13ms
step:1576/1770 train_time:155244ms step_avg:99.13ms
step:1577/1770 train_time:155348ms step_avg:99.14ms
step:1578/1770 train_time:155453ms step_avg:99.14ms
step:1579/1770 train_time:155556ms step_avg:99.14ms
step:1580/1770 train_time:155660ms step_avg:99.15ms
step:1581/1770 train_time:155766ms step_avg:99.15ms
step:1582/1770 train_time:155870ms step_avg:99.15ms
step:1583/1770 train_time:155974ms step_avg:99.16ms
step:1584/1770 train_time:156079ms step_avg:99.16ms
step:1585/1770 train_time:156182ms step_avg:99.16ms
step:1586/1770 train_time:156288ms step_avg:99.17ms
step:1587/1770 train_time:156392ms step_avg:99.17ms
step:1588/1770 train_time:156496ms step_avg:99.17ms
step:1589/1770 train_time:156600ms step_avg:99.18ms
step:1590/1770 train_time:156703ms step_avg:99.18ms
step:1591/1770 train_time:156806ms step_avg:99.18ms
step:1592/1770 train_time:156911ms step_avg:99.18ms
step:1593/1770 train_time:157013ms step_avg:99.19ms
step:1594/1770 train_time:157116ms step_avg:99.19ms
step:1595/1770 train_time:157220ms step_avg:99.19ms
step:1596/1770 train_time:157324ms step_avg:99.20ms
step:1597/1770 train_time:157427ms step_avg:99.20ms
step:1598/1770 train_time:157532ms step_avg:99.20ms
step:1599/1770 train_time:157636ms step_avg:99.20ms
step:1600/1770 train_time:157742ms step_avg:99.21ms
step:1601/1770 train_time:157846ms step_avg:99.21ms
step:1602/1770 train_time:157950ms step_avg:99.22ms
step:1603/1770 train_time:158053ms step_avg:99.22ms
step:1604/1770 train_time:158155ms step_avg:99.22ms
step:1605/1770 train_time:158259ms step_avg:99.22ms
step:1606/1770 train_time:158363ms step_avg:99.22ms
step:1607/1770 train_time:158469ms step_avg:99.23ms
step:1608/1770 train_time:158573ms step_avg:99.23ms
step:1609/1770 train_time:158676ms step_avg:99.23ms
step:1610/1770 train_time:158780ms step_avg:99.24ms
step:1611/1770 train_time:158886ms step_avg:99.24ms
step:1612/1770 train_time:158991ms step_avg:99.25ms
step:1613/1770 train_time:159094ms step_avg:99.25ms
step:1614/1770 train_time:159198ms step_avg:99.25ms
step:1615/1770 train_time:159301ms step_avg:99.25ms
step:1616/1770 train_time:159404ms step_avg:99.26ms
step:1617/1770 train_time:159509ms step_avg:99.26ms
step:1618/1770 train_time:159614ms step_avg:99.26ms
step:1619/1770 train_time:159718ms step_avg:99.27ms
step:1620/1770 train_time:159821ms step_avg:99.27ms
step:1621/1770 train_time:159925ms step_avg:99.27ms
step:1622/1770 train_time:160030ms step_avg:99.27ms
step:1623/1770 train_time:160136ms step_avg:99.28ms
step:1624/1770 train_time:160239ms step_avg:99.28ms
step:1625/1770 train_time:160341ms step_avg:99.28ms
step:1625/1770 val_loss:3.3103 train_time:160444ms step_avg:99.35ms
step:1626/1770 train_time:160465ms step_avg:99.30ms
step:1627/1770 train_time:160555ms step_avg:99.29ms
step:1628/1770 train_time:160659ms step_avg:99.29ms
step:1629/1770 train_time:160761ms step_avg:99.30ms
step:1630/1770 train_time:160864ms step_avg:99.30ms
step:1631/1770 train_time:160967ms step_avg:99.30ms
step:1632/1770 train_time:161070ms step_avg:99.30ms
step:1633/1770 train_time:161173ms step_avg:99.31ms
step:1634/1770 train_time:161276ms step_avg:99.31ms
step:1635/1770 train_time:161378ms step_avg:99.31ms
step:1636/1770 train_time:161483ms step_avg:99.31ms
step:1637/1770 train_time:161588ms step_avg:99.32ms
step:1638/1770 train_time:161692ms step_avg:99.32ms
step:1639/1770 train_time:161796ms step_avg:99.32ms
step:1640/1770 train_time:161900ms step_avg:99.33ms
step:1641/1770 train_time:162004ms step_avg:99.33ms
step:1642/1770 train_time:162107ms step_avg:99.33ms
step:1643/1770 train_time:162210ms step_avg:99.33ms
step:1644/1770 train_time:162315ms step_avg:99.34ms
step:1645/1770 train_time:162418ms step_avg:99.34ms
step:1646/1770 train_time:162523ms step_avg:99.34ms
step:1647/1770 train_time:162629ms step_avg:99.35ms
step:1648/1770 train_time:162731ms step_avg:99.35ms
step:1649/1770 train_time:162835ms step_avg:99.35ms
step:1650/1770 train_time:162938ms step_avg:99.35ms
step:1651/1770 train_time:163041ms step_avg:99.35ms
step:1652/1770 train_time:163144ms step_avg:99.36ms
step:1653/1770 train_time:163248ms step_avg:99.36ms
step:1654/1770 train_time:163354ms step_avg:99.36ms
step:1655/1770 train_time:163460ms step_avg:99.37ms
step:1656/1770 train_time:163564ms step_avg:99.37ms
step:1657/1770 train_time:163669ms step_avg:99.37ms
step:1658/1770 train_time:163772ms step_avg:99.38ms
step:1659/1770 train_time:163877ms step_avg:99.38ms
step:1660/1770 train_time:163981ms step_avg:99.38ms
step:1661/1770 train_time:164085ms step_avg:99.39ms
step:1662/1770 train_time:164189ms step_avg:99.39ms
step:1663/1770 train_time:164292ms step_avg:99.39ms
step:1664/1770 train_time:164396ms step_avg:99.39ms
step:1665/1770 train_time:164499ms step_avg:99.40ms
step:1666/1770 train_time:164603ms step_avg:99.40ms
step:1667/1770 train_time:164707ms step_avg:99.40ms
step:1668/1770 train_time:164809ms step_avg:99.40ms
step:1669/1770 train_time:164913ms step_avg:99.40ms
step:1670/1770 train_time:165016ms step_avg:99.41ms
step:1671/1770 train_time:165120ms step_avg:99.41ms
step:1672/1770 train_time:165225ms step_avg:99.41ms
step:1673/1770 train_time:165330ms step_avg:99.42ms
step:1674/1770 train_time:165432ms step_avg:99.42ms
step:1675/1770 train_time:165537ms step_avg:99.42ms
step:1676/1770 train_time:165642ms step_avg:99.42ms
step:1677/1770 train_time:165750ms step_avg:99.43ms
step:1678/1770 train_time:165853ms step_avg:99.43ms
step:1679/1770 train_time:165956ms step_avg:99.43ms
step:1680/1770 train_time:166058ms step_avg:99.44ms
step:1681/1770 train_time:166162ms step_avg:99.44ms
step:1682/1770 train_time:166268ms step_avg:99.44ms
step:1683/1770 train_time:166371ms step_avg:99.44ms
step:1684/1770 train_time:166474ms step_avg:99.45ms
step:1685/1770 train_time:166578ms step_avg:99.45ms
step:1686/1770 train_time:166682ms step_avg:99.45ms
step:1687/1770 train_time:166787ms step_avg:99.46ms
step:1688/1770 train_time:166890ms step_avg:99.46ms
step:1689/1770 train_time:166994ms step_avg:99.46ms
step:1690/1770 train_time:167097ms step_avg:99.46ms
step:1691/1770 train_time:167200ms step_avg:99.46ms
step:1692/1770 train_time:167304ms step_avg:99.47ms
step:1693/1770 train_time:167409ms step_avg:99.47ms
step:1694/1770 train_time:167512ms step_avg:99.47ms
step:1695/1770 train_time:167616ms step_avg:99.48ms
step:1696/1770 train_time:167721ms step_avg:99.48ms
step:1697/1770 train_time:167827ms step_avg:99.48ms
step:1698/1770 train_time:167932ms step_avg:99.49ms
step:1699/1770 train_time:168036ms step_avg:99.49ms
step:1700/1770 train_time:168139ms step_avg:99.49ms
step:1701/1770 train_time:168242ms step_avg:99.49ms
step:1702/1770 train_time:168345ms step_avg:99.49ms
step:1703/1770 train_time:168448ms step_avg:99.50ms
step:1704/1770 train_time:168551ms step_avg:99.50ms
step:1705/1770 train_time:168653ms step_avg:99.50ms
step:1706/1770 train_time:168756ms step_avg:99.50ms
step:1707/1770 train_time:168860ms step_avg:99.51ms
step:1708/1770 train_time:168964ms step_avg:99.51ms
step:1709/1770 train_time:169069ms step_avg:99.51ms
step:1710/1770 train_time:169176ms step_avg:99.52ms
step:1711/1770 train_time:169282ms step_avg:99.52ms
step:1712/1770 train_time:169386ms step_avg:99.52ms
step:1713/1770 train_time:169489ms step_avg:99.52ms
step:1714/1770 train_time:169593ms step_avg:99.53ms
step:1715/1770 train_time:169696ms step_avg:99.53ms
step:1716/1770 train_time:169800ms step_avg:99.53ms
step:1717/1770 train_time:169905ms step_avg:99.53ms
step:1718/1770 train_time:170010ms step_avg:99.54ms
step:1719/1770 train_time:170115ms step_avg:99.54ms
step:1720/1770 train_time:170220ms step_avg:99.54ms
step:1721/1770 train_time:170324ms step_avg:99.55ms
step:1722/1770 train_time:170431ms step_avg:99.55ms
step:1723/1770 train_time:170537ms step_avg:99.55ms
step:1724/1770 train_time:170643ms step_avg:99.56ms
step:1725/1770 train_time:170750ms step_avg:99.56ms
step:1726/1770 train_time:170856ms step_avg:99.57ms
step:1727/1770 train_time:170959ms step_avg:99.57ms
step:1728/1770 train_time:171065ms step_avg:99.57ms
step:1729/1770 train_time:171169ms step_avg:99.57ms
step:1730/1770 train_time:171275ms step_avg:99.58ms
step:1731/1770 train_time:171381ms step_avg:99.58ms
step:1732/1770 train_time:171484ms step_avg:99.58ms
step:1733/1770 train_time:171591ms step_avg:99.59ms
step:1734/1770 train_time:171695ms step_avg:99.59ms
step:1735/1770 train_time:171801ms step_avg:99.59ms
step:1736/1770 train_time:171905ms step_avg:99.60ms
step:1737/1770 train_time:172010ms step_avg:99.60ms
step:1738/1770 train_time:172113ms step_avg:99.60ms
step:1739/1770 train_time:172218ms step_avg:99.61ms
step:1740/1770 train_time:172321ms step_avg:99.61ms
step:1741/1770 train_time:172428ms step_avg:99.61ms
step:1742/1770 train_time:172535ms step_avg:99.62ms
step:1743/1770 train_time:172640ms step_avg:99.62ms
step:1744/1770 train_time:172744ms step_avg:99.62ms
step:1745/1770 train_time:172848ms step_avg:99.62ms
step:1746/1770 train_time:172955ms step_avg:99.63ms
step:1747/1770 train_time:173057ms step_avg:99.63ms
step:1748/1770 train_time:173163ms step_avg:99.63ms
step:1749/1770 train_time:173269ms step_avg:99.64ms
step:1750/1770 train_time:173373ms step_avg:99.64ms
step:1750/1770 val_loss:3.2836 train_time:173475ms step_avg:99.70ms
step:1751/1770 train_time:173497ms step_avg:99.65ms
step:1752/1770 train_time:173588ms step_avg:99.65ms
step:1753/1770 train_time:173692ms step_avg:99.65ms
step:1754/1770 train_time:173797ms step_avg:99.65ms
step:1755/1770 train_time:173900ms step_avg:99.66ms
step:1756/1770 train_time:174005ms step_avg:99.66ms
step:1757/1770 train_time:174110ms step_avg:99.66ms
step:1758/1770 train_time:174213ms step_avg:99.66ms
step:1759/1770 train_time:174318ms step_avg:99.67ms
step:1760/1770 train_time:174423ms step_avg:99.67ms
step:1761/1770 train_time:174529ms step_avg:99.67ms
step:1762/1770 train_time:174638ms step_avg:99.68ms
step:1763/1770 train_time:174741ms step_avg:99.68ms
step:1764/1770 train_time:174846ms step_avg:99.68ms
step:1765/1770 train_time:174951ms step_avg:99.69ms
step:1766/1770 train_time:175059ms step_avg:99.69ms
step:1767/1770 train_time:175162ms step_avg:99.69ms
step:1768/1770 train_time:175266ms step_avg:99.70ms
step:1769/1770 train_time:175369ms step_avg:99.70ms
step:1770/1770 train_time:175472ms step_avg:99.70ms
step:1770/1770 val_loss:3.2806 train_time:175577ms step_avg:99.76ms
peak memory allocated: 28840 MiB reserved: 32252 MiB
