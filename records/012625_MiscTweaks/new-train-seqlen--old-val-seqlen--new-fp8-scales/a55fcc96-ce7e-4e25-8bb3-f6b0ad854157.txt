import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 14:52:06 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23610ms step_avg:nanms
step:2/1770 train_time:24046ms step_avg:nanms
step:3/1770 train_time:24200ms step_avg:nanms
step:4/1770 train_time:24290ms step_avg:nanms
step:5/1770 train_time:24384ms step_avg:nanms
step:6/1770 train_time:24477ms step_avg:nanms
step:7/1770 train_time:24571ms step_avg:nanms
step:8/1770 train_time:24664ms step_avg:nanms
step:9/1770 train_time:24758ms step_avg:nanms
step:10/1770 train_time:24852ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:186ms step_avg:nanms
step:13/1770 train_time:281ms step_avg:93.65ms
step:14/1770 train_time:376ms step_avg:94.08ms
step:15/1770 train_time:469ms step_avg:93.82ms
step:16/1770 train_time:564ms step_avg:93.95ms
step:17/1770 train_time:657ms step_avg:93.91ms
step:18/1770 train_time:752ms step_avg:93.96ms
step:19/1770 train_time:845ms step_avg:93.90ms
step:20/1770 train_time:939ms step_avg:93.88ms
step:21/1770 train_time:1032ms step_avg:93.84ms
step:22/1770 train_time:1126ms step_avg:93.87ms
step:23/1770 train_time:1220ms step_avg:93.86ms
step:24/1770 train_time:1314ms step_avg:93.84ms
step:25/1770 train_time:1408ms step_avg:93.83ms
step:26/1770 train_time:1502ms step_avg:93.89ms
step:27/1770 train_time:1596ms step_avg:93.88ms
step:28/1770 train_time:1690ms step_avg:93.87ms
step:29/1770 train_time:1784ms step_avg:93.87ms
step:30/1770 train_time:1878ms step_avg:93.89ms
step:31/1770 train_time:1972ms step_avg:93.89ms
step:32/1770 train_time:2065ms step_avg:93.87ms
step:33/1770 train_time:2159ms step_avg:93.87ms
step:34/1770 train_time:2253ms step_avg:93.87ms
step:35/1770 train_time:2346ms step_avg:93.85ms
step:36/1770 train_time:2440ms step_avg:93.85ms
step:37/1770 train_time:2534ms step_avg:93.85ms
step:38/1770 train_time:2628ms step_avg:93.87ms
step:39/1770 train_time:2722ms step_avg:93.88ms
step:40/1770 train_time:2816ms step_avg:93.88ms
step:41/1770 train_time:2910ms step_avg:93.88ms
step:42/1770 train_time:3005ms step_avg:93.91ms
step:43/1770 train_time:3099ms step_avg:93.91ms
step:44/1770 train_time:3193ms step_avg:93.91ms
step:45/1770 train_time:3287ms step_avg:93.92ms
step:46/1770 train_time:3382ms step_avg:93.94ms
step:47/1770 train_time:3475ms step_avg:93.92ms
step:48/1770 train_time:3569ms step_avg:93.92ms
step:49/1770 train_time:3663ms step_avg:93.93ms
step:50/1770 train_time:3757ms step_avg:93.93ms
step:51/1770 train_time:3851ms step_avg:93.93ms
step:52/1770 train_time:3945ms step_avg:93.93ms
step:53/1770 train_time:4039ms step_avg:93.93ms
step:54/1770 train_time:4132ms step_avg:93.92ms
step:55/1770 train_time:4227ms step_avg:93.94ms
step:56/1770 train_time:4320ms step_avg:93.91ms
step:57/1770 train_time:4413ms step_avg:93.90ms
step:58/1770 train_time:4507ms step_avg:93.90ms
step:59/1770 train_time:4601ms step_avg:93.89ms
step:60/1770 train_time:4694ms step_avg:93.89ms
step:61/1770 train_time:4789ms step_avg:93.89ms
step:62/1770 train_time:4882ms step_avg:93.89ms
step:63/1770 train_time:4976ms step_avg:93.89ms
step:64/1770 train_time:5070ms step_avg:93.90ms
step:65/1770 train_time:5164ms step_avg:93.90ms
step:66/1770 train_time:5258ms step_avg:93.89ms
step:67/1770 train_time:5352ms step_avg:93.89ms
step:68/1770 train_time:5445ms step_avg:93.88ms
step:69/1770 train_time:5539ms step_avg:93.88ms
step:70/1770 train_time:5633ms step_avg:93.88ms
step:71/1770 train_time:5727ms step_avg:93.88ms
step:72/1770 train_time:5820ms step_avg:93.88ms
step:73/1770 train_time:5914ms step_avg:93.88ms
step:74/1770 train_time:6008ms step_avg:93.88ms
step:75/1770 train_time:6102ms step_avg:93.88ms
step:76/1770 train_time:6196ms step_avg:93.88ms
step:77/1770 train_time:6290ms step_avg:93.88ms
step:78/1770 train_time:6384ms step_avg:93.89ms
step:79/1770 train_time:6478ms step_avg:93.88ms
step:80/1770 train_time:6571ms step_avg:93.87ms
step:81/1770 train_time:6665ms step_avg:93.88ms
step:82/1770 train_time:6759ms step_avg:93.87ms
step:83/1770 train_time:6852ms step_avg:93.87ms
step:84/1770 train_time:6946ms step_avg:93.87ms
step:85/1770 train_time:7040ms step_avg:93.87ms
step:86/1770 train_time:7134ms step_avg:93.86ms
step:87/1770 train_time:7227ms step_avg:93.86ms
step:88/1770 train_time:7322ms step_avg:93.87ms
step:89/1770 train_time:7416ms step_avg:93.87ms
step:90/1770 train_time:7510ms step_avg:93.87ms
step:91/1770 train_time:7604ms step_avg:93.87ms
step:92/1770 train_time:7697ms step_avg:93.87ms
step:93/1770 train_time:7791ms step_avg:93.87ms
step:94/1770 train_time:7885ms step_avg:93.86ms
step:95/1770 train_time:7979ms step_avg:93.87ms
step:96/1770 train_time:8072ms step_avg:93.86ms
step:97/1770 train_time:8166ms step_avg:93.86ms
step:98/1770 train_time:8260ms step_avg:93.86ms
step:99/1770 train_time:8354ms step_avg:93.87ms
step:100/1770 train_time:8448ms step_avg:93.87ms
step:101/1770 train_time:8543ms step_avg:93.88ms
step:102/1770 train_time:8637ms step_avg:93.88ms
step:103/1770 train_time:8731ms step_avg:93.88ms
step:104/1770 train_time:8825ms step_avg:93.88ms
step:105/1770 train_time:8919ms step_avg:93.89ms
step:106/1770 train_time:9013ms step_avg:93.88ms
step:107/1770 train_time:9107ms step_avg:93.89ms
step:108/1770 train_time:9201ms step_avg:93.89ms
step:109/1770 train_time:9294ms step_avg:93.88ms
step:110/1770 train_time:9389ms step_avg:93.89ms
step:111/1770 train_time:9483ms step_avg:93.89ms
step:112/1770 train_time:9577ms step_avg:93.89ms
step:113/1770 train_time:9671ms step_avg:93.90ms
step:114/1770 train_time:9766ms step_avg:93.90ms
step:115/1770 train_time:9860ms step_avg:93.90ms
step:116/1770 train_time:9953ms step_avg:93.90ms
step:117/1770 train_time:10047ms step_avg:93.90ms
step:118/1770 train_time:10141ms step_avg:93.90ms
step:119/1770 train_time:10234ms step_avg:93.89ms
step:120/1770 train_time:10328ms step_avg:93.89ms
step:121/1770 train_time:10422ms step_avg:93.89ms
step:122/1770 train_time:10516ms step_avg:93.89ms
step:123/1770 train_time:10610ms step_avg:93.89ms
step:124/1770 train_time:10704ms step_avg:93.89ms
step:125/1770 train_time:10798ms step_avg:93.89ms
step:125/1770 val_loss:4.6427 train_time:10890ms step_avg:94.69ms
step:126/1770 train_time:10916ms step_avg:94.10ms
step:127/1770 train_time:10996ms step_avg:93.98ms
step:128/1770 train_time:11096ms step_avg:94.03ms
step:129/1770 train_time:11191ms step_avg:94.04ms
step:130/1770 train_time:11285ms step_avg:94.04ms
step:131/1770 train_time:11379ms step_avg:94.04ms
step:132/1770 train_time:11473ms step_avg:94.04ms
step:133/1770 train_time:11567ms step_avg:94.04ms
step:134/1770 train_time:11661ms step_avg:94.04ms
step:135/1770 train_time:11756ms step_avg:94.05ms
step:136/1770 train_time:11849ms step_avg:94.04ms
step:137/1770 train_time:11944ms step_avg:94.04ms
step:138/1770 train_time:12039ms step_avg:94.05ms
step:139/1770 train_time:12133ms step_avg:94.05ms
step:140/1770 train_time:12227ms step_avg:94.06ms
step:141/1770 train_time:12323ms step_avg:94.07ms
step:142/1770 train_time:12417ms step_avg:94.07ms
step:143/1770 train_time:12511ms step_avg:94.07ms
step:144/1770 train_time:12607ms step_avg:94.08ms
step:145/1770 train_time:12700ms step_avg:94.08ms
step:146/1770 train_time:12794ms step_avg:94.08ms
step:147/1770 train_time:12889ms step_avg:94.08ms
step:148/1770 train_time:12984ms step_avg:94.09ms
step:149/1770 train_time:13079ms step_avg:94.09ms
step:150/1770 train_time:13173ms step_avg:94.09ms
step:151/1770 train_time:13268ms step_avg:94.10ms
step:152/1770 train_time:13363ms step_avg:94.10ms
step:153/1770 train_time:13457ms step_avg:94.11ms
step:154/1770 train_time:13551ms step_avg:94.11ms
step:155/1770 train_time:13646ms step_avg:94.11ms
step:156/1770 train_time:13741ms step_avg:94.11ms
step:157/1770 train_time:13835ms step_avg:94.11ms
step:158/1770 train_time:13929ms step_avg:94.12ms
step:159/1770 train_time:14024ms step_avg:94.12ms
step:160/1770 train_time:14118ms step_avg:94.12ms
step:161/1770 train_time:14212ms step_avg:94.12ms
step:162/1770 train_time:14307ms step_avg:94.13ms
step:163/1770 train_time:14403ms step_avg:94.13ms
step:164/1770 train_time:14497ms step_avg:94.13ms
step:165/1770 train_time:14591ms step_avg:94.14ms
step:166/1770 train_time:14686ms step_avg:94.14ms
step:167/1770 train_time:14781ms step_avg:94.15ms
step:168/1770 train_time:14875ms step_avg:94.14ms
step:169/1770 train_time:14969ms step_avg:94.15ms
step:170/1770 train_time:15064ms step_avg:94.15ms
step:171/1770 train_time:15158ms step_avg:94.15ms
step:172/1770 train_time:15253ms step_avg:94.15ms
step:173/1770 train_time:15348ms step_avg:94.16ms
step:174/1770 train_time:15442ms step_avg:94.16ms
step:175/1770 train_time:15536ms step_avg:94.16ms
step:176/1770 train_time:15631ms step_avg:94.16ms
step:177/1770 train_time:15726ms step_avg:94.17ms
step:178/1770 train_time:15821ms step_avg:94.17ms
step:179/1770 train_time:15915ms step_avg:94.17ms
step:180/1770 train_time:16010ms step_avg:94.18ms
step:181/1770 train_time:16105ms step_avg:94.18ms
step:182/1770 train_time:16199ms step_avg:94.18ms
step:183/1770 train_time:16293ms step_avg:94.18ms
step:184/1770 train_time:16388ms step_avg:94.18ms
step:185/1770 train_time:16483ms step_avg:94.19ms
step:186/1770 train_time:16578ms step_avg:94.19ms
step:187/1770 train_time:16672ms step_avg:94.19ms
step:188/1770 train_time:16767ms step_avg:94.20ms
step:189/1770 train_time:16863ms step_avg:94.21ms
step:190/1770 train_time:16957ms step_avg:94.21ms
step:191/1770 train_time:17052ms step_avg:94.21ms
step:192/1770 train_time:17147ms step_avg:94.21ms
step:193/1770 train_time:17241ms step_avg:94.21ms
step:194/1770 train_time:17336ms step_avg:94.21ms
step:195/1770 train_time:17431ms step_avg:94.22ms
step:196/1770 train_time:17525ms step_avg:94.22ms
step:197/1770 train_time:17619ms step_avg:94.22ms
step:198/1770 train_time:17714ms step_avg:94.22ms
step:199/1770 train_time:17809ms step_avg:94.23ms
step:200/1770 train_time:17903ms step_avg:94.23ms
step:201/1770 train_time:17998ms step_avg:94.23ms
step:202/1770 train_time:18092ms step_avg:94.23ms
step:203/1770 train_time:18186ms step_avg:94.23ms
step:204/1770 train_time:18281ms step_avg:94.23ms
step:205/1770 train_time:18375ms step_avg:94.23ms
step:206/1770 train_time:18470ms step_avg:94.23ms
step:207/1770 train_time:18564ms step_avg:94.24ms
step:208/1770 train_time:18659ms step_avg:94.24ms
step:209/1770 train_time:18753ms step_avg:94.24ms
step:210/1770 train_time:18848ms step_avg:94.24ms
step:211/1770 train_time:18943ms step_avg:94.24ms
step:212/1770 train_time:19037ms step_avg:94.24ms
step:213/1770 train_time:19131ms step_avg:94.24ms
step:214/1770 train_time:19226ms step_avg:94.25ms
step:215/1770 train_time:19321ms step_avg:94.25ms
step:216/1770 train_time:19415ms step_avg:94.25ms
step:217/1770 train_time:19509ms step_avg:94.25ms
step:218/1770 train_time:19604ms step_avg:94.25ms
step:219/1770 train_time:19699ms step_avg:94.25ms
step:220/1770 train_time:19793ms step_avg:94.25ms
step:221/1770 train_time:19889ms step_avg:94.26ms
step:222/1770 train_time:19983ms step_avg:94.26ms
step:223/1770 train_time:20077ms step_avg:94.26ms
step:224/1770 train_time:20171ms step_avg:94.26ms
step:225/1770 train_time:20266ms step_avg:94.26ms
step:226/1770 train_time:20360ms step_avg:94.26ms
step:227/1770 train_time:20454ms step_avg:94.26ms
step:228/1770 train_time:20549ms step_avg:94.26ms
step:229/1770 train_time:20644ms step_avg:94.26ms
step:230/1770 train_time:20738ms step_avg:94.26ms
step:231/1770 train_time:20832ms step_avg:94.26ms
step:232/1770 train_time:20927ms step_avg:94.27ms
step:233/1770 train_time:21022ms step_avg:94.27ms
step:234/1770 train_time:21116ms step_avg:94.27ms
step:235/1770 train_time:21210ms step_avg:94.27ms
step:236/1770 train_time:21305ms step_avg:94.27ms
step:237/1770 train_time:21400ms step_avg:94.27ms
step:238/1770 train_time:21494ms step_avg:94.27ms
step:239/1770 train_time:21589ms step_avg:94.27ms
step:240/1770 train_time:21683ms step_avg:94.28ms
step:241/1770 train_time:21777ms step_avg:94.27ms
step:242/1770 train_time:21872ms step_avg:94.27ms
step:243/1770 train_time:21967ms step_avg:94.28ms
step:244/1770 train_time:22062ms step_avg:94.28ms
step:245/1770 train_time:22156ms step_avg:94.28ms
step:246/1770 train_time:22251ms step_avg:94.29ms
step:247/1770 train_time:22346ms step_avg:94.29ms
step:248/1770 train_time:22441ms step_avg:94.29ms
step:249/1770 train_time:22535ms step_avg:94.29ms
step:250/1770 train_time:22630ms step_avg:94.29ms
step:250/1770 val_loss:4.1121 train_time:22723ms step_avg:94.68ms
step:251/1770 train_time:22748ms step_avg:94.39ms
step:252/1770 train_time:22833ms step_avg:94.35ms
step:253/1770 train_time:22931ms step_avg:94.37ms
step:254/1770 train_time:23027ms step_avg:94.37ms
step:255/1770 train_time:23121ms step_avg:94.37ms
step:256/1770 train_time:23215ms step_avg:94.37ms
step:257/1770 train_time:23309ms step_avg:94.37ms
step:258/1770 train_time:23403ms step_avg:94.37ms
step:259/1770 train_time:23497ms step_avg:94.36ms
step:260/1770 train_time:23591ms step_avg:94.36ms
step:261/1770 train_time:23685ms step_avg:94.36ms
step:262/1770 train_time:23779ms step_avg:94.36ms
step:263/1770 train_time:23875ms step_avg:94.37ms
step:264/1770 train_time:23971ms step_avg:94.37ms
step:265/1770 train_time:24068ms step_avg:94.38ms
step:266/1770 train_time:24161ms step_avg:94.38ms
step:267/1770 train_time:24257ms step_avg:94.38ms
step:268/1770 train_time:24352ms step_avg:94.39ms
step:269/1770 train_time:24447ms step_avg:94.39ms
step:270/1770 train_time:24541ms step_avg:94.39ms
step:271/1770 train_time:24636ms step_avg:94.39ms
step:272/1770 train_time:24731ms step_avg:94.39ms
step:273/1770 train_time:24826ms step_avg:94.40ms
step:274/1770 train_time:24921ms step_avg:94.40ms
step:275/1770 train_time:25016ms step_avg:94.40ms
step:276/1770 train_time:25112ms step_avg:94.41ms
step:277/1770 train_time:25207ms step_avg:94.41ms
step:278/1770 train_time:25302ms step_avg:94.41ms
step:279/1770 train_time:25397ms step_avg:94.41ms
step:280/1770 train_time:25492ms step_avg:94.41ms
step:281/1770 train_time:25587ms step_avg:94.42ms
step:282/1770 train_time:25681ms step_avg:94.42ms
step:283/1770 train_time:25776ms step_avg:94.42ms
step:284/1770 train_time:25872ms step_avg:94.42ms
step:285/1770 train_time:25967ms step_avg:94.42ms
step:286/1770 train_time:26061ms step_avg:94.42ms
step:287/1770 train_time:26157ms step_avg:94.43ms
step:288/1770 train_time:26252ms step_avg:94.43ms
step:289/1770 train_time:26347ms step_avg:94.43ms
step:290/1770 train_time:26442ms step_avg:94.44ms
step:291/1770 train_time:26537ms step_avg:94.44ms
step:292/1770 train_time:26632ms step_avg:94.44ms
step:293/1770 train_time:26727ms step_avg:94.44ms
step:294/1770 train_time:26822ms step_avg:94.44ms
step:295/1770 train_time:26917ms step_avg:94.45ms
step:296/1770 train_time:27012ms step_avg:94.45ms
step:297/1770 train_time:27108ms step_avg:94.45ms
step:298/1770 train_time:27202ms step_avg:94.45ms
step:299/1770 train_time:27297ms step_avg:94.45ms
step:300/1770 train_time:27393ms step_avg:94.46ms
step:301/1770 train_time:27489ms step_avg:94.46ms
step:302/1770 train_time:27584ms step_avg:94.46ms
step:303/1770 train_time:27679ms step_avg:94.47ms
step:304/1770 train_time:27775ms step_avg:94.47ms
step:305/1770 train_time:27870ms step_avg:94.47ms
step:306/1770 train_time:27964ms step_avg:94.47ms
step:307/1770 train_time:28059ms step_avg:94.48ms
step:308/1770 train_time:28155ms step_avg:94.48ms
step:309/1770 train_time:28250ms step_avg:94.48ms
step:310/1770 train_time:28344ms step_avg:94.48ms
step:311/1770 train_time:28439ms step_avg:94.48ms
step:312/1770 train_time:28535ms step_avg:94.49ms
step:313/1770 train_time:28632ms step_avg:94.49ms
step:314/1770 train_time:28725ms step_avg:94.49ms
step:315/1770 train_time:28819ms step_avg:94.49ms
step:316/1770 train_time:28914ms step_avg:94.49ms
step:317/1770 train_time:29010ms step_avg:94.49ms
step:318/1770 train_time:29105ms step_avg:94.50ms
step:319/1770 train_time:29199ms step_avg:94.50ms
step:320/1770 train_time:29294ms step_avg:94.50ms
step:321/1770 train_time:29390ms step_avg:94.50ms
step:322/1770 train_time:29484ms step_avg:94.50ms
step:323/1770 train_time:29579ms step_avg:94.50ms
step:324/1770 train_time:29674ms step_avg:94.50ms
step:325/1770 train_time:29769ms step_avg:94.51ms
step:326/1770 train_time:29864ms step_avg:94.51ms
step:327/1770 train_time:29959ms step_avg:94.51ms
step:328/1770 train_time:30055ms step_avg:94.51ms
step:329/1770 train_time:30150ms step_avg:94.52ms
step:330/1770 train_time:30245ms step_avg:94.52ms
step:331/1770 train_time:30340ms step_avg:94.52ms
step:332/1770 train_time:30435ms step_avg:94.52ms
step:333/1770 train_time:30530ms step_avg:94.52ms
step:334/1770 train_time:30625ms step_avg:94.52ms
step:335/1770 train_time:30719ms step_avg:94.52ms
step:336/1770 train_time:30815ms step_avg:94.52ms
step:337/1770 train_time:30909ms step_avg:94.52ms
step:338/1770 train_time:31004ms step_avg:94.53ms
step:339/1770 train_time:31099ms step_avg:94.52ms
step:340/1770 train_time:31194ms step_avg:94.53ms
step:341/1770 train_time:31290ms step_avg:94.53ms
step:342/1770 train_time:31384ms step_avg:94.53ms
step:343/1770 train_time:31480ms step_avg:94.53ms
step:344/1770 train_time:31576ms step_avg:94.54ms
step:345/1770 train_time:31670ms step_avg:94.54ms
step:346/1770 train_time:31765ms step_avg:94.54ms
step:347/1770 train_time:31859ms step_avg:94.54ms
step:348/1770 train_time:31954ms step_avg:94.54ms
step:349/1770 train_time:32050ms step_avg:94.54ms
step:350/1770 train_time:32145ms step_avg:94.54ms
step:351/1770 train_time:32240ms step_avg:94.54ms
step:352/1770 train_time:32335ms step_avg:94.55ms
step:353/1770 train_time:32430ms step_avg:94.55ms
step:354/1770 train_time:32524ms step_avg:94.55ms
step:355/1770 train_time:32619ms step_avg:94.55ms
step:356/1770 train_time:32714ms step_avg:94.55ms
step:357/1770 train_time:32809ms step_avg:94.55ms
step:358/1770 train_time:32904ms step_avg:94.55ms
step:359/1770 train_time:32999ms step_avg:94.55ms
step:360/1770 train_time:33094ms step_avg:94.55ms
step:361/1770 train_time:33189ms step_avg:94.56ms
step:362/1770 train_time:33284ms step_avg:94.56ms
step:363/1770 train_time:33378ms step_avg:94.56ms
step:364/1770 train_time:33474ms step_avg:94.56ms
step:365/1770 train_time:33569ms step_avg:94.56ms
step:366/1770 train_time:33664ms step_avg:94.56ms
step:367/1770 train_time:33758ms step_avg:94.56ms
step:368/1770 train_time:33853ms step_avg:94.56ms
step:369/1770 train_time:33949ms step_avg:94.57ms
step:370/1770 train_time:34043ms step_avg:94.56ms
step:371/1770 train_time:34138ms step_avg:94.57ms
step:372/1770 train_time:34234ms step_avg:94.57ms
step:373/1770 train_time:34329ms step_avg:94.57ms
step:374/1770 train_time:34424ms step_avg:94.57ms
step:375/1770 train_time:34519ms step_avg:94.57ms
step:375/1770 val_loss:3.9048 train_time:34612ms step_avg:94.83ms
step:376/1770 train_time:34636ms step_avg:94.63ms
step:377/1770 train_time:34719ms step_avg:94.60ms
step:378/1770 train_time:34819ms step_avg:94.62ms
step:379/1770 train_time:34914ms step_avg:94.62ms
step:380/1770 train_time:35009ms step_avg:94.62ms
step:381/1770 train_time:35104ms step_avg:94.62ms
step:382/1770 train_time:35197ms step_avg:94.62ms
step:383/1770 train_time:35292ms step_avg:94.62ms
step:384/1770 train_time:35386ms step_avg:94.62ms
step:385/1770 train_time:35480ms step_avg:94.61ms
step:386/1770 train_time:35575ms step_avg:94.62ms
step:387/1770 train_time:35671ms step_avg:94.62ms
step:388/1770 train_time:35768ms step_avg:94.62ms
step:389/1770 train_time:35863ms step_avg:94.62ms
step:390/1770 train_time:35957ms step_avg:94.62ms
step:391/1770 train_time:36053ms step_avg:94.63ms
step:392/1770 train_time:36148ms step_avg:94.63ms
step:393/1770 train_time:36243ms step_avg:94.63ms
step:394/1770 train_time:36338ms step_avg:94.63ms
step:395/1770 train_time:36432ms step_avg:94.63ms
step:396/1770 train_time:36529ms step_avg:94.63ms
step:397/1770 train_time:36625ms step_avg:94.64ms
step:398/1770 train_time:36721ms step_avg:94.64ms
step:399/1770 train_time:36819ms step_avg:94.65ms
step:400/1770 train_time:36916ms step_avg:94.66ms
step:401/1770 train_time:37013ms step_avg:94.66ms
step:402/1770 train_time:37110ms step_avg:94.67ms
step:403/1770 train_time:37207ms step_avg:94.68ms
step:404/1770 train_time:37303ms step_avg:94.68ms
step:405/1770 train_time:37399ms step_avg:94.68ms
step:406/1770 train_time:37497ms step_avg:94.69ms
step:407/1770 train_time:37594ms step_avg:94.69ms
step:408/1770 train_time:37691ms step_avg:94.70ms
step:409/1770 train_time:37787ms step_avg:94.71ms
step:410/1770 train_time:37884ms step_avg:94.71ms
step:411/1770 train_time:37981ms step_avg:94.71ms
step:412/1770 train_time:38078ms step_avg:94.72ms
step:413/1770 train_time:38175ms step_avg:94.73ms
step:414/1770 train_time:38273ms step_avg:94.74ms
step:415/1770 train_time:38370ms step_avg:94.74ms
step:416/1770 train_time:38466ms step_avg:94.74ms
step:417/1770 train_time:38562ms step_avg:94.75ms
step:418/1770 train_time:38659ms step_avg:94.75ms
step:419/1770 train_time:38756ms step_avg:94.76ms
step:420/1770 train_time:38853ms step_avg:94.76ms
step:421/1770 train_time:38950ms step_avg:94.77ms
step:422/1770 train_time:39048ms step_avg:94.78ms
step:423/1770 train_time:39145ms step_avg:94.78ms
step:424/1770 train_time:39242ms step_avg:94.79ms
step:425/1770 train_time:39338ms step_avg:94.79ms
step:426/1770 train_time:39436ms step_avg:94.80ms
step:427/1770 train_time:39533ms step_avg:94.80ms
step:428/1770 train_time:39631ms step_avg:94.81ms
step:429/1770 train_time:39727ms step_avg:94.81ms
step:430/1770 train_time:39824ms step_avg:94.82ms
step:431/1770 train_time:39920ms step_avg:94.82ms
step:432/1770 train_time:40017ms step_avg:94.83ms
step:433/1770 train_time:40114ms step_avg:94.83ms
step:434/1770 train_time:40211ms step_avg:94.84ms
step:435/1770 train_time:40308ms step_avg:94.84ms
step:436/1770 train_time:40405ms step_avg:94.85ms
step:437/1770 train_time:40502ms step_avg:94.85ms
step:438/1770 train_time:40598ms step_avg:94.86ms
step:439/1770 train_time:40695ms step_avg:94.86ms
step:440/1770 train_time:40792ms step_avg:94.87ms
step:441/1770 train_time:40889ms step_avg:94.87ms
step:442/1770 train_time:40986ms step_avg:94.87ms
step:443/1770 train_time:41082ms step_avg:94.88ms
step:444/1770 train_time:41179ms step_avg:94.88ms
step:445/1770 train_time:41277ms step_avg:94.89ms
step:446/1770 train_time:41374ms step_avg:94.89ms
step:447/1770 train_time:41471ms step_avg:94.90ms
step:448/1770 train_time:41568ms step_avg:94.90ms
step:449/1770 train_time:41665ms step_avg:94.91ms
step:450/1770 train_time:41761ms step_avg:94.91ms
step:451/1770 train_time:41858ms step_avg:94.92ms
step:452/1770 train_time:41956ms step_avg:94.92ms
step:453/1770 train_time:42053ms step_avg:94.93ms
step:454/1770 train_time:42149ms step_avg:94.93ms
step:455/1770 train_time:42247ms step_avg:94.94ms
step:456/1770 train_time:42342ms step_avg:94.94ms
step:457/1770 train_time:42439ms step_avg:94.94ms
step:458/1770 train_time:42537ms step_avg:94.95ms
step:459/1770 train_time:42634ms step_avg:94.95ms
step:460/1770 train_time:42731ms step_avg:94.96ms
step:461/1770 train_time:42828ms step_avg:94.96ms
step:462/1770 train_time:42924ms step_avg:94.97ms
step:463/1770 train_time:43020ms step_avg:94.97ms
step:464/1770 train_time:43118ms step_avg:94.97ms
step:465/1770 train_time:43216ms step_avg:94.98ms
step:466/1770 train_time:43314ms step_avg:94.99ms
step:467/1770 train_time:43411ms step_avg:94.99ms
step:468/1770 train_time:43508ms step_avg:95.00ms
step:469/1770 train_time:43605ms step_avg:95.00ms
step:470/1770 train_time:43701ms step_avg:95.00ms
step:471/1770 train_time:43798ms step_avg:95.01ms
step:472/1770 train_time:43896ms step_avg:95.01ms
step:473/1770 train_time:43993ms step_avg:95.02ms
step:474/1770 train_time:44090ms step_avg:95.02ms
step:475/1770 train_time:44186ms step_avg:95.02ms
step:476/1770 train_time:44283ms step_avg:95.03ms
step:477/1770 train_time:44379ms step_avg:95.03ms
step:478/1770 train_time:44476ms step_avg:95.03ms
step:479/1770 train_time:44573ms step_avg:95.04ms
step:480/1770 train_time:44671ms step_avg:95.05ms
step:481/1770 train_time:44768ms step_avg:95.05ms
step:482/1770 train_time:44865ms step_avg:95.05ms
step:483/1770 train_time:44961ms step_avg:95.06ms
step:484/1770 train_time:45059ms step_avg:95.06ms
step:485/1770 train_time:45156ms step_avg:95.07ms
step:486/1770 train_time:45254ms step_avg:95.07ms
step:487/1770 train_time:45351ms step_avg:95.08ms
step:488/1770 train_time:45448ms step_avg:95.08ms
step:489/1770 train_time:45545ms step_avg:95.08ms
step:490/1770 train_time:45641ms step_avg:95.09ms
step:491/1770 train_time:45738ms step_avg:95.09ms
step:492/1770 train_time:45837ms step_avg:95.10ms
step:493/1770 train_time:45934ms step_avg:95.10ms
step:494/1770 train_time:46032ms step_avg:95.11ms
step:495/1770 train_time:46129ms step_avg:95.11ms
step:496/1770 train_time:46225ms step_avg:95.11ms
step:497/1770 train_time:46321ms step_avg:95.12ms
step:498/1770 train_time:46418ms step_avg:95.12ms
step:499/1770 train_time:46516ms step_avg:95.12ms
step:500/1770 train_time:46613ms step_avg:95.13ms
step:500/1770 val_loss:3.7538 train_time:46709ms step_avg:95.32ms
step:501/1770 train_time:46732ms step_avg:95.18ms
step:502/1770 train_time:46819ms step_avg:95.16ms
step:503/1770 train_time:46918ms step_avg:95.17ms
step:504/1770 train_time:47015ms step_avg:95.17ms
step:505/1770 train_time:47112ms step_avg:95.18ms
step:506/1770 train_time:47209ms step_avg:95.18ms
step:507/1770 train_time:47306ms step_avg:95.18ms
step:508/1770 train_time:47402ms step_avg:95.18ms
step:509/1770 train_time:47498ms step_avg:95.19ms
step:510/1770 train_time:47595ms step_avg:95.19ms
step:511/1770 train_time:47692ms step_avg:95.19ms
step:512/1770 train_time:47790ms step_avg:95.20ms
step:513/1770 train_time:47887ms step_avg:95.20ms
step:514/1770 train_time:47985ms step_avg:95.21ms
step:515/1770 train_time:48081ms step_avg:95.21ms
step:516/1770 train_time:48178ms step_avg:95.21ms
step:517/1770 train_time:48274ms step_avg:95.22ms
step:518/1770 train_time:48371ms step_avg:95.22ms
step:519/1770 train_time:48468ms step_avg:95.22ms
step:520/1770 train_time:48564ms step_avg:95.22ms
step:521/1770 train_time:48661ms step_avg:95.23ms
step:522/1770 train_time:48758ms step_avg:95.23ms
step:523/1770 train_time:48855ms step_avg:95.23ms
step:524/1770 train_time:48953ms step_avg:95.24ms
step:525/1770 train_time:49051ms step_avg:95.25ms
step:526/1770 train_time:49148ms step_avg:95.25ms
step:527/1770 train_time:49246ms step_avg:95.25ms
step:528/1770 train_time:49342ms step_avg:95.26ms
step:529/1770 train_time:49439ms step_avg:95.26ms
step:530/1770 train_time:49536ms step_avg:95.26ms
step:531/1770 train_time:49634ms step_avg:95.27ms
step:532/1770 train_time:49732ms step_avg:95.27ms
step:533/1770 train_time:49829ms step_avg:95.28ms
step:534/1770 train_time:49928ms step_avg:95.28ms
step:535/1770 train_time:50025ms step_avg:95.29ms
step:536/1770 train_time:50121ms step_avg:95.29ms
step:537/1770 train_time:50218ms step_avg:95.29ms
step:538/1770 train_time:50315ms step_avg:95.29ms
step:539/1770 train_time:50413ms step_avg:95.30ms
step:540/1770 train_time:50510ms step_avg:95.30ms
step:541/1770 train_time:50607ms step_avg:95.31ms
step:542/1770 train_time:50704ms step_avg:95.31ms
step:543/1770 train_time:50801ms step_avg:95.31ms
step:544/1770 train_time:50897ms step_avg:95.31ms
step:545/1770 train_time:50995ms step_avg:95.32ms
step:546/1770 train_time:51093ms step_avg:95.32ms
step:547/1770 train_time:51191ms step_avg:95.33ms
step:548/1770 train_time:51289ms step_avg:95.33ms
step:549/1770 train_time:51386ms step_avg:95.34ms
step:550/1770 train_time:51483ms step_avg:95.34ms
step:551/1770 train_time:51579ms step_avg:95.34ms
step:552/1770 train_time:51677ms step_avg:95.34ms
step:553/1770 train_time:51774ms step_avg:95.35ms
step:554/1770 train_time:51872ms step_avg:95.35ms
step:555/1770 train_time:51970ms step_avg:95.36ms
step:556/1770 train_time:52067ms step_avg:95.36ms
step:557/1770 train_time:52164ms step_avg:95.36ms
step:558/1770 train_time:52261ms step_avg:95.37ms
step:559/1770 train_time:52358ms step_avg:95.37ms
step:560/1770 train_time:52456ms step_avg:95.37ms
step:561/1770 train_time:52554ms step_avg:95.38ms
step:562/1770 train_time:52652ms step_avg:95.38ms
step:563/1770 train_time:52749ms step_avg:95.39ms
step:564/1770 train_time:52847ms step_avg:95.39ms
step:565/1770 train_time:52944ms step_avg:95.39ms
step:566/1770 train_time:53040ms step_avg:95.40ms
step:567/1770 train_time:53137ms step_avg:95.40ms
step:568/1770 train_time:53235ms step_avg:95.40ms
step:569/1770 train_time:53333ms step_avg:95.41ms
step:570/1770 train_time:53430ms step_avg:95.41ms
step:571/1770 train_time:53528ms step_avg:95.42ms
step:572/1770 train_time:53625ms step_avg:95.42ms
step:573/1770 train_time:53722ms step_avg:95.42ms
step:574/1770 train_time:53819ms step_avg:95.42ms
step:575/1770 train_time:53916ms step_avg:95.43ms
step:576/1770 train_time:54013ms step_avg:95.43ms
step:577/1770 train_time:54111ms step_avg:95.43ms
step:578/1770 train_time:54209ms step_avg:95.44ms
step:579/1770 train_time:54305ms step_avg:95.44ms
step:580/1770 train_time:54402ms step_avg:95.44ms
step:581/1770 train_time:54499ms step_avg:95.44ms
step:582/1770 train_time:54596ms step_avg:95.45ms
step:583/1770 train_time:54694ms step_avg:95.45ms
step:584/1770 train_time:54791ms step_avg:95.46ms
step:585/1770 train_time:54889ms step_avg:95.46ms
step:586/1770 train_time:54986ms step_avg:95.46ms
step:587/1770 train_time:55082ms step_avg:95.46ms
step:588/1770 train_time:55179ms step_avg:95.47ms
step:589/1770 train_time:55276ms step_avg:95.47ms
step:590/1770 train_time:55374ms step_avg:95.47ms
step:591/1770 train_time:55472ms step_avg:95.48ms
step:592/1770 train_time:55569ms step_avg:95.48ms
step:593/1770 train_time:55665ms step_avg:95.48ms
step:594/1770 train_time:55762ms step_avg:95.48ms
step:595/1770 train_time:55859ms step_avg:95.49ms
step:596/1770 train_time:55956ms step_avg:95.49ms
step:597/1770 train_time:56054ms step_avg:95.49ms
step:598/1770 train_time:56152ms step_avg:95.50ms
step:599/1770 train_time:56250ms step_avg:95.50ms
step:600/1770 train_time:56347ms step_avg:95.50ms
step:601/1770 train_time:56444ms step_avg:95.51ms
step:602/1770 train_time:56540ms step_avg:95.51ms
step:603/1770 train_time:56637ms step_avg:95.51ms
step:604/1770 train_time:56735ms step_avg:95.51ms
step:605/1770 train_time:56833ms step_avg:95.52ms
step:606/1770 train_time:56931ms step_avg:95.52ms
step:607/1770 train_time:57028ms step_avg:95.52ms
step:608/1770 train_time:57125ms step_avg:95.53ms
step:609/1770 train_time:57221ms step_avg:95.53ms
step:610/1770 train_time:57318ms step_avg:95.53ms
step:611/1770 train_time:57416ms step_avg:95.53ms
step:612/1770 train_time:57514ms step_avg:95.54ms
step:613/1770 train_time:57613ms step_avg:95.54ms
step:614/1770 train_time:57710ms step_avg:95.55ms
step:615/1770 train_time:57807ms step_avg:95.55ms
step:616/1770 train_time:57904ms step_avg:95.55ms
step:617/1770 train_time:58001ms step_avg:95.55ms
step:618/1770 train_time:58098ms step_avg:95.56ms
step:619/1770 train_time:58196ms step_avg:95.56ms
step:620/1770 train_time:58293ms step_avg:95.56ms
step:621/1770 train_time:58391ms step_avg:95.57ms
step:622/1770 train_time:58488ms step_avg:95.57ms
step:623/1770 train_time:58585ms step_avg:95.57ms
step:624/1770 train_time:58682ms step_avg:95.57ms
step:625/1770 train_time:58779ms step_avg:95.58ms
step:625/1770 val_loss:3.6651 train_time:58875ms step_avg:95.73ms
step:626/1770 train_time:58897ms step_avg:95.61ms
step:627/1770 train_time:58984ms step_avg:95.60ms
step:628/1770 train_time:59082ms step_avg:95.60ms
step:629/1770 train_time:59178ms step_avg:95.60ms
step:630/1770 train_time:59275ms step_avg:95.61ms
step:631/1770 train_time:59372ms step_avg:95.61ms
step:632/1770 train_time:59469ms step_avg:95.61ms
step:633/1770 train_time:59566ms step_avg:95.61ms
step:634/1770 train_time:59663ms step_avg:95.61ms
step:635/1770 train_time:59759ms step_avg:95.61ms
step:636/1770 train_time:59857ms step_avg:95.62ms
step:637/1770 train_time:59955ms step_avg:95.62ms
step:638/1770 train_time:60053ms step_avg:95.63ms
step:639/1770 train_time:60151ms step_avg:95.63ms
step:640/1770 train_time:60249ms step_avg:95.63ms
step:641/1770 train_time:60346ms step_avg:95.64ms
step:642/1770 train_time:60442ms step_avg:95.64ms
step:643/1770 train_time:60539ms step_avg:95.64ms
step:644/1770 train_time:60636ms step_avg:95.64ms
step:645/1770 train_time:60733ms step_avg:95.64ms
step:646/1770 train_time:60831ms step_avg:95.65ms
step:647/1770 train_time:60928ms step_avg:95.65ms
step:648/1770 train_time:61026ms step_avg:95.65ms
step:649/1770 train_time:61124ms step_avg:95.66ms
step:650/1770 train_time:61221ms step_avg:95.66ms
step:651/1770 train_time:61318ms step_avg:95.66ms
step:652/1770 train_time:61415ms step_avg:95.66ms
step:653/1770 train_time:61513ms step_avg:95.66ms
step:654/1770 train_time:61610ms step_avg:95.67ms
step:655/1770 train_time:61707ms step_avg:95.67ms
step:656/1770 train_time:61805ms step_avg:95.67ms
step:657/1770 train_time:61902ms step_avg:95.67ms
step:658/1770 train_time:62000ms step_avg:95.68ms
step:659/1770 train_time:62098ms step_avg:95.68ms
step:660/1770 train_time:62198ms step_avg:95.69ms
step:661/1770 train_time:62296ms step_avg:95.69ms
step:662/1770 train_time:62395ms step_avg:95.70ms
step:663/1770 train_time:62494ms step_avg:95.70ms
step:664/1770 train_time:62593ms step_avg:95.71ms
step:665/1770 train_time:62693ms step_avg:95.71ms
step:666/1770 train_time:62792ms step_avg:95.72ms
step:667/1770 train_time:62891ms step_avg:95.72ms
step:668/1770 train_time:62991ms step_avg:95.73ms
step:669/1770 train_time:63091ms step_avg:95.74ms
step:670/1770 train_time:63191ms step_avg:95.74ms
step:671/1770 train_time:63291ms step_avg:95.75ms
step:672/1770 train_time:63391ms step_avg:95.76ms
step:673/1770 train_time:63490ms step_avg:95.76ms
step:674/1770 train_time:63589ms step_avg:95.77ms
step:675/1770 train_time:63688ms step_avg:95.77ms
step:676/1770 train_time:63788ms step_avg:95.78ms
step:677/1770 train_time:63887ms step_avg:95.78ms
step:678/1770 train_time:63986ms step_avg:95.79ms
step:679/1770 train_time:64085ms step_avg:95.79ms
step:680/1770 train_time:64184ms step_avg:95.80ms
step:681/1770 train_time:64282ms step_avg:95.80ms
step:682/1770 train_time:64380ms step_avg:95.80ms
step:683/1770 train_time:64479ms step_avg:95.81ms
step:684/1770 train_time:64578ms step_avg:95.81ms
step:685/1770 train_time:64676ms step_avg:95.82ms
step:686/1770 train_time:64774ms step_avg:95.82ms
step:687/1770 train_time:64874ms step_avg:95.83ms
step:688/1770 train_time:64973ms step_avg:95.83ms
step:689/1770 train_time:65073ms step_avg:95.84ms
step:690/1770 train_time:65173ms step_avg:95.84ms
step:691/1770 train_time:65273ms step_avg:95.85ms
step:692/1770 train_time:65372ms step_avg:95.85ms
step:693/1770 train_time:65472ms step_avg:95.86ms
step:694/1770 train_time:65571ms step_avg:95.86ms
step:695/1770 train_time:65671ms step_avg:95.87ms
step:696/1770 train_time:65771ms step_avg:95.88ms
step:697/1770 train_time:65870ms step_avg:95.88ms
step:698/1770 train_time:65969ms step_avg:95.89ms
step:699/1770 train_time:66069ms step_avg:95.89ms
step:700/1770 train_time:66168ms step_avg:95.90ms
step:701/1770 train_time:66268ms step_avg:95.90ms
step:702/1770 train_time:66366ms step_avg:95.91ms
step:703/1770 train_time:66465ms step_avg:95.91ms
step:704/1770 train_time:66564ms step_avg:95.91ms
step:705/1770 train_time:66663ms step_avg:95.92ms
step:706/1770 train_time:66761ms step_avg:95.92ms
step:707/1770 train_time:66860ms step_avg:95.92ms
step:708/1770 train_time:66958ms step_avg:95.93ms
step:709/1770 train_time:67057ms step_avg:95.93ms
step:710/1770 train_time:67156ms step_avg:95.94ms
step:711/1770 train_time:67255ms step_avg:95.94ms
step:712/1770 train_time:67355ms step_avg:95.95ms
step:713/1770 train_time:67454ms step_avg:95.95ms
step:714/1770 train_time:67554ms step_avg:95.96ms
step:715/1770 train_time:67653ms step_avg:95.96ms
step:716/1770 train_time:67753ms step_avg:95.97ms
step:717/1770 train_time:67852ms step_avg:95.97ms
step:718/1770 train_time:67952ms step_avg:95.98ms
step:719/1770 train_time:68053ms step_avg:95.98ms
step:720/1770 train_time:68152ms step_avg:95.99ms
step:721/1770 train_time:68251ms step_avg:95.99ms
step:722/1770 train_time:68351ms step_avg:96.00ms
step:723/1770 train_time:68450ms step_avg:96.00ms
step:724/1770 train_time:68550ms step_avg:96.01ms
step:725/1770 train_time:68649ms step_avg:96.01ms
step:726/1770 train_time:68748ms step_avg:96.02ms
step:727/1770 train_time:68847ms step_avg:96.02ms
step:728/1770 train_time:68945ms step_avg:96.02ms
step:729/1770 train_time:69045ms step_avg:96.03ms
step:730/1770 train_time:69144ms step_avg:96.03ms
step:731/1770 train_time:69243ms step_avg:96.04ms
step:732/1770 train_time:69341ms step_avg:96.04ms
step:733/1770 train_time:69440ms step_avg:96.04ms
step:734/1770 train_time:69538ms step_avg:96.05ms
step:735/1770 train_time:69637ms step_avg:96.05ms
step:736/1770 train_time:69736ms step_avg:96.05ms
step:737/1770 train_time:69834ms step_avg:96.06ms
step:738/1770 train_time:69933ms step_avg:96.06ms
step:739/1770 train_time:70033ms step_avg:96.07ms
step:740/1770 train_time:70133ms step_avg:96.07ms
step:741/1770 train_time:70233ms step_avg:96.08ms
step:742/1770 train_time:70332ms step_avg:96.08ms
step:743/1770 train_time:70432ms step_avg:96.09ms
step:744/1770 train_time:70531ms step_avg:96.09ms
step:745/1770 train_time:70631ms step_avg:96.10ms
step:746/1770 train_time:70730ms step_avg:96.10ms
step:747/1770 train_time:70829ms step_avg:96.10ms
step:748/1770 train_time:70928ms step_avg:96.11ms
step:749/1770 train_time:71027ms step_avg:96.11ms
step:750/1770 train_time:71126ms step_avg:96.12ms
step:750/1770 val_loss:3.6012 train_time:71224ms step_avg:96.25ms
step:751/1770 train_time:71246ms step_avg:96.15ms
step:752/1770 train_time:71338ms step_avg:96.14ms
step:753/1770 train_time:71438ms step_avg:96.15ms
step:754/1770 train_time:71537ms step_avg:96.15ms
step:755/1770 train_time:71636ms step_avg:96.16ms
step:756/1770 train_time:71734ms step_avg:96.16ms
step:757/1770 train_time:71833ms step_avg:96.16ms
step:758/1770 train_time:71931ms step_avg:96.16ms
step:759/1770 train_time:72030ms step_avg:96.17ms
step:760/1770 train_time:72128ms step_avg:96.17ms
step:761/1770 train_time:72228ms step_avg:96.18ms
step:762/1770 train_time:72329ms step_avg:96.18ms
step:763/1770 train_time:72429ms step_avg:96.19ms
step:764/1770 train_time:72528ms step_avg:96.19ms
step:765/1770 train_time:72627ms step_avg:96.20ms
step:766/1770 train_time:72726ms step_avg:96.20ms
step:767/1770 train_time:72825ms step_avg:96.20ms
step:768/1770 train_time:72923ms step_avg:96.20ms
step:769/1770 train_time:73021ms step_avg:96.21ms
step:770/1770 train_time:73119ms step_avg:96.21ms
step:771/1770 train_time:73218ms step_avg:96.21ms
step:772/1770 train_time:73316ms step_avg:96.22ms
step:773/1770 train_time:73416ms step_avg:96.22ms
step:774/1770 train_time:73516ms step_avg:96.23ms
step:775/1770 train_time:73616ms step_avg:96.23ms
step:776/1770 train_time:73716ms step_avg:96.23ms
step:777/1770 train_time:73816ms step_avg:96.24ms
step:778/1770 train_time:73915ms step_avg:96.24ms
step:779/1770 train_time:74014ms step_avg:96.25ms
step:780/1770 train_time:74113ms step_avg:96.25ms
step:781/1770 train_time:74213ms step_avg:96.25ms
step:782/1770 train_time:74311ms step_avg:96.26ms
step:783/1770 train_time:74411ms step_avg:96.26ms
step:784/1770 train_time:74511ms step_avg:96.27ms
step:785/1770 train_time:74610ms step_avg:96.27ms
step:786/1770 train_time:74710ms step_avg:96.28ms
step:787/1770 train_time:74810ms step_avg:96.28ms
step:788/1770 train_time:74909ms step_avg:96.28ms
step:789/1770 train_time:75008ms step_avg:96.29ms
step:790/1770 train_time:75108ms step_avg:96.29ms
step:791/1770 train_time:75207ms step_avg:96.30ms
step:792/1770 train_time:75305ms step_avg:96.30ms
step:793/1770 train_time:75405ms step_avg:96.30ms
step:794/1770 train_time:75504ms step_avg:96.31ms
step:795/1770 train_time:75603ms step_avg:96.31ms
step:796/1770 train_time:75701ms step_avg:96.31ms
step:797/1770 train_time:75800ms step_avg:96.32ms
step:798/1770 train_time:75900ms step_avg:96.32ms
step:799/1770 train_time:75998ms step_avg:96.32ms
step:800/1770 train_time:76097ms step_avg:96.33ms
step:801/1770 train_time:76197ms step_avg:96.33ms
step:802/1770 train_time:76296ms step_avg:96.33ms
step:803/1770 train_time:76396ms step_avg:96.34ms
step:804/1770 train_time:76496ms step_avg:96.34ms
step:805/1770 train_time:76596ms step_avg:96.35ms
step:806/1770 train_time:76695ms step_avg:96.35ms
step:807/1770 train_time:76796ms step_avg:96.36ms
step:808/1770 train_time:76896ms step_avg:96.36ms
step:809/1770 train_time:76996ms step_avg:96.36ms
step:810/1770 train_time:77095ms step_avg:96.37ms
step:811/1770 train_time:77195ms step_avg:96.37ms
step:812/1770 train_time:77294ms step_avg:96.38ms
step:813/1770 train_time:77394ms step_avg:96.38ms
step:814/1770 train_time:77494ms step_avg:96.39ms
step:815/1770 train_time:77594ms step_avg:96.39ms
step:816/1770 train_time:77694ms step_avg:96.39ms
step:817/1770 train_time:77794ms step_avg:96.40ms
step:818/1770 train_time:77894ms step_avg:96.40ms
step:819/1770 train_time:77994ms step_avg:96.41ms
step:820/1770 train_time:78093ms step_avg:96.41ms
step:821/1770 train_time:78193ms step_avg:96.42ms
step:822/1770 train_time:78292ms step_avg:96.42ms
step:823/1770 train_time:78392ms step_avg:96.42ms
step:824/1770 train_time:78491ms step_avg:96.43ms
step:825/1770 train_time:78590ms step_avg:96.43ms
step:826/1770 train_time:78689ms step_avg:96.43ms
step:827/1770 train_time:78789ms step_avg:96.44ms
step:828/1770 train_time:78888ms step_avg:96.44ms
step:829/1770 train_time:78988ms step_avg:96.44ms
step:830/1770 train_time:79087ms step_avg:96.45ms
step:831/1770 train_time:79187ms step_avg:96.45ms
step:832/1770 train_time:79286ms step_avg:96.45ms
step:833/1770 train_time:79385ms step_avg:96.46ms
step:834/1770 train_time:79484ms step_avg:96.46ms
step:835/1770 train_time:79583ms step_avg:96.46ms
step:836/1770 train_time:79682ms step_avg:96.47ms
step:837/1770 train_time:79781ms step_avg:96.47ms
step:838/1770 train_time:79880ms step_avg:96.47ms
step:839/1770 train_time:79978ms step_avg:96.48ms
step:840/1770 train_time:80077ms step_avg:96.48ms
step:841/1770 train_time:80176ms step_avg:96.48ms
step:842/1770 train_time:80275ms step_avg:96.48ms
step:843/1770 train_time:80374ms step_avg:96.49ms
step:844/1770 train_time:80474ms step_avg:96.49ms
step:845/1770 train_time:80574ms step_avg:96.50ms
step:846/1770 train_time:80674ms step_avg:96.50ms
step:847/1770 train_time:80774ms step_avg:96.50ms
step:848/1770 train_time:80873ms step_avg:96.51ms
step:849/1770 train_time:80973ms step_avg:96.51ms
step:850/1770 train_time:81072ms step_avg:96.51ms
step:851/1770 train_time:81172ms step_avg:96.52ms
step:852/1770 train_time:81271ms step_avg:96.52ms
step:853/1770 train_time:81372ms step_avg:96.53ms
step:854/1770 train_time:81471ms step_avg:96.53ms
step:855/1770 train_time:81571ms step_avg:96.53ms
step:856/1770 train_time:81671ms step_avg:96.54ms
step:857/1770 train_time:81771ms step_avg:96.54ms
step:858/1770 train_time:81870ms step_avg:96.55ms
step:859/1770 train_time:81971ms step_avg:96.55ms
step:860/1770 train_time:82070ms step_avg:96.55ms
step:861/1770 train_time:82169ms step_avg:96.56ms
step:862/1770 train_time:82268ms step_avg:96.56ms
step:863/1770 train_time:82367ms step_avg:96.56ms
step:864/1770 train_time:82466ms step_avg:96.56ms
step:865/1770 train_time:82565ms step_avg:96.57ms
step:866/1770 train_time:82664ms step_avg:96.57ms
step:867/1770 train_time:82763ms step_avg:96.57ms
step:868/1770 train_time:82862ms step_avg:96.58ms
step:869/1770 train_time:82960ms step_avg:96.58ms
step:870/1770 train_time:83059ms step_avg:96.58ms
step:871/1770 train_time:83159ms step_avg:96.58ms
step:872/1770 train_time:83257ms step_avg:96.59ms
step:873/1770 train_time:83356ms step_avg:96.59ms
step:874/1770 train_time:83457ms step_avg:96.59ms
step:875/1770 train_time:83556ms step_avg:96.60ms
step:875/1770 val_loss:3.5522 train_time:83654ms step_avg:96.71ms
step:876/1770 train_time:83677ms step_avg:96.62ms
step:877/1770 train_time:83764ms step_avg:96.61ms
step:878/1770 train_time:83864ms step_avg:96.62ms
step:879/1770 train_time:83963ms step_avg:96.62ms
step:880/1770 train_time:84061ms step_avg:96.62ms
step:881/1770 train_time:84159ms step_avg:96.62ms
step:882/1770 train_time:84258ms step_avg:96.63ms
step:883/1770 train_time:84357ms step_avg:96.63ms
step:884/1770 train_time:84456ms step_avg:96.63ms
step:885/1770 train_time:84555ms step_avg:96.63ms
step:886/1770 train_time:84654ms step_avg:96.64ms
step:887/1770 train_time:84754ms step_avg:96.64ms
step:888/1770 train_time:84855ms step_avg:96.65ms
step:889/1770 train_time:84956ms step_avg:96.65ms
step:890/1770 train_time:85056ms step_avg:96.65ms
step:891/1770 train_time:85156ms step_avg:96.66ms
step:892/1770 train_time:85256ms step_avg:96.66ms
step:893/1770 train_time:85355ms step_avg:96.67ms
step:894/1770 train_time:85455ms step_avg:96.67ms
step:895/1770 train_time:85554ms step_avg:96.67ms
step:896/1770 train_time:85653ms step_avg:96.67ms
step:897/1770 train_time:85753ms step_avg:96.68ms
step:898/1770 train_time:85853ms step_avg:96.68ms
step:899/1770 train_time:85954ms step_avg:96.69ms
step:900/1770 train_time:86054ms step_avg:96.69ms
step:901/1770 train_time:86153ms step_avg:96.69ms
step:902/1770 train_time:86253ms step_avg:96.70ms
step:903/1770 train_time:86353ms step_avg:96.70ms
step:904/1770 train_time:86452ms step_avg:96.70ms
step:905/1770 train_time:86552ms step_avg:96.71ms
step:906/1770 train_time:86651ms step_avg:96.71ms
step:907/1770 train_time:86751ms step_avg:96.71ms
step:908/1770 train_time:86852ms step_avg:96.72ms
step:909/1770 train_time:86952ms step_avg:96.72ms
step:910/1770 train_time:87054ms step_avg:96.73ms
step:911/1770 train_time:87154ms step_avg:96.73ms
step:912/1770 train_time:87254ms step_avg:96.73ms
step:913/1770 train_time:87354ms step_avg:96.74ms
step:914/1770 train_time:87453ms step_avg:96.74ms
step:915/1770 train_time:87552ms step_avg:96.74ms
step:916/1770 train_time:87651ms step_avg:96.75ms
step:917/1770 train_time:87750ms step_avg:96.75ms
step:918/1770 train_time:87849ms step_avg:96.75ms
step:919/1770 train_time:87948ms step_avg:96.75ms
step:920/1770 train_time:88050ms step_avg:96.76ms
step:921/1770 train_time:88152ms step_avg:96.76ms
step:922/1770 train_time:88255ms step_avg:96.77ms
step:923/1770 train_time:88355ms step_avg:96.77ms
step:924/1770 train_time:88456ms step_avg:96.78ms
step:925/1770 train_time:88557ms step_avg:96.78ms
step:926/1770 train_time:88657ms step_avg:96.79ms
step:927/1770 train_time:88759ms step_avg:96.79ms
step:928/1770 train_time:88859ms step_avg:96.80ms
step:929/1770 train_time:88959ms step_avg:96.80ms
step:930/1770 train_time:89060ms step_avg:96.80ms
step:931/1770 train_time:89161ms step_avg:96.81ms
step:932/1770 train_time:89261ms step_avg:96.81ms
step:933/1770 train_time:89361ms step_avg:96.82ms
step:934/1770 train_time:89461ms step_avg:96.82ms
step:935/1770 train_time:89560ms step_avg:96.82ms
step:936/1770 train_time:89660ms step_avg:96.82ms
step:937/1770 train_time:89760ms step_avg:96.83ms
step:938/1770 train_time:89860ms step_avg:96.83ms
step:939/1770 train_time:89960ms step_avg:96.84ms
step:940/1770 train_time:90060ms step_avg:96.84ms
step:941/1770 train_time:90161ms step_avg:96.84ms
step:942/1770 train_time:90261ms step_avg:96.85ms
step:943/1770 train_time:90362ms step_avg:96.85ms
step:944/1770 train_time:90462ms step_avg:96.85ms
step:945/1770 train_time:90561ms step_avg:96.86ms
step:946/1770 train_time:90662ms step_avg:96.86ms
step:947/1770 train_time:90762ms step_avg:96.86ms
step:948/1770 train_time:90861ms step_avg:96.87ms
step:949/1770 train_time:90961ms step_avg:96.87ms
step:950/1770 train_time:91062ms step_avg:96.87ms
step:951/1770 train_time:91162ms step_avg:96.88ms
step:952/1770 train_time:91263ms step_avg:96.88ms
step:953/1770 train_time:91363ms step_avg:96.89ms
step:954/1770 train_time:91463ms step_avg:96.89ms
step:955/1770 train_time:91563ms step_avg:96.89ms
step:956/1770 train_time:91663ms step_avg:96.90ms
step:957/1770 train_time:91763ms step_avg:96.90ms
step:958/1770 train_time:91864ms step_avg:96.90ms
step:959/1770 train_time:91964ms step_avg:96.91ms
step:960/1770 train_time:92064ms step_avg:96.91ms
step:961/1770 train_time:92165ms step_avg:96.91ms
step:962/1770 train_time:92266ms step_avg:96.92ms
step:963/1770 train_time:92367ms step_avg:96.92ms
step:964/1770 train_time:92468ms step_avg:96.93ms
step:965/1770 train_time:92568ms step_avg:96.93ms
step:966/1770 train_time:92668ms step_avg:96.93ms
step:967/1770 train_time:92769ms step_avg:96.94ms
step:968/1770 train_time:92870ms step_avg:96.94ms
step:969/1770 train_time:92971ms step_avg:96.95ms
step:970/1770 train_time:93072ms step_avg:96.95ms
step:971/1770 train_time:93173ms step_avg:96.95ms
step:972/1770 train_time:93274ms step_avg:96.96ms
step:973/1770 train_time:93376ms step_avg:96.96ms
step:974/1770 train_time:93477ms step_avg:96.97ms
step:975/1770 train_time:93577ms step_avg:96.97ms
step:976/1770 train_time:93678ms step_avg:96.98ms
step:977/1770 train_time:93779ms step_avg:96.98ms
step:978/1770 train_time:93879ms step_avg:96.98ms
step:979/1770 train_time:93980ms step_avg:96.99ms
step:980/1770 train_time:94080ms step_avg:96.99ms
step:981/1770 train_time:94180ms step_avg:96.99ms
step:982/1770 train_time:94280ms step_avg:97.00ms
step:983/1770 train_time:94380ms step_avg:97.00ms
step:984/1770 train_time:94481ms step_avg:97.00ms
step:985/1770 train_time:94581ms step_avg:97.01ms
step:986/1770 train_time:94681ms step_avg:97.01ms
step:987/1770 train_time:94781ms step_avg:97.01ms
step:988/1770 train_time:94881ms step_avg:97.02ms
step:989/1770 train_time:94983ms step_avg:97.02ms
step:990/1770 train_time:95083ms step_avg:97.02ms
step:991/1770 train_time:95183ms step_avg:97.03ms
step:992/1770 train_time:95283ms step_avg:97.03ms
step:993/1770 train_time:95383ms step_avg:97.03ms
step:994/1770 train_time:95484ms step_avg:97.04ms
step:995/1770 train_time:95585ms step_avg:97.04ms
step:996/1770 train_time:95685ms step_avg:97.04ms
step:997/1770 train_time:95785ms step_avg:97.05ms
step:998/1770 train_time:95885ms step_avg:97.05ms
step:999/1770 train_time:95985ms step_avg:97.05ms
step:1000/1770 train_time:96085ms step_avg:97.06ms
step:1000/1770 val_loss:3.5129 train_time:96183ms step_avg:97.15ms
step:1001/1770 train_time:96206ms step_avg:97.08ms
step:1002/1770 train_time:96296ms step_avg:97.07ms
step:1003/1770 train_time:96398ms step_avg:97.08ms
step:1004/1770 train_time:96499ms step_avg:97.08ms
step:1005/1770 train_time:96599ms step_avg:97.08ms
step:1006/1770 train_time:96699ms step_avg:97.09ms
step:1007/1770 train_time:96800ms step_avg:97.09ms
step:1008/1770 train_time:96900ms step_avg:97.09ms
step:1009/1770 train_time:97000ms step_avg:97.10ms
step:1010/1770 train_time:97100ms step_avg:97.10ms
step:1011/1770 train_time:97203ms step_avg:97.11ms
step:1012/1770 train_time:97305ms step_avg:97.11ms
step:1013/1770 train_time:97406ms step_avg:97.11ms
step:1014/1770 train_time:97506ms step_avg:97.12ms
step:1015/1770 train_time:97606ms step_avg:97.12ms
step:1016/1770 train_time:97706ms step_avg:97.12ms
step:1017/1770 train_time:97807ms step_avg:97.13ms
step:1018/1770 train_time:97906ms step_avg:97.13ms
step:1019/1770 train_time:98006ms step_avg:97.13ms
step:1020/1770 train_time:98105ms step_avg:97.13ms
step:1021/1770 train_time:98206ms step_avg:97.14ms
step:1022/1770 train_time:98306ms step_avg:97.14ms
step:1023/1770 train_time:98406ms step_avg:97.14ms
step:1024/1770 train_time:98506ms step_avg:97.15ms
step:1025/1770 train_time:98606ms step_avg:97.15ms
step:1026/1770 train_time:98707ms step_avg:97.15ms
step:1027/1770 train_time:98807ms step_avg:97.15ms
step:1028/1770 train_time:98907ms step_avg:97.16ms
step:1029/1770 train_time:99007ms step_avg:97.16ms
step:1030/1770 train_time:99107ms step_avg:97.16ms
step:1031/1770 train_time:99206ms step_avg:97.17ms
step:1032/1770 train_time:99307ms step_avg:97.17ms
step:1033/1770 train_time:99407ms step_avg:97.17ms
step:1034/1770 train_time:99507ms step_avg:97.18ms
step:1035/1770 train_time:99607ms step_avg:97.18ms
step:1036/1770 train_time:99706ms step_avg:97.18ms
step:1037/1770 train_time:99806ms step_avg:97.18ms
step:1038/1770 train_time:99906ms step_avg:97.18ms
step:1039/1770 train_time:100006ms step_avg:97.19ms
step:1040/1770 train_time:100106ms step_avg:97.19ms
step:1041/1770 train_time:100206ms step_avg:97.19ms
step:1042/1770 train_time:100306ms step_avg:97.20ms
step:1043/1770 train_time:100406ms step_avg:97.20ms
step:1044/1770 train_time:100507ms step_avg:97.20ms
step:1045/1770 train_time:100607ms step_avg:97.20ms
step:1046/1770 train_time:100706ms step_avg:97.21ms
step:1047/1770 train_time:100807ms step_avg:97.21ms
step:1048/1770 train_time:100908ms step_avg:97.21ms
step:1049/1770 train_time:101008ms step_avg:97.22ms
step:1050/1770 train_time:101108ms step_avg:97.22ms
step:1051/1770 train_time:101208ms step_avg:97.22ms
step:1052/1770 train_time:101308ms step_avg:97.22ms
step:1053/1770 train_time:101409ms step_avg:97.23ms
step:1054/1770 train_time:101508ms step_avg:97.23ms
step:1055/1770 train_time:101609ms step_avg:97.23ms
step:1056/1770 train_time:101709ms step_avg:97.24ms
step:1057/1770 train_time:101809ms step_avg:97.24ms
step:1058/1770 train_time:101910ms step_avg:97.24ms
step:1059/1770 train_time:102010ms step_avg:97.24ms
step:1060/1770 train_time:102110ms step_avg:97.25ms
step:1061/1770 train_time:102211ms step_avg:97.25ms
step:1062/1770 train_time:102312ms step_avg:97.25ms
step:1063/1770 train_time:102414ms step_avg:97.26ms
step:1064/1770 train_time:102515ms step_avg:97.26ms
step:1065/1770 train_time:102616ms step_avg:97.27ms
step:1066/1770 train_time:102717ms step_avg:97.27ms
step:1067/1770 train_time:102819ms step_avg:97.27ms
step:1068/1770 train_time:102922ms step_avg:97.28ms
step:1069/1770 train_time:103023ms step_avg:97.28ms
step:1070/1770 train_time:103125ms step_avg:97.29ms
step:1071/1770 train_time:103225ms step_avg:97.29ms
step:1072/1770 train_time:103325ms step_avg:97.29ms
step:1073/1770 train_time:103426ms step_avg:97.30ms
step:1074/1770 train_time:103526ms step_avg:97.30ms
step:1075/1770 train_time:103627ms step_avg:97.30ms
step:1076/1770 train_time:103728ms step_avg:97.31ms
step:1077/1770 train_time:103827ms step_avg:97.31ms
step:1078/1770 train_time:103927ms step_avg:97.31ms
step:1079/1770 train_time:104027ms step_avg:97.31ms
step:1080/1770 train_time:104127ms step_avg:97.31ms
step:1081/1770 train_time:104227ms step_avg:97.32ms
step:1082/1770 train_time:104327ms step_avg:97.32ms
step:1083/1770 train_time:104427ms step_avg:97.32ms
step:1084/1770 train_time:104528ms step_avg:97.33ms
step:1085/1770 train_time:104628ms step_avg:97.33ms
step:1086/1770 train_time:104728ms step_avg:97.33ms
step:1087/1770 train_time:104828ms step_avg:97.33ms
step:1088/1770 train_time:104928ms step_avg:97.34ms
step:1089/1770 train_time:105028ms step_avg:97.34ms
step:1090/1770 train_time:105129ms step_avg:97.34ms
step:1091/1770 train_time:105229ms step_avg:97.34ms
step:1092/1770 train_time:105330ms step_avg:97.35ms
step:1093/1770 train_time:105431ms step_avg:97.35ms
step:1094/1770 train_time:105532ms step_avg:97.35ms
step:1095/1770 train_time:105633ms step_avg:97.36ms
step:1096/1770 train_time:105733ms step_avg:97.36ms
step:1097/1770 train_time:105834ms step_avg:97.36ms
step:1098/1770 train_time:105935ms step_avg:97.37ms
step:1099/1770 train_time:106036ms step_avg:97.37ms
step:1100/1770 train_time:106138ms step_avg:97.37ms
step:1101/1770 train_time:106240ms step_avg:97.38ms
step:1102/1770 train_time:106340ms step_avg:97.38ms
step:1103/1770 train_time:106441ms step_avg:97.38ms
step:1104/1770 train_time:106543ms step_avg:97.39ms
step:1105/1770 train_time:106644ms step_avg:97.39ms
step:1106/1770 train_time:106745ms step_avg:97.40ms
step:1107/1770 train_time:106847ms step_avg:97.40ms
step:1108/1770 train_time:106947ms step_avg:97.40ms
step:1109/1770 train_time:107047ms step_avg:97.40ms
step:1110/1770 train_time:107147ms step_avg:97.41ms
step:1111/1770 train_time:107248ms step_avg:97.41ms
step:1112/1770 train_time:107349ms step_avg:97.41ms
step:1113/1770 train_time:107449ms step_avg:97.41ms
step:1114/1770 train_time:107549ms step_avg:97.42ms
step:1115/1770 train_time:107649ms step_avg:97.42ms
step:1116/1770 train_time:107750ms step_avg:97.42ms
step:1117/1770 train_time:107850ms step_avg:97.43ms
step:1118/1770 train_time:107950ms step_avg:97.43ms
step:1119/1770 train_time:108051ms step_avg:97.43ms
step:1120/1770 train_time:108151ms step_avg:97.43ms
step:1121/1770 train_time:108251ms step_avg:97.44ms
step:1122/1770 train_time:108353ms step_avg:97.44ms
step:1123/1770 train_time:108453ms step_avg:97.44ms
step:1124/1770 train_time:108554ms step_avg:97.45ms
step:1125/1770 train_time:108656ms step_avg:97.45ms
step:1125/1770 val_loss:3.4738 train_time:108755ms step_avg:97.54ms
step:1126/1770 train_time:108777ms step_avg:97.47ms
step:1127/1770 train_time:108869ms step_avg:97.47ms
step:1128/1770 train_time:108972ms step_avg:97.47ms
step:1129/1770 train_time:109072ms step_avg:97.47ms
step:1130/1770 train_time:109173ms step_avg:97.48ms
step:1131/1770 train_time:109274ms step_avg:97.48ms
step:1132/1770 train_time:109374ms step_avg:97.48ms
step:1133/1770 train_time:109474ms step_avg:97.48ms
step:1134/1770 train_time:109574ms step_avg:97.49ms
step:1135/1770 train_time:109674ms step_avg:97.49ms
step:1136/1770 train_time:109774ms step_avg:97.49ms
step:1137/1770 train_time:109874ms step_avg:97.49ms
step:1138/1770 train_time:109975ms step_avg:97.50ms
step:1139/1770 train_time:110075ms step_avg:97.50ms
step:1140/1770 train_time:110175ms step_avg:97.50ms
step:1141/1770 train_time:110275ms step_avg:97.50ms
step:1142/1770 train_time:110375ms step_avg:97.50ms
step:1143/1770 train_time:110475ms step_avg:97.51ms
step:1144/1770 train_time:110575ms step_avg:97.51ms
step:1145/1770 train_time:110675ms step_avg:97.51ms
step:1146/1770 train_time:110776ms step_avg:97.51ms
step:1147/1770 train_time:110876ms step_avg:97.52ms
step:1148/1770 train_time:110976ms step_avg:97.52ms
step:1149/1770 train_time:111076ms step_avg:97.52ms
step:1150/1770 train_time:111176ms step_avg:97.52ms
step:1151/1770 train_time:111277ms step_avg:97.53ms
step:1152/1770 train_time:111377ms step_avg:97.53ms
step:1153/1770 train_time:111477ms step_avg:97.53ms
step:1154/1770 train_time:111579ms step_avg:97.53ms
step:1155/1770 train_time:111679ms step_avg:97.54ms
step:1156/1770 train_time:111779ms step_avg:97.54ms
step:1157/1770 train_time:111882ms step_avg:97.54ms
step:1158/1770 train_time:111983ms step_avg:97.55ms
step:1159/1770 train_time:112084ms step_avg:97.55ms
step:1160/1770 train_time:112186ms step_avg:97.55ms
step:1161/1770 train_time:112287ms step_avg:97.56ms
step:1162/1770 train_time:112388ms step_avg:97.56ms
step:1163/1770 train_time:112490ms step_avg:97.56ms
step:1164/1770 train_time:112591ms step_avg:97.57ms
step:1165/1770 train_time:112692ms step_avg:97.57ms
step:1166/1770 train_time:112793ms step_avg:97.57ms
step:1167/1770 train_time:112893ms step_avg:97.57ms
step:1168/1770 train_time:112994ms step_avg:97.58ms
step:1169/1770 train_time:113094ms step_avg:97.58ms
step:1170/1770 train_time:113194ms step_avg:97.58ms
step:1171/1770 train_time:113295ms step_avg:97.58ms
step:1172/1770 train_time:113395ms step_avg:97.59ms
step:1173/1770 train_time:113495ms step_avg:97.59ms
step:1174/1770 train_time:113596ms step_avg:97.59ms
step:1175/1770 train_time:113696ms step_avg:97.59ms
step:1176/1770 train_time:113797ms step_avg:97.60ms
step:1177/1770 train_time:113899ms step_avg:97.60ms
step:1178/1770 train_time:114000ms step_avg:97.60ms
step:1179/1770 train_time:114101ms step_avg:97.61ms
step:1180/1770 train_time:114203ms step_avg:97.61ms
step:1181/1770 train_time:114305ms step_avg:97.61ms
step:1182/1770 train_time:114406ms step_avg:97.62ms
step:1183/1770 train_time:114508ms step_avg:97.62ms
step:1184/1770 train_time:114613ms step_avg:97.63ms
step:1185/1770 train_time:114714ms step_avg:97.63ms
step:1186/1770 train_time:114816ms step_avg:97.63ms
step:1187/1770 train_time:114920ms step_avg:97.64ms
step:1188/1770 train_time:115022ms step_avg:97.64ms
step:1189/1770 train_time:115123ms step_avg:97.64ms
step:1190/1770 train_time:115224ms step_avg:97.65ms
step:1191/1770 train_time:115327ms step_avg:97.65ms
step:1192/1770 train_time:115430ms step_avg:97.66ms
step:1193/1770 train_time:115533ms step_avg:97.66ms
step:1194/1770 train_time:115635ms step_avg:97.66ms
step:1195/1770 train_time:115738ms step_avg:97.67ms
step:1196/1770 train_time:115841ms step_avg:97.67ms
step:1197/1770 train_time:115943ms step_avg:97.68ms
step:1198/1770 train_time:116045ms step_avg:97.68ms
step:1199/1770 train_time:116147ms step_avg:97.68ms
step:1200/1770 train_time:116249ms step_avg:97.69ms
step:1201/1770 train_time:116351ms step_avg:97.69ms
step:1202/1770 train_time:116453ms step_avg:97.70ms
step:1203/1770 train_time:116555ms step_avg:97.70ms
step:1204/1770 train_time:116657ms step_avg:97.70ms
step:1205/1770 train_time:116759ms step_avg:97.71ms
step:1206/1770 train_time:116862ms step_avg:97.71ms
step:1207/1770 train_time:116963ms step_avg:97.71ms
step:1208/1770 train_time:117065ms step_avg:97.72ms
step:1209/1770 train_time:117166ms step_avg:97.72ms
step:1210/1770 train_time:117269ms step_avg:97.72ms
step:1211/1770 train_time:117372ms step_avg:97.73ms
step:1212/1770 train_time:117476ms step_avg:97.73ms
step:1213/1770 train_time:117577ms step_avg:97.74ms
step:1214/1770 train_time:117679ms step_avg:97.74ms
step:1215/1770 train_time:117780ms step_avg:97.74ms
step:1216/1770 train_time:117885ms step_avg:97.75ms
step:1217/1770 train_time:117987ms step_avg:97.75ms
step:1218/1770 train_time:118089ms step_avg:97.76ms
step:1219/1770 train_time:118192ms step_avg:97.76ms
step:1220/1770 train_time:118294ms step_avg:97.76ms
step:1221/1770 train_time:118395ms step_avg:97.77ms
step:1222/1770 train_time:118499ms step_avg:97.77ms
step:1223/1770 train_time:118600ms step_avg:97.77ms
step:1224/1770 train_time:118703ms step_avg:97.78ms
step:1225/1770 train_time:118805ms step_avg:97.78ms
step:1226/1770 train_time:118907ms step_avg:97.79ms
step:1227/1770 train_time:119011ms step_avg:97.79ms
step:1228/1770 train_time:119115ms step_avg:97.80ms
step:1229/1770 train_time:119216ms step_avg:97.80ms
step:1230/1770 train_time:119317ms step_avg:97.80ms
step:1231/1770 train_time:119419ms step_avg:97.80ms
step:1232/1770 train_time:119520ms step_avg:97.81ms
step:1233/1770 train_time:119622ms step_avg:97.81ms
step:1234/1770 train_time:119724ms step_avg:97.81ms
step:1235/1770 train_time:119827ms step_avg:97.82ms
step:1236/1770 train_time:119929ms step_avg:97.82ms
step:1237/1770 train_time:120032ms step_avg:97.83ms
step:1238/1770 train_time:120134ms step_avg:97.83ms
step:1239/1770 train_time:120236ms step_avg:97.83ms
step:1240/1770 train_time:120338ms step_avg:97.84ms
step:1241/1770 train_time:120439ms step_avg:97.84ms
step:1242/1770 train_time:120541ms step_avg:97.84ms
step:1243/1770 train_time:120643ms step_avg:97.84ms
step:1244/1770 train_time:120744ms step_avg:97.85ms
step:1245/1770 train_time:120846ms step_avg:97.85ms
step:1246/1770 train_time:120949ms step_avg:97.86ms
step:1247/1770 train_time:121051ms step_avg:97.86ms
step:1248/1770 train_time:121154ms step_avg:97.86ms
step:1249/1770 train_time:121255ms step_avg:97.87ms
step:1250/1770 train_time:121357ms step_avg:97.87ms
step:1250/1770 val_loss:3.4249 train_time:121459ms step_avg:97.95ms
step:1251/1770 train_time:121481ms step_avg:97.89ms
step:1252/1770 train_time:121570ms step_avg:97.88ms
step:1253/1770 train_time:121672ms step_avg:97.89ms
step:1254/1770 train_time:121774ms step_avg:97.89ms
step:1255/1770 train_time:121877ms step_avg:97.89ms
step:1256/1770 train_time:121979ms step_avg:97.90ms
step:1257/1770 train_time:122081ms step_avg:97.90ms
step:1258/1770 train_time:122184ms step_avg:97.90ms
step:1259/1770 train_time:122285ms step_avg:97.91ms
step:1260/1770 train_time:122386ms step_avg:97.91ms
step:1261/1770 train_time:122490ms step_avg:97.91ms
step:1262/1770 train_time:122593ms step_avg:97.92ms
step:1263/1770 train_time:122695ms step_avg:97.92ms
step:1264/1770 train_time:122798ms step_avg:97.93ms
step:1265/1770 train_time:122900ms step_avg:97.93ms
step:1266/1770 train_time:123003ms step_avg:97.93ms
step:1267/1770 train_time:123107ms step_avg:97.94ms
step:1268/1770 train_time:123208ms step_avg:97.94ms
step:1269/1770 train_time:123309ms step_avg:97.94ms
step:1270/1770 train_time:123411ms step_avg:97.95ms
step:1271/1770 train_time:123514ms step_avg:97.95ms
step:1272/1770 train_time:123615ms step_avg:97.95ms
step:1273/1770 train_time:123718ms step_avg:97.96ms
step:1274/1770 train_time:123821ms step_avg:97.96ms
step:1275/1770 train_time:123923ms step_avg:97.96ms
step:1276/1770 train_time:124026ms step_avg:97.97ms
step:1277/1770 train_time:124127ms step_avg:97.97ms
step:1278/1770 train_time:124231ms step_avg:97.97ms
step:1279/1770 train_time:124333ms step_avg:97.98ms
step:1280/1770 train_time:124436ms step_avg:97.98ms
step:1281/1770 train_time:124537ms step_avg:97.98ms
step:1282/1770 train_time:124639ms step_avg:97.99ms
step:1283/1770 train_time:124742ms step_avg:97.99ms
step:1284/1770 train_time:124844ms step_avg:97.99ms
step:1285/1770 train_time:124946ms step_avg:98.00ms
step:1286/1770 train_time:125049ms step_avg:98.00ms
step:1287/1770 train_time:125153ms step_avg:98.01ms
step:1288/1770 train_time:125255ms step_avg:98.01ms
step:1289/1770 train_time:125357ms step_avg:98.01ms
step:1290/1770 train_time:125459ms step_avg:98.02ms
step:1291/1770 train_time:125562ms step_avg:98.02ms
step:1292/1770 train_time:125664ms step_avg:98.02ms
step:1293/1770 train_time:125767ms step_avg:98.03ms
step:1294/1770 train_time:125868ms step_avg:98.03ms
step:1295/1770 train_time:125970ms step_avg:98.03ms
step:1296/1770 train_time:126073ms step_avg:98.03ms
step:1297/1770 train_time:126174ms step_avg:98.04ms
step:1298/1770 train_time:126276ms step_avg:98.04ms
step:1299/1770 train_time:126378ms step_avg:98.04ms
step:1300/1770 train_time:126480ms step_avg:98.05ms
step:1301/1770 train_time:126582ms step_avg:98.05ms
step:1302/1770 train_time:126685ms step_avg:98.05ms
step:1303/1770 train_time:126787ms step_avg:98.06ms
step:1304/1770 train_time:126889ms step_avg:98.06ms
step:1305/1770 train_time:126991ms step_avg:98.06ms
step:1306/1770 train_time:127092ms step_avg:98.07ms
step:1307/1770 train_time:127194ms step_avg:98.07ms
step:1308/1770 train_time:127295ms step_avg:98.07ms
step:1309/1770 train_time:127397ms step_avg:98.07ms
step:1310/1770 train_time:127499ms step_avg:98.08ms
step:1311/1770 train_time:127601ms step_avg:98.08ms
step:1312/1770 train_time:127704ms step_avg:98.08ms
step:1313/1770 train_time:127806ms step_avg:98.09ms
step:1314/1770 train_time:127908ms step_avg:98.09ms
step:1315/1770 train_time:128010ms step_avg:98.09ms
step:1316/1770 train_time:128112ms step_avg:98.09ms
step:1317/1770 train_time:128214ms step_avg:98.10ms
step:1318/1770 train_time:128318ms step_avg:98.10ms
step:1319/1770 train_time:128420ms step_avg:98.11ms
step:1320/1770 train_time:128523ms step_avg:98.11ms
step:1321/1770 train_time:128626ms step_avg:98.11ms
step:1322/1770 train_time:128728ms step_avg:98.12ms
step:1323/1770 train_time:128831ms step_avg:98.12ms
step:1324/1770 train_time:128934ms step_avg:98.12ms
step:1325/1770 train_time:129037ms step_avg:98.13ms
step:1326/1770 train_time:129139ms step_avg:98.13ms
step:1327/1770 train_time:129244ms step_avg:98.14ms
step:1328/1770 train_time:129345ms step_avg:98.14ms
step:1329/1770 train_time:129448ms step_avg:98.14ms
step:1330/1770 train_time:129549ms step_avg:98.14ms
step:1331/1770 train_time:129650ms step_avg:98.15ms
step:1332/1770 train_time:129751ms step_avg:98.15ms
step:1333/1770 train_time:129853ms step_avg:98.15ms
step:1334/1770 train_time:129954ms step_avg:98.15ms
step:1335/1770 train_time:130057ms step_avg:98.16ms
step:1336/1770 train_time:130159ms step_avg:98.16ms
step:1337/1770 train_time:130262ms step_avg:98.16ms
step:1338/1770 train_time:130364ms step_avg:98.17ms
step:1339/1770 train_time:130467ms step_avg:98.17ms
step:1340/1770 train_time:130571ms step_avg:98.17ms
step:1341/1770 train_time:130672ms step_avg:98.18ms
step:1342/1770 train_time:130775ms step_avg:98.18ms
step:1343/1770 train_time:130879ms step_avg:98.18ms
step:1344/1770 train_time:130981ms step_avg:98.19ms
step:1345/1770 train_time:131084ms step_avg:98.19ms
step:1346/1770 train_time:131185ms step_avg:98.19ms
step:1347/1770 train_time:131287ms step_avg:98.20ms
step:1348/1770 train_time:131392ms step_avg:98.20ms
step:1349/1770 train_time:131494ms step_avg:98.20ms
step:1350/1770 train_time:131595ms step_avg:98.21ms
step:1351/1770 train_time:131698ms step_avg:98.21ms
step:1352/1770 train_time:131800ms step_avg:98.21ms
step:1353/1770 train_time:131903ms step_avg:98.21ms
step:1354/1770 train_time:132005ms step_avg:98.22ms
step:1355/1770 train_time:132106ms step_avg:98.22ms
step:1356/1770 train_time:132208ms step_avg:98.22ms
step:1357/1770 train_time:132310ms step_avg:98.23ms
step:1358/1770 train_time:132411ms step_avg:98.23ms
step:1359/1770 train_time:132513ms step_avg:98.23ms
step:1360/1770 train_time:132615ms step_avg:98.23ms
step:1361/1770 train_time:132717ms step_avg:98.24ms
step:1362/1770 train_time:132820ms step_avg:98.24ms
step:1363/1770 train_time:132923ms step_avg:98.24ms
step:1364/1770 train_time:133026ms step_avg:98.25ms
step:1365/1770 train_time:133127ms step_avg:98.25ms
step:1366/1770 train_time:133230ms step_avg:98.25ms
step:1367/1770 train_time:133332ms step_avg:98.25ms
step:1368/1770 train_time:133433ms step_avg:98.26ms
step:1369/1770 train_time:133535ms step_avg:98.26ms
step:1370/1770 train_time:133638ms step_avg:98.26ms
step:1371/1770 train_time:133740ms step_avg:98.27ms
step:1372/1770 train_time:133843ms step_avg:98.27ms
step:1373/1770 train_time:133946ms step_avg:98.27ms
step:1374/1770 train_time:134049ms step_avg:98.28ms
step:1375/1770 train_time:134151ms step_avg:98.28ms
step:1375/1770 val_loss:3.3815 train_time:134252ms step_avg:98.35ms
step:1376/1770 train_time:134274ms step_avg:98.30ms
step:1377/1770 train_time:134364ms step_avg:98.29ms
step:1378/1770 train_time:134466ms step_avg:98.29ms
step:1379/1770 train_time:134567ms step_avg:98.30ms
step:1380/1770 train_time:134669ms step_avg:98.30ms
step:1381/1770 train_time:134771ms step_avg:98.30ms
step:1382/1770 train_time:134873ms step_avg:98.30ms
step:1383/1770 train_time:134976ms step_avg:98.31ms
step:1384/1770 train_time:135078ms step_avg:98.31ms
step:1385/1770 train_time:135180ms step_avg:98.31ms
step:1386/1770 train_time:135283ms step_avg:98.32ms
step:1387/1770 train_time:135386ms step_avg:98.32ms
step:1388/1770 train_time:135487ms step_avg:98.32ms
step:1389/1770 train_time:135590ms step_avg:98.32ms
step:1390/1770 train_time:135692ms step_avg:98.33ms
step:1391/1770 train_time:135794ms step_avg:98.33ms
step:1392/1770 train_time:135896ms step_avg:98.33ms
step:1393/1770 train_time:135998ms step_avg:98.34ms
step:1394/1770 train_time:136100ms step_avg:98.34ms
step:1395/1770 train_time:136203ms step_avg:98.34ms
step:1396/1770 train_time:136306ms step_avg:98.35ms
step:1397/1770 train_time:136408ms step_avg:98.35ms
step:1398/1770 train_time:136511ms step_avg:98.35ms
step:1399/1770 train_time:136613ms step_avg:98.35ms
step:1400/1770 train_time:136716ms step_avg:98.36ms
step:1401/1770 train_time:136818ms step_avg:98.36ms
step:1402/1770 train_time:136920ms step_avg:98.36ms
step:1403/1770 train_time:137021ms step_avg:98.36ms
step:1404/1770 train_time:137123ms step_avg:98.37ms
step:1405/1770 train_time:137225ms step_avg:98.37ms
step:1406/1770 train_time:137327ms step_avg:98.37ms
step:1407/1770 train_time:137429ms step_avg:98.37ms
step:1408/1770 train_time:137531ms step_avg:98.38ms
step:1409/1770 train_time:137634ms step_avg:98.38ms
step:1410/1770 train_time:137737ms step_avg:98.38ms
step:1411/1770 train_time:137838ms step_avg:98.39ms
step:1412/1770 train_time:137940ms step_avg:98.39ms
step:1413/1770 train_time:138041ms step_avg:98.39ms
step:1414/1770 train_time:138143ms step_avg:98.39ms
step:1415/1770 train_time:138246ms step_avg:98.40ms
step:1416/1770 train_time:138349ms step_avg:98.40ms
step:1417/1770 train_time:138450ms step_avg:98.40ms
step:1418/1770 train_time:138553ms step_avg:98.40ms
step:1419/1770 train_time:138656ms step_avg:98.41ms
step:1420/1770 train_time:138758ms step_avg:98.41ms
step:1421/1770 train_time:138859ms step_avg:98.41ms
step:1422/1770 train_time:138960ms step_avg:98.41ms
step:1423/1770 train_time:139061ms step_avg:98.42ms
step:1424/1770 train_time:139163ms step_avg:98.42ms
step:1425/1770 train_time:139265ms step_avg:98.42ms
step:1426/1770 train_time:139366ms step_avg:98.42ms
step:1427/1770 train_time:139468ms step_avg:98.42ms
step:1428/1770 train_time:139572ms step_avg:98.43ms
step:1429/1770 train_time:139675ms step_avg:98.43ms
step:1430/1770 train_time:139777ms step_avg:98.43ms
step:1431/1770 train_time:139880ms step_avg:98.44ms
step:1432/1770 train_time:139981ms step_avg:98.44ms
step:1433/1770 train_time:140082ms step_avg:98.44ms
step:1434/1770 train_time:140183ms step_avg:98.44ms
step:1435/1770 train_time:140284ms step_avg:98.45ms
step:1436/1770 train_time:140388ms step_avg:98.45ms
step:1437/1770 train_time:140491ms step_avg:98.45ms
step:1438/1770 train_time:140593ms step_avg:98.45ms
step:1439/1770 train_time:140696ms step_avg:98.46ms
step:1440/1770 train_time:140798ms step_avg:98.46ms
step:1441/1770 train_time:140903ms step_avg:98.46ms
step:1442/1770 train_time:141004ms step_avg:98.47ms
step:1443/1770 train_time:141106ms step_avg:98.47ms
step:1444/1770 train_time:141209ms step_avg:98.47ms
step:1445/1770 train_time:141312ms step_avg:98.48ms
step:1446/1770 train_time:141414ms step_avg:98.48ms
step:1447/1770 train_time:141517ms step_avg:98.48ms
step:1448/1770 train_time:141620ms step_avg:98.48ms
step:1449/1770 train_time:141723ms step_avg:98.49ms
step:1450/1770 train_time:141826ms step_avg:98.49ms
step:1451/1770 train_time:141930ms step_avg:98.49ms
step:1452/1770 train_time:142034ms step_avg:98.50ms
step:1453/1770 train_time:142137ms step_avg:98.50ms
step:1454/1770 train_time:142240ms step_avg:98.50ms
step:1455/1770 train_time:142344ms step_avg:98.51ms
step:1456/1770 train_time:142450ms step_avg:98.51ms
step:1457/1770 train_time:142553ms step_avg:98.52ms
step:1458/1770 train_time:142657ms step_avg:98.52ms
step:1459/1770 train_time:142760ms step_avg:98.52ms
step:1460/1770 train_time:142863ms step_avg:98.53ms
step:1461/1770 train_time:142966ms step_avg:98.53ms
step:1462/1770 train_time:143069ms step_avg:98.53ms
step:1463/1770 train_time:143173ms step_avg:98.54ms
step:1464/1770 train_time:143277ms step_avg:98.54ms
step:1465/1770 train_time:143380ms step_avg:98.54ms
step:1466/1770 train_time:143484ms step_avg:98.55ms
step:1467/1770 train_time:143589ms step_avg:98.55ms
step:1468/1770 train_time:143692ms step_avg:98.55ms
step:1469/1770 train_time:143795ms step_avg:98.56ms
step:1470/1770 train_time:143898ms step_avg:98.56ms
step:1471/1770 train_time:144001ms step_avg:98.56ms
step:1472/1770 train_time:144103ms step_avg:98.57ms
step:1473/1770 train_time:144207ms step_avg:98.57ms
step:1474/1770 train_time:144312ms step_avg:98.57ms
step:1475/1770 train_time:144415ms step_avg:98.58ms
step:1476/1770 train_time:144518ms step_avg:98.58ms
step:1477/1770 train_time:144623ms step_avg:98.58ms
step:1478/1770 train_time:144727ms step_avg:98.59ms
step:1479/1770 train_time:144830ms step_avg:98.59ms
step:1480/1770 train_time:144934ms step_avg:98.59ms
step:1481/1770 train_time:145041ms step_avg:98.60ms
step:1482/1770 train_time:145144ms step_avg:98.60ms
step:1483/1770 train_time:145248ms step_avg:98.61ms
step:1484/1770 train_time:145351ms step_avg:98.61ms
step:1485/1770 train_time:145455ms step_avg:98.61ms
step:1486/1770 train_time:145557ms step_avg:98.62ms
step:1487/1770 train_time:145659ms step_avg:98.62ms
step:1488/1770 train_time:145763ms step_avg:98.62ms
step:1489/1770 train_time:145867ms step_avg:98.63ms
step:1490/1770 train_time:145970ms step_avg:98.63ms
step:1491/1770 train_time:146073ms step_avg:98.63ms
step:1492/1770 train_time:146176ms step_avg:98.63ms
step:1493/1770 train_time:146282ms step_avg:98.64ms
step:1494/1770 train_time:146388ms step_avg:98.64ms
step:1495/1770 train_time:146491ms step_avg:98.65ms
step:1496/1770 train_time:146594ms step_avg:98.65ms
step:1497/1770 train_time:146699ms step_avg:98.65ms
step:1498/1770 train_time:146801ms step_avg:98.66ms
step:1499/1770 train_time:146903ms step_avg:98.66ms
step:1500/1770 train_time:147005ms step_avg:98.66ms
step:1500/1770 val_loss:3.3439 train_time:147106ms step_avg:98.73ms
step:1501/1770 train_time:147128ms step_avg:98.68ms
step:1502/1770 train_time:147220ms step_avg:98.67ms
step:1503/1770 train_time:147323ms step_avg:98.68ms
step:1504/1770 train_time:147426ms step_avg:98.68ms
step:1505/1770 train_time:147532ms step_avg:98.68ms
step:1506/1770 train_time:147635ms step_avg:98.69ms
step:1507/1770 train_time:147739ms step_avg:98.69ms
step:1508/1770 train_time:147843ms step_avg:98.69ms
step:1509/1770 train_time:147946ms step_avg:98.70ms
step:1510/1770 train_time:148047ms step_avg:98.70ms
step:1511/1770 train_time:148153ms step_avg:98.70ms
step:1512/1770 train_time:148259ms step_avg:98.71ms
step:1513/1770 train_time:148363ms step_avg:98.71ms
step:1514/1770 train_time:148466ms step_avg:98.71ms
step:1515/1770 train_time:148569ms step_avg:98.72ms
step:1516/1770 train_time:148671ms step_avg:98.72ms
step:1517/1770 train_time:148775ms step_avg:98.72ms
step:1518/1770 train_time:148880ms step_avg:98.73ms
step:1519/1770 train_time:148982ms step_avg:98.73ms
step:1520/1770 train_time:149086ms step_avg:98.73ms
step:1521/1770 train_time:149189ms step_avg:98.74ms
step:1522/1770 train_time:149293ms step_avg:98.74ms
step:1523/1770 train_time:149397ms step_avg:98.74ms
step:1524/1770 train_time:149500ms step_avg:98.75ms
step:1525/1770 train_time:149603ms step_avg:98.75ms
step:1526/1770 train_time:149706ms step_avg:98.75ms
step:1527/1770 train_time:149809ms step_avg:98.75ms
step:1528/1770 train_time:149914ms step_avg:98.76ms
step:1529/1770 train_time:150017ms step_avg:98.76ms
step:1530/1770 train_time:150120ms step_avg:98.76ms
step:1531/1770 train_time:150222ms step_avg:98.77ms
step:1532/1770 train_time:150326ms step_avg:98.77ms
step:1533/1770 train_time:150430ms step_avg:98.77ms
step:1534/1770 train_time:150534ms step_avg:98.78ms
step:1535/1770 train_time:150637ms step_avg:98.78ms
step:1536/1770 train_time:150741ms step_avg:98.78ms
step:1537/1770 train_time:150844ms step_avg:98.78ms
step:1538/1770 train_time:150948ms step_avg:98.79ms
step:1539/1770 train_time:151050ms step_avg:98.79ms
step:1540/1770 train_time:151158ms step_avg:98.80ms
step:1541/1770 train_time:151262ms step_avg:98.80ms
step:1542/1770 train_time:151365ms step_avg:98.80ms
step:1543/1770 train_time:151466ms step_avg:98.80ms
step:1544/1770 train_time:151572ms step_avg:98.81ms
step:1545/1770 train_time:151675ms step_avg:98.81ms
step:1546/1770 train_time:151779ms step_avg:98.81ms
step:1547/1770 train_time:151883ms step_avg:98.82ms
step:1548/1770 train_time:151985ms step_avg:98.82ms
step:1549/1770 train_time:152088ms step_avg:98.82ms
step:1550/1770 train_time:152193ms step_avg:98.83ms
step:1551/1770 train_time:152296ms step_avg:98.83ms
step:1552/1770 train_time:152402ms step_avg:98.83ms
step:1553/1770 train_time:152505ms step_avg:98.84ms
step:1554/1770 train_time:152607ms step_avg:98.84ms
step:1555/1770 train_time:152711ms step_avg:98.84ms
step:1556/1770 train_time:152814ms step_avg:98.84ms
step:1557/1770 train_time:152918ms step_avg:98.85ms
step:1558/1770 train_time:153021ms step_avg:98.85ms
step:1559/1770 train_time:153124ms step_avg:98.85ms
step:1560/1770 train_time:153227ms step_avg:98.86ms
step:1561/1770 train_time:153332ms step_avg:98.86ms
step:1562/1770 train_time:153435ms step_avg:98.86ms
step:1563/1770 train_time:153539ms step_avg:98.87ms
step:1564/1770 train_time:153642ms step_avg:98.87ms
step:1565/1770 train_time:153744ms step_avg:98.87ms
step:1566/1770 train_time:153847ms step_avg:98.87ms
step:1567/1770 train_time:153950ms step_avg:98.88ms
step:1568/1770 train_time:154053ms step_avg:98.88ms
step:1569/1770 train_time:154160ms step_avg:98.88ms
step:1570/1770 train_time:154263ms step_avg:98.89ms
step:1571/1770 train_time:154365ms step_avg:98.89ms
step:1572/1770 train_time:154469ms step_avg:98.89ms
step:1573/1770 train_time:154575ms step_avg:98.90ms
step:1574/1770 train_time:154678ms step_avg:98.90ms
step:1575/1770 train_time:154780ms step_avg:98.90ms
step:1576/1770 train_time:154884ms step_avg:98.90ms
step:1577/1770 train_time:154988ms step_avg:98.91ms
step:1578/1770 train_time:155093ms step_avg:98.91ms
step:1579/1770 train_time:155197ms step_avg:98.91ms
step:1580/1770 train_time:155300ms step_avg:98.92ms
step:1581/1770 train_time:155406ms step_avg:98.92ms
step:1582/1770 train_time:155510ms step_avg:98.92ms
step:1583/1770 train_time:155612ms step_avg:98.93ms
step:1584/1770 train_time:155717ms step_avg:98.93ms
step:1585/1770 train_time:155822ms step_avg:98.93ms
step:1586/1770 train_time:155928ms step_avg:98.94ms
step:1587/1770 train_time:156032ms step_avg:98.94ms
step:1588/1770 train_time:156135ms step_avg:98.95ms
step:1589/1770 train_time:156241ms step_avg:98.95ms
step:1590/1770 train_time:156344ms step_avg:98.95ms
step:1591/1770 train_time:156446ms step_avg:98.95ms
step:1592/1770 train_time:156550ms step_avg:98.96ms
step:1593/1770 train_time:156654ms step_avg:98.96ms
step:1594/1770 train_time:156757ms step_avg:98.96ms
step:1595/1770 train_time:156861ms step_avg:98.97ms
step:1596/1770 train_time:156965ms step_avg:98.97ms
step:1597/1770 train_time:157068ms step_avg:98.97ms
step:1598/1770 train_time:157171ms step_avg:98.97ms
step:1599/1770 train_time:157276ms step_avg:98.98ms
step:1600/1770 train_time:157382ms step_avg:98.98ms
step:1601/1770 train_time:157486ms step_avg:98.99ms
step:1602/1770 train_time:157590ms step_avg:98.99ms
step:1603/1770 train_time:157693ms step_avg:98.99ms
step:1604/1770 train_time:157797ms step_avg:98.99ms
step:1605/1770 train_time:157900ms step_avg:99.00ms
step:1606/1770 train_time:158003ms step_avg:99.00ms
step:1607/1770 train_time:158110ms step_avg:99.00ms
step:1608/1770 train_time:158213ms step_avg:99.01ms
step:1609/1770 train_time:158316ms step_avg:99.01ms
step:1610/1770 train_time:158422ms step_avg:99.01ms
step:1611/1770 train_time:158526ms step_avg:99.02ms
step:1612/1770 train_time:158630ms step_avg:99.02ms
step:1613/1770 train_time:158733ms step_avg:99.02ms
step:1614/1770 train_time:158836ms step_avg:99.03ms
step:1615/1770 train_time:158940ms step_avg:99.03ms
step:1616/1770 train_time:159043ms step_avg:99.03ms
step:1617/1770 train_time:159149ms step_avg:99.03ms
step:1618/1770 train_time:159253ms step_avg:99.04ms
step:1619/1770 train_time:159357ms step_avg:99.04ms
step:1620/1770 train_time:159460ms step_avg:99.04ms
step:1621/1770 train_time:159563ms step_avg:99.05ms
step:1622/1770 train_time:159667ms step_avg:99.05ms
step:1623/1770 train_time:159772ms step_avg:99.05ms
step:1624/1770 train_time:159875ms step_avg:99.06ms
step:1625/1770 train_time:159979ms step_avg:99.06ms
step:1625/1770 val_loss:3.3091 train_time:160082ms step_avg:99.12ms
step:1626/1770 train_time:160103ms step_avg:99.07ms
step:1627/1770 train_time:160192ms step_avg:99.07ms
step:1628/1770 train_time:160295ms step_avg:99.07ms
step:1629/1770 train_time:160398ms step_avg:99.07ms
step:1630/1770 train_time:160501ms step_avg:99.07ms
step:1631/1770 train_time:160604ms step_avg:99.08ms
step:1632/1770 train_time:160706ms step_avg:99.08ms
step:1633/1770 train_time:160809ms step_avg:99.08ms
step:1634/1770 train_time:160912ms step_avg:99.08ms
step:1635/1770 train_time:161015ms step_avg:99.09ms
step:1636/1770 train_time:161119ms step_avg:99.09ms
step:1637/1770 train_time:161225ms step_avg:99.09ms
step:1638/1770 train_time:161328ms step_avg:99.10ms
step:1639/1770 train_time:161431ms step_avg:99.10ms
step:1640/1770 train_time:161534ms step_avg:99.10ms
step:1641/1770 train_time:161637ms step_avg:99.10ms
step:1642/1770 train_time:161741ms step_avg:99.11ms
step:1643/1770 train_time:161845ms step_avg:99.11ms
step:1644/1770 train_time:161949ms step_avg:99.11ms
step:1645/1770 train_time:162052ms step_avg:99.11ms
step:1646/1770 train_time:162157ms step_avg:99.12ms
step:1647/1770 train_time:162263ms step_avg:99.12ms
step:1648/1770 train_time:162366ms step_avg:99.12ms
step:1649/1770 train_time:162468ms step_avg:99.13ms
step:1650/1770 train_time:162571ms step_avg:99.13ms
step:1651/1770 train_time:162674ms step_avg:99.13ms
step:1652/1770 train_time:162777ms step_avg:99.13ms
step:1653/1770 train_time:162881ms step_avg:99.14ms
step:1654/1770 train_time:162988ms step_avg:99.14ms
step:1655/1770 train_time:163094ms step_avg:99.15ms
step:1656/1770 train_time:163197ms step_avg:99.15ms
step:1657/1770 train_time:163302ms step_avg:99.15ms
step:1658/1770 train_time:163405ms step_avg:99.15ms
step:1659/1770 train_time:163510ms step_avg:99.16ms
step:1660/1770 train_time:163612ms step_avg:99.16ms
step:1661/1770 train_time:163717ms step_avg:99.16ms
step:1662/1770 train_time:163821ms step_avg:99.17ms
step:1663/1770 train_time:163923ms step_avg:99.17ms
step:1664/1770 train_time:164027ms step_avg:99.17ms
step:1665/1770 train_time:164129ms step_avg:99.17ms
step:1666/1770 train_time:164233ms step_avg:99.17ms
step:1667/1770 train_time:164336ms step_avg:99.18ms
step:1668/1770 train_time:164440ms step_avg:99.18ms
step:1669/1770 train_time:164543ms step_avg:99.18ms
step:1670/1770 train_time:164646ms step_avg:99.18ms
step:1671/1770 train_time:164749ms step_avg:99.19ms
step:1672/1770 train_time:164853ms step_avg:99.19ms
step:1673/1770 train_time:164958ms step_avg:99.19ms
step:1674/1770 train_time:165062ms step_avg:99.20ms
step:1675/1770 train_time:165164ms step_avg:99.20ms
step:1676/1770 train_time:165269ms step_avg:99.20ms
step:1677/1770 train_time:165376ms step_avg:99.21ms
step:1678/1770 train_time:165478ms step_avg:99.21ms
step:1679/1770 train_time:165582ms step_avg:99.21ms
step:1680/1770 train_time:165685ms step_avg:99.21ms
step:1681/1770 train_time:165789ms step_avg:99.22ms
step:1682/1770 train_time:165895ms step_avg:99.22ms
step:1683/1770 train_time:165997ms step_avg:99.22ms
step:1684/1770 train_time:166100ms step_avg:99.22ms
step:1685/1770 train_time:166205ms step_avg:99.23ms
step:1686/1770 train_time:166309ms step_avg:99.23ms
step:1687/1770 train_time:166413ms step_avg:99.23ms
step:1688/1770 train_time:166517ms step_avg:99.24ms
step:1689/1770 train_time:166620ms step_avg:99.24ms
step:1690/1770 train_time:166723ms step_avg:99.24ms
step:1691/1770 train_time:166826ms step_avg:99.24ms
step:1692/1770 train_time:166929ms step_avg:99.24ms
step:1693/1770 train_time:167034ms step_avg:99.25ms
step:1694/1770 train_time:167137ms step_avg:99.25ms
step:1695/1770 train_time:167241ms step_avg:99.25ms
step:1696/1770 train_time:167346ms step_avg:99.26ms
step:1697/1770 train_time:167451ms step_avg:99.26ms
step:1698/1770 train_time:167554ms step_avg:99.26ms
step:1699/1770 train_time:167657ms step_avg:99.26ms
step:1700/1770 train_time:167761ms step_avg:99.27ms
step:1701/1770 train_time:167864ms step_avg:99.27ms
step:1702/1770 train_time:167968ms step_avg:99.27ms
step:1703/1770 train_time:168070ms step_avg:99.27ms
step:1704/1770 train_time:168173ms step_avg:99.28ms
step:1705/1770 train_time:168277ms step_avg:99.28ms
step:1706/1770 train_time:168380ms step_avg:99.28ms
step:1707/1770 train_time:168485ms step_avg:99.28ms
step:1708/1770 train_time:168588ms step_avg:99.29ms
step:1709/1770 train_time:168694ms step_avg:99.29ms
step:1710/1770 train_time:168800ms step_avg:99.29ms
step:1711/1770 train_time:168906ms step_avg:99.30ms
step:1712/1770 train_time:169010ms step_avg:99.30ms
step:1713/1770 train_time:169113ms step_avg:99.30ms
step:1714/1770 train_time:169218ms step_avg:99.31ms
step:1715/1770 train_time:169320ms step_avg:99.31ms
step:1716/1770 train_time:169426ms step_avg:99.31ms
step:1717/1770 train_time:169529ms step_avg:99.31ms
step:1718/1770 train_time:169634ms step_avg:99.32ms
step:1719/1770 train_time:169739ms step_avg:99.32ms
step:1720/1770 train_time:169843ms step_avg:99.32ms
step:1721/1770 train_time:169946ms step_avg:99.33ms
step:1722/1770 train_time:170053ms step_avg:99.33ms
step:1723/1770 train_time:170158ms step_avg:99.33ms
step:1724/1770 train_time:170264ms step_avg:99.34ms
step:1725/1770 train_time:170370ms step_avg:99.34ms
step:1726/1770 train_time:170477ms step_avg:99.35ms
step:1727/1770 train_time:170580ms step_avg:99.35ms
step:1728/1770 train_time:170686ms step_avg:99.35ms
step:1729/1770 train_time:170791ms step_avg:99.35ms
step:1730/1770 train_time:170896ms step_avg:99.36ms
step:1731/1770 train_time:171002ms step_avg:99.36ms
step:1732/1770 train_time:171105ms step_avg:99.36ms
step:1733/1770 train_time:171211ms step_avg:99.37ms
step:1734/1770 train_time:171314ms step_avg:99.37ms
step:1735/1770 train_time:171419ms step_avg:99.37ms
step:1736/1770 train_time:171523ms step_avg:99.38ms
step:1737/1770 train_time:171628ms step_avg:99.38ms
step:1738/1770 train_time:171732ms step_avg:99.38ms
step:1739/1770 train_time:171835ms step_avg:99.38ms
step:1740/1770 train_time:171939ms step_avg:99.39ms
step:1741/1770 train_time:172046ms step_avg:99.39ms
step:1742/1770 train_time:172152ms step_avg:99.39ms
step:1743/1770 train_time:172257ms step_avg:99.40ms
step:1744/1770 train_time:172362ms step_avg:99.40ms
step:1745/1770 train_time:172466ms step_avg:99.40ms
step:1746/1770 train_time:172572ms step_avg:99.41ms
step:1747/1770 train_time:172675ms step_avg:99.41ms
step:1748/1770 train_time:172782ms step_avg:99.41ms
step:1749/1770 train_time:172886ms step_avg:99.42ms
step:1750/1770 train_time:172990ms step_avg:99.42ms
step:1750/1770 val_loss:3.2824 train_time:173092ms step_avg:99.48ms
step:1751/1770 train_time:173114ms step_avg:99.43ms
step:1752/1770 train_time:173205ms step_avg:99.43ms
step:1753/1770 train_time:173309ms step_avg:99.43ms
step:1754/1770 train_time:173413ms step_avg:99.43ms
step:1755/1770 train_time:173517ms step_avg:99.44ms
step:1756/1770 train_time:173622ms step_avg:99.44ms
step:1757/1770 train_time:173726ms step_avg:99.44ms
step:1758/1770 train_time:173830ms step_avg:99.44ms
step:1759/1770 train_time:173934ms step_avg:99.45ms
step:1760/1770 train_time:174039ms step_avg:99.45ms
step:1761/1770 train_time:174146ms step_avg:99.45ms
step:1762/1770 train_time:174254ms step_avg:99.46ms
step:1763/1770 train_time:174357ms step_avg:99.46ms
step:1764/1770 train_time:174462ms step_avg:99.47ms
step:1765/1770 train_time:174566ms step_avg:99.47ms
step:1766/1770 train_time:174675ms step_avg:99.47ms
step:1767/1770 train_time:174778ms step_avg:99.48ms
step:1768/1770 train_time:174882ms step_avg:99.48ms
step:1769/1770 train_time:174985ms step_avg:99.48ms
step:1770/1770 train_time:175087ms step_avg:99.48ms
step:1770/1770 val_loss:3.2800 train_time:175191ms step_avg:99.54ms
peak memory allocated: 28840 MiB reserved: 32272 MiB
