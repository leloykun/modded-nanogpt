import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 16:07:25 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24369ms step_avg:nanms
step:2/1770 train_time:24874ms step_avg:nanms
step:3/1770 train_time:24971ms step_avg:nanms
step:4/1770 train_time:25063ms step_avg:nanms
step:5/1770 train_time:25157ms step_avg:nanms
step:6/1770 train_time:25252ms step_avg:nanms
step:7/1770 train_time:25345ms step_avg:nanms
step:8/1770 train_time:25439ms step_avg:nanms
step:9/1770 train_time:25532ms step_avg:nanms
step:10/1770 train_time:25626ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:93.94ms
step:14/1770 train_time:376ms step_avg:93.95ms
step:15/1770 train_time:470ms step_avg:93.96ms
step:16/1770 train_time:564ms step_avg:93.96ms
step:17/1770 train_time:658ms step_avg:94.03ms
step:18/1770 train_time:752ms step_avg:94.04ms
step:19/1770 train_time:846ms step_avg:94.03ms
step:20/1770 train_time:941ms step_avg:94.07ms
step:21/1770 train_time:1036ms step_avg:94.15ms
step:22/1770 train_time:1128ms step_avg:94.04ms
step:23/1770 train_time:1223ms step_avg:94.06ms
step:24/1770 train_time:1317ms step_avg:94.08ms
step:25/1770 train_time:1411ms step_avg:94.07ms
step:26/1770 train_time:1505ms step_avg:94.07ms
step:27/1770 train_time:1600ms step_avg:94.10ms
step:28/1770 train_time:1693ms step_avg:94.08ms
step:29/1770 train_time:1787ms step_avg:94.06ms
step:30/1770 train_time:1881ms step_avg:94.07ms
step:31/1770 train_time:1975ms step_avg:94.05ms
step:32/1770 train_time:2069ms step_avg:94.04ms
step:33/1770 train_time:2163ms step_avg:94.03ms
step:34/1770 train_time:2257ms step_avg:94.05ms
step:35/1770 train_time:2351ms step_avg:94.05ms
step:36/1770 train_time:2446ms step_avg:94.06ms
step:37/1770 train_time:2539ms step_avg:94.04ms
step:38/1770 train_time:2633ms step_avg:94.04ms
step:39/1770 train_time:2727ms step_avg:94.04ms
step:40/1770 train_time:2821ms step_avg:94.04ms
step:41/1770 train_time:2916ms step_avg:94.05ms
step:42/1770 train_time:3009ms step_avg:94.05ms
step:43/1770 train_time:3103ms step_avg:94.04ms
step:44/1770 train_time:3198ms step_avg:94.05ms
step:45/1770 train_time:3292ms step_avg:94.06ms
step:46/1770 train_time:3386ms step_avg:94.05ms
step:47/1770 train_time:3480ms step_avg:94.06ms
step:48/1770 train_time:3574ms step_avg:94.05ms
step:49/1770 train_time:3667ms step_avg:94.04ms
step:50/1770 train_time:3761ms step_avg:94.04ms
step:51/1770 train_time:3856ms step_avg:94.04ms
step:52/1770 train_time:3949ms step_avg:94.03ms
step:53/1770 train_time:4043ms step_avg:94.03ms
step:54/1770 train_time:4138ms step_avg:94.04ms
step:55/1770 train_time:4232ms step_avg:94.04ms
step:56/1770 train_time:4326ms step_avg:94.04ms
step:57/1770 train_time:4420ms step_avg:94.05ms
step:58/1770 train_time:4515ms step_avg:94.05ms
step:59/1770 train_time:4609ms step_avg:94.06ms
step:60/1770 train_time:4703ms step_avg:94.06ms
step:61/1770 train_time:4798ms step_avg:94.08ms
step:62/1770 train_time:4892ms step_avg:94.08ms
step:63/1770 train_time:4986ms step_avg:94.07ms
step:64/1770 train_time:5079ms step_avg:94.06ms
step:65/1770 train_time:5173ms step_avg:94.06ms
step:66/1770 train_time:5267ms step_avg:94.05ms
step:67/1770 train_time:5361ms step_avg:94.06ms
step:68/1770 train_time:5456ms step_avg:94.06ms
step:69/1770 train_time:5550ms step_avg:94.07ms
step:70/1770 train_time:5644ms step_avg:94.07ms
step:71/1770 train_time:5738ms step_avg:94.07ms
step:72/1770 train_time:5832ms step_avg:94.06ms
step:73/1770 train_time:5927ms step_avg:94.07ms
step:74/1770 train_time:6021ms step_avg:94.08ms
step:75/1770 train_time:6115ms step_avg:94.08ms
step:76/1770 train_time:6210ms step_avg:94.08ms
step:77/1770 train_time:6303ms step_avg:94.08ms
step:78/1770 train_time:6398ms step_avg:94.08ms
step:79/1770 train_time:6492ms step_avg:94.08ms
step:80/1770 train_time:6585ms step_avg:94.07ms
step:81/1770 train_time:6679ms step_avg:94.08ms
step:82/1770 train_time:6774ms step_avg:94.09ms
step:83/1770 train_time:6868ms step_avg:94.08ms
step:84/1770 train_time:6962ms step_avg:94.08ms
step:85/1770 train_time:7056ms step_avg:94.08ms
step:86/1770 train_time:7150ms step_avg:94.08ms
step:87/1770 train_time:7244ms step_avg:94.08ms
step:88/1770 train_time:7338ms step_avg:94.08ms
step:89/1770 train_time:7432ms step_avg:94.07ms
step:90/1770 train_time:7527ms step_avg:94.08ms
step:91/1770 train_time:7620ms step_avg:94.07ms
step:92/1770 train_time:7713ms step_avg:94.07ms
step:93/1770 train_time:7807ms step_avg:94.06ms
step:94/1770 train_time:7902ms step_avg:94.07ms
step:95/1770 train_time:7996ms step_avg:94.07ms
step:96/1770 train_time:8090ms step_avg:94.07ms
step:97/1770 train_time:8185ms step_avg:94.08ms
step:98/1770 train_time:8278ms step_avg:94.07ms
step:99/1770 train_time:8372ms step_avg:94.07ms
step:100/1770 train_time:8466ms step_avg:94.07ms
step:101/1770 train_time:8560ms step_avg:94.07ms
step:102/1770 train_time:8654ms step_avg:94.07ms
step:103/1770 train_time:8749ms step_avg:94.07ms
step:104/1770 train_time:8843ms step_avg:94.08ms
step:105/1770 train_time:8937ms step_avg:94.07ms
step:106/1770 train_time:9031ms step_avg:94.07ms
step:107/1770 train_time:9125ms step_avg:94.07ms
step:108/1770 train_time:9219ms step_avg:94.07ms
step:109/1770 train_time:9313ms step_avg:94.07ms
step:110/1770 train_time:9408ms step_avg:94.08ms
step:111/1770 train_time:9502ms step_avg:94.08ms
step:112/1770 train_time:9596ms step_avg:94.08ms
step:113/1770 train_time:9690ms step_avg:94.08ms
step:114/1770 train_time:9784ms step_avg:94.08ms
step:115/1770 train_time:9878ms step_avg:94.08ms
step:116/1770 train_time:9972ms step_avg:94.08ms
step:117/1770 train_time:10066ms step_avg:94.08ms
step:118/1770 train_time:10160ms step_avg:94.07ms
step:119/1770 train_time:10253ms step_avg:94.07ms
step:120/1770 train_time:10347ms step_avg:94.06ms
step:121/1770 train_time:10441ms step_avg:94.06ms
step:122/1770 train_time:10535ms step_avg:94.07ms
step:123/1770 train_time:10630ms step_avg:94.07ms
step:124/1770 train_time:10723ms step_avg:94.06ms
step:125/1770 train_time:10817ms step_avg:94.06ms
step:125/1770 val_loss:4.6583 train_time:10909ms step_avg:94.86ms
step:126/1770 train_time:10931ms step_avg:94.23ms
step:127/1770 train_time:11017ms step_avg:94.16ms
step:128/1770 train_time:11118ms step_avg:94.22ms
step:129/1770 train_time:11212ms step_avg:94.22ms
step:130/1770 train_time:11306ms step_avg:94.22ms
step:131/1770 train_time:11400ms step_avg:94.21ms
step:132/1770 train_time:11494ms step_avg:94.21ms
step:133/1770 train_time:11587ms step_avg:94.21ms
step:134/1770 train_time:11682ms step_avg:94.21ms
step:135/1770 train_time:11775ms step_avg:94.20ms
step:136/1770 train_time:11870ms step_avg:94.20ms
step:137/1770 train_time:11964ms step_avg:94.20ms
step:138/1770 train_time:12059ms step_avg:94.21ms
step:139/1770 train_time:12154ms step_avg:94.21ms
step:140/1770 train_time:12248ms step_avg:94.22ms
step:141/1770 train_time:12343ms step_avg:94.22ms
step:142/1770 train_time:12439ms step_avg:94.24ms
step:143/1770 train_time:12535ms step_avg:94.24ms
step:144/1770 train_time:12629ms step_avg:94.25ms
step:145/1770 train_time:12723ms step_avg:94.25ms
step:146/1770 train_time:12818ms step_avg:94.25ms
step:147/1770 train_time:12912ms step_avg:94.25ms
step:148/1770 train_time:13006ms step_avg:94.25ms
step:149/1770 train_time:13101ms step_avg:94.25ms
step:150/1770 train_time:13197ms step_avg:94.26ms
step:151/1770 train_time:13290ms step_avg:94.26ms
step:152/1770 train_time:13385ms step_avg:94.26ms
step:153/1770 train_time:13479ms step_avg:94.26ms
step:154/1770 train_time:13574ms step_avg:94.27ms
step:155/1770 train_time:13669ms step_avg:94.27ms
step:156/1770 train_time:13763ms step_avg:94.27ms
step:157/1770 train_time:13858ms step_avg:94.27ms
step:158/1770 train_time:13952ms step_avg:94.27ms
step:159/1770 train_time:14046ms step_avg:94.27ms
step:160/1770 train_time:14141ms step_avg:94.27ms
step:161/1770 train_time:14236ms step_avg:94.28ms
step:162/1770 train_time:14330ms step_avg:94.28ms
step:163/1770 train_time:14425ms step_avg:94.28ms
step:164/1770 train_time:14519ms step_avg:94.28ms
step:165/1770 train_time:14615ms step_avg:94.29ms
step:166/1770 train_time:14710ms step_avg:94.29ms
step:167/1770 train_time:14805ms step_avg:94.30ms
step:168/1770 train_time:14899ms step_avg:94.30ms
step:169/1770 train_time:14994ms step_avg:94.30ms
step:170/1770 train_time:15088ms step_avg:94.30ms
step:171/1770 train_time:15182ms step_avg:94.30ms
step:172/1770 train_time:15277ms step_avg:94.30ms
step:173/1770 train_time:15371ms step_avg:94.30ms
step:174/1770 train_time:15466ms step_avg:94.30ms
step:175/1770 train_time:15560ms step_avg:94.30ms
step:176/1770 train_time:15655ms step_avg:94.31ms
step:177/1770 train_time:15750ms step_avg:94.31ms
step:178/1770 train_time:15844ms step_avg:94.31ms
step:179/1770 train_time:15939ms step_avg:94.31ms
step:180/1770 train_time:16033ms step_avg:94.31ms
step:181/1770 train_time:16128ms step_avg:94.32ms
step:182/1770 train_time:16224ms step_avg:94.32ms
step:183/1770 train_time:16318ms step_avg:94.33ms
step:184/1770 train_time:16413ms step_avg:94.33ms
step:185/1770 train_time:16508ms step_avg:94.33ms
step:186/1770 train_time:16603ms step_avg:94.33ms
step:187/1770 train_time:16698ms step_avg:94.34ms
step:188/1770 train_time:16793ms step_avg:94.34ms
step:189/1770 train_time:16888ms step_avg:94.34ms
step:190/1770 train_time:16982ms step_avg:94.35ms
step:191/1770 train_time:17077ms step_avg:94.35ms
step:192/1770 train_time:17172ms step_avg:94.35ms
step:193/1770 train_time:17266ms step_avg:94.35ms
step:194/1770 train_time:17361ms step_avg:94.35ms
step:195/1770 train_time:17456ms step_avg:94.36ms
step:196/1770 train_time:17551ms step_avg:94.36ms
step:197/1770 train_time:17646ms step_avg:94.36ms
step:198/1770 train_time:17740ms step_avg:94.36ms
step:199/1770 train_time:17835ms step_avg:94.37ms
step:200/1770 train_time:17930ms step_avg:94.37ms
step:201/1770 train_time:18026ms step_avg:94.38ms
step:202/1770 train_time:18120ms step_avg:94.37ms
step:203/1770 train_time:18215ms step_avg:94.38ms
step:204/1770 train_time:18310ms step_avg:94.38ms
step:205/1770 train_time:18404ms step_avg:94.38ms
step:206/1770 train_time:18499ms step_avg:94.38ms
step:207/1770 train_time:18594ms step_avg:94.39ms
step:208/1770 train_time:18689ms step_avg:94.39ms
step:209/1770 train_time:18783ms step_avg:94.39ms
step:210/1770 train_time:18878ms step_avg:94.39ms
step:211/1770 train_time:18973ms step_avg:94.39ms
step:212/1770 train_time:19067ms step_avg:94.39ms
step:213/1770 train_time:19162ms step_avg:94.39ms
step:214/1770 train_time:19257ms step_avg:94.39ms
step:215/1770 train_time:19351ms step_avg:94.40ms
step:216/1770 train_time:19446ms step_avg:94.40ms
step:217/1770 train_time:19540ms step_avg:94.40ms
step:218/1770 train_time:19635ms step_avg:94.40ms
step:219/1770 train_time:19730ms step_avg:94.40ms
step:220/1770 train_time:19824ms step_avg:94.40ms
step:221/1770 train_time:19919ms step_avg:94.40ms
step:222/1770 train_time:20014ms step_avg:94.41ms
step:223/1770 train_time:20109ms step_avg:94.41ms
step:224/1770 train_time:20203ms step_avg:94.41ms
step:225/1770 train_time:20298ms step_avg:94.41ms
step:226/1770 train_time:20394ms step_avg:94.42ms
step:227/1770 train_time:20489ms step_avg:94.42ms
step:228/1770 train_time:20583ms step_avg:94.42ms
step:229/1770 train_time:20678ms step_avg:94.42ms
step:230/1770 train_time:20773ms step_avg:94.42ms
step:231/1770 train_time:20868ms step_avg:94.42ms
step:232/1770 train_time:20962ms step_avg:94.42ms
step:233/1770 train_time:21058ms step_avg:94.43ms
step:234/1770 train_time:21152ms step_avg:94.43ms
step:235/1770 train_time:21246ms step_avg:94.43ms
step:236/1770 train_time:21341ms step_avg:94.43ms
step:237/1770 train_time:21436ms step_avg:94.43ms
step:238/1770 train_time:21531ms step_avg:94.43ms
step:239/1770 train_time:21626ms step_avg:94.44ms
step:240/1770 train_time:21721ms step_avg:94.44ms
step:241/1770 train_time:21815ms step_avg:94.44ms
step:242/1770 train_time:21909ms step_avg:94.44ms
step:243/1770 train_time:22003ms step_avg:94.44ms
step:244/1770 train_time:22099ms step_avg:94.44ms
step:245/1770 train_time:22194ms step_avg:94.44ms
step:246/1770 train_time:22289ms step_avg:94.44ms
step:247/1770 train_time:22383ms step_avg:94.44ms
step:248/1770 train_time:22479ms step_avg:94.45ms
step:249/1770 train_time:22574ms step_avg:94.45ms
step:250/1770 train_time:22668ms step_avg:94.45ms
step:250/1770 val_loss:4.1127 train_time:22762ms step_avg:94.84ms
step:251/1770 train_time:22784ms step_avg:94.54ms
step:252/1770 train_time:22870ms step_avg:94.50ms
step:253/1770 train_time:22969ms step_avg:94.52ms
step:254/1770 train_time:23065ms step_avg:94.53ms
step:255/1770 train_time:23159ms step_avg:94.53ms
step:256/1770 train_time:23253ms step_avg:94.53ms
step:257/1770 train_time:23348ms step_avg:94.53ms
step:258/1770 train_time:23442ms step_avg:94.52ms
step:259/1770 train_time:23536ms step_avg:94.52ms
step:260/1770 train_time:23630ms step_avg:94.52ms
step:261/1770 train_time:23725ms step_avg:94.52ms
step:262/1770 train_time:23819ms step_avg:94.52ms
step:263/1770 train_time:23915ms step_avg:94.52ms
step:264/1770 train_time:24010ms step_avg:94.53ms
step:265/1770 train_time:24106ms step_avg:94.53ms
step:266/1770 train_time:24202ms step_avg:94.54ms
step:267/1770 train_time:24297ms step_avg:94.54ms
step:268/1770 train_time:24392ms step_avg:94.54ms
step:269/1770 train_time:24487ms step_avg:94.54ms
step:270/1770 train_time:24582ms step_avg:94.55ms
step:271/1770 train_time:24677ms step_avg:94.55ms
step:272/1770 train_time:24773ms step_avg:94.55ms
step:273/1770 train_time:24869ms step_avg:94.56ms
step:274/1770 train_time:24965ms step_avg:94.56ms
step:275/1770 train_time:25061ms step_avg:94.57ms
step:276/1770 train_time:25156ms step_avg:94.57ms
step:277/1770 train_time:25251ms step_avg:94.57ms
step:278/1770 train_time:25347ms step_avg:94.58ms
step:279/1770 train_time:25442ms step_avg:94.58ms
step:280/1770 train_time:25536ms step_avg:94.58ms
step:281/1770 train_time:25631ms step_avg:94.58ms
step:282/1770 train_time:25726ms step_avg:94.58ms
step:283/1770 train_time:25821ms step_avg:94.58ms
step:284/1770 train_time:25916ms step_avg:94.58ms
step:285/1770 train_time:26011ms step_avg:94.59ms
step:286/1770 train_time:26107ms step_avg:94.59ms
step:287/1770 train_time:26203ms step_avg:94.60ms
step:288/1770 train_time:26298ms step_avg:94.60ms
step:289/1770 train_time:26394ms step_avg:94.60ms
step:290/1770 train_time:26488ms step_avg:94.60ms
step:291/1770 train_time:26584ms step_avg:94.60ms
step:292/1770 train_time:26678ms step_avg:94.60ms
step:293/1770 train_time:26774ms step_avg:94.61ms
step:294/1770 train_time:26870ms step_avg:94.61ms
step:295/1770 train_time:26965ms step_avg:94.62ms
step:296/1770 train_time:27061ms step_avg:94.62ms
step:297/1770 train_time:27157ms step_avg:94.62ms
step:298/1770 train_time:27252ms step_avg:94.63ms
step:299/1770 train_time:27348ms step_avg:94.63ms
step:300/1770 train_time:27443ms step_avg:94.63ms
step:301/1770 train_time:27538ms step_avg:94.63ms
step:302/1770 train_time:27633ms step_avg:94.63ms
step:303/1770 train_time:27728ms step_avg:94.64ms
step:304/1770 train_time:27824ms step_avg:94.64ms
step:305/1770 train_time:27919ms step_avg:94.64ms
step:306/1770 train_time:28014ms step_avg:94.64ms
step:307/1770 train_time:28110ms step_avg:94.65ms
step:308/1770 train_time:28206ms step_avg:94.65ms
step:309/1770 train_time:28301ms step_avg:94.65ms
step:310/1770 train_time:28396ms step_avg:94.65ms
step:311/1770 train_time:28491ms step_avg:94.65ms
step:312/1770 train_time:28586ms step_avg:94.66ms
step:313/1770 train_time:28681ms step_avg:94.66ms
step:314/1770 train_time:28777ms step_avg:94.66ms
step:315/1770 train_time:28872ms step_avg:94.66ms
step:316/1770 train_time:28967ms step_avg:94.66ms
step:317/1770 train_time:29062ms step_avg:94.67ms
step:318/1770 train_time:29157ms step_avg:94.67ms
step:319/1770 train_time:29252ms step_avg:94.67ms
step:320/1770 train_time:29348ms step_avg:94.67ms
step:321/1770 train_time:29443ms step_avg:94.67ms
step:322/1770 train_time:29538ms step_avg:94.67ms
step:323/1770 train_time:29634ms step_avg:94.68ms
step:324/1770 train_time:29729ms step_avg:94.68ms
step:325/1770 train_time:29824ms step_avg:94.68ms
step:326/1770 train_time:29919ms step_avg:94.68ms
step:327/1770 train_time:30014ms step_avg:94.68ms
step:328/1770 train_time:30109ms step_avg:94.68ms
step:329/1770 train_time:30204ms step_avg:94.68ms
step:330/1770 train_time:30298ms step_avg:94.68ms
step:331/1770 train_time:30393ms step_avg:94.68ms
step:332/1770 train_time:30489ms step_avg:94.69ms
step:333/1770 train_time:30584ms step_avg:94.69ms
step:334/1770 train_time:30679ms step_avg:94.69ms
step:335/1770 train_time:30774ms step_avg:94.69ms
step:336/1770 train_time:30869ms step_avg:94.69ms
step:337/1770 train_time:30965ms step_avg:94.69ms
step:338/1770 train_time:31060ms step_avg:94.70ms
step:339/1770 train_time:31155ms step_avg:94.70ms
step:340/1770 train_time:31250ms step_avg:94.70ms
step:341/1770 train_time:31346ms step_avg:94.70ms
step:342/1770 train_time:31441ms step_avg:94.70ms
step:343/1770 train_time:31536ms step_avg:94.70ms
step:344/1770 train_time:31631ms step_avg:94.70ms
step:345/1770 train_time:31727ms step_avg:94.71ms
step:346/1770 train_time:31822ms step_avg:94.71ms
step:347/1770 train_time:31916ms step_avg:94.71ms
step:348/1770 train_time:32011ms step_avg:94.71ms
step:349/1770 train_time:32107ms step_avg:94.71ms
step:350/1770 train_time:32202ms step_avg:94.71ms
step:351/1770 train_time:32297ms step_avg:94.71ms
step:352/1770 train_time:32392ms step_avg:94.71ms
step:353/1770 train_time:32487ms step_avg:94.71ms
step:354/1770 train_time:32582ms step_avg:94.72ms
step:355/1770 train_time:32678ms step_avg:94.72ms
step:356/1770 train_time:32773ms step_avg:94.72ms
step:357/1770 train_time:32869ms step_avg:94.72ms
step:358/1770 train_time:32964ms step_avg:94.72ms
step:359/1770 train_time:33059ms step_avg:94.72ms
step:360/1770 train_time:33155ms step_avg:94.73ms
step:361/1770 train_time:33250ms step_avg:94.73ms
step:362/1770 train_time:33346ms step_avg:94.73ms
step:363/1770 train_time:33441ms step_avg:94.73ms
step:364/1770 train_time:33535ms step_avg:94.73ms
step:365/1770 train_time:33631ms step_avg:94.73ms
step:366/1770 train_time:33726ms step_avg:94.74ms
step:367/1770 train_time:33822ms step_avg:94.74ms
step:368/1770 train_time:33917ms step_avg:94.74ms
step:369/1770 train_time:34012ms step_avg:94.74ms
step:370/1770 train_time:34107ms step_avg:94.74ms
step:371/1770 train_time:34203ms step_avg:94.74ms
step:372/1770 train_time:34297ms step_avg:94.74ms
step:373/1770 train_time:34393ms step_avg:94.75ms
step:374/1770 train_time:34488ms step_avg:94.75ms
step:375/1770 train_time:34583ms step_avg:94.75ms
step:375/1770 val_loss:3.9003 train_time:34676ms step_avg:95.00ms
step:376/1770 train_time:34700ms step_avg:94.81ms
step:377/1770 train_time:34786ms step_avg:94.78ms
step:378/1770 train_time:34884ms step_avg:94.79ms
step:379/1770 train_time:34982ms step_avg:94.80ms
step:380/1770 train_time:35076ms step_avg:94.80ms
step:381/1770 train_time:35171ms step_avg:94.80ms
step:382/1770 train_time:35266ms step_avg:94.80ms
step:383/1770 train_time:35361ms step_avg:94.80ms
step:384/1770 train_time:35456ms step_avg:94.80ms
step:385/1770 train_time:35550ms step_avg:94.80ms
step:386/1770 train_time:35645ms step_avg:94.80ms
step:387/1770 train_time:35740ms step_avg:94.80ms
step:388/1770 train_time:35836ms step_avg:94.80ms
step:389/1770 train_time:35932ms step_avg:94.81ms
step:390/1770 train_time:36027ms step_avg:94.81ms
step:391/1770 train_time:36122ms step_avg:94.81ms
step:392/1770 train_time:36217ms step_avg:94.81ms
step:393/1770 train_time:36312ms step_avg:94.81ms
step:394/1770 train_time:36407ms step_avg:94.81ms
step:395/1770 train_time:36502ms step_avg:94.81ms
step:396/1770 train_time:36599ms step_avg:94.82ms
step:397/1770 train_time:36696ms step_avg:94.82ms
step:398/1770 train_time:36793ms step_avg:94.83ms
step:399/1770 train_time:36890ms step_avg:94.83ms
step:400/1770 train_time:36987ms step_avg:94.84ms
step:401/1770 train_time:37085ms step_avg:94.85ms
step:402/1770 train_time:37181ms step_avg:94.85ms
step:403/1770 train_time:37278ms step_avg:94.85ms
step:404/1770 train_time:37374ms step_avg:94.86ms
step:405/1770 train_time:37471ms step_avg:94.86ms
step:406/1770 train_time:37568ms step_avg:94.87ms
step:407/1770 train_time:37665ms step_avg:94.87ms
step:408/1770 train_time:37763ms step_avg:94.88ms
step:409/1770 train_time:37860ms step_avg:94.89ms
step:410/1770 train_time:37956ms step_avg:94.89ms
step:411/1770 train_time:38053ms step_avg:94.90ms
step:412/1770 train_time:38151ms step_avg:94.90ms
step:413/1770 train_time:38248ms step_avg:94.91ms
step:414/1770 train_time:38345ms step_avg:94.91ms
step:415/1770 train_time:38442ms step_avg:94.92ms
step:416/1770 train_time:38538ms step_avg:94.92ms
step:417/1770 train_time:38635ms step_avg:94.93ms
step:418/1770 train_time:38733ms step_avg:94.93ms
step:419/1770 train_time:38829ms step_avg:94.94ms
step:420/1770 train_time:38927ms step_avg:94.94ms
step:421/1770 train_time:39024ms step_avg:94.95ms
step:422/1770 train_time:39121ms step_avg:94.95ms
step:423/1770 train_time:39217ms step_avg:94.96ms
step:424/1770 train_time:39314ms step_avg:94.96ms
step:425/1770 train_time:39411ms step_avg:94.97ms
step:426/1770 train_time:39509ms step_avg:94.97ms
step:427/1770 train_time:39606ms step_avg:94.98ms
step:428/1770 train_time:39702ms step_avg:94.98ms
step:429/1770 train_time:39798ms step_avg:94.98ms
step:430/1770 train_time:39895ms step_avg:94.99ms
step:431/1770 train_time:39992ms step_avg:94.99ms
step:432/1770 train_time:40090ms step_avg:95.00ms
step:433/1770 train_time:40187ms step_avg:95.00ms
step:434/1770 train_time:40284ms step_avg:95.01ms
step:435/1770 train_time:40380ms step_avg:95.01ms
step:436/1770 train_time:40477ms step_avg:95.02ms
step:437/1770 train_time:40574ms step_avg:95.02ms
step:438/1770 train_time:40671ms step_avg:95.03ms
step:439/1770 train_time:40769ms step_avg:95.03ms
step:440/1770 train_time:40866ms step_avg:95.04ms
step:441/1770 train_time:40963ms step_avg:95.04ms
step:442/1770 train_time:41060ms step_avg:95.05ms
step:443/1770 train_time:41157ms step_avg:95.05ms
step:444/1770 train_time:41254ms step_avg:95.06ms
step:445/1770 train_time:41351ms step_avg:95.06ms
step:446/1770 train_time:41449ms step_avg:95.07ms
step:447/1770 train_time:41546ms step_avg:95.07ms
step:448/1770 train_time:41643ms step_avg:95.08ms
step:449/1770 train_time:41740ms step_avg:95.08ms
step:450/1770 train_time:41837ms step_avg:95.09ms
step:451/1770 train_time:41935ms step_avg:95.09ms
step:452/1770 train_time:42032ms step_avg:95.10ms
step:453/1770 train_time:42130ms step_avg:95.10ms
step:454/1770 train_time:42227ms step_avg:95.11ms
step:455/1770 train_time:42324ms step_avg:95.11ms
step:456/1770 train_time:42421ms step_avg:95.11ms
step:457/1770 train_time:42518ms step_avg:95.12ms
step:458/1770 train_time:42619ms step_avg:95.13ms
step:459/1770 train_time:42712ms step_avg:95.13ms
step:460/1770 train_time:42810ms step_avg:95.13ms
step:461/1770 train_time:42907ms step_avg:95.14ms
step:462/1770 train_time:43004ms step_avg:95.14ms
step:463/1770 train_time:43100ms step_avg:95.14ms
step:464/1770 train_time:43198ms step_avg:95.15ms
step:465/1770 train_time:43294ms step_avg:95.15ms
step:466/1770 train_time:43392ms step_avg:95.16ms
step:467/1770 train_time:43489ms step_avg:95.16ms
step:468/1770 train_time:43586ms step_avg:95.17ms
step:469/1770 train_time:43684ms step_avg:95.17ms
step:470/1770 train_time:43780ms step_avg:95.17ms
step:471/1770 train_time:43878ms step_avg:95.18ms
step:472/1770 train_time:43975ms step_avg:95.18ms
step:473/1770 train_time:44072ms step_avg:95.19ms
step:474/1770 train_time:44169ms step_avg:95.19ms
step:475/1770 train_time:44266ms step_avg:95.20ms
step:476/1770 train_time:44363ms step_avg:95.20ms
step:477/1770 train_time:44460ms step_avg:95.20ms
step:478/1770 train_time:44557ms step_avg:95.21ms
step:479/1770 train_time:44655ms step_avg:95.21ms
step:480/1770 train_time:44751ms step_avg:95.22ms
step:481/1770 train_time:44849ms step_avg:95.22ms
step:482/1770 train_time:44946ms step_avg:95.22ms
step:483/1770 train_time:45043ms step_avg:95.23ms
step:484/1770 train_time:45140ms step_avg:95.23ms
step:485/1770 train_time:45237ms step_avg:95.24ms
step:486/1770 train_time:45335ms step_avg:95.24ms
step:487/1770 train_time:45432ms step_avg:95.24ms
step:488/1770 train_time:45529ms step_avg:95.25ms
step:489/1770 train_time:45627ms step_avg:95.25ms
step:490/1770 train_time:45725ms step_avg:95.26ms
step:491/1770 train_time:45820ms step_avg:95.26ms
step:492/1770 train_time:45917ms step_avg:95.26ms
step:493/1770 train_time:46015ms step_avg:95.27ms
step:494/1770 train_time:46112ms step_avg:95.27ms
step:495/1770 train_time:46210ms step_avg:95.28ms
step:496/1770 train_time:46307ms step_avg:95.28ms
step:497/1770 train_time:46404ms step_avg:95.29ms
step:498/1770 train_time:46501ms step_avg:95.29ms
step:499/1770 train_time:46598ms step_avg:95.29ms
step:500/1770 train_time:46696ms step_avg:95.30ms
step:500/1770 val_loss:3.7539 train_time:46791ms step_avg:95.49ms
step:501/1770 train_time:46813ms step_avg:95.34ms
step:502/1770 train_time:46903ms step_avg:95.33ms
step:503/1770 train_time:47003ms step_avg:95.34ms
step:504/1770 train_time:47101ms step_avg:95.35ms
step:505/1770 train_time:47198ms step_avg:95.35ms
step:506/1770 train_time:47294ms step_avg:95.35ms
step:507/1770 train_time:47391ms step_avg:95.35ms
step:508/1770 train_time:47488ms step_avg:95.36ms
step:509/1770 train_time:47585ms step_avg:95.36ms
step:510/1770 train_time:47681ms step_avg:95.36ms
step:511/1770 train_time:47778ms step_avg:95.37ms
step:512/1770 train_time:47875ms step_avg:95.37ms
step:513/1770 train_time:47973ms step_avg:95.37ms
step:514/1770 train_time:48070ms step_avg:95.38ms
step:515/1770 train_time:48167ms step_avg:95.38ms
step:516/1770 train_time:48265ms step_avg:95.38ms
step:517/1770 train_time:48362ms step_avg:95.39ms
step:518/1770 train_time:48459ms step_avg:95.39ms
step:519/1770 train_time:48557ms step_avg:95.40ms
step:520/1770 train_time:48653ms step_avg:95.40ms
step:521/1770 train_time:48750ms step_avg:95.40ms
step:522/1770 train_time:48847ms step_avg:95.40ms
step:523/1770 train_time:48944ms step_avg:95.41ms
step:524/1770 train_time:49042ms step_avg:95.41ms
step:525/1770 train_time:49140ms step_avg:95.42ms
step:526/1770 train_time:49237ms step_avg:95.42ms
step:527/1770 train_time:49334ms step_avg:95.42ms
step:528/1770 train_time:49431ms step_avg:95.43ms
step:529/1770 train_time:49529ms step_avg:95.43ms
step:530/1770 train_time:49627ms step_avg:95.44ms
step:531/1770 train_time:49724ms step_avg:95.44ms
step:532/1770 train_time:49822ms step_avg:95.44ms
step:533/1770 train_time:49920ms step_avg:95.45ms
step:534/1770 train_time:50017ms step_avg:95.45ms
step:535/1770 train_time:50114ms step_avg:95.46ms
step:536/1770 train_time:50211ms step_avg:95.46ms
step:537/1770 train_time:50308ms step_avg:95.46ms
step:538/1770 train_time:50405ms step_avg:95.46ms
step:539/1770 train_time:50504ms step_avg:95.47ms
step:540/1770 train_time:50602ms step_avg:95.47ms
step:541/1770 train_time:50699ms step_avg:95.48ms
step:542/1770 train_time:50797ms step_avg:95.48ms
step:543/1770 train_time:50895ms step_avg:95.49ms
step:544/1770 train_time:50992ms step_avg:95.49ms
step:545/1770 train_time:51089ms step_avg:95.49ms
step:546/1770 train_time:51187ms step_avg:95.50ms
step:547/1770 train_time:51285ms step_avg:95.50ms
step:548/1770 train_time:51382ms step_avg:95.51ms
step:549/1770 train_time:51480ms step_avg:95.51ms
step:550/1770 train_time:51577ms step_avg:95.51ms
step:551/1770 train_time:51675ms step_avg:95.52ms
step:552/1770 train_time:51772ms step_avg:95.52ms
step:553/1770 train_time:51869ms step_avg:95.52ms
step:554/1770 train_time:51967ms step_avg:95.53ms
step:555/1770 train_time:52063ms step_avg:95.53ms
step:556/1770 train_time:52161ms step_avg:95.53ms
step:557/1770 train_time:52258ms step_avg:95.54ms
step:558/1770 train_time:52355ms step_avg:95.54ms
step:559/1770 train_time:52453ms step_avg:95.54ms
step:560/1770 train_time:52550ms step_avg:95.55ms
step:561/1770 train_time:52648ms step_avg:95.55ms
step:562/1770 train_time:52745ms step_avg:95.55ms
step:563/1770 train_time:52843ms step_avg:95.56ms
step:564/1770 train_time:52941ms step_avg:95.56ms
step:565/1770 train_time:53039ms step_avg:95.57ms
step:566/1770 train_time:53136ms step_avg:95.57ms
step:567/1770 train_time:53233ms step_avg:95.57ms
step:568/1770 train_time:53330ms step_avg:95.57ms
step:569/1770 train_time:53428ms step_avg:95.58ms
step:570/1770 train_time:53525ms step_avg:95.58ms
step:571/1770 train_time:53623ms step_avg:95.58ms
step:572/1770 train_time:53720ms step_avg:95.59ms
step:573/1770 train_time:53818ms step_avg:95.59ms
step:574/1770 train_time:53915ms step_avg:95.59ms
step:575/1770 train_time:54013ms step_avg:95.60ms
step:576/1770 train_time:54110ms step_avg:95.60ms
step:577/1770 train_time:54207ms step_avg:95.60ms
step:578/1770 train_time:54304ms step_avg:95.61ms
step:579/1770 train_time:54402ms step_avg:95.61ms
step:580/1770 train_time:54500ms step_avg:95.61ms
step:581/1770 train_time:54597ms step_avg:95.62ms
step:582/1770 train_time:54694ms step_avg:95.62ms
step:583/1770 train_time:54792ms step_avg:95.62ms
step:584/1770 train_time:54889ms step_avg:95.63ms
step:585/1770 train_time:54987ms step_avg:95.63ms
step:586/1770 train_time:55084ms step_avg:95.63ms
step:587/1770 train_time:55182ms step_avg:95.64ms
step:588/1770 train_time:55280ms step_avg:95.64ms
step:589/1770 train_time:55377ms step_avg:95.64ms
step:590/1770 train_time:55473ms step_avg:95.64ms
step:591/1770 train_time:55571ms step_avg:95.65ms
step:592/1770 train_time:55668ms step_avg:95.65ms
step:593/1770 train_time:55766ms step_avg:95.65ms
step:594/1770 train_time:55865ms step_avg:95.66ms
step:595/1770 train_time:55964ms step_avg:95.66ms
step:596/1770 train_time:56062ms step_avg:95.67ms
step:597/1770 train_time:56161ms step_avg:95.67ms
step:598/1770 train_time:56258ms step_avg:95.68ms
step:599/1770 train_time:56356ms step_avg:95.68ms
step:600/1770 train_time:56453ms step_avg:95.68ms
step:601/1770 train_time:56550ms step_avg:95.69ms
step:602/1770 train_time:56647ms step_avg:95.69ms
step:603/1770 train_time:56745ms step_avg:95.69ms
step:604/1770 train_time:56842ms step_avg:95.69ms
step:605/1770 train_time:56940ms step_avg:95.70ms
step:606/1770 train_time:57037ms step_avg:95.70ms
step:607/1770 train_time:57134ms step_avg:95.70ms
step:608/1770 train_time:57232ms step_avg:95.71ms
step:609/1770 train_time:57329ms step_avg:95.71ms
step:610/1770 train_time:57427ms step_avg:95.71ms
step:611/1770 train_time:57524ms step_avg:95.71ms
step:612/1770 train_time:57624ms step_avg:95.72ms
step:613/1770 train_time:57719ms step_avg:95.72ms
step:614/1770 train_time:57817ms step_avg:95.72ms
step:615/1770 train_time:57913ms step_avg:95.72ms
step:616/1770 train_time:58010ms step_avg:95.73ms
step:617/1770 train_time:58108ms step_avg:95.73ms
step:618/1770 train_time:58206ms step_avg:95.73ms
step:619/1770 train_time:58304ms step_avg:95.74ms
step:620/1770 train_time:58402ms step_avg:95.74ms
step:621/1770 train_time:58500ms step_avg:95.74ms
step:622/1770 train_time:58597ms step_avg:95.75ms
step:623/1770 train_time:58694ms step_avg:95.75ms
step:624/1770 train_time:58791ms step_avg:95.75ms
step:625/1770 train_time:58888ms step_avg:95.75ms
step:625/1770 val_loss:3.6627 train_time:58984ms step_avg:95.91ms
step:626/1770 train_time:59007ms step_avg:95.79ms
step:627/1770 train_time:59097ms step_avg:95.78ms
step:628/1770 train_time:59196ms step_avg:95.79ms
step:629/1770 train_time:59294ms step_avg:95.79ms
step:630/1770 train_time:59391ms step_avg:95.79ms
step:631/1770 train_time:59487ms step_avg:95.79ms
step:632/1770 train_time:59584ms step_avg:95.79ms
step:633/1770 train_time:59681ms step_avg:95.80ms
step:634/1770 train_time:59778ms step_avg:95.80ms
step:635/1770 train_time:59875ms step_avg:95.80ms
step:636/1770 train_time:59973ms step_avg:95.80ms
step:637/1770 train_time:60073ms step_avg:95.81ms
step:638/1770 train_time:60172ms step_avg:95.81ms
step:639/1770 train_time:60269ms step_avg:95.82ms
step:640/1770 train_time:60367ms step_avg:95.82ms
step:641/1770 train_time:60465ms step_avg:95.82ms
step:642/1770 train_time:60562ms step_avg:95.83ms
step:643/1770 train_time:60660ms step_avg:95.83ms
step:644/1770 train_time:60757ms step_avg:95.83ms
step:645/1770 train_time:60854ms step_avg:95.83ms
step:646/1770 train_time:60951ms step_avg:95.83ms
step:647/1770 train_time:61048ms step_avg:95.84ms
step:648/1770 train_time:61146ms step_avg:95.84ms
step:649/1770 train_time:61244ms step_avg:95.84ms
step:650/1770 train_time:61343ms step_avg:95.85ms
step:651/1770 train_time:61440ms step_avg:95.85ms
step:652/1770 train_time:61537ms step_avg:95.85ms
step:653/1770 train_time:61635ms step_avg:95.86ms
step:654/1770 train_time:61732ms step_avg:95.86ms
step:655/1770 train_time:61829ms step_avg:95.86ms
step:656/1770 train_time:61927ms step_avg:95.86ms
step:657/1770 train_time:62024ms step_avg:95.86ms
step:658/1770 train_time:62123ms step_avg:95.87ms
step:659/1770 train_time:62222ms step_avg:95.87ms
step:660/1770 train_time:62321ms step_avg:95.88ms
step:661/1770 train_time:62419ms step_avg:95.88ms
step:662/1770 train_time:62519ms step_avg:95.89ms
step:663/1770 train_time:62618ms step_avg:95.89ms
step:664/1770 train_time:62717ms step_avg:95.90ms
step:665/1770 train_time:62816ms step_avg:95.90ms
step:666/1770 train_time:62915ms step_avg:95.91ms
step:667/1770 train_time:63013ms step_avg:95.91ms
step:668/1770 train_time:63112ms step_avg:95.91ms
step:669/1770 train_time:63211ms step_avg:95.92ms
step:670/1770 train_time:63310ms step_avg:95.92ms
step:671/1770 train_time:63410ms step_avg:95.93ms
step:672/1770 train_time:63509ms step_avg:95.94ms
step:673/1770 train_time:63608ms step_avg:95.94ms
step:674/1770 train_time:63707ms step_avg:95.95ms
step:675/1770 train_time:63807ms step_avg:95.95ms
step:676/1770 train_time:63907ms step_avg:95.96ms
step:677/1770 train_time:64006ms step_avg:95.96ms
step:678/1770 train_time:64106ms step_avg:95.97ms
step:679/1770 train_time:64206ms step_avg:95.97ms
step:680/1770 train_time:64305ms step_avg:95.98ms
step:681/1770 train_time:64404ms step_avg:95.98ms
step:682/1770 train_time:64504ms step_avg:95.99ms
step:683/1770 train_time:64603ms step_avg:95.99ms
step:684/1770 train_time:64703ms step_avg:96.00ms
step:685/1770 train_time:64803ms step_avg:96.00ms
step:686/1770 train_time:64902ms step_avg:96.01ms
step:687/1770 train_time:65001ms step_avg:96.01ms
step:688/1770 train_time:65100ms step_avg:96.02ms
step:689/1770 train_time:65199ms step_avg:96.02ms
step:690/1770 train_time:65298ms step_avg:96.03ms
step:691/1770 train_time:65396ms step_avg:96.03ms
step:692/1770 train_time:65496ms step_avg:96.03ms
step:693/1770 train_time:65595ms step_avg:96.04ms
step:694/1770 train_time:65695ms step_avg:96.05ms
step:695/1770 train_time:65795ms step_avg:96.05ms
step:696/1770 train_time:65895ms step_avg:96.06ms
step:697/1770 train_time:65994ms step_avg:96.06ms
step:698/1770 train_time:66094ms step_avg:96.07ms
step:699/1770 train_time:66193ms step_avg:96.07ms
step:700/1770 train_time:66293ms step_avg:96.08ms
step:701/1770 train_time:66392ms step_avg:96.08ms
step:702/1770 train_time:66491ms step_avg:96.08ms
step:703/1770 train_time:66590ms step_avg:96.09ms
step:704/1770 train_time:66689ms step_avg:96.09ms
step:705/1770 train_time:66789ms step_avg:96.10ms
step:706/1770 train_time:66889ms step_avg:96.10ms
step:707/1770 train_time:66988ms step_avg:96.11ms
step:708/1770 train_time:67087ms step_avg:96.11ms
step:709/1770 train_time:67188ms step_avg:96.12ms
step:710/1770 train_time:67288ms step_avg:96.13ms
step:711/1770 train_time:67388ms step_avg:96.13ms
step:712/1770 train_time:67487ms step_avg:96.14ms
step:713/1770 train_time:67587ms step_avg:96.14ms
step:714/1770 train_time:67686ms step_avg:96.14ms
step:715/1770 train_time:67785ms step_avg:96.15ms
step:716/1770 train_time:67884ms step_avg:96.15ms
step:717/1770 train_time:67983ms step_avg:96.16ms
step:718/1770 train_time:68083ms step_avg:96.16ms
step:719/1770 train_time:68182ms step_avg:96.17ms
step:720/1770 train_time:68281ms step_avg:96.17ms
step:721/1770 train_time:68380ms step_avg:96.18ms
step:722/1770 train_time:68480ms step_avg:96.18ms
step:723/1770 train_time:68579ms step_avg:96.18ms
step:724/1770 train_time:68678ms step_avg:96.19ms
step:725/1770 train_time:68777ms step_avg:96.19ms
step:726/1770 train_time:68875ms step_avg:96.19ms
step:727/1770 train_time:68974ms step_avg:96.20ms
step:728/1770 train_time:69073ms step_avg:96.20ms
step:729/1770 train_time:69172ms step_avg:96.21ms
step:730/1770 train_time:69271ms step_avg:96.21ms
step:731/1770 train_time:69371ms step_avg:96.21ms
step:732/1770 train_time:69470ms step_avg:96.22ms
step:733/1770 train_time:69569ms step_avg:96.22ms
step:734/1770 train_time:69668ms step_avg:96.23ms
step:735/1770 train_time:69768ms step_avg:96.23ms
step:736/1770 train_time:69867ms step_avg:96.24ms
step:737/1770 train_time:69966ms step_avg:96.24ms
step:738/1770 train_time:70066ms step_avg:96.24ms
step:739/1770 train_time:70165ms step_avg:96.25ms
step:740/1770 train_time:70265ms step_avg:96.25ms
step:741/1770 train_time:70366ms step_avg:96.26ms
step:742/1770 train_time:70466ms step_avg:96.26ms
step:743/1770 train_time:70565ms step_avg:96.27ms
step:744/1770 train_time:70664ms step_avg:96.27ms
step:745/1770 train_time:70764ms step_avg:96.28ms
step:746/1770 train_time:70864ms step_avg:96.28ms
step:747/1770 train_time:70963ms step_avg:96.29ms
step:748/1770 train_time:71063ms step_avg:96.29ms
step:749/1770 train_time:71163ms step_avg:96.30ms
step:750/1770 train_time:71262ms step_avg:96.30ms
step:750/1770 val_loss:3.5991 train_time:71360ms step_avg:96.43ms
step:751/1770 train_time:71382ms step_avg:96.33ms
step:752/1770 train_time:71472ms step_avg:96.32ms
step:753/1770 train_time:71573ms step_avg:96.33ms
step:754/1770 train_time:71671ms step_avg:96.33ms
step:755/1770 train_time:71770ms step_avg:96.34ms
step:756/1770 train_time:71868ms step_avg:96.34ms
step:757/1770 train_time:71966ms step_avg:96.34ms
step:758/1770 train_time:72064ms step_avg:96.34ms
step:759/1770 train_time:72163ms step_avg:96.35ms
step:760/1770 train_time:72261ms step_avg:96.35ms
step:761/1770 train_time:72360ms step_avg:96.35ms
step:762/1770 train_time:72460ms step_avg:96.36ms
step:763/1770 train_time:72560ms step_avg:96.36ms
step:764/1770 train_time:72659ms step_avg:96.36ms
step:765/1770 train_time:72758ms step_avg:96.37ms
step:766/1770 train_time:72857ms step_avg:96.37ms
step:767/1770 train_time:72958ms step_avg:96.38ms
step:768/1770 train_time:73058ms step_avg:96.38ms
step:769/1770 train_time:73159ms step_avg:96.39ms
step:770/1770 train_time:73258ms step_avg:96.39ms
step:771/1770 train_time:73357ms step_avg:96.40ms
step:772/1770 train_time:73457ms step_avg:96.40ms
step:773/1770 train_time:73557ms step_avg:96.40ms
step:774/1770 train_time:73655ms step_avg:96.41ms
step:775/1770 train_time:73754ms step_avg:96.41ms
step:776/1770 train_time:73854ms step_avg:96.41ms
step:777/1770 train_time:73953ms step_avg:96.42ms
step:778/1770 train_time:74052ms step_avg:96.42ms
step:779/1770 train_time:74152ms step_avg:96.43ms
step:780/1770 train_time:74251ms step_avg:96.43ms
step:781/1770 train_time:74350ms step_avg:96.43ms
step:782/1770 train_time:74449ms step_avg:96.44ms
step:783/1770 train_time:74547ms step_avg:96.44ms
step:784/1770 train_time:74647ms step_avg:96.44ms
step:785/1770 train_time:74747ms step_avg:96.45ms
step:786/1770 train_time:74846ms step_avg:96.45ms
step:787/1770 train_time:74946ms step_avg:96.46ms
step:788/1770 train_time:75045ms step_avg:96.46ms
step:789/1770 train_time:75145ms step_avg:96.46ms
step:790/1770 train_time:75244ms step_avg:96.47ms
step:791/1770 train_time:75344ms step_avg:96.47ms
step:792/1770 train_time:75444ms step_avg:96.48ms
step:793/1770 train_time:75544ms step_avg:96.48ms
step:794/1770 train_time:75643ms step_avg:96.48ms
step:795/1770 train_time:75743ms step_avg:96.49ms
step:796/1770 train_time:75843ms step_avg:96.49ms
step:797/1770 train_time:75942ms step_avg:96.50ms
step:798/1770 train_time:76042ms step_avg:96.50ms
step:799/1770 train_time:76141ms step_avg:96.50ms
step:800/1770 train_time:76240ms step_avg:96.51ms
step:801/1770 train_time:76339ms step_avg:96.51ms
step:802/1770 train_time:76439ms step_avg:96.51ms
step:803/1770 train_time:76539ms step_avg:96.52ms
step:804/1770 train_time:76639ms step_avg:96.52ms
step:805/1770 train_time:76739ms step_avg:96.53ms
step:806/1770 train_time:76839ms step_avg:96.53ms
step:807/1770 train_time:76940ms step_avg:96.54ms
step:808/1770 train_time:77039ms step_avg:96.54ms
step:809/1770 train_time:77139ms step_avg:96.54ms
step:810/1770 train_time:77239ms step_avg:96.55ms
step:811/1770 train_time:77338ms step_avg:96.55ms
step:812/1770 train_time:77438ms step_avg:96.56ms
step:813/1770 train_time:77538ms step_avg:96.56ms
step:814/1770 train_time:77637ms step_avg:96.56ms
step:815/1770 train_time:77736ms step_avg:96.57ms
step:816/1770 train_time:77835ms step_avg:96.57ms
step:817/1770 train_time:77935ms step_avg:96.57ms
step:818/1770 train_time:78035ms step_avg:96.58ms
step:819/1770 train_time:78135ms step_avg:96.58ms
step:820/1770 train_time:78235ms step_avg:96.59ms
step:821/1770 train_time:78335ms step_avg:96.59ms
step:822/1770 train_time:78435ms step_avg:96.59ms
step:823/1770 train_time:78534ms step_avg:96.60ms
step:824/1770 train_time:78633ms step_avg:96.60ms
step:825/1770 train_time:78734ms step_avg:96.61ms
step:826/1770 train_time:78833ms step_avg:96.61ms
step:827/1770 train_time:78932ms step_avg:96.61ms
step:828/1770 train_time:79032ms step_avg:96.62ms
step:829/1770 train_time:79132ms step_avg:96.62ms
step:830/1770 train_time:79232ms step_avg:96.62ms
step:831/1770 train_time:79332ms step_avg:96.63ms
step:832/1770 train_time:79431ms step_avg:96.63ms
step:833/1770 train_time:79530ms step_avg:96.63ms
step:834/1770 train_time:79630ms step_avg:96.64ms
step:835/1770 train_time:79729ms step_avg:96.64ms
step:836/1770 train_time:79828ms step_avg:96.64ms
step:837/1770 train_time:79928ms step_avg:96.65ms
step:838/1770 train_time:80028ms step_avg:96.65ms
step:839/1770 train_time:80128ms step_avg:96.66ms
step:840/1770 train_time:80227ms step_avg:96.66ms
step:841/1770 train_time:80327ms step_avg:96.66ms
step:842/1770 train_time:80426ms step_avg:96.67ms
step:843/1770 train_time:80526ms step_avg:96.67ms
step:844/1770 train_time:80625ms step_avg:96.67ms
step:845/1770 train_time:80726ms step_avg:96.68ms
step:846/1770 train_time:80825ms step_avg:96.68ms
step:847/1770 train_time:80924ms step_avg:96.68ms
step:848/1770 train_time:81024ms step_avg:96.69ms
step:849/1770 train_time:81124ms step_avg:96.69ms
step:850/1770 train_time:81224ms step_avg:96.70ms
step:851/1770 train_time:81323ms step_avg:96.70ms
step:852/1770 train_time:81422ms step_avg:96.70ms
step:853/1770 train_time:81522ms step_avg:96.70ms
step:854/1770 train_time:81621ms step_avg:96.71ms
step:855/1770 train_time:81720ms step_avg:96.71ms
step:856/1770 train_time:81820ms step_avg:96.71ms
step:857/1770 train_time:81919ms step_avg:96.72ms
step:858/1770 train_time:82019ms step_avg:96.72ms
step:859/1770 train_time:82119ms step_avg:96.72ms
step:860/1770 train_time:82219ms step_avg:96.73ms
step:861/1770 train_time:82319ms step_avg:96.73ms
step:862/1770 train_time:82419ms step_avg:96.74ms
step:863/1770 train_time:82519ms step_avg:96.74ms
step:864/1770 train_time:82618ms step_avg:96.74ms
step:865/1770 train_time:82718ms step_avg:96.75ms
step:866/1770 train_time:82819ms step_avg:96.75ms
step:867/1770 train_time:82919ms step_avg:96.76ms
step:868/1770 train_time:83019ms step_avg:96.76ms
step:869/1770 train_time:83119ms step_avg:96.76ms
step:870/1770 train_time:83219ms step_avg:96.77ms
step:871/1770 train_time:83319ms step_avg:96.77ms
step:872/1770 train_time:83419ms step_avg:96.77ms
step:873/1770 train_time:83520ms step_avg:96.78ms
step:874/1770 train_time:83620ms step_avg:96.78ms
step:875/1770 train_time:83720ms step_avg:96.79ms
step:875/1770 val_loss:3.5507 train_time:83818ms step_avg:96.90ms
step:876/1770 train_time:83840ms step_avg:96.81ms
step:877/1770 train_time:83929ms step_avg:96.80ms
step:878/1770 train_time:84030ms step_avg:96.81ms
step:879/1770 train_time:84130ms step_avg:96.81ms
step:880/1770 train_time:84229ms step_avg:96.81ms
step:881/1770 train_time:84328ms step_avg:96.82ms
step:882/1770 train_time:84427ms step_avg:96.82ms
step:883/1770 train_time:84526ms step_avg:96.82ms
step:884/1770 train_time:84625ms step_avg:96.83ms
step:885/1770 train_time:84724ms step_avg:96.83ms
step:886/1770 train_time:84823ms step_avg:96.83ms
step:887/1770 train_time:84924ms step_avg:96.83ms
step:888/1770 train_time:85025ms step_avg:96.84ms
step:889/1770 train_time:85126ms step_avg:96.84ms
step:890/1770 train_time:85225ms step_avg:96.85ms
step:891/1770 train_time:85326ms step_avg:96.85ms
step:892/1770 train_time:85425ms step_avg:96.85ms
step:893/1770 train_time:85524ms step_avg:96.86ms
step:894/1770 train_time:85623ms step_avg:96.86ms
step:895/1770 train_time:85722ms step_avg:96.86ms
step:896/1770 train_time:85822ms step_avg:96.86ms
step:897/1770 train_time:85922ms step_avg:96.87ms
step:898/1770 train_time:86022ms step_avg:96.87ms
step:899/1770 train_time:86122ms step_avg:96.88ms
step:900/1770 train_time:86221ms step_avg:96.88ms
step:901/1770 train_time:86320ms step_avg:96.88ms
step:902/1770 train_time:86420ms step_avg:96.88ms
step:903/1770 train_time:86519ms step_avg:96.89ms
step:904/1770 train_time:86619ms step_avg:96.89ms
step:905/1770 train_time:86718ms step_avg:96.89ms
step:906/1770 train_time:86818ms step_avg:96.90ms
step:907/1770 train_time:86918ms step_avg:96.90ms
step:908/1770 train_time:87018ms step_avg:96.90ms
step:909/1770 train_time:87118ms step_avg:96.91ms
step:910/1770 train_time:87217ms step_avg:96.91ms
step:911/1770 train_time:87317ms step_avg:96.91ms
step:912/1770 train_time:87417ms step_avg:96.91ms
step:913/1770 train_time:87516ms step_avg:96.92ms
step:914/1770 train_time:87617ms step_avg:96.92ms
step:915/1770 train_time:87716ms step_avg:96.92ms
step:916/1770 train_time:87816ms step_avg:96.93ms
step:917/1770 train_time:87914ms step_avg:96.93ms
step:918/1770 train_time:88014ms step_avg:96.93ms
step:919/1770 train_time:88114ms step_avg:96.93ms
step:920/1770 train_time:88216ms step_avg:96.94ms
step:921/1770 train_time:88317ms step_avg:96.94ms
step:922/1770 train_time:88418ms step_avg:96.95ms
step:923/1770 train_time:88518ms step_avg:96.95ms
step:924/1770 train_time:88618ms step_avg:96.96ms
step:925/1770 train_time:88719ms step_avg:96.96ms
step:926/1770 train_time:88819ms step_avg:96.96ms
step:927/1770 train_time:88919ms step_avg:96.97ms
step:928/1770 train_time:89020ms step_avg:96.97ms
step:929/1770 train_time:89120ms step_avg:96.98ms
step:930/1770 train_time:89222ms step_avg:96.98ms
step:931/1770 train_time:89323ms step_avg:96.98ms
step:932/1770 train_time:89424ms step_avg:96.99ms
step:933/1770 train_time:89525ms step_avg:96.99ms
step:934/1770 train_time:89626ms step_avg:97.00ms
step:935/1770 train_time:89727ms step_avg:97.00ms
step:936/1770 train_time:89828ms step_avg:97.01ms
step:937/1770 train_time:89929ms step_avg:97.01ms
step:938/1770 train_time:90031ms step_avg:97.02ms
step:939/1770 train_time:90133ms step_avg:97.02ms
step:940/1770 train_time:90233ms step_avg:97.03ms
step:941/1770 train_time:90333ms step_avg:97.03ms
step:942/1770 train_time:90435ms step_avg:97.03ms
step:943/1770 train_time:90536ms step_avg:97.04ms
step:944/1770 train_time:90637ms step_avg:97.04ms
step:945/1770 train_time:90738ms step_avg:97.05ms
step:946/1770 train_time:90839ms step_avg:97.05ms
step:947/1770 train_time:90940ms step_avg:97.05ms
step:948/1770 train_time:91042ms step_avg:97.06ms
step:949/1770 train_time:91143ms step_avg:97.06ms
step:950/1770 train_time:91243ms step_avg:97.07ms
step:951/1770 train_time:91344ms step_avg:97.07ms
step:952/1770 train_time:91445ms step_avg:97.07ms
step:953/1770 train_time:91546ms step_avg:97.08ms
step:954/1770 train_time:91647ms step_avg:97.08ms
step:955/1770 train_time:91749ms step_avg:97.09ms
step:956/1770 train_time:91850ms step_avg:97.09ms
step:957/1770 train_time:91951ms step_avg:97.10ms
step:958/1770 train_time:92051ms step_avg:97.10ms
step:959/1770 train_time:92152ms step_avg:97.10ms
step:960/1770 train_time:92252ms step_avg:97.11ms
step:961/1770 train_time:92352ms step_avg:97.11ms
step:962/1770 train_time:92453ms step_avg:97.11ms
step:963/1770 train_time:92554ms step_avg:97.12ms
step:964/1770 train_time:92656ms step_avg:97.12ms
step:965/1770 train_time:92757ms step_avg:97.13ms
step:966/1770 train_time:92858ms step_avg:97.13ms
step:967/1770 train_time:92960ms step_avg:97.14ms
step:968/1770 train_time:93060ms step_avg:97.14ms
step:969/1770 train_time:93161ms step_avg:97.14ms
step:970/1770 train_time:93261ms step_avg:97.15ms
step:971/1770 train_time:93362ms step_avg:97.15ms
step:972/1770 train_time:93462ms step_avg:97.15ms
step:973/1770 train_time:93564ms step_avg:97.16ms
step:974/1770 train_time:93665ms step_avg:97.16ms
step:975/1770 train_time:93766ms step_avg:97.17ms
step:976/1770 train_time:93867ms step_avg:97.17ms
step:977/1770 train_time:93968ms step_avg:97.17ms
step:978/1770 train_time:94070ms step_avg:97.18ms
step:979/1770 train_time:94171ms step_avg:97.18ms
step:980/1770 train_time:94271ms step_avg:97.19ms
step:981/1770 train_time:94371ms step_avg:97.19ms
step:982/1770 train_time:94472ms step_avg:97.19ms
step:983/1770 train_time:94572ms step_avg:97.20ms
step:984/1770 train_time:94674ms step_avg:97.20ms
step:985/1770 train_time:94776ms step_avg:97.21ms
step:986/1770 train_time:94877ms step_avg:97.21ms
step:987/1770 train_time:94978ms step_avg:97.21ms
step:988/1770 train_time:95078ms step_avg:97.22ms
step:989/1770 train_time:95180ms step_avg:97.22ms
step:990/1770 train_time:95281ms step_avg:97.23ms
step:991/1770 train_time:95382ms step_avg:97.23ms
step:992/1770 train_time:95482ms step_avg:97.23ms
step:993/1770 train_time:95583ms step_avg:97.24ms
step:994/1770 train_time:95685ms step_avg:97.24ms
step:995/1770 train_time:95786ms step_avg:97.24ms
step:996/1770 train_time:95888ms step_avg:97.25ms
step:997/1770 train_time:95990ms step_avg:97.25ms
step:998/1770 train_time:96091ms step_avg:97.26ms
step:999/1770 train_time:96191ms step_avg:97.26ms
step:1000/1770 train_time:96292ms step_avg:97.26ms
step:1000/1770 val_loss:3.5100 train_time:96390ms step_avg:97.36ms
step:1001/1770 train_time:96414ms step_avg:97.29ms
step:1002/1770 train_time:96503ms step_avg:97.28ms
step:1003/1770 train_time:96606ms step_avg:97.29ms
step:1004/1770 train_time:96707ms step_avg:97.29ms
step:1005/1770 train_time:96807ms step_avg:97.29ms
step:1006/1770 train_time:96907ms step_avg:97.30ms
step:1007/1770 train_time:97007ms step_avg:97.30ms
step:1008/1770 train_time:97107ms step_avg:97.30ms
step:1009/1770 train_time:97207ms step_avg:97.30ms
step:1010/1770 train_time:97307ms step_avg:97.31ms
step:1011/1770 train_time:97408ms step_avg:97.31ms
step:1012/1770 train_time:97512ms step_avg:97.32ms
step:1013/1770 train_time:97615ms step_avg:97.32ms
step:1014/1770 train_time:97716ms step_avg:97.33ms
step:1015/1770 train_time:97816ms step_avg:97.33ms
step:1016/1770 train_time:97916ms step_avg:97.33ms
step:1017/1770 train_time:98018ms step_avg:97.34ms
step:1018/1770 train_time:98116ms step_avg:97.34ms
step:1019/1770 train_time:98217ms step_avg:97.34ms
step:1020/1770 train_time:98318ms step_avg:97.34ms
step:1021/1770 train_time:98418ms step_avg:97.35ms
step:1022/1770 train_time:98520ms step_avg:97.35ms
step:1023/1770 train_time:98622ms step_avg:97.36ms
step:1024/1770 train_time:98724ms step_avg:97.36ms
step:1025/1770 train_time:98825ms step_avg:97.36ms
step:1026/1770 train_time:98927ms step_avg:97.37ms
step:1027/1770 train_time:99028ms step_avg:97.37ms
step:1028/1770 train_time:99128ms step_avg:97.38ms
step:1029/1770 train_time:99231ms step_avg:97.38ms
step:1030/1770 train_time:99332ms step_avg:97.38ms
step:1031/1770 train_time:99433ms step_avg:97.39ms
step:1032/1770 train_time:99534ms step_avg:97.39ms
step:1033/1770 train_time:99636ms step_avg:97.40ms
step:1034/1770 train_time:99736ms step_avg:97.40ms
step:1035/1770 train_time:99837ms step_avg:97.40ms
step:1036/1770 train_time:99937ms step_avg:97.40ms
step:1037/1770 train_time:100038ms step_avg:97.41ms
step:1038/1770 train_time:100138ms step_avg:97.41ms
step:1039/1770 train_time:100240ms step_avg:97.41ms
step:1040/1770 train_time:100342ms step_avg:97.42ms
step:1041/1770 train_time:100444ms step_avg:97.42ms
step:1042/1770 train_time:100545ms step_avg:97.43ms
step:1043/1770 train_time:100647ms step_avg:97.43ms
step:1044/1770 train_time:100748ms step_avg:97.44ms
step:1045/1770 train_time:100848ms step_avg:97.44ms
step:1046/1770 train_time:100949ms step_avg:97.44ms
step:1047/1770 train_time:101050ms step_avg:97.44ms
step:1048/1770 train_time:101150ms step_avg:97.45ms
step:1049/1770 train_time:101253ms step_avg:97.45ms
step:1050/1770 train_time:101353ms step_avg:97.46ms
step:1051/1770 train_time:101455ms step_avg:97.46ms
step:1052/1770 train_time:101556ms step_avg:97.46ms
step:1053/1770 train_time:101656ms step_avg:97.47ms
step:1054/1770 train_time:101756ms step_avg:97.47ms
step:1055/1770 train_time:101857ms step_avg:97.47ms
step:1056/1770 train_time:101958ms step_avg:97.47ms
step:1057/1770 train_time:102059ms step_avg:97.48ms
step:1058/1770 train_time:102160ms step_avg:97.48ms
step:1059/1770 train_time:102262ms step_avg:97.49ms
step:1060/1770 train_time:102365ms step_avg:97.49ms
step:1061/1770 train_time:102467ms step_avg:97.49ms
step:1062/1770 train_time:102568ms step_avg:97.50ms
step:1063/1770 train_time:102671ms step_avg:97.50ms
step:1064/1770 train_time:102771ms step_avg:97.51ms
step:1065/1770 train_time:102873ms step_avg:97.51ms
step:1066/1770 train_time:102974ms step_avg:97.51ms
step:1067/1770 train_time:103075ms step_avg:97.52ms
step:1068/1770 train_time:103178ms step_avg:97.52ms
step:1069/1770 train_time:103279ms step_avg:97.53ms
step:1070/1770 train_time:103379ms step_avg:97.53ms
step:1071/1770 train_time:103480ms step_avg:97.53ms
step:1072/1770 train_time:103582ms step_avg:97.53ms
step:1073/1770 train_time:103683ms step_avg:97.54ms
step:1074/1770 train_time:103784ms step_avg:97.54ms
step:1075/1770 train_time:103886ms step_avg:97.55ms
step:1076/1770 train_time:103987ms step_avg:97.55ms
step:1077/1770 train_time:104088ms step_avg:97.55ms
step:1078/1770 train_time:104189ms step_avg:97.55ms
step:1079/1770 train_time:104289ms step_avg:97.56ms
step:1080/1770 train_time:104390ms step_avg:97.56ms
step:1081/1770 train_time:104492ms step_avg:97.57ms
step:1082/1770 train_time:104594ms step_avg:97.57ms
step:1083/1770 train_time:104696ms step_avg:97.57ms
step:1084/1770 train_time:104796ms step_avg:97.58ms
step:1085/1770 train_time:104897ms step_avg:97.58ms
step:1086/1770 train_time:104997ms step_avg:97.58ms
step:1087/1770 train_time:105097ms step_avg:97.58ms
step:1088/1770 train_time:105198ms step_avg:97.59ms
step:1089/1770 train_time:105299ms step_avg:97.59ms
step:1090/1770 train_time:105402ms step_avg:97.59ms
step:1091/1770 train_time:105503ms step_avg:97.60ms
step:1092/1770 train_time:105604ms step_avg:97.60ms
step:1093/1770 train_time:105705ms step_avg:97.60ms
step:1094/1770 train_time:105806ms step_avg:97.61ms
step:1095/1770 train_time:105907ms step_avg:97.61ms
step:1096/1770 train_time:106007ms step_avg:97.61ms
step:1097/1770 train_time:106108ms step_avg:97.62ms
step:1098/1770 train_time:106208ms step_avg:97.62ms
step:1099/1770 train_time:106308ms step_avg:97.62ms
step:1100/1770 train_time:106409ms step_avg:97.62ms
step:1101/1770 train_time:106514ms step_avg:97.63ms
step:1102/1770 train_time:106611ms step_avg:97.63ms
step:1103/1770 train_time:106711ms step_avg:97.63ms
step:1104/1770 train_time:106813ms step_avg:97.64ms
step:1105/1770 train_time:106914ms step_avg:97.64ms
step:1106/1770 train_time:107016ms step_avg:97.64ms
step:1107/1770 train_time:107116ms step_avg:97.64ms
step:1108/1770 train_time:107218ms step_avg:97.65ms
step:1109/1770 train_time:107320ms step_avg:97.65ms
step:1110/1770 train_time:107421ms step_avg:97.66ms
step:1111/1770 train_time:107522ms step_avg:97.66ms
step:1112/1770 train_time:107623ms step_avg:97.66ms
step:1113/1770 train_time:107725ms step_avg:97.67ms
step:1114/1770 train_time:107827ms step_avg:97.67ms
step:1115/1770 train_time:107928ms step_avg:97.67ms
step:1116/1770 train_time:108030ms step_avg:97.68ms
step:1117/1770 train_time:108131ms step_avg:97.68ms
step:1118/1770 train_time:108231ms step_avg:97.68ms
step:1119/1770 train_time:108333ms step_avg:97.69ms
step:1120/1770 train_time:108435ms step_avg:97.69ms
step:1121/1770 train_time:108536ms step_avg:97.69ms
step:1122/1770 train_time:108637ms step_avg:97.69ms
step:1123/1770 train_time:108737ms step_avg:97.70ms
step:1124/1770 train_time:108839ms step_avg:97.70ms
step:1125/1770 train_time:108940ms step_avg:97.70ms
step:1125/1770 val_loss:3.4713 train_time:109039ms step_avg:97.79ms
step:1126/1770 train_time:109061ms step_avg:97.73ms
step:1127/1770 train_time:109155ms step_avg:97.72ms
step:1128/1770 train_time:109256ms step_avg:97.72ms
step:1129/1770 train_time:109357ms step_avg:97.73ms
step:1130/1770 train_time:109459ms step_avg:97.73ms
step:1131/1770 train_time:109560ms step_avg:97.73ms
step:1132/1770 train_time:109661ms step_avg:97.74ms
step:1133/1770 train_time:109762ms step_avg:97.74ms
step:1134/1770 train_time:109862ms step_avg:97.74ms
step:1135/1770 train_time:109962ms step_avg:97.74ms
step:1136/1770 train_time:110064ms step_avg:97.75ms
step:1137/1770 train_time:110168ms step_avg:97.75ms
step:1138/1770 train_time:110270ms step_avg:97.76ms
step:1139/1770 train_time:110371ms step_avg:97.76ms
step:1140/1770 train_time:110472ms step_avg:97.76ms
step:1141/1770 train_time:110572ms step_avg:97.76ms
step:1142/1770 train_time:110672ms step_avg:97.77ms
step:1143/1770 train_time:110772ms step_avg:97.77ms
step:1144/1770 train_time:110872ms step_avg:97.77ms
step:1145/1770 train_time:110973ms step_avg:97.77ms
step:1146/1770 train_time:111076ms step_avg:97.78ms
step:1147/1770 train_time:111179ms step_avg:97.78ms
step:1148/1770 train_time:111281ms step_avg:97.79ms
step:1149/1770 train_time:111381ms step_avg:97.79ms
step:1150/1770 train_time:111484ms step_avg:97.79ms
step:1151/1770 train_time:111583ms step_avg:97.79ms
step:1152/1770 train_time:111685ms step_avg:97.80ms
step:1153/1770 train_time:111785ms step_avg:97.80ms
step:1154/1770 train_time:111887ms step_avg:97.80ms
step:1155/1770 train_time:111989ms step_avg:97.81ms
step:1156/1770 train_time:112089ms step_avg:97.81ms
step:1157/1770 train_time:112191ms step_avg:97.81ms
step:1158/1770 train_time:112293ms step_avg:97.82ms
step:1159/1770 train_time:112394ms step_avg:97.82ms
step:1160/1770 train_time:112494ms step_avg:97.82ms
step:1161/1770 train_time:112595ms step_avg:97.82ms
step:1162/1770 train_time:112697ms step_avg:97.83ms
step:1163/1770 train_time:112798ms step_avg:97.83ms
step:1164/1770 train_time:112900ms step_avg:97.83ms
step:1165/1770 train_time:113001ms step_avg:97.84ms
step:1166/1770 train_time:113103ms step_avg:97.84ms
step:1167/1770 train_time:113203ms step_avg:97.84ms
step:1168/1770 train_time:113304ms step_avg:97.84ms
step:1169/1770 train_time:113405ms step_avg:97.85ms
step:1170/1770 train_time:113507ms step_avg:97.85ms
step:1171/1770 train_time:113609ms step_avg:97.85ms
step:1172/1770 train_time:113711ms step_avg:97.86ms
step:1173/1770 train_time:113812ms step_avg:97.86ms
step:1174/1770 train_time:113914ms step_avg:97.86ms
step:1175/1770 train_time:114014ms step_avg:97.87ms
step:1176/1770 train_time:114114ms step_avg:97.87ms
step:1177/1770 train_time:114214ms step_avg:97.87ms
step:1178/1770 train_time:114316ms step_avg:97.87ms
step:1179/1770 train_time:114419ms step_avg:97.88ms
step:1180/1770 train_time:114521ms step_avg:97.88ms
step:1181/1770 train_time:114622ms step_avg:97.88ms
step:1182/1770 train_time:114723ms step_avg:97.89ms
step:1183/1770 train_time:114826ms step_avg:97.89ms
step:1184/1770 train_time:114930ms step_avg:97.90ms
step:1185/1770 train_time:115032ms step_avg:97.90ms
step:1186/1770 train_time:115134ms step_avg:97.90ms
step:1187/1770 train_time:115239ms step_avg:97.91ms
step:1188/1770 train_time:115341ms step_avg:97.91ms
step:1189/1770 train_time:115443ms step_avg:97.92ms
step:1190/1770 train_time:115545ms step_avg:97.92ms
step:1191/1770 train_time:115647ms step_avg:97.92ms
step:1192/1770 train_time:115749ms step_avg:97.93ms
step:1193/1770 train_time:115851ms step_avg:97.93ms
step:1194/1770 train_time:115954ms step_avg:97.93ms
step:1195/1770 train_time:116056ms step_avg:97.94ms
step:1196/1770 train_time:116160ms step_avg:97.94ms
step:1197/1770 train_time:116261ms step_avg:97.95ms
step:1198/1770 train_time:116363ms step_avg:97.95ms
step:1199/1770 train_time:116465ms step_avg:97.95ms
step:1200/1770 train_time:116567ms step_avg:97.96ms
step:1201/1770 train_time:116669ms step_avg:97.96ms
step:1202/1770 train_time:116771ms step_avg:97.96ms
step:1203/1770 train_time:116873ms step_avg:97.97ms
step:1204/1770 train_time:116976ms step_avg:97.97ms
step:1205/1770 train_time:117077ms step_avg:97.97ms
step:1206/1770 train_time:117180ms step_avg:97.98ms
step:1207/1770 train_time:117283ms step_avg:97.98ms
step:1208/1770 train_time:117386ms step_avg:97.98ms
step:1209/1770 train_time:117487ms step_avg:97.99ms
step:1210/1770 train_time:117588ms step_avg:97.99ms
step:1211/1770 train_time:117691ms step_avg:97.99ms
step:1212/1770 train_time:117794ms step_avg:98.00ms
step:1213/1770 train_time:117896ms step_avg:98.00ms
step:1214/1770 train_time:117997ms step_avg:98.00ms
step:1215/1770 train_time:118100ms step_avg:98.01ms
step:1216/1770 train_time:118205ms step_avg:98.01ms
step:1217/1770 train_time:118307ms step_avg:98.02ms
step:1218/1770 train_time:118409ms step_avg:98.02ms
step:1219/1770 train_time:118511ms step_avg:98.02ms
step:1220/1770 train_time:118613ms step_avg:98.03ms
step:1221/1770 train_time:118714ms step_avg:98.03ms
step:1222/1770 train_time:118818ms step_avg:98.03ms
step:1223/1770 train_time:118920ms step_avg:98.04ms
step:1224/1770 train_time:119023ms step_avg:98.04ms
step:1225/1770 train_time:119125ms step_avg:98.05ms
step:1226/1770 train_time:119227ms step_avg:98.05ms
step:1227/1770 train_time:119330ms step_avg:98.05ms
step:1228/1770 train_time:119434ms step_avg:98.06ms
step:1229/1770 train_time:119536ms step_avg:98.06ms
step:1230/1770 train_time:119640ms step_avg:98.07ms
step:1231/1770 train_time:119742ms step_avg:98.07ms
step:1232/1770 train_time:119845ms step_avg:98.07ms
step:1233/1770 train_time:119946ms step_avg:98.08ms
step:1234/1770 train_time:120049ms step_avg:98.08ms
step:1235/1770 train_time:120150ms step_avg:98.08ms
step:1236/1770 train_time:120253ms step_avg:98.09ms
step:1237/1770 train_time:120354ms step_avg:98.09ms
step:1238/1770 train_time:120457ms step_avg:98.09ms
step:1239/1770 train_time:120560ms step_avg:98.10ms
step:1240/1770 train_time:120662ms step_avg:98.10ms
step:1241/1770 train_time:120765ms step_avg:98.10ms
step:1242/1770 train_time:120867ms step_avg:98.11ms
step:1243/1770 train_time:120969ms step_avg:98.11ms
step:1244/1770 train_time:121071ms step_avg:98.11ms
step:1245/1770 train_time:121172ms step_avg:98.12ms
step:1246/1770 train_time:121275ms step_avg:98.12ms
step:1247/1770 train_time:121377ms step_avg:98.12ms
step:1248/1770 train_time:121480ms step_avg:98.13ms
step:1249/1770 train_time:121582ms step_avg:98.13ms
step:1250/1770 train_time:121684ms step_avg:98.13ms
step:1250/1770 val_loss:3.4243 train_time:121786ms step_avg:98.21ms
step:1251/1770 train_time:121808ms step_avg:98.15ms
step:1252/1770 train_time:121899ms step_avg:98.15ms
step:1253/1770 train_time:122003ms step_avg:98.15ms
step:1254/1770 train_time:122107ms step_avg:98.16ms
step:1255/1770 train_time:122211ms step_avg:98.16ms
step:1256/1770 train_time:122312ms step_avg:98.16ms
step:1257/1770 train_time:122413ms step_avg:98.17ms
step:1258/1770 train_time:122515ms step_avg:98.17ms
step:1259/1770 train_time:122617ms step_avg:98.17ms
step:1260/1770 train_time:122718ms step_avg:98.17ms
step:1261/1770 train_time:122821ms step_avg:98.18ms
step:1262/1770 train_time:122924ms step_avg:98.18ms
step:1263/1770 train_time:123027ms step_avg:98.19ms
step:1264/1770 train_time:123132ms step_avg:98.19ms
step:1265/1770 train_time:123233ms step_avg:98.19ms
step:1266/1770 train_time:123335ms step_avg:98.20ms
step:1267/1770 train_time:123437ms step_avg:98.20ms
step:1268/1770 train_time:123540ms step_avg:98.20ms
step:1269/1770 train_time:123642ms step_avg:98.21ms
step:1270/1770 train_time:123745ms step_avg:98.21ms
step:1271/1770 train_time:123847ms step_avg:98.21ms
step:1272/1770 train_time:123949ms step_avg:98.22ms
step:1273/1770 train_time:124051ms step_avg:98.22ms
step:1274/1770 train_time:124153ms step_avg:98.22ms
step:1275/1770 train_time:124255ms step_avg:98.23ms
step:1276/1770 train_time:124357ms step_avg:98.23ms
step:1277/1770 train_time:124458ms step_avg:98.23ms
step:1278/1770 train_time:124562ms step_avg:98.23ms
step:1279/1770 train_time:124665ms step_avg:98.24ms
step:1280/1770 train_time:124769ms step_avg:98.24ms
step:1281/1770 train_time:124870ms step_avg:98.25ms
step:1282/1770 train_time:124972ms step_avg:98.25ms
step:1283/1770 train_time:125076ms step_avg:98.25ms
step:1284/1770 train_time:125178ms step_avg:98.26ms
step:1285/1770 train_time:125280ms step_avg:98.26ms
step:1286/1770 train_time:125384ms step_avg:98.26ms
step:1287/1770 train_time:125488ms step_avg:98.27ms
step:1288/1770 train_time:125590ms step_avg:98.27ms
step:1289/1770 train_time:125692ms step_avg:98.27ms
step:1290/1770 train_time:125793ms step_avg:98.28ms
step:1291/1770 train_time:125895ms step_avg:98.28ms
step:1292/1770 train_time:125997ms step_avg:98.28ms
step:1293/1770 train_time:126100ms step_avg:98.29ms
step:1294/1770 train_time:126201ms step_avg:98.29ms
step:1295/1770 train_time:126302ms step_avg:98.29ms
step:1296/1770 train_time:126405ms step_avg:98.29ms
step:1297/1770 train_time:126508ms step_avg:98.30ms
step:1298/1770 train_time:126610ms step_avg:98.30ms
step:1299/1770 train_time:126712ms step_avg:98.30ms
step:1300/1770 train_time:126813ms step_avg:98.30ms
step:1301/1770 train_time:126915ms step_avg:98.31ms
step:1302/1770 train_time:127017ms step_avg:98.31ms
step:1303/1770 train_time:127119ms step_avg:98.31ms
step:1304/1770 train_time:127221ms step_avg:98.32ms
step:1305/1770 train_time:127323ms step_avg:98.32ms
step:1306/1770 train_time:127424ms step_avg:98.32ms
step:1307/1770 train_time:127527ms step_avg:98.32ms
step:1308/1770 train_time:127629ms step_avg:98.33ms
step:1309/1770 train_time:127731ms step_avg:98.33ms
step:1310/1770 train_time:127833ms step_avg:98.33ms
step:1311/1770 train_time:127935ms step_avg:98.34ms
step:1312/1770 train_time:128037ms step_avg:98.34ms
step:1313/1770 train_time:128139ms step_avg:98.34ms
step:1314/1770 train_time:128241ms step_avg:98.34ms
step:1315/1770 train_time:128344ms step_avg:98.35ms
step:1316/1770 train_time:128446ms step_avg:98.35ms
step:1317/1770 train_time:128549ms step_avg:98.35ms
step:1318/1770 train_time:128655ms step_avg:98.36ms
step:1319/1770 train_time:128758ms step_avg:98.36ms
step:1320/1770 train_time:128860ms step_avg:98.37ms
step:1321/1770 train_time:128962ms step_avg:98.37ms
step:1322/1770 train_time:129064ms step_avg:98.37ms
step:1323/1770 train_time:129168ms step_avg:98.38ms
step:1324/1770 train_time:129271ms step_avg:98.38ms
step:1325/1770 train_time:129374ms step_avg:98.38ms
step:1326/1770 train_time:129475ms step_avg:98.39ms
step:1327/1770 train_time:129580ms step_avg:98.39ms
step:1328/1770 train_time:129682ms step_avg:98.39ms
step:1329/1770 train_time:129785ms step_avg:98.40ms
step:1330/1770 train_time:129886ms step_avg:98.40ms
step:1331/1770 train_time:129988ms step_avg:98.40ms
step:1332/1770 train_time:130090ms step_avg:98.40ms
step:1333/1770 train_time:130192ms step_avg:98.41ms
step:1334/1770 train_time:130294ms step_avg:98.41ms
step:1335/1770 train_time:130396ms step_avg:98.41ms
step:1336/1770 train_time:130498ms step_avg:98.42ms
step:1337/1770 train_time:130600ms step_avg:98.42ms
step:1338/1770 train_time:130702ms step_avg:98.42ms
step:1339/1770 train_time:130805ms step_avg:98.42ms
step:1340/1770 train_time:130909ms step_avg:98.43ms
step:1341/1770 train_time:131011ms step_avg:98.43ms
step:1342/1770 train_time:131113ms step_avg:98.43ms
step:1343/1770 train_time:131216ms step_avg:98.44ms
step:1344/1770 train_time:131319ms step_avg:98.44ms
step:1345/1770 train_time:131421ms step_avg:98.44ms
step:1346/1770 train_time:131523ms step_avg:98.45ms
step:1347/1770 train_time:131626ms step_avg:98.45ms
step:1348/1770 train_time:131730ms step_avg:98.45ms
step:1349/1770 train_time:131832ms step_avg:98.46ms
step:1350/1770 train_time:131937ms step_avg:98.46ms
step:1351/1770 train_time:132037ms step_avg:98.46ms
step:1352/1770 train_time:132140ms step_avg:98.46ms
step:1353/1770 train_time:132244ms step_avg:98.47ms
step:1354/1770 train_time:132346ms step_avg:98.47ms
step:1355/1770 train_time:132449ms step_avg:98.47ms
step:1356/1770 train_time:132550ms step_avg:98.48ms
step:1357/1770 train_time:132652ms step_avg:98.48ms
step:1358/1770 train_time:132755ms step_avg:98.48ms
step:1359/1770 train_time:132858ms step_avg:98.49ms
step:1360/1770 train_time:132960ms step_avg:98.49ms
step:1361/1770 train_time:133063ms step_avg:98.49ms
step:1362/1770 train_time:133165ms step_avg:98.50ms
step:1363/1770 train_time:133268ms step_avg:98.50ms
step:1364/1770 train_time:133371ms step_avg:98.50ms
step:1365/1770 train_time:133473ms step_avg:98.50ms
step:1366/1770 train_time:133575ms step_avg:98.51ms
step:1367/1770 train_time:133678ms step_avg:98.51ms
step:1368/1770 train_time:133780ms step_avg:98.51ms
step:1369/1770 train_time:133884ms step_avg:98.52ms
step:1370/1770 train_time:133986ms step_avg:98.52ms
step:1371/1770 train_time:134089ms step_avg:98.52ms
step:1372/1770 train_time:134191ms step_avg:98.52ms
step:1373/1770 train_time:134293ms step_avg:98.53ms
step:1374/1770 train_time:134396ms step_avg:98.53ms
step:1375/1770 train_time:134498ms step_avg:98.53ms
step:1375/1770 val_loss:3.3807 train_time:134600ms step_avg:98.61ms
step:1376/1770 train_time:134622ms step_avg:98.55ms
step:1377/1770 train_time:134713ms step_avg:98.55ms
step:1378/1770 train_time:134815ms step_avg:98.55ms
step:1379/1770 train_time:134917ms step_avg:98.55ms
step:1380/1770 train_time:135019ms step_avg:98.55ms
step:1381/1770 train_time:135121ms step_avg:98.56ms
step:1382/1770 train_time:135223ms step_avg:98.56ms
step:1383/1770 train_time:135325ms step_avg:98.56ms
step:1384/1770 train_time:135427ms step_avg:98.56ms
step:1385/1770 train_time:135529ms step_avg:98.57ms
step:1386/1770 train_time:135632ms step_avg:98.57ms
step:1387/1770 train_time:135734ms step_avg:98.57ms
step:1388/1770 train_time:135836ms step_avg:98.57ms
step:1389/1770 train_time:135939ms step_avg:98.58ms
step:1390/1770 train_time:136041ms step_avg:98.58ms
step:1391/1770 train_time:136143ms step_avg:98.58ms
step:1392/1770 train_time:136245ms step_avg:98.59ms
step:1393/1770 train_time:136346ms step_avg:98.59ms
step:1394/1770 train_time:136448ms step_avg:98.59ms
step:1395/1770 train_time:136552ms step_avg:98.59ms
step:1396/1770 train_time:136656ms step_avg:98.60ms
step:1397/1770 train_time:136759ms step_avg:98.60ms
step:1398/1770 train_time:136862ms step_avg:98.60ms
step:1399/1770 train_time:136964ms step_avg:98.61ms
step:1400/1770 train_time:137067ms step_avg:98.61ms
step:1401/1770 train_time:137169ms step_avg:98.61ms
step:1402/1770 train_time:137272ms step_avg:98.61ms
step:1403/1770 train_time:137373ms step_avg:98.62ms
step:1404/1770 train_time:137476ms step_avg:98.62ms
step:1405/1770 train_time:137578ms step_avg:98.62ms
step:1406/1770 train_time:137680ms step_avg:98.62ms
step:1407/1770 train_time:137782ms step_avg:98.63ms
step:1408/1770 train_time:137884ms step_avg:98.63ms
step:1409/1770 train_time:137986ms step_avg:98.63ms
step:1410/1770 train_time:138089ms step_avg:98.64ms
step:1411/1770 train_time:138192ms step_avg:98.64ms
step:1412/1770 train_time:138295ms step_avg:98.64ms
step:1413/1770 train_time:138396ms step_avg:98.64ms
step:1414/1770 train_time:138499ms step_avg:98.65ms
step:1415/1770 train_time:138601ms step_avg:98.65ms
step:1416/1770 train_time:138704ms step_avg:98.65ms
step:1417/1770 train_time:138806ms step_avg:98.65ms
step:1418/1770 train_time:138908ms step_avg:98.66ms
step:1419/1770 train_time:139011ms step_avg:98.66ms
step:1420/1770 train_time:139114ms step_avg:98.66ms
step:1421/1770 train_time:139215ms step_avg:98.66ms
step:1422/1770 train_time:139317ms step_avg:98.67ms
step:1423/1770 train_time:139420ms step_avg:98.67ms
step:1424/1770 train_time:139522ms step_avg:98.67ms
step:1425/1770 train_time:139624ms step_avg:98.67ms
step:1426/1770 train_time:139727ms step_avg:98.68ms
step:1427/1770 train_time:139829ms step_avg:98.68ms
step:1428/1770 train_time:139932ms step_avg:98.68ms
step:1429/1770 train_time:140035ms step_avg:98.69ms
step:1430/1770 train_time:140136ms step_avg:98.69ms
step:1431/1770 train_time:140239ms step_avg:98.69ms
step:1432/1770 train_time:140341ms step_avg:98.69ms
step:1433/1770 train_time:140443ms step_avg:98.69ms
step:1434/1770 train_time:140544ms step_avg:98.70ms
step:1435/1770 train_time:140647ms step_avg:98.70ms
step:1436/1770 train_time:140750ms step_avg:98.70ms
step:1437/1770 train_time:140853ms step_avg:98.71ms
step:1438/1770 train_time:140954ms step_avg:98.71ms
step:1439/1770 train_time:141056ms step_avg:98.71ms
step:1440/1770 train_time:141158ms step_avg:98.71ms
step:1441/1770 train_time:141264ms step_avg:98.72ms
step:1442/1770 train_time:141365ms step_avg:98.72ms
step:1443/1770 train_time:141468ms step_avg:98.72ms
step:1444/1770 train_time:141570ms step_avg:98.72ms
step:1445/1770 train_time:141673ms step_avg:98.73ms
step:1446/1770 train_time:141776ms step_avg:98.73ms
step:1447/1770 train_time:141880ms step_avg:98.73ms
step:1448/1770 train_time:141986ms step_avg:98.74ms
step:1449/1770 train_time:142087ms step_avg:98.74ms
step:1450/1770 train_time:142191ms step_avg:98.74ms
step:1451/1770 train_time:142296ms step_avg:98.75ms
step:1452/1770 train_time:142399ms step_avg:98.75ms
step:1453/1770 train_time:142502ms step_avg:98.75ms
step:1454/1770 train_time:142605ms step_avg:98.76ms
step:1455/1770 train_time:142709ms step_avg:98.76ms
step:1456/1770 train_time:142813ms step_avg:98.76ms
step:1457/1770 train_time:142917ms step_avg:98.77ms
step:1458/1770 train_time:143021ms step_avg:98.77ms
step:1459/1770 train_time:143124ms step_avg:98.77ms
step:1460/1770 train_time:143228ms step_avg:98.78ms
step:1461/1770 train_time:143333ms step_avg:98.78ms
step:1462/1770 train_time:143436ms step_avg:98.79ms
step:1463/1770 train_time:143540ms step_avg:98.79ms
step:1464/1770 train_time:143644ms step_avg:98.79ms
step:1465/1770 train_time:143747ms step_avg:98.80ms
step:1466/1770 train_time:143852ms step_avg:98.80ms
step:1467/1770 train_time:143955ms step_avg:98.80ms
step:1468/1770 train_time:144058ms step_avg:98.81ms
step:1469/1770 train_time:144161ms step_avg:98.81ms
step:1470/1770 train_time:144264ms step_avg:98.81ms
step:1471/1770 train_time:144368ms step_avg:98.81ms
step:1472/1770 train_time:144471ms step_avg:98.82ms
step:1473/1770 train_time:144576ms step_avg:98.82ms
step:1474/1770 train_time:144680ms step_avg:98.83ms
step:1475/1770 train_time:144783ms step_avg:98.83ms
step:1476/1770 train_time:144886ms step_avg:98.83ms
step:1477/1770 train_time:144991ms step_avg:98.83ms
step:1478/1770 train_time:145094ms step_avg:98.84ms
step:1479/1770 train_time:145197ms step_avg:98.84ms
step:1480/1770 train_time:145302ms step_avg:98.85ms
step:1481/1770 train_time:145410ms step_avg:98.85ms
step:1482/1770 train_time:145513ms step_avg:98.85ms
step:1483/1770 train_time:145616ms step_avg:98.86ms
step:1484/1770 train_time:145720ms step_avg:98.86ms
step:1485/1770 train_time:145823ms step_avg:98.86ms
step:1486/1770 train_time:145925ms step_avg:98.87ms
step:1487/1770 train_time:146029ms step_avg:98.87ms
step:1488/1770 train_time:146134ms step_avg:98.87ms
step:1489/1770 train_time:146238ms step_avg:98.88ms
step:1490/1770 train_time:146341ms step_avg:98.88ms
step:1491/1770 train_time:146443ms step_avg:98.88ms
step:1492/1770 train_time:146547ms step_avg:98.88ms
step:1493/1770 train_time:146653ms step_avg:98.89ms
step:1494/1770 train_time:146759ms step_avg:98.89ms
step:1495/1770 train_time:146862ms step_avg:98.90ms
step:1496/1770 train_time:146965ms step_avg:98.90ms
step:1497/1770 train_time:147068ms step_avg:98.90ms
step:1498/1770 train_time:147172ms step_avg:98.91ms
step:1499/1770 train_time:147274ms step_avg:98.91ms
step:1500/1770 train_time:147377ms step_avg:98.91ms
step:1500/1770 val_loss:3.3427 train_time:147478ms step_avg:98.98ms
step:1501/1770 train_time:147500ms step_avg:98.93ms
step:1502/1770 train_time:147593ms step_avg:98.92ms
step:1503/1770 train_time:147696ms step_avg:98.93ms
step:1504/1770 train_time:147799ms step_avg:98.93ms
step:1505/1770 train_time:147904ms step_avg:98.93ms
step:1506/1770 train_time:148008ms step_avg:98.94ms
step:1507/1770 train_time:148111ms step_avg:98.94ms
step:1508/1770 train_time:148216ms step_avg:98.94ms
step:1509/1770 train_time:148319ms step_avg:98.95ms
step:1510/1770 train_time:148421ms step_avg:98.95ms
step:1511/1770 train_time:148526ms step_avg:98.95ms
step:1512/1770 train_time:148630ms step_avg:98.95ms
step:1513/1770 train_time:148736ms step_avg:98.96ms
step:1514/1770 train_time:148838ms step_avg:98.96ms
step:1515/1770 train_time:148942ms step_avg:98.96ms
step:1516/1770 train_time:149046ms step_avg:98.97ms
step:1517/1770 train_time:149149ms step_avg:98.97ms
step:1518/1770 train_time:149254ms step_avg:98.97ms
step:1519/1770 train_time:149356ms step_avg:98.98ms
step:1520/1770 train_time:149461ms step_avg:98.98ms
step:1521/1770 train_time:149564ms step_avg:98.98ms
step:1522/1770 train_time:149668ms step_avg:98.99ms
step:1523/1770 train_time:149771ms step_avg:98.99ms
step:1524/1770 train_time:149874ms step_avg:98.99ms
step:1525/1770 train_time:149976ms step_avg:98.99ms
step:1526/1770 train_time:150080ms step_avg:99.00ms
step:1527/1770 train_time:150183ms step_avg:99.00ms
step:1528/1770 train_time:150288ms step_avg:99.00ms
step:1529/1770 train_time:150391ms step_avg:99.01ms
step:1530/1770 train_time:150494ms step_avg:99.01ms
step:1531/1770 train_time:150597ms step_avg:99.01ms
step:1532/1770 train_time:150700ms step_avg:99.01ms
step:1533/1770 train_time:150804ms step_avg:99.02ms
step:1534/1770 train_time:150908ms step_avg:99.02ms
step:1535/1770 train_time:151015ms step_avg:99.03ms
step:1536/1770 train_time:151115ms step_avg:99.03ms
step:1537/1770 train_time:151218ms step_avg:99.03ms
step:1538/1770 train_time:151322ms step_avg:99.03ms
step:1539/1770 train_time:151426ms step_avg:99.04ms
step:1540/1770 train_time:151532ms step_avg:99.04ms
step:1541/1770 train_time:151636ms step_avg:99.04ms
step:1542/1770 train_time:151740ms step_avg:99.05ms
step:1543/1770 train_time:151842ms step_avg:99.05ms
step:1544/1770 train_time:151948ms step_avg:99.05ms
step:1545/1770 train_time:152051ms step_avg:99.06ms
step:1546/1770 train_time:152155ms step_avg:99.06ms
step:1547/1770 train_time:152258ms step_avg:99.06ms
step:1548/1770 train_time:152361ms step_avg:99.06ms
step:1549/1770 train_time:152464ms step_avg:99.07ms
step:1550/1770 train_time:152568ms step_avg:99.07ms
step:1551/1770 train_time:152672ms step_avg:99.07ms
step:1552/1770 train_time:152777ms step_avg:99.08ms
step:1553/1770 train_time:152881ms step_avg:99.08ms
step:1554/1770 train_time:152983ms step_avg:99.08ms
step:1555/1770 train_time:153087ms step_avg:99.09ms
step:1556/1770 train_time:153189ms step_avg:99.09ms
step:1557/1770 train_time:153293ms step_avg:99.09ms
step:1558/1770 train_time:153396ms step_avg:99.09ms
step:1559/1770 train_time:153500ms step_avg:99.10ms
step:1560/1770 train_time:153603ms step_avg:99.10ms
step:1561/1770 train_time:153708ms step_avg:99.10ms
step:1562/1770 train_time:153811ms step_avg:99.10ms
step:1563/1770 train_time:153914ms step_avg:99.11ms
step:1564/1770 train_time:154016ms step_avg:99.11ms
step:1565/1770 train_time:154122ms step_avg:99.11ms
step:1566/1770 train_time:154226ms step_avg:99.12ms
step:1567/1770 train_time:154331ms step_avg:99.12ms
step:1568/1770 train_time:154432ms step_avg:99.12ms
step:1569/1770 train_time:154539ms step_avg:99.13ms
step:1570/1770 train_time:154642ms step_avg:99.13ms
step:1571/1770 train_time:154746ms step_avg:99.13ms
step:1572/1770 train_time:154851ms step_avg:99.14ms
step:1573/1770 train_time:154956ms step_avg:99.14ms
step:1574/1770 train_time:155059ms step_avg:99.14ms
step:1575/1770 train_time:155162ms step_avg:99.14ms
step:1576/1770 train_time:155265ms step_avg:99.15ms
step:1577/1770 train_time:155369ms step_avg:99.15ms
step:1578/1770 train_time:155475ms step_avg:99.16ms
step:1579/1770 train_time:155579ms step_avg:99.16ms
step:1580/1770 train_time:155681ms step_avg:99.16ms
step:1581/1770 train_time:155787ms step_avg:99.16ms
step:1582/1770 train_time:155892ms step_avg:99.17ms
step:1583/1770 train_time:155996ms step_avg:99.17ms
step:1584/1770 train_time:156100ms step_avg:99.17ms
step:1585/1770 train_time:156204ms step_avg:99.18ms
step:1586/1770 train_time:156315ms step_avg:99.18ms
step:1587/1770 train_time:156416ms step_avg:99.19ms
step:1588/1770 train_time:156520ms step_avg:99.19ms
step:1589/1770 train_time:156624ms step_avg:99.19ms
step:1590/1770 train_time:156727ms step_avg:99.19ms
step:1591/1770 train_time:156829ms step_avg:99.20ms
step:1592/1770 train_time:156933ms step_avg:99.20ms
step:1593/1770 train_time:157037ms step_avg:99.20ms
step:1594/1770 train_time:157140ms step_avg:99.20ms
step:1595/1770 train_time:157243ms step_avg:99.21ms
step:1596/1770 train_time:157348ms step_avg:99.21ms
step:1597/1770 train_time:157451ms step_avg:99.21ms
step:1598/1770 train_time:157554ms step_avg:99.22ms
step:1599/1770 train_time:157658ms step_avg:99.22ms
step:1600/1770 train_time:157764ms step_avg:99.22ms
step:1601/1770 train_time:157868ms step_avg:99.23ms
step:1602/1770 train_time:157973ms step_avg:99.23ms
step:1603/1770 train_time:158076ms step_avg:99.23ms
step:1604/1770 train_time:158178ms step_avg:99.23ms
step:1605/1770 train_time:158282ms step_avg:99.24ms
step:1606/1770 train_time:158385ms step_avg:99.24ms
step:1607/1770 train_time:158493ms step_avg:99.24ms
step:1608/1770 train_time:158596ms step_avg:99.25ms
step:1609/1770 train_time:158699ms step_avg:99.25ms
step:1610/1770 train_time:158803ms step_avg:99.25ms
step:1611/1770 train_time:158909ms step_avg:99.26ms
step:1612/1770 train_time:159014ms step_avg:99.26ms
step:1613/1770 train_time:159118ms step_avg:99.26ms
step:1614/1770 train_time:159221ms step_avg:99.27ms
step:1615/1770 train_time:159324ms step_avg:99.27ms
step:1616/1770 train_time:159428ms step_avg:99.27ms
step:1617/1770 train_time:159534ms step_avg:99.27ms
step:1618/1770 train_time:159639ms step_avg:99.28ms
step:1619/1770 train_time:159742ms step_avg:99.28ms
step:1620/1770 train_time:159846ms step_avg:99.28ms
step:1621/1770 train_time:159949ms step_avg:99.29ms
step:1622/1770 train_time:160054ms step_avg:99.29ms
step:1623/1770 train_time:160160ms step_avg:99.29ms
step:1624/1770 train_time:160263ms step_avg:99.30ms
step:1625/1770 train_time:160365ms step_avg:99.30ms
step:1625/1770 val_loss:3.3085 train_time:160467ms step_avg:99.36ms
step:1626/1770 train_time:160489ms step_avg:99.31ms
step:1627/1770 train_time:160580ms step_avg:99.31ms
step:1628/1770 train_time:160684ms step_avg:99.31ms
step:1629/1770 train_time:160787ms step_avg:99.31ms
step:1630/1770 train_time:160890ms step_avg:99.31ms
step:1631/1770 train_time:160993ms step_avg:99.32ms
step:1632/1770 train_time:161097ms step_avg:99.32ms
step:1633/1770 train_time:161200ms step_avg:99.32ms
step:1634/1770 train_time:161302ms step_avg:99.32ms
step:1635/1770 train_time:161405ms step_avg:99.33ms
step:1636/1770 train_time:161510ms step_avg:99.33ms
step:1637/1770 train_time:161613ms step_avg:99.33ms
step:1638/1770 train_time:161717ms step_avg:99.33ms
step:1639/1770 train_time:161821ms step_avg:99.34ms
step:1640/1770 train_time:161925ms step_avg:99.34ms
step:1641/1770 train_time:162029ms step_avg:99.34ms
step:1642/1770 train_time:162131ms step_avg:99.35ms
step:1643/1770 train_time:162234ms step_avg:99.35ms
step:1644/1770 train_time:162340ms step_avg:99.35ms
step:1645/1770 train_time:162447ms step_avg:99.36ms
step:1646/1770 train_time:162548ms step_avg:99.36ms
step:1647/1770 train_time:162652ms step_avg:99.36ms
step:1648/1770 train_time:162754ms step_avg:99.36ms
step:1649/1770 train_time:162858ms step_avg:99.36ms
step:1650/1770 train_time:162962ms step_avg:99.37ms
step:1651/1770 train_time:163065ms step_avg:99.37ms
step:1652/1770 train_time:163168ms step_avg:99.37ms
step:1653/1770 train_time:163272ms step_avg:99.37ms
step:1654/1770 train_time:163379ms step_avg:99.38ms
step:1655/1770 train_time:163485ms step_avg:99.38ms
step:1656/1770 train_time:163588ms step_avg:99.39ms
step:1657/1770 train_time:163693ms step_avg:99.39ms
step:1658/1770 train_time:163797ms step_avg:99.39ms
step:1659/1770 train_time:163902ms step_avg:99.39ms
step:1660/1770 train_time:164005ms step_avg:99.40ms
step:1661/1770 train_time:164110ms step_avg:99.40ms
step:1662/1770 train_time:164214ms step_avg:99.40ms
step:1663/1770 train_time:164317ms step_avg:99.41ms
step:1664/1770 train_time:164420ms step_avg:99.41ms
step:1665/1770 train_time:164523ms step_avg:99.41ms
step:1666/1770 train_time:164627ms step_avg:99.41ms
step:1667/1770 train_time:164730ms step_avg:99.41ms
step:1668/1770 train_time:164832ms step_avg:99.42ms
step:1669/1770 train_time:164935ms step_avg:99.42ms
step:1670/1770 train_time:165038ms step_avg:99.42ms
step:1671/1770 train_time:165140ms step_avg:99.42ms
step:1672/1770 train_time:165246ms step_avg:99.43ms
step:1673/1770 train_time:165351ms step_avg:99.43ms
step:1674/1770 train_time:165454ms step_avg:99.43ms
step:1675/1770 train_time:165558ms step_avg:99.43ms
step:1676/1770 train_time:165663ms step_avg:99.44ms
step:1677/1770 train_time:165771ms step_avg:99.44ms
step:1678/1770 train_time:165874ms step_avg:99.44ms
step:1679/1770 train_time:165977ms step_avg:99.45ms
step:1680/1770 train_time:166080ms step_avg:99.45ms
step:1681/1770 train_time:166185ms step_avg:99.45ms
step:1682/1770 train_time:166291ms step_avg:99.46ms
step:1683/1770 train_time:166394ms step_avg:99.46ms
step:1684/1770 train_time:166498ms step_avg:99.46ms
step:1685/1770 train_time:166602ms step_avg:99.46ms
step:1686/1770 train_time:166706ms step_avg:99.47ms
step:1687/1770 train_time:166812ms step_avg:99.47ms
step:1688/1770 train_time:166916ms step_avg:99.47ms
step:1689/1770 train_time:167020ms step_avg:99.48ms
step:1690/1770 train_time:167122ms step_avg:99.48ms
step:1691/1770 train_time:167225ms step_avg:99.48ms
step:1692/1770 train_time:167329ms step_avg:99.48ms
step:1693/1770 train_time:167434ms step_avg:99.49ms
step:1694/1770 train_time:167538ms step_avg:99.49ms
step:1695/1770 train_time:167641ms step_avg:99.49ms
step:1696/1770 train_time:167746ms step_avg:99.49ms
step:1697/1770 train_time:167852ms step_avg:99.50ms
step:1698/1770 train_time:167956ms step_avg:99.50ms
step:1699/1770 train_time:168061ms step_avg:99.50ms
step:1700/1770 train_time:168163ms step_avg:99.50ms
step:1701/1770 train_time:168266ms step_avg:99.51ms
step:1702/1770 train_time:168370ms step_avg:99.51ms
step:1703/1770 train_time:168473ms step_avg:99.51ms
step:1704/1770 train_time:168577ms step_avg:99.51ms
step:1705/1770 train_time:168681ms step_avg:99.52ms
step:1706/1770 train_time:168784ms step_avg:99.52ms
step:1707/1770 train_time:168888ms step_avg:99.52ms
step:1708/1770 train_time:168992ms step_avg:99.52ms
step:1709/1770 train_time:169097ms step_avg:99.53ms
step:1710/1770 train_time:169205ms step_avg:99.53ms
step:1711/1770 train_time:169311ms step_avg:99.54ms
step:1712/1770 train_time:169415ms step_avg:99.54ms
step:1713/1770 train_time:169519ms step_avg:99.54ms
step:1714/1770 train_time:169622ms step_avg:99.54ms
step:1715/1770 train_time:169726ms step_avg:99.55ms
step:1716/1770 train_time:169830ms step_avg:99.55ms
step:1717/1770 train_time:169933ms step_avg:99.55ms
step:1718/1770 train_time:170042ms step_avg:99.56ms
step:1719/1770 train_time:170144ms step_avg:99.56ms
step:1720/1770 train_time:170248ms step_avg:99.56ms
step:1721/1770 train_time:170352ms step_avg:99.56ms
step:1722/1770 train_time:170459ms step_avg:99.57ms
step:1723/1770 train_time:170564ms step_avg:99.57ms
step:1724/1770 train_time:170670ms step_avg:99.57ms
step:1725/1770 train_time:170776ms step_avg:99.58ms
step:1726/1770 train_time:170882ms step_avg:99.58ms
step:1727/1770 train_time:170986ms step_avg:99.58ms
step:1728/1770 train_time:171091ms step_avg:99.59ms
step:1729/1770 train_time:171195ms step_avg:99.59ms
step:1730/1770 train_time:171301ms step_avg:99.59ms
step:1731/1770 train_time:171406ms step_avg:99.60ms
step:1732/1770 train_time:171510ms step_avg:99.60ms
step:1733/1770 train_time:171616ms step_avg:99.60ms
step:1734/1770 train_time:171719ms step_avg:99.60ms
step:1735/1770 train_time:171824ms step_avg:99.61ms
step:1736/1770 train_time:171928ms step_avg:99.61ms
step:1737/1770 train_time:172033ms step_avg:99.61ms
step:1738/1770 train_time:172137ms step_avg:99.62ms
step:1739/1770 train_time:172241ms step_avg:99.62ms
step:1740/1770 train_time:172346ms step_avg:99.62ms
step:1741/1770 train_time:172453ms step_avg:99.63ms
step:1742/1770 train_time:172560ms step_avg:99.63ms
step:1743/1770 train_time:172665ms step_avg:99.63ms
step:1744/1770 train_time:172770ms step_avg:99.64ms
step:1745/1770 train_time:172874ms step_avg:99.64ms
step:1746/1770 train_time:172981ms step_avg:99.64ms
step:1747/1770 train_time:173083ms step_avg:99.65ms
step:1748/1770 train_time:173189ms step_avg:99.65ms
step:1749/1770 train_time:173295ms step_avg:99.65ms
step:1750/1770 train_time:173399ms step_avg:99.65ms
step:1750/1770 val_loss:3.2817 train_time:173501ms step_avg:99.71ms
step:1751/1770 train_time:173524ms step_avg:99.67ms
step:1752/1770 train_time:173614ms step_avg:99.66ms
step:1753/1770 train_time:173719ms step_avg:99.67ms
step:1754/1770 train_time:173824ms step_avg:99.67ms
step:1755/1770 train_time:173928ms step_avg:99.67ms
step:1756/1770 train_time:174033ms step_avg:99.68ms
step:1757/1770 train_time:174137ms step_avg:99.68ms
step:1758/1770 train_time:174240ms step_avg:99.68ms
step:1759/1770 train_time:174345ms step_avg:99.68ms
step:1760/1770 train_time:174450ms step_avg:99.69ms
step:1761/1770 train_time:174556ms step_avg:99.69ms
step:1762/1770 train_time:174665ms step_avg:99.69ms
step:1763/1770 train_time:174768ms step_avg:99.70ms
step:1764/1770 train_time:174873ms step_avg:99.70ms
step:1765/1770 train_time:174978ms step_avg:99.70ms
step:1766/1770 train_time:175086ms step_avg:99.71ms
step:1767/1770 train_time:175189ms step_avg:99.71ms
step:1768/1770 train_time:175293ms step_avg:99.71ms
step:1769/1770 train_time:175396ms step_avg:99.71ms
step:1770/1770 train_time:175499ms step_avg:99.72ms
step:1770/1770 val_loss:3.2787 train_time:175604ms step_avg:99.77ms
peak memory allocated: 28840 MiB reserved: 32272 MiB
