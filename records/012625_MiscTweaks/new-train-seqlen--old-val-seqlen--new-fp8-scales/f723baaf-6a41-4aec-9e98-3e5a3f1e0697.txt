import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 16:29:15 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23349ms step_avg:nanms
step:2/1770 train_time:23780ms step_avg:nanms
step:3/1770 train_time:23876ms step_avg:nanms
step:4/1770 train_time:23969ms step_avg:nanms
step:5/1770 train_time:24062ms step_avg:nanms
step:6/1770 train_time:24156ms step_avg:nanms
step:7/1770 train_time:24249ms step_avg:nanms
step:8/1770 train_time:24343ms step_avg:nanms
step:9/1770 train_time:24437ms step_avg:nanms
step:10/1770 train_time:24530ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:281ms step_avg:93.64ms
step:14/1770 train_time:375ms step_avg:93.76ms
step:15/1770 train_time:469ms step_avg:93.82ms
step:16/1770 train_time:563ms step_avg:93.91ms
step:17/1770 train_time:658ms step_avg:93.96ms
step:18/1770 train_time:752ms step_avg:94.00ms
step:19/1770 train_time:845ms step_avg:93.94ms
step:20/1770 train_time:939ms step_avg:93.93ms
step:21/1770 train_time:1033ms step_avg:93.94ms
step:22/1770 train_time:1128ms step_avg:93.99ms
step:23/1770 train_time:1222ms step_avg:93.97ms
step:24/1770 train_time:1315ms step_avg:93.96ms
step:25/1770 train_time:1409ms step_avg:93.94ms
step:26/1770 train_time:1503ms step_avg:93.95ms
step:27/1770 train_time:1598ms step_avg:93.98ms
step:28/1770 train_time:1692ms step_avg:94.01ms
step:29/1770 train_time:1786ms step_avg:94.02ms
step:30/1770 train_time:1880ms step_avg:94.02ms
step:31/1770 train_time:1974ms step_avg:94.01ms
step:32/1770 train_time:2069ms step_avg:94.04ms
step:33/1770 train_time:2162ms step_avg:94.01ms
step:34/1770 train_time:2256ms step_avg:94.01ms
step:35/1770 train_time:2350ms step_avg:94.01ms
step:36/1770 train_time:2445ms step_avg:94.06ms
step:37/1770 train_time:2539ms step_avg:94.04ms
step:38/1770 train_time:2634ms step_avg:94.06ms
step:39/1770 train_time:2728ms step_avg:94.06ms
step:40/1770 train_time:2822ms step_avg:94.06ms
step:41/1770 train_time:2915ms step_avg:94.04ms
step:42/1770 train_time:3010ms step_avg:94.05ms
step:43/1770 train_time:3104ms step_avg:94.06ms
step:44/1770 train_time:3198ms step_avg:94.06ms
step:45/1770 train_time:3292ms step_avg:94.07ms
step:46/1770 train_time:3387ms step_avg:94.07ms
step:47/1770 train_time:3481ms step_avg:94.07ms
step:48/1770 train_time:3575ms step_avg:94.09ms
step:49/1770 train_time:3669ms step_avg:94.08ms
step:50/1770 train_time:3763ms step_avg:94.08ms
step:51/1770 train_time:3857ms step_avg:94.08ms
step:52/1770 train_time:3952ms step_avg:94.09ms
step:53/1770 train_time:4046ms step_avg:94.08ms
step:54/1770 train_time:4140ms step_avg:94.09ms
step:55/1770 train_time:4234ms step_avg:94.10ms
step:56/1770 train_time:4328ms step_avg:94.09ms
step:57/1770 train_time:4422ms step_avg:94.08ms
step:58/1770 train_time:4515ms step_avg:94.06ms
step:59/1770 train_time:4609ms step_avg:94.06ms
step:60/1770 train_time:4703ms step_avg:94.06ms
step:61/1770 train_time:4797ms step_avg:94.05ms
step:62/1770 train_time:4891ms step_avg:94.05ms
step:63/1770 train_time:4985ms step_avg:94.05ms
step:64/1770 train_time:5079ms step_avg:94.05ms
step:65/1770 train_time:5172ms step_avg:94.04ms
step:66/1770 train_time:5267ms step_avg:94.04ms
step:67/1770 train_time:5361ms step_avg:94.05ms
step:68/1770 train_time:5455ms step_avg:94.05ms
step:69/1770 train_time:5548ms step_avg:94.04ms
step:70/1770 train_time:5643ms step_avg:94.04ms
step:71/1770 train_time:5737ms step_avg:94.05ms
step:72/1770 train_time:5831ms step_avg:94.05ms
step:73/1770 train_time:5925ms step_avg:94.05ms
step:74/1770 train_time:6019ms step_avg:94.04ms
step:75/1770 train_time:6113ms step_avg:94.04ms
step:76/1770 train_time:6207ms step_avg:94.05ms
step:77/1770 train_time:6301ms step_avg:94.04ms
step:78/1770 train_time:6395ms step_avg:94.04ms
step:79/1770 train_time:6489ms step_avg:94.04ms
step:80/1770 train_time:6582ms step_avg:94.04ms
step:81/1770 train_time:6676ms step_avg:94.02ms
step:82/1770 train_time:6769ms step_avg:94.02ms
step:83/1770 train_time:6863ms step_avg:94.01ms
step:84/1770 train_time:6957ms step_avg:94.01ms
step:85/1770 train_time:7051ms step_avg:94.01ms
step:86/1770 train_time:7145ms step_avg:94.02ms
step:87/1770 train_time:7239ms step_avg:94.01ms
step:88/1770 train_time:7333ms step_avg:94.01ms
step:89/1770 train_time:7426ms step_avg:94.00ms
step:90/1770 train_time:7521ms step_avg:94.01ms
step:91/1770 train_time:7615ms step_avg:94.01ms
step:92/1770 train_time:7709ms step_avg:94.01ms
step:93/1770 train_time:7802ms step_avg:94.00ms
step:94/1770 train_time:7897ms step_avg:94.01ms
step:95/1770 train_time:7991ms step_avg:94.01ms
step:96/1770 train_time:8085ms step_avg:94.01ms
step:97/1770 train_time:8179ms step_avg:94.01ms
step:98/1770 train_time:8273ms step_avg:94.01ms
step:99/1770 train_time:8367ms step_avg:94.02ms
step:100/1770 train_time:8462ms step_avg:94.02ms
step:101/1770 train_time:8556ms step_avg:94.02ms
step:102/1770 train_time:8650ms step_avg:94.02ms
step:103/1770 train_time:8744ms step_avg:94.02ms
step:104/1770 train_time:8838ms step_avg:94.02ms
step:105/1770 train_time:8933ms step_avg:94.03ms
step:106/1770 train_time:9026ms step_avg:94.02ms
step:107/1770 train_time:9120ms step_avg:94.02ms
step:108/1770 train_time:9215ms step_avg:94.03ms
step:109/1770 train_time:9309ms step_avg:94.03ms
step:110/1770 train_time:9404ms step_avg:94.04ms
step:111/1770 train_time:9497ms step_avg:94.03ms
step:112/1770 train_time:9592ms step_avg:94.04ms
step:113/1770 train_time:9686ms step_avg:94.04ms
step:114/1770 train_time:9780ms step_avg:94.04ms
step:115/1770 train_time:9873ms step_avg:94.03ms
step:116/1770 train_time:9967ms step_avg:94.03ms
step:117/1770 train_time:10061ms step_avg:94.03ms
step:118/1770 train_time:10155ms step_avg:94.03ms
step:119/1770 train_time:10249ms step_avg:94.03ms
step:120/1770 train_time:10343ms step_avg:94.03ms
step:121/1770 train_time:10437ms step_avg:94.03ms
step:122/1770 train_time:10531ms step_avg:94.03ms
step:123/1770 train_time:10625ms step_avg:94.03ms
step:124/1770 train_time:10719ms step_avg:94.02ms
step:125/1770 train_time:10813ms step_avg:94.02ms
step:125/1770 val_loss:4.6508 train_time:10905ms step_avg:94.82ms
step:126/1770 train_time:10927ms step_avg:94.20ms
step:127/1770 train_time:11013ms step_avg:94.13ms
step:128/1770 train_time:11110ms step_avg:94.15ms
step:129/1770 train_time:11205ms step_avg:94.16ms
step:130/1770 train_time:11299ms step_avg:94.15ms
step:131/1770 train_time:11393ms step_avg:94.15ms
step:132/1770 train_time:11487ms step_avg:94.15ms
step:133/1770 train_time:11580ms step_avg:94.15ms
step:134/1770 train_time:11675ms step_avg:94.15ms
step:135/1770 train_time:11769ms step_avg:94.15ms
step:136/1770 train_time:11863ms step_avg:94.15ms
step:137/1770 train_time:11957ms step_avg:94.15ms
step:138/1770 train_time:12052ms step_avg:94.16ms
step:139/1770 train_time:12148ms step_avg:94.17ms
step:140/1770 train_time:12315ms step_avg:94.73ms
step:141/1770 train_time:12364ms step_avg:94.38ms
step:142/1770 train_time:12458ms step_avg:94.38ms
step:143/1770 train_time:12552ms step_avg:94.37ms
step:144/1770 train_time:12646ms step_avg:94.37ms
step:145/1770 train_time:12740ms step_avg:94.37ms
step:146/1770 train_time:12835ms step_avg:94.37ms
step:147/1770 train_time:12929ms step_avg:94.37ms
step:148/1770 train_time:13023ms step_avg:94.37ms
step:149/1770 train_time:13117ms step_avg:94.37ms
step:150/1770 train_time:13213ms step_avg:94.38ms
step:151/1770 train_time:13309ms step_avg:94.39ms
step:152/1770 train_time:13404ms step_avg:94.39ms
step:153/1770 train_time:13498ms step_avg:94.39ms
step:154/1770 train_time:13593ms step_avg:94.40ms
step:155/1770 train_time:13688ms step_avg:94.40ms
step:156/1770 train_time:13782ms step_avg:94.40ms
step:157/1770 train_time:13876ms step_avg:94.39ms
step:158/1770 train_time:13971ms step_avg:94.40ms
step:159/1770 train_time:14065ms step_avg:94.40ms
step:160/1770 train_time:14159ms step_avg:94.40ms
step:161/1770 train_time:14254ms step_avg:94.40ms
step:162/1770 train_time:14350ms step_avg:94.41ms
step:163/1770 train_time:14444ms step_avg:94.41ms
step:164/1770 train_time:14539ms step_avg:94.41ms
step:165/1770 train_time:14634ms step_avg:94.41ms
step:166/1770 train_time:14728ms step_avg:94.41ms
step:167/1770 train_time:14822ms step_avg:94.41ms
step:168/1770 train_time:14917ms step_avg:94.41ms
step:169/1770 train_time:15011ms step_avg:94.41ms
step:170/1770 train_time:15106ms step_avg:94.41ms
step:171/1770 train_time:15200ms step_avg:94.41ms
step:172/1770 train_time:15295ms step_avg:94.41ms
step:173/1770 train_time:15390ms step_avg:94.42ms
step:174/1770 train_time:15485ms step_avg:94.42ms
step:175/1770 train_time:15579ms step_avg:94.42ms
step:176/1770 train_time:15674ms step_avg:94.42ms
step:177/1770 train_time:15769ms step_avg:94.43ms
step:178/1770 train_time:15863ms step_avg:94.42ms
step:179/1770 train_time:15958ms step_avg:94.42ms
step:180/1770 train_time:16053ms step_avg:94.43ms
step:181/1770 train_time:16147ms step_avg:94.43ms
step:182/1770 train_time:16242ms step_avg:94.43ms
step:183/1770 train_time:16336ms step_avg:94.43ms
step:184/1770 train_time:16431ms step_avg:94.43ms
step:185/1770 train_time:16525ms step_avg:94.43ms
step:186/1770 train_time:16619ms step_avg:94.43ms
step:187/1770 train_time:16714ms step_avg:94.43ms
step:188/1770 train_time:16809ms step_avg:94.43ms
step:189/1770 train_time:16904ms step_avg:94.43ms
step:190/1770 train_time:16998ms step_avg:94.43ms
step:191/1770 train_time:17093ms step_avg:94.44ms
step:192/1770 train_time:17188ms step_avg:94.44ms
step:193/1770 train_time:17283ms step_avg:94.44ms
step:194/1770 train_time:17378ms step_avg:94.45ms
step:195/1770 train_time:17473ms step_avg:94.45ms
step:196/1770 train_time:17567ms step_avg:94.45ms
step:197/1770 train_time:17662ms step_avg:94.45ms
step:198/1770 train_time:17757ms step_avg:94.45ms
step:199/1770 train_time:17851ms step_avg:94.45ms
step:200/1770 train_time:17946ms step_avg:94.45ms
step:201/1770 train_time:18040ms step_avg:94.45ms
step:202/1770 train_time:18135ms step_avg:94.45ms
step:203/1770 train_time:18229ms step_avg:94.45ms
step:204/1770 train_time:18324ms step_avg:94.45ms
step:205/1770 train_time:18419ms step_avg:94.46ms
step:206/1770 train_time:18514ms step_avg:94.46ms
step:207/1770 train_time:18609ms step_avg:94.46ms
step:208/1770 train_time:18703ms step_avg:94.46ms
step:209/1770 train_time:18798ms step_avg:94.46ms
step:210/1770 train_time:18893ms step_avg:94.47ms
step:211/1770 train_time:18987ms step_avg:94.46ms
step:212/1770 train_time:19082ms step_avg:94.47ms
step:213/1770 train_time:19177ms step_avg:94.47ms
step:214/1770 train_time:19273ms step_avg:94.48ms
step:215/1770 train_time:19368ms step_avg:94.48ms
step:216/1770 train_time:19463ms step_avg:94.48ms
step:217/1770 train_time:19558ms step_avg:94.48ms
step:218/1770 train_time:19652ms step_avg:94.48ms
step:219/1770 train_time:19746ms step_avg:94.48ms
step:220/1770 train_time:19840ms step_avg:94.48ms
step:221/1770 train_time:19935ms step_avg:94.48ms
step:222/1770 train_time:20030ms step_avg:94.48ms
step:223/1770 train_time:20124ms step_avg:94.48ms
step:224/1770 train_time:20218ms step_avg:94.48ms
step:225/1770 train_time:20313ms step_avg:94.48ms
step:226/1770 train_time:20408ms step_avg:94.48ms
step:227/1770 train_time:20502ms step_avg:94.48ms
step:228/1770 train_time:20596ms step_avg:94.48ms
step:229/1770 train_time:20691ms step_avg:94.48ms
step:230/1770 train_time:20785ms step_avg:94.48ms
step:231/1770 train_time:20880ms step_avg:94.48ms
step:232/1770 train_time:20975ms step_avg:94.48ms
step:233/1770 train_time:21070ms step_avg:94.48ms
step:234/1770 train_time:21164ms step_avg:94.48ms
step:235/1770 train_time:21258ms step_avg:94.48ms
step:236/1770 train_time:21353ms step_avg:94.48ms
step:237/1770 train_time:21448ms step_avg:94.48ms
step:238/1770 train_time:21542ms step_avg:94.48ms
step:239/1770 train_time:21637ms step_avg:94.49ms
step:240/1770 train_time:21732ms step_avg:94.49ms
step:241/1770 train_time:21826ms step_avg:94.49ms
step:242/1770 train_time:21922ms step_avg:94.49ms
step:243/1770 train_time:22017ms step_avg:94.49ms
step:244/1770 train_time:22111ms step_avg:94.49ms
step:245/1770 train_time:22206ms step_avg:94.49ms
step:246/1770 train_time:22301ms step_avg:94.49ms
step:247/1770 train_time:22396ms step_avg:94.50ms
step:248/1770 train_time:22491ms step_avg:94.50ms
step:249/1770 train_time:22586ms step_avg:94.50ms
step:250/1770 train_time:22680ms step_avg:94.50ms
step:250/1770 val_loss:4.1067 train_time:22773ms step_avg:94.89ms
step:251/1770 train_time:22796ms step_avg:94.59ms
step:252/1770 train_time:22878ms step_avg:94.54ms
step:253/1770 train_time:22976ms step_avg:94.55ms
step:254/1770 train_time:23071ms step_avg:94.55ms
step:255/1770 train_time:23166ms step_avg:94.55ms
step:256/1770 train_time:23260ms step_avg:94.55ms
step:257/1770 train_time:23354ms step_avg:94.55ms
step:258/1770 train_time:23448ms step_avg:94.55ms
step:259/1770 train_time:23542ms step_avg:94.55ms
step:260/1770 train_time:23637ms step_avg:94.55ms
step:261/1770 train_time:23731ms step_avg:94.55ms
step:262/1770 train_time:23826ms step_avg:94.55ms
step:263/1770 train_time:23921ms step_avg:94.55ms
step:264/1770 train_time:24017ms step_avg:94.56ms
step:265/1770 train_time:24114ms step_avg:94.56ms
step:266/1770 train_time:24209ms step_avg:94.57ms
step:267/1770 train_time:24304ms step_avg:94.57ms
step:268/1770 train_time:24400ms step_avg:94.57ms
step:269/1770 train_time:24495ms step_avg:94.57ms
step:270/1770 train_time:24590ms step_avg:94.58ms
step:271/1770 train_time:24684ms step_avg:94.58ms
step:272/1770 train_time:24779ms step_avg:94.57ms
step:273/1770 train_time:24874ms step_avg:94.58ms
step:274/1770 train_time:24969ms step_avg:94.58ms
step:275/1770 train_time:25065ms step_avg:94.58ms
step:276/1770 train_time:25160ms step_avg:94.59ms
step:277/1770 train_time:25255ms step_avg:94.59ms
step:278/1770 train_time:25351ms step_avg:94.59ms
step:279/1770 train_time:25446ms step_avg:94.59ms
step:280/1770 train_time:25541ms step_avg:94.60ms
step:281/1770 train_time:25636ms step_avg:94.60ms
step:282/1770 train_time:25731ms step_avg:94.60ms
step:283/1770 train_time:25826ms step_avg:94.60ms
step:284/1770 train_time:25921ms step_avg:94.60ms
step:285/1770 train_time:26016ms step_avg:94.60ms
step:286/1770 train_time:26112ms step_avg:94.61ms
step:287/1770 train_time:26208ms step_avg:94.61ms
step:288/1770 train_time:26303ms step_avg:94.62ms
step:289/1770 train_time:26399ms step_avg:94.62ms
step:290/1770 train_time:26494ms step_avg:94.62ms
step:291/1770 train_time:26589ms step_avg:94.62ms
step:292/1770 train_time:26685ms step_avg:94.63ms
step:293/1770 train_time:26780ms step_avg:94.63ms
step:294/1770 train_time:26875ms step_avg:94.63ms
step:295/1770 train_time:26970ms step_avg:94.63ms
step:296/1770 train_time:27065ms step_avg:94.63ms
step:297/1770 train_time:27160ms step_avg:94.63ms
step:298/1770 train_time:27255ms step_avg:94.64ms
step:299/1770 train_time:27350ms step_avg:94.64ms
step:300/1770 train_time:27445ms step_avg:94.64ms
step:301/1770 train_time:27540ms step_avg:94.64ms
step:302/1770 train_time:27635ms step_avg:94.64ms
step:303/1770 train_time:27731ms step_avg:94.64ms
step:304/1770 train_time:27825ms step_avg:94.64ms
step:305/1770 train_time:27920ms step_avg:94.64ms
step:306/1770 train_time:28016ms step_avg:94.65ms
step:307/1770 train_time:28111ms step_avg:94.65ms
step:308/1770 train_time:28206ms step_avg:94.65ms
step:309/1770 train_time:28301ms step_avg:94.65ms
step:310/1770 train_time:28397ms step_avg:94.66ms
step:311/1770 train_time:28492ms step_avg:94.66ms
step:312/1770 train_time:28587ms step_avg:94.66ms
step:313/1770 train_time:28682ms step_avg:94.66ms
step:314/1770 train_time:28777ms step_avg:94.66ms
step:315/1770 train_time:28872ms step_avg:94.66ms
step:316/1770 train_time:28968ms step_avg:94.67ms
step:317/1770 train_time:29063ms step_avg:94.67ms
step:318/1770 train_time:29158ms step_avg:94.67ms
step:319/1770 train_time:29253ms step_avg:94.67ms
step:320/1770 train_time:29348ms step_avg:94.67ms
step:321/1770 train_time:29443ms step_avg:94.67ms
step:322/1770 train_time:29538ms step_avg:94.67ms
step:323/1770 train_time:29634ms step_avg:94.68ms
step:324/1770 train_time:29729ms step_avg:94.68ms
step:325/1770 train_time:29824ms step_avg:94.68ms
step:326/1770 train_time:29919ms step_avg:94.68ms
step:327/1770 train_time:30014ms step_avg:94.68ms
step:328/1770 train_time:30110ms step_avg:94.68ms
step:329/1770 train_time:30204ms step_avg:94.68ms
step:330/1770 train_time:30299ms step_avg:94.69ms
step:331/1770 train_time:30394ms step_avg:94.69ms
step:332/1770 train_time:30489ms step_avg:94.69ms
step:333/1770 train_time:30584ms step_avg:94.69ms
step:334/1770 train_time:30679ms step_avg:94.69ms
step:335/1770 train_time:30775ms step_avg:94.69ms
step:336/1770 train_time:30870ms step_avg:94.69ms
step:337/1770 train_time:30965ms step_avg:94.69ms
step:338/1770 train_time:31061ms step_avg:94.70ms
step:339/1770 train_time:31156ms step_avg:94.70ms
step:340/1770 train_time:31251ms step_avg:94.70ms
step:341/1770 train_time:31346ms step_avg:94.70ms
step:342/1770 train_time:31441ms step_avg:94.70ms
step:343/1770 train_time:31537ms step_avg:94.70ms
step:344/1770 train_time:31632ms step_avg:94.71ms
step:345/1770 train_time:31727ms step_avg:94.71ms
step:346/1770 train_time:31822ms step_avg:94.71ms
step:347/1770 train_time:31917ms step_avg:94.71ms
step:348/1770 train_time:32013ms step_avg:94.71ms
step:349/1770 train_time:32108ms step_avg:94.71ms
step:350/1770 train_time:32203ms step_avg:94.71ms
step:351/1770 train_time:32298ms step_avg:94.71ms
step:352/1770 train_time:32393ms step_avg:94.72ms
step:353/1770 train_time:32488ms step_avg:94.72ms
step:354/1770 train_time:32583ms step_avg:94.72ms
step:355/1770 train_time:32678ms step_avg:94.72ms
step:356/1770 train_time:32774ms step_avg:94.72ms
step:357/1770 train_time:32870ms step_avg:94.73ms
step:358/1770 train_time:32964ms step_avg:94.73ms
step:359/1770 train_time:33060ms step_avg:94.73ms
step:360/1770 train_time:33155ms step_avg:94.73ms
step:361/1770 train_time:33250ms step_avg:94.73ms
step:362/1770 train_time:33345ms step_avg:94.73ms
step:363/1770 train_time:33440ms step_avg:94.73ms
step:364/1770 train_time:33535ms step_avg:94.73ms
step:365/1770 train_time:33630ms step_avg:94.73ms
step:366/1770 train_time:33725ms step_avg:94.73ms
step:367/1770 train_time:33819ms step_avg:94.73ms
step:368/1770 train_time:33915ms step_avg:94.74ms
step:369/1770 train_time:34010ms step_avg:94.74ms
step:370/1770 train_time:34105ms step_avg:94.74ms
step:371/1770 train_time:34200ms step_avg:94.74ms
step:372/1770 train_time:34296ms step_avg:94.74ms
step:373/1770 train_time:34391ms step_avg:94.74ms
step:374/1770 train_time:34486ms step_avg:94.74ms
step:375/1770 train_time:34580ms step_avg:94.74ms
step:375/1770 val_loss:3.9003 train_time:34674ms step_avg:95.00ms
step:376/1770 train_time:34696ms step_avg:94.80ms
step:377/1770 train_time:34780ms step_avg:94.77ms
step:378/1770 train_time:34878ms step_avg:94.78ms
step:379/1770 train_time:34973ms step_avg:94.78ms
step:380/1770 train_time:35068ms step_avg:94.78ms
step:381/1770 train_time:35163ms step_avg:94.78ms
step:382/1770 train_time:35258ms step_avg:94.78ms
step:383/1770 train_time:35352ms step_avg:94.78ms
step:384/1770 train_time:35447ms step_avg:94.78ms
step:385/1770 train_time:35541ms step_avg:94.78ms
step:386/1770 train_time:35636ms step_avg:94.78ms
step:387/1770 train_time:35733ms step_avg:94.78ms
step:388/1770 train_time:35829ms step_avg:94.79ms
step:389/1770 train_time:35925ms step_avg:94.79ms
step:390/1770 train_time:36019ms step_avg:94.79ms
step:391/1770 train_time:36115ms step_avg:94.79ms
step:392/1770 train_time:36210ms step_avg:94.79ms
step:393/1770 train_time:36306ms step_avg:94.79ms
step:394/1770 train_time:36400ms step_avg:94.79ms
step:395/1770 train_time:36494ms step_avg:94.79ms
step:396/1770 train_time:36591ms step_avg:94.80ms
step:397/1770 train_time:36689ms step_avg:94.80ms
step:398/1770 train_time:36786ms step_avg:94.81ms
step:399/1770 train_time:36883ms step_avg:94.82ms
step:400/1770 train_time:36979ms step_avg:94.82ms
step:401/1770 train_time:37076ms step_avg:94.82ms
step:402/1770 train_time:37173ms step_avg:94.83ms
step:403/1770 train_time:37270ms step_avg:94.83ms
step:404/1770 train_time:37367ms step_avg:94.84ms
step:405/1770 train_time:37464ms step_avg:94.85ms
step:406/1770 train_time:37560ms step_avg:94.85ms
step:407/1770 train_time:37657ms step_avg:94.85ms
step:408/1770 train_time:37754ms step_avg:94.86ms
step:409/1770 train_time:37852ms step_avg:94.87ms
step:410/1770 train_time:37949ms step_avg:94.87ms
step:411/1770 train_time:38046ms step_avg:94.88ms
step:412/1770 train_time:38143ms step_avg:94.88ms
step:413/1770 train_time:38240ms step_avg:94.89ms
step:414/1770 train_time:38337ms step_avg:94.89ms
step:415/1770 train_time:38434ms step_avg:94.90ms
step:416/1770 train_time:38531ms step_avg:94.90ms
step:417/1770 train_time:38628ms step_avg:94.91ms
step:418/1770 train_time:38725ms step_avg:94.91ms
step:419/1770 train_time:38822ms step_avg:94.92ms
step:420/1770 train_time:38919ms step_avg:94.92ms
step:421/1770 train_time:39016ms step_avg:94.93ms
step:422/1770 train_time:39113ms step_avg:94.93ms
step:423/1770 train_time:39210ms step_avg:94.94ms
step:424/1770 train_time:39307ms step_avg:94.95ms
step:425/1770 train_time:39404ms step_avg:94.95ms
step:426/1770 train_time:39501ms step_avg:94.96ms
step:427/1770 train_time:39598ms step_avg:94.96ms
step:428/1770 train_time:39695ms step_avg:94.96ms
step:429/1770 train_time:39792ms step_avg:94.97ms
step:430/1770 train_time:39889ms step_avg:94.97ms
step:431/1770 train_time:39986ms step_avg:94.98ms
step:432/1770 train_time:40082ms step_avg:94.98ms
step:433/1770 train_time:40178ms step_avg:94.98ms
step:434/1770 train_time:40275ms step_avg:94.99ms
step:435/1770 train_time:40373ms step_avg:94.99ms
step:436/1770 train_time:40470ms step_avg:95.00ms
step:437/1770 train_time:40567ms step_avg:95.00ms
step:438/1770 train_time:40664ms step_avg:95.01ms
step:439/1770 train_time:40761ms step_avg:95.01ms
step:440/1770 train_time:40857ms step_avg:95.02ms
step:441/1770 train_time:40954ms step_avg:95.02ms
step:442/1770 train_time:41051ms step_avg:95.03ms
step:443/1770 train_time:41148ms step_avg:95.03ms
step:444/1770 train_time:41245ms step_avg:95.03ms
step:445/1770 train_time:41342ms step_avg:95.04ms
step:446/1770 train_time:41438ms step_avg:95.04ms
step:447/1770 train_time:41535ms step_avg:95.05ms
step:448/1770 train_time:41633ms step_avg:95.05ms
step:449/1770 train_time:41730ms step_avg:95.06ms
step:450/1770 train_time:41827ms step_avg:95.06ms
step:451/1770 train_time:41924ms step_avg:95.07ms
step:452/1770 train_time:42021ms step_avg:95.07ms
step:453/1770 train_time:42117ms step_avg:95.07ms
step:454/1770 train_time:42214ms step_avg:95.08ms
step:455/1770 train_time:42312ms step_avg:95.08ms
step:456/1770 train_time:42409ms step_avg:95.09ms
step:457/1770 train_time:42507ms step_avg:95.09ms
step:458/1770 train_time:42603ms step_avg:95.10ms
step:459/1770 train_time:42700ms step_avg:95.10ms
step:460/1770 train_time:42797ms step_avg:95.10ms
step:461/1770 train_time:42895ms step_avg:95.11ms
step:462/1770 train_time:42992ms step_avg:95.11ms
step:463/1770 train_time:43089ms step_avg:95.12ms
step:464/1770 train_time:43186ms step_avg:95.12ms
step:465/1770 train_time:43283ms step_avg:95.13ms
step:466/1770 train_time:43380ms step_avg:95.13ms
step:467/1770 train_time:43477ms step_avg:95.13ms
step:468/1770 train_time:43573ms step_avg:95.14ms
step:469/1770 train_time:43671ms step_avg:95.14ms
step:470/1770 train_time:43769ms step_avg:95.15ms
step:471/1770 train_time:43866ms step_avg:95.15ms
step:472/1770 train_time:43963ms step_avg:95.16ms
step:473/1770 train_time:44060ms step_avg:95.16ms
step:474/1770 train_time:44156ms step_avg:95.16ms
step:475/1770 train_time:44253ms step_avg:95.17ms
step:476/1770 train_time:44350ms step_avg:95.17ms
step:477/1770 train_time:44448ms step_avg:95.18ms
step:478/1770 train_time:44545ms step_avg:95.18ms
step:479/1770 train_time:44641ms step_avg:95.18ms
step:480/1770 train_time:44738ms step_avg:95.19ms
step:481/1770 train_time:44835ms step_avg:95.19ms
step:482/1770 train_time:44932ms step_avg:95.19ms
step:483/1770 train_time:45029ms step_avg:95.20ms
step:484/1770 train_time:45126ms step_avg:95.20ms
step:485/1770 train_time:45223ms step_avg:95.21ms
step:486/1770 train_time:45320ms step_avg:95.21ms
step:487/1770 train_time:45417ms step_avg:95.21ms
step:488/1770 train_time:45515ms step_avg:95.22ms
step:489/1770 train_time:45612ms step_avg:95.22ms
step:490/1770 train_time:45709ms step_avg:95.23ms
step:491/1770 train_time:45807ms step_avg:95.23ms
step:492/1770 train_time:45903ms step_avg:95.24ms
step:493/1770 train_time:46000ms step_avg:95.24ms
step:494/1770 train_time:46097ms step_avg:95.24ms
step:495/1770 train_time:46194ms step_avg:95.24ms
step:496/1770 train_time:46291ms step_avg:95.25ms
step:497/1770 train_time:46389ms step_avg:95.25ms
step:498/1770 train_time:46486ms step_avg:95.26ms
step:499/1770 train_time:46583ms step_avg:95.26ms
step:500/1770 train_time:46680ms step_avg:95.26ms
step:500/1770 val_loss:3.7561 train_time:46774ms step_avg:95.46ms
step:501/1770 train_time:46797ms step_avg:95.31ms
step:502/1770 train_time:46886ms step_avg:95.30ms
step:503/1770 train_time:46984ms step_avg:95.30ms
step:504/1770 train_time:47081ms step_avg:95.31ms
step:505/1770 train_time:47178ms step_avg:95.31ms
step:506/1770 train_time:47274ms step_avg:95.31ms
step:507/1770 train_time:47371ms step_avg:95.31ms
step:508/1770 train_time:47468ms step_avg:95.32ms
step:509/1770 train_time:47564ms step_avg:95.32ms
step:510/1770 train_time:47661ms step_avg:95.32ms
step:511/1770 train_time:47758ms step_avg:95.32ms
step:512/1770 train_time:47855ms step_avg:95.33ms
step:513/1770 train_time:47953ms step_avg:95.33ms
step:514/1770 train_time:48051ms step_avg:95.34ms
step:515/1770 train_time:48149ms step_avg:95.34ms
step:516/1770 train_time:48246ms step_avg:95.35ms
step:517/1770 train_time:48342ms step_avg:95.35ms
step:518/1770 train_time:48439ms step_avg:95.35ms
step:519/1770 train_time:48536ms step_avg:95.36ms
step:520/1770 train_time:48633ms step_avg:95.36ms
step:521/1770 train_time:48730ms step_avg:95.36ms
step:522/1770 train_time:48827ms step_avg:95.36ms
step:523/1770 train_time:48924ms step_avg:95.37ms
step:524/1770 train_time:49022ms step_avg:95.37ms
step:525/1770 train_time:49118ms step_avg:95.37ms
step:526/1770 train_time:49216ms step_avg:95.38ms
step:527/1770 train_time:49313ms step_avg:95.38ms
step:528/1770 train_time:49411ms step_avg:95.39ms
step:529/1770 train_time:49509ms step_avg:95.39ms
step:530/1770 train_time:49606ms step_avg:95.40ms
step:531/1770 train_time:49703ms step_avg:95.40ms
step:532/1770 train_time:49800ms step_avg:95.40ms
step:533/1770 train_time:49897ms step_avg:95.41ms
step:534/1770 train_time:49994ms step_avg:95.41ms
step:535/1770 train_time:50092ms step_avg:95.41ms
step:536/1770 train_time:50190ms step_avg:95.42ms
step:537/1770 train_time:50287ms step_avg:95.42ms
step:538/1770 train_time:50384ms step_avg:95.43ms
step:539/1770 train_time:50482ms step_avg:95.43ms
step:540/1770 train_time:50580ms step_avg:95.43ms
step:541/1770 train_time:50677ms step_avg:95.44ms
step:542/1770 train_time:50775ms step_avg:95.44ms
step:543/1770 train_time:50872ms step_avg:95.44ms
step:544/1770 train_time:50970ms step_avg:95.45ms
step:545/1770 train_time:51067ms step_avg:95.45ms
step:546/1770 train_time:51164ms step_avg:95.45ms
step:547/1770 train_time:51261ms step_avg:95.46ms
step:548/1770 train_time:51358ms step_avg:95.46ms
step:549/1770 train_time:51455ms step_avg:95.46ms
step:550/1770 train_time:51553ms step_avg:95.47ms
step:551/1770 train_time:51650ms step_avg:95.47ms
step:552/1770 train_time:51748ms step_avg:95.48ms
step:553/1770 train_time:51846ms step_avg:95.48ms
step:554/1770 train_time:51943ms step_avg:95.48ms
step:555/1770 train_time:52041ms step_avg:95.49ms
step:556/1770 train_time:52138ms step_avg:95.49ms
step:557/1770 train_time:52236ms step_avg:95.49ms
step:558/1770 train_time:52333ms step_avg:95.50ms
step:559/1770 train_time:52431ms step_avg:95.50ms
step:560/1770 train_time:52529ms step_avg:95.51ms
step:561/1770 train_time:52626ms step_avg:95.51ms
step:562/1770 train_time:52723ms step_avg:95.51ms
step:563/1770 train_time:52820ms step_avg:95.52ms
step:564/1770 train_time:52918ms step_avg:95.52ms
step:565/1770 train_time:53015ms step_avg:95.52ms
step:566/1770 train_time:53113ms step_avg:95.53ms
step:567/1770 train_time:53211ms step_avg:95.53ms
step:568/1770 train_time:53308ms step_avg:95.53ms
step:569/1770 train_time:53405ms step_avg:95.54ms
step:570/1770 train_time:53503ms step_avg:95.54ms
step:571/1770 train_time:53600ms step_avg:95.54ms
step:572/1770 train_time:53696ms step_avg:95.55ms
step:573/1770 train_time:53794ms step_avg:95.55ms
step:574/1770 train_time:53893ms step_avg:95.55ms
step:575/1770 train_time:53990ms step_avg:95.56ms
step:576/1770 train_time:54087ms step_avg:95.56ms
step:577/1770 train_time:54184ms step_avg:95.56ms
step:578/1770 train_time:54281ms step_avg:95.57ms
step:579/1770 train_time:54379ms step_avg:95.57ms
step:580/1770 train_time:54477ms step_avg:95.57ms
step:581/1770 train_time:54574ms step_avg:95.58ms
step:582/1770 train_time:54672ms step_avg:95.58ms
step:583/1770 train_time:54769ms step_avg:95.58ms
step:584/1770 train_time:54867ms step_avg:95.59ms
step:585/1770 train_time:54964ms step_avg:95.59ms
step:586/1770 train_time:55061ms step_avg:95.59ms
step:587/1770 train_time:55158ms step_avg:95.59ms
step:588/1770 train_time:55255ms step_avg:95.60ms
step:589/1770 train_time:55353ms step_avg:95.60ms
step:590/1770 train_time:55451ms step_avg:95.60ms
step:591/1770 train_time:55548ms step_avg:95.61ms
step:592/1770 train_time:55645ms step_avg:95.61ms
step:593/1770 train_time:55742ms step_avg:95.61ms
step:594/1770 train_time:55840ms step_avg:95.62ms
step:595/1770 train_time:55938ms step_avg:95.62ms
step:596/1770 train_time:56035ms step_avg:95.62ms
step:597/1770 train_time:56133ms step_avg:95.63ms
step:598/1770 train_time:56230ms step_avg:95.63ms
step:599/1770 train_time:56327ms step_avg:95.63ms
step:600/1770 train_time:56424ms step_avg:95.63ms
step:601/1770 train_time:56522ms step_avg:95.64ms
step:602/1770 train_time:56619ms step_avg:95.64ms
step:603/1770 train_time:56717ms step_avg:95.64ms
step:604/1770 train_time:56814ms step_avg:95.65ms
step:605/1770 train_time:56912ms step_avg:95.65ms
step:606/1770 train_time:57010ms step_avg:95.65ms
step:607/1770 train_time:57108ms step_avg:95.66ms
step:608/1770 train_time:57205ms step_avg:95.66ms
step:609/1770 train_time:57302ms step_avg:95.66ms
step:610/1770 train_time:57399ms step_avg:95.66ms
step:611/1770 train_time:57496ms step_avg:95.67ms
step:612/1770 train_time:57594ms step_avg:95.67ms
step:613/1770 train_time:57692ms step_avg:95.67ms
step:614/1770 train_time:57789ms step_avg:95.68ms
step:615/1770 train_time:57886ms step_avg:95.68ms
step:616/1770 train_time:57984ms step_avg:95.68ms
step:617/1770 train_time:58080ms step_avg:95.68ms
step:618/1770 train_time:58178ms step_avg:95.69ms
step:619/1770 train_time:58276ms step_avg:95.69ms
step:620/1770 train_time:58374ms step_avg:95.69ms
step:621/1770 train_time:58471ms step_avg:95.70ms
step:622/1770 train_time:58569ms step_avg:95.70ms
step:623/1770 train_time:58666ms step_avg:95.70ms
step:624/1770 train_time:58762ms step_avg:95.70ms
step:625/1770 train_time:58860ms step_avg:95.71ms
step:625/1770 val_loss:3.6675 train_time:58956ms step_avg:95.86ms
step:626/1770 train_time:58978ms step_avg:95.74ms
step:627/1770 train_time:59065ms step_avg:95.73ms
step:628/1770 train_time:59164ms step_avg:95.73ms
step:629/1770 train_time:59261ms step_avg:95.74ms
step:630/1770 train_time:59358ms step_avg:95.74ms
step:631/1770 train_time:59455ms step_avg:95.74ms
step:632/1770 train_time:59551ms step_avg:95.74ms
step:633/1770 train_time:59648ms step_avg:95.74ms
step:634/1770 train_time:59745ms step_avg:95.75ms
step:635/1770 train_time:59842ms step_avg:95.75ms
step:636/1770 train_time:59939ms step_avg:95.75ms
step:637/1770 train_time:60037ms step_avg:95.75ms
step:638/1770 train_time:60136ms step_avg:95.76ms
step:639/1770 train_time:60234ms step_avg:95.76ms
step:640/1770 train_time:60332ms step_avg:95.76ms
step:641/1770 train_time:60430ms step_avg:95.77ms
step:642/1770 train_time:60528ms step_avg:95.77ms
step:643/1770 train_time:60625ms step_avg:95.77ms
step:644/1770 train_time:60722ms step_avg:95.78ms
step:645/1770 train_time:60819ms step_avg:95.78ms
step:646/1770 train_time:60916ms step_avg:95.78ms
step:647/1770 train_time:61013ms step_avg:95.78ms
step:648/1770 train_time:61110ms step_avg:95.78ms
step:649/1770 train_time:61208ms step_avg:95.79ms
step:650/1770 train_time:61306ms step_avg:95.79ms
step:651/1770 train_time:61404ms step_avg:95.79ms
step:652/1770 train_time:61501ms step_avg:95.80ms
step:653/1770 train_time:61599ms step_avg:95.80ms
step:654/1770 train_time:61696ms step_avg:95.80ms
step:655/1770 train_time:61793ms step_avg:95.80ms
step:656/1770 train_time:61891ms step_avg:95.81ms
step:657/1770 train_time:61989ms step_avg:95.81ms
step:658/1770 train_time:62089ms step_avg:95.82ms
step:659/1770 train_time:62188ms step_avg:95.82ms
step:660/1770 train_time:62287ms step_avg:95.83ms
step:661/1770 train_time:62386ms step_avg:95.83ms
step:662/1770 train_time:62485ms step_avg:95.84ms
step:663/1770 train_time:62584ms step_avg:95.84ms
step:664/1770 train_time:62684ms step_avg:95.85ms
step:665/1770 train_time:62783ms step_avg:95.85ms
step:666/1770 train_time:62881ms step_avg:95.86ms
step:667/1770 train_time:62980ms step_avg:95.86ms
step:668/1770 train_time:63079ms step_avg:95.86ms
step:669/1770 train_time:63178ms step_avg:95.87ms
step:670/1770 train_time:63277ms step_avg:95.87ms
step:671/1770 train_time:63377ms step_avg:95.88ms
step:672/1770 train_time:63477ms step_avg:95.89ms
step:673/1770 train_time:63577ms step_avg:95.89ms
step:674/1770 train_time:63675ms step_avg:95.90ms
step:675/1770 train_time:63775ms step_avg:95.90ms
step:676/1770 train_time:63874ms step_avg:95.91ms
step:677/1770 train_time:63973ms step_avg:95.91ms
step:678/1770 train_time:64072ms step_avg:95.92ms
step:679/1770 train_time:64171ms step_avg:95.92ms
step:680/1770 train_time:64271ms step_avg:95.93ms
step:681/1770 train_time:64371ms step_avg:95.93ms
step:682/1770 train_time:64470ms step_avg:95.94ms
step:683/1770 train_time:64570ms step_avg:95.94ms
step:684/1770 train_time:64670ms step_avg:95.95ms
step:685/1770 train_time:64771ms step_avg:95.96ms
step:686/1770 train_time:64870ms step_avg:95.96ms
step:687/1770 train_time:64970ms step_avg:95.97ms
step:688/1770 train_time:65070ms step_avg:95.97ms
step:689/1770 train_time:65170ms step_avg:95.98ms
step:690/1770 train_time:65269ms step_avg:95.98ms
step:691/1770 train_time:65369ms step_avg:95.99ms
step:692/1770 train_time:65469ms step_avg:96.00ms
step:693/1770 train_time:65569ms step_avg:96.00ms
step:694/1770 train_time:65669ms step_avg:96.01ms
step:695/1770 train_time:65769ms step_avg:96.01ms
step:696/1770 train_time:65869ms step_avg:96.02ms
step:697/1770 train_time:65968ms step_avg:96.02ms
step:698/1770 train_time:66067ms step_avg:96.03ms
step:699/1770 train_time:66167ms step_avg:96.03ms
step:700/1770 train_time:66266ms step_avg:96.04ms
step:701/1770 train_time:66365ms step_avg:96.04ms
step:702/1770 train_time:66464ms step_avg:96.05ms
step:703/1770 train_time:66563ms step_avg:96.05ms
step:704/1770 train_time:66662ms step_avg:96.06ms
step:705/1770 train_time:66761ms step_avg:96.06ms
step:706/1770 train_time:66860ms step_avg:96.06ms
step:707/1770 train_time:66958ms step_avg:96.07ms
step:708/1770 train_time:67057ms step_avg:96.07ms
step:709/1770 train_time:67157ms step_avg:96.08ms
step:710/1770 train_time:67257ms step_avg:96.08ms
step:711/1770 train_time:67357ms step_avg:96.09ms
step:712/1770 train_time:67457ms step_avg:96.09ms
step:713/1770 train_time:67556ms step_avg:96.10ms
step:714/1770 train_time:67655ms step_avg:96.10ms
step:715/1770 train_time:67754ms step_avg:96.11ms
step:716/1770 train_time:67854ms step_avg:96.11ms
step:717/1770 train_time:67953ms step_avg:96.11ms
step:718/1770 train_time:68052ms step_avg:96.12ms
step:719/1770 train_time:68151ms step_avg:96.12ms
step:720/1770 train_time:68251ms step_avg:96.13ms
step:721/1770 train_time:68350ms step_avg:96.13ms
step:722/1770 train_time:68450ms step_avg:96.14ms
step:723/1770 train_time:68550ms step_avg:96.14ms
step:724/1770 train_time:68651ms step_avg:96.15ms
step:725/1770 train_time:68750ms step_avg:96.15ms
step:726/1770 train_time:68849ms step_avg:96.16ms
step:727/1770 train_time:68949ms step_avg:96.16ms
step:728/1770 train_time:69048ms step_avg:96.17ms
step:729/1770 train_time:69146ms step_avg:96.17ms
step:730/1770 train_time:69246ms step_avg:96.17ms
step:731/1770 train_time:69345ms step_avg:96.18ms
step:732/1770 train_time:69444ms step_avg:96.18ms
step:733/1770 train_time:69544ms step_avg:96.19ms
step:734/1770 train_time:69643ms step_avg:96.19ms
step:735/1770 train_time:69743ms step_avg:96.20ms
step:736/1770 train_time:69842ms step_avg:96.20ms
step:737/1770 train_time:69942ms step_avg:96.21ms
step:738/1770 train_time:70041ms step_avg:96.21ms
step:739/1770 train_time:70141ms step_avg:96.22ms
step:740/1770 train_time:70240ms step_avg:96.22ms
step:741/1770 train_time:70339ms step_avg:96.22ms
step:742/1770 train_time:70438ms step_avg:96.23ms
step:743/1770 train_time:70537ms step_avg:96.23ms
step:744/1770 train_time:70637ms step_avg:96.24ms
step:745/1770 train_time:70737ms step_avg:96.24ms
step:746/1770 train_time:70837ms step_avg:96.25ms
step:747/1770 train_time:70937ms step_avg:96.25ms
step:748/1770 train_time:71037ms step_avg:96.26ms
step:749/1770 train_time:71136ms step_avg:96.26ms
step:750/1770 train_time:71235ms step_avg:96.26ms
step:750/1770 val_loss:3.6027 train_time:71332ms step_avg:96.39ms
step:751/1770 train_time:71354ms step_avg:96.29ms
step:752/1770 train_time:71445ms step_avg:96.29ms
step:753/1770 train_time:71545ms step_avg:96.29ms
step:754/1770 train_time:71644ms step_avg:96.30ms
step:755/1770 train_time:71743ms step_avg:96.30ms
step:756/1770 train_time:71841ms step_avg:96.30ms
step:757/1770 train_time:71940ms step_avg:96.30ms
step:758/1770 train_time:72038ms step_avg:96.31ms
step:759/1770 train_time:72136ms step_avg:96.31ms
step:760/1770 train_time:72235ms step_avg:96.31ms
step:761/1770 train_time:72334ms step_avg:96.32ms
step:762/1770 train_time:72435ms step_avg:96.32ms
step:763/1770 train_time:72535ms step_avg:96.33ms
step:764/1770 train_time:72635ms step_avg:96.33ms
step:765/1770 train_time:72735ms step_avg:96.34ms
step:766/1770 train_time:72834ms step_avg:96.34ms
step:767/1770 train_time:72935ms step_avg:96.35ms
step:768/1770 train_time:73035ms step_avg:96.35ms
step:769/1770 train_time:73134ms step_avg:96.36ms
step:770/1770 train_time:73233ms step_avg:96.36ms
step:771/1770 train_time:73332ms step_avg:96.36ms
step:772/1770 train_time:73432ms step_avg:96.37ms
step:773/1770 train_time:73531ms step_avg:96.37ms
step:774/1770 train_time:73630ms step_avg:96.37ms
step:775/1770 train_time:73730ms step_avg:96.38ms
step:776/1770 train_time:73829ms step_avg:96.38ms
step:777/1770 train_time:73930ms step_avg:96.39ms
step:778/1770 train_time:74029ms step_avg:96.39ms
step:779/1770 train_time:74128ms step_avg:96.40ms
step:780/1770 train_time:74227ms step_avg:96.40ms
step:781/1770 train_time:74326ms step_avg:96.40ms
step:782/1770 train_time:74425ms step_avg:96.41ms
step:783/1770 train_time:74524ms step_avg:96.41ms
step:784/1770 train_time:74623ms step_avg:96.41ms
step:785/1770 train_time:74721ms step_avg:96.41ms
step:786/1770 train_time:74821ms step_avg:96.42ms
step:787/1770 train_time:74920ms step_avg:96.42ms
step:788/1770 train_time:75020ms step_avg:96.43ms
step:789/1770 train_time:75120ms step_avg:96.43ms
step:790/1770 train_time:75220ms step_avg:96.44ms
step:791/1770 train_time:75319ms step_avg:96.44ms
step:792/1770 train_time:75419ms step_avg:96.44ms
step:793/1770 train_time:75519ms step_avg:96.45ms
step:794/1770 train_time:75619ms step_avg:96.45ms
step:795/1770 train_time:75718ms step_avg:96.46ms
step:796/1770 train_time:75818ms step_avg:96.46ms
step:797/1770 train_time:75917ms step_avg:96.46ms
step:798/1770 train_time:76017ms step_avg:96.47ms
step:799/1770 train_time:76116ms step_avg:96.47ms
step:800/1770 train_time:76216ms step_avg:96.48ms
step:801/1770 train_time:76315ms step_avg:96.48ms
step:802/1770 train_time:76415ms step_avg:96.48ms
step:803/1770 train_time:76515ms step_avg:96.49ms
step:804/1770 train_time:76615ms step_avg:96.49ms
step:805/1770 train_time:76714ms step_avg:96.50ms
step:806/1770 train_time:76814ms step_avg:96.50ms
step:807/1770 train_time:76914ms step_avg:96.50ms
step:808/1770 train_time:77014ms step_avg:96.51ms
step:809/1770 train_time:77113ms step_avg:96.51ms
step:810/1770 train_time:77214ms step_avg:96.52ms
step:811/1770 train_time:77313ms step_avg:96.52ms
step:812/1770 train_time:77413ms step_avg:96.53ms
step:813/1770 train_time:77513ms step_avg:96.53ms
step:814/1770 train_time:77613ms step_avg:96.53ms
step:815/1770 train_time:77713ms step_avg:96.54ms
step:816/1770 train_time:77812ms step_avg:96.54ms
step:817/1770 train_time:77912ms step_avg:96.55ms
step:818/1770 train_time:78012ms step_avg:96.55ms
step:819/1770 train_time:78111ms step_avg:96.55ms
step:820/1770 train_time:78211ms step_avg:96.56ms
step:821/1770 train_time:78311ms step_avg:96.56ms
step:822/1770 train_time:78411ms step_avg:96.57ms
step:823/1770 train_time:78512ms step_avg:96.57ms
step:824/1770 train_time:78612ms step_avg:96.58ms
step:825/1770 train_time:78712ms step_avg:96.58ms
step:826/1770 train_time:78812ms step_avg:96.58ms
step:827/1770 train_time:78911ms step_avg:96.59ms
step:828/1770 train_time:79011ms step_avg:96.59ms
step:829/1770 train_time:79111ms step_avg:96.59ms
step:830/1770 train_time:79210ms step_avg:96.60ms
step:831/1770 train_time:79309ms step_avg:96.60ms
step:832/1770 train_time:79408ms step_avg:96.60ms
step:833/1770 train_time:79508ms step_avg:96.61ms
step:834/1770 train_time:79609ms step_avg:96.61ms
step:835/1770 train_time:79708ms step_avg:96.62ms
step:836/1770 train_time:79807ms step_avg:96.62ms
step:837/1770 train_time:79907ms step_avg:96.62ms
step:838/1770 train_time:80006ms step_avg:96.63ms
step:839/1770 train_time:80106ms step_avg:96.63ms
step:840/1770 train_time:80205ms step_avg:96.63ms
step:841/1770 train_time:80304ms step_avg:96.64ms
step:842/1770 train_time:80403ms step_avg:96.64ms
step:843/1770 train_time:80502ms step_avg:96.64ms
step:844/1770 train_time:80601ms step_avg:96.64ms
step:845/1770 train_time:80701ms step_avg:96.65ms
step:846/1770 train_time:80801ms step_avg:96.65ms
step:847/1770 train_time:80900ms step_avg:96.65ms
step:848/1770 train_time:80999ms step_avg:96.66ms
step:849/1770 train_time:81098ms step_avg:96.66ms
step:850/1770 train_time:81197ms step_avg:96.66ms
step:851/1770 train_time:81297ms step_avg:96.67ms
step:852/1770 train_time:81397ms step_avg:96.67ms
step:853/1770 train_time:81496ms step_avg:96.67ms
step:854/1770 train_time:81595ms step_avg:96.68ms
step:855/1770 train_time:81695ms step_avg:96.68ms
step:856/1770 train_time:81794ms step_avg:96.68ms
step:857/1770 train_time:81894ms step_avg:96.69ms
step:858/1770 train_time:81994ms step_avg:96.69ms
step:859/1770 train_time:82094ms step_avg:96.70ms
step:860/1770 train_time:82194ms step_avg:96.70ms
step:861/1770 train_time:82293ms step_avg:96.70ms
step:862/1770 train_time:82393ms step_avg:96.70ms
step:863/1770 train_time:82492ms step_avg:96.71ms
step:864/1770 train_time:82592ms step_avg:96.71ms
step:865/1770 train_time:82692ms step_avg:96.72ms
step:866/1770 train_time:82792ms step_avg:96.72ms
step:867/1770 train_time:82892ms step_avg:96.72ms
step:868/1770 train_time:82992ms step_avg:96.73ms
step:869/1770 train_time:83092ms step_avg:96.73ms
step:870/1770 train_time:83191ms step_avg:96.73ms
step:871/1770 train_time:83291ms step_avg:96.74ms
step:872/1770 train_time:83390ms step_avg:96.74ms
step:873/1770 train_time:83489ms step_avg:96.74ms
step:874/1770 train_time:83589ms step_avg:96.75ms
step:875/1770 train_time:83687ms step_avg:96.75ms
step:875/1770 val_loss:3.5555 train_time:83785ms step_avg:96.86ms
step:876/1770 train_time:83808ms step_avg:96.78ms
step:877/1770 train_time:83895ms step_avg:96.76ms
step:878/1770 train_time:83995ms step_avg:96.77ms
step:879/1770 train_time:84094ms step_avg:96.77ms
step:880/1770 train_time:84194ms step_avg:96.77ms
step:881/1770 train_time:84293ms step_avg:96.78ms
step:882/1770 train_time:84392ms step_avg:96.78ms
step:883/1770 train_time:84490ms step_avg:96.78ms
step:884/1770 train_time:84589ms step_avg:96.78ms
step:885/1770 train_time:84688ms step_avg:96.79ms
step:886/1770 train_time:84788ms step_avg:96.79ms
step:887/1770 train_time:84889ms step_avg:96.79ms
step:888/1770 train_time:84989ms step_avg:96.80ms
step:889/1770 train_time:85089ms step_avg:96.80ms
step:890/1770 train_time:85188ms step_avg:96.80ms
step:891/1770 train_time:85288ms step_avg:96.81ms
step:892/1770 train_time:85387ms step_avg:96.81ms
step:893/1770 train_time:85486ms step_avg:96.81ms
step:894/1770 train_time:85586ms step_avg:96.82ms
step:895/1770 train_time:85685ms step_avg:96.82ms
step:896/1770 train_time:85784ms step_avg:96.82ms
step:897/1770 train_time:85883ms step_avg:96.82ms
step:898/1770 train_time:85983ms step_avg:96.83ms
step:899/1770 train_time:86082ms step_avg:96.83ms
step:900/1770 train_time:86183ms step_avg:96.83ms
step:901/1770 train_time:86282ms step_avg:96.84ms
step:902/1770 train_time:86382ms step_avg:96.84ms
step:903/1770 train_time:86482ms step_avg:96.84ms
step:904/1770 train_time:86583ms step_avg:96.85ms
step:905/1770 train_time:86682ms step_avg:96.85ms
step:906/1770 train_time:86781ms step_avg:96.85ms
step:907/1770 train_time:86881ms step_avg:96.86ms
step:908/1770 train_time:86980ms step_avg:96.86ms
step:909/1770 train_time:87080ms step_avg:96.86ms
step:910/1770 train_time:87180ms step_avg:96.87ms
step:911/1770 train_time:87279ms step_avg:96.87ms
step:912/1770 train_time:87379ms step_avg:96.87ms
step:913/1770 train_time:87478ms step_avg:96.87ms
step:914/1770 train_time:87577ms step_avg:96.88ms
step:915/1770 train_time:87677ms step_avg:96.88ms
step:916/1770 train_time:87777ms step_avg:96.88ms
step:917/1770 train_time:87877ms step_avg:96.89ms
step:918/1770 train_time:87976ms step_avg:96.89ms
step:919/1770 train_time:88076ms step_avg:96.89ms
step:920/1770 train_time:88177ms step_avg:96.90ms
step:921/1770 train_time:88279ms step_avg:96.90ms
step:922/1770 train_time:88380ms step_avg:96.91ms
step:923/1770 train_time:88481ms step_avg:96.91ms
step:924/1770 train_time:88583ms step_avg:96.92ms
step:925/1770 train_time:88683ms step_avg:96.92ms
step:926/1770 train_time:88784ms step_avg:96.93ms
step:927/1770 train_time:88884ms step_avg:96.93ms
step:928/1770 train_time:88985ms step_avg:96.93ms
step:929/1770 train_time:89085ms step_avg:96.94ms
step:930/1770 train_time:89186ms step_avg:96.94ms
step:931/1770 train_time:89287ms step_avg:96.95ms
step:932/1770 train_time:89388ms step_avg:96.95ms
step:933/1770 train_time:89489ms step_avg:96.95ms
step:934/1770 train_time:89589ms step_avg:96.96ms
step:935/1770 train_time:89690ms step_avg:96.96ms
step:936/1770 train_time:89791ms step_avg:96.97ms
step:937/1770 train_time:89892ms step_avg:96.97ms
step:938/1770 train_time:89993ms step_avg:96.97ms
step:939/1770 train_time:90094ms step_avg:96.98ms
step:940/1770 train_time:90196ms step_avg:96.98ms
step:941/1770 train_time:90297ms step_avg:96.99ms
step:942/1770 train_time:90399ms step_avg:96.99ms
step:943/1770 train_time:90500ms step_avg:97.00ms
step:944/1770 train_time:90601ms step_avg:97.00ms
step:945/1770 train_time:90701ms step_avg:97.01ms
step:946/1770 train_time:90802ms step_avg:97.01ms
step:947/1770 train_time:90904ms step_avg:97.02ms
step:948/1770 train_time:91005ms step_avg:97.02ms
step:949/1770 train_time:91107ms step_avg:97.03ms
step:950/1770 train_time:91208ms step_avg:97.03ms
step:951/1770 train_time:91309ms step_avg:97.03ms
step:952/1770 train_time:91409ms step_avg:97.04ms
step:953/1770 train_time:91510ms step_avg:97.04ms
step:954/1770 train_time:91611ms step_avg:97.05ms
step:955/1770 train_time:91712ms step_avg:97.05ms
step:956/1770 train_time:91814ms step_avg:97.05ms
step:957/1770 train_time:91916ms step_avg:97.06ms
step:958/1770 train_time:92018ms step_avg:97.07ms
step:959/1770 train_time:92119ms step_avg:97.07ms
step:960/1770 train_time:92220ms step_avg:97.07ms
step:961/1770 train_time:92321ms step_avg:97.08ms
step:962/1770 train_time:92422ms step_avg:97.08ms
step:963/1770 train_time:92523ms step_avg:97.09ms
step:964/1770 train_time:92623ms step_avg:97.09ms
step:965/1770 train_time:92725ms step_avg:97.09ms
step:966/1770 train_time:92826ms step_avg:97.10ms
step:967/1770 train_time:92927ms step_avg:97.10ms
step:968/1770 train_time:93028ms step_avg:97.11ms
step:969/1770 train_time:93129ms step_avg:97.11ms
step:970/1770 train_time:93228ms step_avg:97.11ms
step:971/1770 train_time:93328ms step_avg:97.12ms
step:972/1770 train_time:93429ms step_avg:97.12ms
step:973/1770 train_time:93529ms step_avg:97.12ms
step:974/1770 train_time:93630ms step_avg:97.13ms
step:975/1770 train_time:93732ms step_avg:97.13ms
step:976/1770 train_time:93833ms step_avg:97.14ms
step:977/1770 train_time:93935ms step_avg:97.14ms
step:978/1770 train_time:94036ms step_avg:97.14ms
step:979/1770 train_time:94137ms step_avg:97.15ms
step:980/1770 train_time:94239ms step_avg:97.15ms
step:981/1770 train_time:94339ms step_avg:97.16ms
step:982/1770 train_time:94440ms step_avg:97.16ms
step:983/1770 train_time:94541ms step_avg:97.16ms
step:984/1770 train_time:94643ms step_avg:97.17ms
step:985/1770 train_time:94745ms step_avg:97.17ms
step:986/1770 train_time:94845ms step_avg:97.18ms
step:987/1770 train_time:94946ms step_avg:97.18ms
step:988/1770 train_time:95046ms step_avg:97.18ms
step:989/1770 train_time:95148ms step_avg:97.19ms
step:990/1770 train_time:95248ms step_avg:97.19ms
step:991/1770 train_time:95348ms step_avg:97.19ms
step:992/1770 train_time:95448ms step_avg:97.20ms
step:993/1770 train_time:95549ms step_avg:97.20ms
step:994/1770 train_time:95652ms step_avg:97.21ms
step:995/1770 train_time:95753ms step_avg:97.21ms
step:996/1770 train_time:95854ms step_avg:97.21ms
step:997/1770 train_time:95955ms step_avg:97.22ms
step:998/1770 train_time:96056ms step_avg:97.22ms
step:999/1770 train_time:96156ms step_avg:97.23ms
step:1000/1770 train_time:96257ms step_avg:97.23ms
step:1000/1770 val_loss:3.5161 train_time:96356ms step_avg:97.33ms
step:1001/1770 train_time:96378ms step_avg:97.25ms
step:1002/1770 train_time:96468ms step_avg:97.25ms
step:1003/1770 train_time:96569ms step_avg:97.25ms
step:1004/1770 train_time:96670ms step_avg:97.25ms
step:1005/1770 train_time:96770ms step_avg:97.26ms
step:1006/1770 train_time:96870ms step_avg:97.26ms
step:1007/1770 train_time:96970ms step_avg:97.26ms
step:1008/1770 train_time:97070ms step_avg:97.26ms
step:1009/1770 train_time:97170ms step_avg:97.27ms
step:1010/1770 train_time:97271ms step_avg:97.27ms
step:1011/1770 train_time:97373ms step_avg:97.28ms
step:1012/1770 train_time:97476ms step_avg:97.28ms
step:1013/1770 train_time:97578ms step_avg:97.29ms
step:1014/1770 train_time:97678ms step_avg:97.29ms
step:1015/1770 train_time:97778ms step_avg:97.29ms
step:1016/1770 train_time:97878ms step_avg:97.29ms
step:1017/1770 train_time:97979ms step_avg:97.30ms
step:1018/1770 train_time:98079ms step_avg:97.30ms
step:1019/1770 train_time:98180ms step_avg:97.30ms
step:1020/1770 train_time:98281ms step_avg:97.31ms
step:1021/1770 train_time:98383ms step_avg:97.31ms
step:1022/1770 train_time:98484ms step_avg:97.32ms
step:1023/1770 train_time:98585ms step_avg:97.32ms
step:1024/1770 train_time:98687ms step_avg:97.32ms
step:1025/1770 train_time:98788ms step_avg:97.33ms
step:1026/1770 train_time:98889ms step_avg:97.33ms
step:1027/1770 train_time:98991ms step_avg:97.34ms
step:1028/1770 train_time:99092ms step_avg:97.34ms
step:1029/1770 train_time:99194ms step_avg:97.34ms
step:1030/1770 train_time:99294ms step_avg:97.35ms
step:1031/1770 train_time:99395ms step_avg:97.35ms
step:1032/1770 train_time:99496ms step_avg:97.35ms
step:1033/1770 train_time:99596ms step_avg:97.36ms
step:1034/1770 train_time:99696ms step_avg:97.36ms
step:1035/1770 train_time:99797ms step_avg:97.36ms
step:1036/1770 train_time:99898ms step_avg:97.37ms
step:1037/1770 train_time:99999ms step_avg:97.37ms
step:1038/1770 train_time:100101ms step_avg:97.37ms
step:1039/1770 train_time:100202ms step_avg:97.38ms
step:1040/1770 train_time:100303ms step_avg:97.38ms
step:1041/1770 train_time:100404ms step_avg:97.38ms
step:1042/1770 train_time:100505ms step_avg:97.39ms
step:1043/1770 train_time:100607ms step_avg:97.39ms
step:1044/1770 train_time:100708ms step_avg:97.40ms
step:1045/1770 train_time:100808ms step_avg:97.40ms
step:1046/1770 train_time:100908ms step_avg:97.40ms
step:1047/1770 train_time:101009ms step_avg:97.41ms
step:1048/1770 train_time:101111ms step_avg:97.41ms
step:1049/1770 train_time:101213ms step_avg:97.41ms
step:1050/1770 train_time:101314ms step_avg:97.42ms
step:1051/1770 train_time:101415ms step_avg:97.42ms
step:1052/1770 train_time:101516ms step_avg:97.42ms
step:1053/1770 train_time:101617ms step_avg:97.43ms
step:1054/1770 train_time:101717ms step_avg:97.43ms
step:1055/1770 train_time:101817ms step_avg:97.43ms
step:1056/1770 train_time:101917ms step_avg:97.44ms
step:1057/1770 train_time:102018ms step_avg:97.44ms
step:1058/1770 train_time:102119ms step_avg:97.44ms
step:1059/1770 train_time:102220ms step_avg:97.45ms
step:1060/1770 train_time:102322ms step_avg:97.45ms
step:1061/1770 train_time:102423ms step_avg:97.45ms
step:1062/1770 train_time:102525ms step_avg:97.46ms
step:1063/1770 train_time:102628ms step_avg:97.46ms
step:1064/1770 train_time:102729ms step_avg:97.47ms
step:1065/1770 train_time:102830ms step_avg:97.47ms
step:1066/1770 train_time:102932ms step_avg:97.47ms
step:1067/1770 train_time:103033ms step_avg:97.48ms
step:1068/1770 train_time:103135ms step_avg:97.48ms
step:1069/1770 train_time:103236ms step_avg:97.48ms
step:1070/1770 train_time:103337ms step_avg:97.49ms
step:1071/1770 train_time:103438ms step_avg:97.49ms
step:1072/1770 train_time:103539ms step_avg:97.49ms
step:1073/1770 train_time:103639ms step_avg:97.50ms
step:1074/1770 train_time:103739ms step_avg:97.50ms
step:1075/1770 train_time:103841ms step_avg:97.50ms
step:1076/1770 train_time:103942ms step_avg:97.51ms
step:1077/1770 train_time:104044ms step_avg:97.51ms
step:1078/1770 train_time:104146ms step_avg:97.51ms
step:1079/1770 train_time:104247ms step_avg:97.52ms
step:1080/1770 train_time:104348ms step_avg:97.52ms
step:1081/1770 train_time:104448ms step_avg:97.52ms
step:1082/1770 train_time:104549ms step_avg:97.53ms
step:1083/1770 train_time:104652ms step_avg:97.53ms
step:1084/1770 train_time:104754ms step_avg:97.54ms
step:1085/1770 train_time:104856ms step_avg:97.54ms
step:1086/1770 train_time:104956ms step_avg:97.54ms
step:1087/1770 train_time:105057ms step_avg:97.55ms
step:1088/1770 train_time:105157ms step_avg:97.55ms
step:1089/1770 train_time:105258ms step_avg:97.55ms
step:1090/1770 train_time:105359ms step_avg:97.55ms
step:1091/1770 train_time:105459ms step_avg:97.56ms
step:1092/1770 train_time:105560ms step_avg:97.56ms
step:1093/1770 train_time:105662ms step_avg:97.56ms
step:1094/1770 train_time:105765ms step_avg:97.57ms
step:1095/1770 train_time:105866ms step_avg:97.57ms
step:1096/1770 train_time:105967ms step_avg:97.58ms
step:1097/1770 train_time:106068ms step_avg:97.58ms
step:1098/1770 train_time:106169ms step_avg:97.58ms
step:1099/1770 train_time:106271ms step_avg:97.59ms
step:1100/1770 train_time:106372ms step_avg:97.59ms
step:1101/1770 train_time:106473ms step_avg:97.59ms
step:1102/1770 train_time:106574ms step_avg:97.60ms
step:1103/1770 train_time:106675ms step_avg:97.60ms
step:1104/1770 train_time:106778ms step_avg:97.60ms
step:1105/1770 train_time:106878ms step_avg:97.61ms
step:1106/1770 train_time:106979ms step_avg:97.61ms
step:1107/1770 train_time:107079ms step_avg:97.61ms
step:1108/1770 train_time:107180ms step_avg:97.61ms
step:1109/1770 train_time:107280ms step_avg:97.62ms
step:1110/1770 train_time:107382ms step_avg:97.62ms
step:1111/1770 train_time:107483ms step_avg:97.62ms
step:1112/1770 train_time:107585ms step_avg:97.63ms
step:1113/1770 train_time:107686ms step_avg:97.63ms
step:1114/1770 train_time:107788ms step_avg:97.63ms
step:1115/1770 train_time:107890ms step_avg:97.64ms
step:1116/1770 train_time:107991ms step_avg:97.64ms
step:1117/1770 train_time:108092ms step_avg:97.64ms
step:1118/1770 train_time:108194ms step_avg:97.65ms
step:1119/1770 train_time:108294ms step_avg:97.65ms
step:1120/1770 train_time:108395ms step_avg:97.65ms
step:1121/1770 train_time:108495ms step_avg:97.66ms
step:1122/1770 train_time:108596ms step_avg:97.66ms
step:1123/1770 train_time:108696ms step_avg:97.66ms
step:1124/1770 train_time:108797ms step_avg:97.66ms
step:1125/1770 train_time:108897ms step_avg:97.67ms
step:1125/1770 val_loss:3.4768 train_time:108997ms step_avg:97.75ms
step:1126/1770 train_time:109020ms step_avg:97.69ms
step:1127/1770 train_time:109110ms step_avg:97.68ms
step:1128/1770 train_time:109211ms step_avg:97.68ms
step:1129/1770 train_time:109312ms step_avg:97.69ms
step:1130/1770 train_time:109412ms step_avg:97.69ms
step:1131/1770 train_time:109514ms step_avg:97.69ms
step:1132/1770 train_time:109615ms step_avg:97.70ms
step:1133/1770 train_time:109715ms step_avg:97.70ms
step:1134/1770 train_time:109817ms step_avg:97.70ms
step:1135/1770 train_time:109918ms step_avg:97.70ms
step:1136/1770 train_time:110021ms step_avg:97.71ms
step:1137/1770 train_time:110124ms step_avg:97.71ms
step:1138/1770 train_time:110226ms step_avg:97.72ms
step:1139/1770 train_time:110326ms step_avg:97.72ms
step:1140/1770 train_time:110427ms step_avg:97.72ms
step:1141/1770 train_time:110527ms step_avg:97.73ms
step:1142/1770 train_time:110627ms step_avg:97.73ms
step:1143/1770 train_time:110727ms step_avg:97.73ms
step:1144/1770 train_time:110828ms step_avg:97.73ms
step:1145/1770 train_time:110928ms step_avg:97.73ms
step:1146/1770 train_time:111030ms step_avg:97.74ms
step:1147/1770 train_time:111131ms step_avg:97.74ms
step:1148/1770 train_time:111233ms step_avg:97.74ms
step:1149/1770 train_time:111334ms step_avg:97.75ms
step:1150/1770 train_time:111436ms step_avg:97.75ms
step:1151/1770 train_time:111539ms step_avg:97.76ms
step:1152/1770 train_time:111640ms step_avg:97.76ms
step:1153/1770 train_time:111741ms step_avg:97.76ms
step:1154/1770 train_time:111842ms step_avg:97.76ms
step:1155/1770 train_time:111944ms step_avg:97.77ms
step:1156/1770 train_time:112045ms step_avg:97.77ms
step:1157/1770 train_time:112147ms step_avg:97.77ms
step:1158/1770 train_time:112249ms step_avg:97.78ms
step:1159/1770 train_time:112350ms step_avg:97.78ms
step:1160/1770 train_time:112450ms step_avg:97.78ms
step:1161/1770 train_time:112551ms step_avg:97.79ms
step:1162/1770 train_time:112653ms step_avg:97.79ms
step:1163/1770 train_time:112755ms step_avg:97.79ms
step:1164/1770 train_time:112856ms step_avg:97.80ms
step:1165/1770 train_time:112957ms step_avg:97.80ms
step:1166/1770 train_time:113060ms step_avg:97.80ms
step:1167/1770 train_time:113161ms step_avg:97.81ms
step:1168/1770 train_time:113262ms step_avg:97.81ms
step:1169/1770 train_time:113363ms step_avg:97.81ms
step:1170/1770 train_time:113464ms step_avg:97.81ms
step:1171/1770 train_time:113567ms step_avg:97.82ms
step:1172/1770 train_time:113668ms step_avg:97.82ms
step:1173/1770 train_time:113768ms step_avg:97.82ms
step:1174/1770 train_time:113869ms step_avg:97.83ms
step:1175/1770 train_time:113970ms step_avg:97.83ms
step:1176/1770 train_time:114071ms step_avg:97.83ms
step:1177/1770 train_time:114172ms step_avg:97.83ms
step:1178/1770 train_time:114273ms step_avg:97.84ms
step:1179/1770 train_time:114374ms step_avg:97.84ms
step:1180/1770 train_time:114476ms step_avg:97.84ms
step:1181/1770 train_time:114579ms step_avg:97.85ms
step:1182/1770 train_time:114680ms step_avg:97.85ms
step:1183/1770 train_time:114781ms step_avg:97.85ms
step:1184/1770 train_time:114885ms step_avg:97.86ms
step:1185/1770 train_time:114987ms step_avg:97.86ms
step:1186/1770 train_time:115090ms step_avg:97.87ms
step:1187/1770 train_time:115193ms step_avg:97.87ms
step:1188/1770 train_time:115295ms step_avg:97.87ms
step:1189/1770 train_time:115396ms step_avg:97.88ms
step:1190/1770 train_time:115498ms step_avg:97.88ms
step:1191/1770 train_time:115601ms step_avg:97.88ms
step:1192/1770 train_time:115704ms step_avg:97.89ms
step:1193/1770 train_time:115806ms step_avg:97.89ms
step:1194/1770 train_time:115908ms step_avg:97.89ms
step:1195/1770 train_time:116010ms step_avg:97.90ms
step:1196/1770 train_time:116113ms step_avg:97.90ms
step:1197/1770 train_time:116215ms step_avg:97.91ms
step:1198/1770 train_time:116317ms step_avg:97.91ms
step:1199/1770 train_time:116419ms step_avg:97.91ms
step:1200/1770 train_time:116522ms step_avg:97.92ms
step:1201/1770 train_time:116624ms step_avg:97.92ms
step:1202/1770 train_time:116726ms step_avg:97.92ms
step:1203/1770 train_time:116828ms step_avg:97.93ms
step:1204/1770 train_time:116930ms step_avg:97.93ms
step:1205/1770 train_time:117031ms step_avg:97.93ms
step:1206/1770 train_time:117134ms step_avg:97.94ms
step:1207/1770 train_time:117236ms step_avg:97.94ms
step:1208/1770 train_time:117338ms step_avg:97.95ms
step:1209/1770 train_time:117440ms step_avg:97.95ms
step:1210/1770 train_time:117542ms step_avg:97.95ms
step:1211/1770 train_time:117645ms step_avg:97.96ms
step:1212/1770 train_time:117748ms step_avg:97.96ms
step:1213/1770 train_time:117850ms step_avg:97.96ms
step:1214/1770 train_time:117952ms step_avg:97.97ms
step:1215/1770 train_time:118055ms step_avg:97.97ms
step:1216/1770 train_time:118159ms step_avg:97.98ms
step:1217/1770 train_time:118261ms step_avg:97.98ms
step:1218/1770 train_time:118363ms step_avg:97.98ms
step:1219/1770 train_time:118464ms step_avg:97.99ms
step:1220/1770 train_time:118567ms step_avg:97.99ms
step:1221/1770 train_time:118669ms step_avg:97.99ms
step:1222/1770 train_time:118772ms step_avg:98.00ms
step:1223/1770 train_time:118873ms step_avg:98.00ms
step:1224/1770 train_time:118977ms step_avg:98.00ms
step:1225/1770 train_time:119079ms step_avg:98.01ms
step:1226/1770 train_time:119181ms step_avg:98.01ms
step:1227/1770 train_time:119285ms step_avg:98.02ms
step:1228/1770 train_time:119389ms step_avg:98.02ms
step:1229/1770 train_time:119491ms step_avg:98.02ms
step:1230/1770 train_time:119594ms step_avg:98.03ms
step:1231/1770 train_time:119697ms step_avg:98.03ms
step:1232/1770 train_time:119798ms step_avg:98.03ms
step:1233/1770 train_time:119901ms step_avg:98.04ms
step:1234/1770 train_time:120003ms step_avg:98.04ms
step:1235/1770 train_time:120104ms step_avg:98.04ms
step:1236/1770 train_time:120207ms step_avg:98.05ms
step:1237/1770 train_time:120309ms step_avg:98.05ms
step:1238/1770 train_time:120411ms step_avg:98.05ms
step:1239/1770 train_time:120513ms step_avg:98.06ms
step:1240/1770 train_time:120614ms step_avg:98.06ms
step:1241/1770 train_time:120717ms step_avg:98.06ms
step:1242/1770 train_time:120820ms step_avg:98.07ms
step:1243/1770 train_time:120922ms step_avg:98.07ms
step:1244/1770 train_time:121024ms step_avg:98.07ms
step:1245/1770 train_time:121126ms step_avg:98.08ms
step:1246/1770 train_time:121229ms step_avg:98.08ms
step:1247/1770 train_time:121330ms step_avg:98.08ms
step:1248/1770 train_time:121433ms step_avg:98.09ms
step:1249/1770 train_time:121534ms step_avg:98.09ms
step:1250/1770 train_time:121637ms step_avg:98.09ms
step:1250/1770 val_loss:3.4280 train_time:121739ms step_avg:98.18ms
step:1251/1770 train_time:121761ms step_avg:98.12ms
step:1252/1770 train_time:121848ms step_avg:98.11ms
step:1253/1770 train_time:121951ms step_avg:98.11ms
step:1254/1770 train_time:122055ms step_avg:98.11ms
step:1255/1770 train_time:122159ms step_avg:98.12ms
step:1256/1770 train_time:122261ms step_avg:98.12ms
step:1257/1770 train_time:122362ms step_avg:98.12ms
step:1258/1770 train_time:122464ms step_avg:98.13ms
step:1259/1770 train_time:122565ms step_avg:98.13ms
step:1260/1770 train_time:122667ms step_avg:98.13ms
step:1261/1770 train_time:122771ms step_avg:98.14ms
step:1262/1770 train_time:122874ms step_avg:98.14ms
step:1263/1770 train_time:122976ms step_avg:98.14ms
step:1264/1770 train_time:123079ms step_avg:98.15ms
step:1265/1770 train_time:123180ms step_avg:98.15ms
step:1266/1770 train_time:123282ms step_avg:98.15ms
step:1267/1770 train_time:123384ms step_avg:98.16ms
step:1268/1770 train_time:123487ms step_avg:98.16ms
step:1269/1770 train_time:123588ms step_avg:98.16ms
step:1270/1770 train_time:123691ms step_avg:98.17ms
step:1271/1770 train_time:123794ms step_avg:98.17ms
step:1272/1770 train_time:123896ms step_avg:98.17ms
step:1273/1770 train_time:123998ms step_avg:98.18ms
step:1274/1770 train_time:124101ms step_avg:98.18ms
step:1275/1770 train_time:124203ms step_avg:98.18ms
step:1276/1770 train_time:124305ms step_avg:98.19ms
step:1277/1770 train_time:124407ms step_avg:98.19ms
step:1278/1770 train_time:124510ms step_avg:98.19ms
step:1279/1770 train_time:124612ms step_avg:98.20ms
step:1280/1770 train_time:124715ms step_avg:98.20ms
step:1281/1770 train_time:124817ms step_avg:98.20ms
step:1282/1770 train_time:124921ms step_avg:98.21ms
step:1283/1770 train_time:125024ms step_avg:98.21ms
step:1284/1770 train_time:125126ms step_avg:98.22ms
step:1285/1770 train_time:125228ms step_avg:98.22ms
step:1286/1770 train_time:125332ms step_avg:98.22ms
step:1287/1770 train_time:125435ms step_avg:98.23ms
step:1288/1770 train_time:125538ms step_avg:98.23ms
step:1289/1770 train_time:125640ms step_avg:98.23ms
step:1290/1770 train_time:125742ms step_avg:98.24ms
step:1291/1770 train_time:125843ms step_avg:98.24ms
step:1292/1770 train_time:125945ms step_avg:98.24ms
step:1293/1770 train_time:126048ms step_avg:98.24ms
step:1294/1770 train_time:126149ms step_avg:98.25ms
step:1295/1770 train_time:126252ms step_avg:98.25ms
step:1296/1770 train_time:126354ms step_avg:98.25ms
step:1297/1770 train_time:126457ms step_avg:98.26ms
step:1298/1770 train_time:126558ms step_avg:98.26ms
step:1299/1770 train_time:126660ms step_avg:98.26ms
step:1300/1770 train_time:126762ms step_avg:98.26ms
step:1301/1770 train_time:126864ms step_avg:98.27ms
step:1302/1770 train_time:126966ms step_avg:98.27ms
step:1303/1770 train_time:127067ms step_avg:98.27ms
step:1304/1770 train_time:127169ms step_avg:98.28ms
step:1305/1770 train_time:127271ms step_avg:98.28ms
step:1306/1770 train_time:127374ms step_avg:98.28ms
step:1307/1770 train_time:127477ms step_avg:98.29ms
step:1308/1770 train_time:127580ms step_avg:98.29ms
step:1309/1770 train_time:127682ms step_avg:98.29ms
step:1310/1770 train_time:127784ms step_avg:98.30ms
step:1311/1770 train_time:127885ms step_avg:98.30ms
step:1312/1770 train_time:127986ms step_avg:98.30ms
step:1313/1770 train_time:128088ms step_avg:98.30ms
step:1314/1770 train_time:128190ms step_avg:98.31ms
step:1315/1770 train_time:128292ms step_avg:98.31ms
step:1316/1770 train_time:128395ms step_avg:98.31ms
step:1317/1770 train_time:128498ms step_avg:98.31ms
step:1318/1770 train_time:128604ms step_avg:98.32ms
step:1319/1770 train_time:128707ms step_avg:98.32ms
step:1320/1770 train_time:128809ms step_avg:98.33ms
step:1321/1770 train_time:128911ms step_avg:98.33ms
step:1322/1770 train_time:129014ms step_avg:98.33ms
step:1323/1770 train_time:129117ms step_avg:98.34ms
step:1324/1770 train_time:129220ms step_avg:98.34ms
step:1325/1770 train_time:129323ms step_avg:98.34ms
step:1326/1770 train_time:129425ms step_avg:98.35ms
step:1327/1770 train_time:129530ms step_avg:98.35ms
step:1328/1770 train_time:129633ms step_avg:98.36ms
step:1329/1770 train_time:129736ms step_avg:98.36ms
step:1330/1770 train_time:129838ms step_avg:98.36ms
step:1331/1770 train_time:129940ms step_avg:98.36ms
step:1332/1770 train_time:130042ms step_avg:98.37ms
step:1333/1770 train_time:130143ms step_avg:98.37ms
step:1334/1770 train_time:130245ms step_avg:98.37ms
step:1335/1770 train_time:130346ms step_avg:98.37ms
step:1336/1770 train_time:130448ms step_avg:98.38ms
step:1337/1770 train_time:130550ms step_avg:98.38ms
step:1338/1770 train_time:130653ms step_avg:98.38ms
step:1339/1770 train_time:130757ms step_avg:98.39ms
step:1340/1770 train_time:130861ms step_avg:98.39ms
step:1341/1770 train_time:130963ms step_avg:98.39ms
step:1342/1770 train_time:131066ms step_avg:98.40ms
step:1343/1770 train_time:131168ms step_avg:98.40ms
step:1344/1770 train_time:131270ms step_avg:98.40ms
step:1345/1770 train_time:131372ms step_avg:98.41ms
step:1346/1770 train_time:131475ms step_avg:98.41ms
step:1347/1770 train_time:131578ms step_avg:98.41ms
step:1348/1770 train_time:131682ms step_avg:98.42ms
step:1349/1770 train_time:131784ms step_avg:98.42ms
step:1350/1770 train_time:131886ms step_avg:98.42ms
step:1351/1770 train_time:131988ms step_avg:98.43ms
step:1352/1770 train_time:132091ms step_avg:98.43ms
step:1353/1770 train_time:132194ms step_avg:98.43ms
step:1354/1770 train_time:132296ms step_avg:98.43ms
step:1355/1770 train_time:132398ms step_avg:98.44ms
step:1356/1770 train_time:132500ms step_avg:98.44ms
step:1357/1770 train_time:132601ms step_avg:98.44ms
step:1358/1770 train_time:132704ms step_avg:98.45ms
step:1359/1770 train_time:132807ms step_avg:98.45ms
step:1360/1770 train_time:132909ms step_avg:98.45ms
step:1361/1770 train_time:133012ms step_avg:98.45ms
step:1362/1770 train_time:133114ms step_avg:98.46ms
step:1363/1770 train_time:133217ms step_avg:98.46ms
step:1364/1770 train_time:133320ms step_avg:98.46ms
step:1365/1770 train_time:133422ms step_avg:98.47ms
step:1366/1770 train_time:133524ms step_avg:98.47ms
step:1367/1770 train_time:133626ms step_avg:98.47ms
step:1368/1770 train_time:133728ms step_avg:98.47ms
step:1369/1770 train_time:133831ms step_avg:98.48ms
step:1370/1770 train_time:133934ms step_avg:98.48ms
step:1371/1770 train_time:134036ms step_avg:98.48ms
step:1372/1770 train_time:134139ms step_avg:98.49ms
step:1373/1770 train_time:134241ms step_avg:98.49ms
step:1374/1770 train_time:134344ms step_avg:98.49ms
step:1375/1770 train_time:134446ms step_avg:98.50ms
step:1375/1770 val_loss:3.3846 train_time:134547ms step_avg:98.57ms
step:1376/1770 train_time:134569ms step_avg:98.51ms
step:1377/1770 train_time:134660ms step_avg:98.51ms
step:1378/1770 train_time:134762ms step_avg:98.51ms
step:1379/1770 train_time:134863ms step_avg:98.51ms
step:1380/1770 train_time:134966ms step_avg:98.52ms
step:1381/1770 train_time:135067ms step_avg:98.52ms
step:1382/1770 train_time:135169ms step_avg:98.52ms
step:1383/1770 train_time:135272ms step_avg:98.52ms
step:1384/1770 train_time:135374ms step_avg:98.53ms
step:1385/1770 train_time:135477ms step_avg:98.53ms
step:1386/1770 train_time:135579ms step_avg:98.53ms
step:1387/1770 train_time:135684ms step_avg:98.54ms
step:1388/1770 train_time:135785ms step_avg:98.54ms
step:1389/1770 train_time:135888ms step_avg:98.54ms
step:1390/1770 train_time:135990ms step_avg:98.54ms
step:1391/1770 train_time:136091ms step_avg:98.55ms
step:1392/1770 train_time:136195ms step_avg:98.55ms
step:1393/1770 train_time:136296ms step_avg:98.55ms
step:1394/1770 train_time:136398ms step_avg:98.55ms
step:1395/1770 train_time:136501ms step_avg:98.56ms
step:1396/1770 train_time:136605ms step_avg:98.56ms
step:1397/1770 train_time:136706ms step_avg:98.56ms
step:1398/1770 train_time:136809ms step_avg:98.57ms
step:1399/1770 train_time:136912ms step_avg:98.57ms
step:1400/1770 train_time:137015ms step_avg:98.57ms
step:1401/1770 train_time:137117ms step_avg:98.57ms
step:1402/1770 train_time:137220ms step_avg:98.58ms
step:1403/1770 train_time:137321ms step_avg:98.58ms
step:1404/1770 train_time:137424ms step_avg:98.58ms
step:1405/1770 train_time:137527ms step_avg:98.59ms
step:1406/1770 train_time:137629ms step_avg:98.59ms
step:1407/1770 train_time:137731ms step_avg:98.59ms
step:1408/1770 train_time:137834ms step_avg:98.59ms
step:1409/1770 train_time:137937ms step_avg:98.60ms
step:1410/1770 train_time:138038ms step_avg:98.60ms
step:1411/1770 train_time:138140ms step_avg:98.60ms
step:1412/1770 train_time:138241ms step_avg:98.60ms
step:1413/1770 train_time:138343ms step_avg:98.61ms
step:1414/1770 train_time:138446ms step_avg:98.61ms
step:1415/1770 train_time:138549ms step_avg:98.61ms
step:1416/1770 train_time:138652ms step_avg:98.61ms
step:1417/1770 train_time:138754ms step_avg:98.62ms
step:1418/1770 train_time:138856ms step_avg:98.62ms
step:1419/1770 train_time:138960ms step_avg:98.62ms
step:1420/1770 train_time:139061ms step_avg:98.63ms
step:1421/1770 train_time:139164ms step_avg:98.63ms
step:1422/1770 train_time:139265ms step_avg:98.63ms
step:1423/1770 train_time:139367ms step_avg:98.63ms
step:1424/1770 train_time:139470ms step_avg:98.64ms
step:1425/1770 train_time:139573ms step_avg:98.64ms
step:1426/1770 train_time:139676ms step_avg:98.64ms
step:1427/1770 train_time:139777ms step_avg:98.64ms
step:1428/1770 train_time:139881ms step_avg:98.65ms
step:1429/1770 train_time:139984ms step_avg:98.65ms
step:1430/1770 train_time:140085ms step_avg:98.65ms
step:1431/1770 train_time:140189ms step_avg:98.65ms
step:1432/1770 train_time:140290ms step_avg:98.66ms
step:1433/1770 train_time:140392ms step_avg:98.66ms
step:1434/1770 train_time:140494ms step_avg:98.66ms
step:1435/1770 train_time:140596ms step_avg:98.66ms
step:1436/1770 train_time:140699ms step_avg:98.67ms
step:1437/1770 train_time:140801ms step_avg:98.67ms
step:1438/1770 train_time:140903ms step_avg:98.67ms
step:1439/1770 train_time:141005ms step_avg:98.67ms
step:1440/1770 train_time:141107ms step_avg:98.68ms
step:1441/1770 train_time:141212ms step_avg:98.68ms
step:1442/1770 train_time:141313ms step_avg:98.68ms
step:1443/1770 train_time:141416ms step_avg:98.68ms
step:1444/1770 train_time:141518ms step_avg:98.69ms
step:1445/1770 train_time:141621ms step_avg:98.69ms
step:1446/1770 train_time:141724ms step_avg:98.69ms
step:1447/1770 train_time:141827ms step_avg:98.70ms
step:1448/1770 train_time:141931ms step_avg:98.70ms
step:1449/1770 train_time:142035ms step_avg:98.70ms
step:1450/1770 train_time:142137ms step_avg:98.71ms
step:1451/1770 train_time:142240ms step_avg:98.71ms
step:1452/1770 train_time:142343ms step_avg:98.71ms
step:1453/1770 train_time:142446ms step_avg:98.72ms
step:1454/1770 train_time:142550ms step_avg:98.72ms
step:1455/1770 train_time:142654ms step_avg:98.72ms
step:1456/1770 train_time:142758ms step_avg:98.73ms
step:1457/1770 train_time:142861ms step_avg:98.73ms
step:1458/1770 train_time:142964ms step_avg:98.73ms
step:1459/1770 train_time:143069ms step_avg:98.74ms
step:1460/1770 train_time:143172ms step_avg:98.74ms
step:1461/1770 train_time:143276ms step_avg:98.74ms
step:1462/1770 train_time:143379ms step_avg:98.75ms
step:1463/1770 train_time:143483ms step_avg:98.75ms
step:1464/1770 train_time:143588ms step_avg:98.75ms
step:1465/1770 train_time:143691ms step_avg:98.76ms
step:1466/1770 train_time:143795ms step_avg:98.76ms
step:1467/1770 train_time:143899ms step_avg:98.76ms
step:1468/1770 train_time:144003ms step_avg:98.77ms
step:1469/1770 train_time:144105ms step_avg:98.77ms
step:1470/1770 train_time:144208ms step_avg:98.77ms
step:1471/1770 train_time:144311ms step_avg:98.78ms
step:1472/1770 train_time:144414ms step_avg:98.78ms
step:1473/1770 train_time:144519ms step_avg:98.78ms
step:1474/1770 train_time:144623ms step_avg:98.79ms
step:1475/1770 train_time:144726ms step_avg:98.79ms
step:1476/1770 train_time:144829ms step_avg:98.79ms
step:1477/1770 train_time:144935ms step_avg:98.80ms
step:1478/1770 train_time:145038ms step_avg:98.80ms
step:1479/1770 train_time:145141ms step_avg:98.80ms
step:1480/1770 train_time:145244ms step_avg:98.81ms
step:1481/1770 train_time:145351ms step_avg:98.81ms
step:1482/1770 train_time:145454ms step_avg:98.81ms
step:1483/1770 train_time:145559ms step_avg:98.82ms
step:1484/1770 train_time:145662ms step_avg:98.82ms
step:1485/1770 train_time:145765ms step_avg:98.82ms
step:1486/1770 train_time:145868ms step_avg:98.83ms
step:1487/1770 train_time:145972ms step_avg:98.83ms
step:1488/1770 train_time:146076ms step_avg:98.83ms
step:1489/1770 train_time:146181ms step_avg:98.84ms
step:1490/1770 train_time:146284ms step_avg:98.84ms
step:1491/1770 train_time:146387ms step_avg:98.84ms
step:1492/1770 train_time:146490ms step_avg:98.85ms
step:1493/1770 train_time:146596ms step_avg:98.85ms
step:1494/1770 train_time:146702ms step_avg:98.86ms
step:1495/1770 train_time:146805ms step_avg:98.86ms
step:1496/1770 train_time:146908ms step_avg:98.86ms
step:1497/1770 train_time:147011ms step_avg:98.86ms
step:1498/1770 train_time:147113ms step_avg:98.87ms
step:1499/1770 train_time:147216ms step_avg:98.87ms
step:1500/1770 train_time:147319ms step_avg:98.87ms
step:1500/1770 val_loss:3.3470 train_time:147421ms step_avg:98.94ms
step:1501/1770 train_time:147443ms step_avg:98.89ms
step:1502/1770 train_time:147535ms step_avg:98.88ms
step:1503/1770 train_time:147636ms step_avg:98.89ms
step:1504/1770 train_time:147740ms step_avg:98.89ms
step:1505/1770 train_time:147844ms step_avg:98.89ms
step:1506/1770 train_time:147948ms step_avg:98.90ms
step:1507/1770 train_time:148052ms step_avg:98.90ms
step:1508/1770 train_time:148157ms step_avg:98.90ms
step:1509/1770 train_time:148260ms step_avg:98.91ms
step:1510/1770 train_time:148363ms step_avg:98.91ms
step:1511/1770 train_time:148469ms step_avg:98.91ms
step:1512/1770 train_time:148574ms step_avg:98.92ms
step:1513/1770 train_time:148678ms step_avg:98.92ms
step:1514/1770 train_time:148781ms step_avg:98.92ms
step:1515/1770 train_time:148884ms step_avg:98.93ms
step:1516/1770 train_time:148987ms step_avg:98.93ms
step:1517/1770 train_time:149090ms step_avg:98.93ms
step:1518/1770 train_time:149196ms step_avg:98.94ms
step:1519/1770 train_time:149297ms step_avg:98.94ms
step:1520/1770 train_time:149402ms step_avg:98.94ms
step:1521/1770 train_time:149505ms step_avg:98.94ms
step:1522/1770 train_time:149609ms step_avg:98.95ms
step:1523/1770 train_time:149713ms step_avg:98.95ms
step:1524/1770 train_time:149816ms step_avg:98.95ms
step:1525/1770 train_time:149919ms step_avg:98.96ms
step:1526/1770 train_time:150023ms step_avg:98.96ms
step:1527/1770 train_time:150127ms step_avg:98.96ms
step:1528/1770 train_time:150233ms step_avg:98.97ms
step:1529/1770 train_time:150336ms step_avg:98.97ms
step:1530/1770 train_time:150438ms step_avg:98.97ms
step:1531/1770 train_time:150542ms step_avg:98.98ms
step:1532/1770 train_time:150646ms step_avg:98.98ms
step:1533/1770 train_time:150750ms step_avg:98.98ms
step:1534/1770 train_time:150854ms step_avg:98.99ms
step:1535/1770 train_time:150957ms step_avg:98.99ms
step:1536/1770 train_time:151060ms step_avg:98.99ms
step:1537/1770 train_time:151165ms step_avg:98.99ms
step:1538/1770 train_time:151269ms step_avg:99.00ms
step:1539/1770 train_time:151371ms step_avg:99.00ms
step:1540/1770 train_time:151477ms step_avg:99.00ms
step:1541/1770 train_time:151582ms step_avg:99.01ms
step:1542/1770 train_time:151685ms step_avg:99.01ms
step:1543/1770 train_time:151787ms step_avg:99.01ms
step:1544/1770 train_time:151893ms step_avg:99.02ms
step:1545/1770 train_time:151996ms step_avg:99.02ms
step:1546/1770 train_time:152099ms step_avg:99.02ms
step:1547/1770 train_time:152203ms step_avg:99.03ms
step:1548/1770 train_time:152306ms step_avg:99.03ms
step:1549/1770 train_time:152411ms step_avg:99.03ms
step:1550/1770 train_time:152515ms step_avg:99.04ms
step:1551/1770 train_time:152617ms step_avg:99.04ms
step:1552/1770 train_time:152723ms step_avg:99.04ms
step:1553/1770 train_time:152827ms step_avg:99.05ms
step:1554/1770 train_time:152930ms step_avg:99.05ms
step:1555/1770 train_time:153034ms step_avg:99.05ms
step:1556/1770 train_time:153137ms step_avg:99.05ms
step:1557/1770 train_time:153240ms step_avg:99.06ms
step:1558/1770 train_time:153343ms step_avg:99.06ms
step:1559/1770 train_time:153447ms step_avg:99.06ms
step:1560/1770 train_time:153549ms step_avg:99.06ms
step:1561/1770 train_time:153655ms step_avg:99.07ms
step:1562/1770 train_time:153758ms step_avg:99.07ms
step:1563/1770 train_time:153862ms step_avg:99.07ms
step:1564/1770 train_time:153965ms step_avg:99.08ms
step:1565/1770 train_time:154068ms step_avg:99.08ms
step:1566/1770 train_time:154172ms step_avg:99.08ms
step:1567/1770 train_time:154274ms step_avg:99.08ms
step:1568/1770 train_time:154378ms step_avg:99.09ms
step:1569/1770 train_time:154485ms step_avg:99.09ms
step:1570/1770 train_time:154588ms step_avg:99.10ms
step:1571/1770 train_time:154691ms step_avg:99.10ms
step:1572/1770 train_time:154795ms step_avg:99.10ms
step:1573/1770 train_time:154901ms step_avg:99.10ms
step:1574/1770 train_time:155004ms step_avg:99.11ms
step:1575/1770 train_time:155106ms step_avg:99.11ms
step:1576/1770 train_time:155209ms step_avg:99.11ms
step:1577/1770 train_time:155314ms step_avg:99.12ms
step:1578/1770 train_time:155420ms step_avg:99.12ms
step:1579/1770 train_time:155523ms step_avg:99.12ms
step:1580/1770 train_time:155626ms step_avg:99.12ms
step:1581/1770 train_time:155732ms step_avg:99.13ms
step:1582/1770 train_time:155836ms step_avg:99.13ms
step:1583/1770 train_time:155939ms step_avg:99.13ms
step:1584/1770 train_time:156043ms step_avg:99.14ms
step:1585/1770 train_time:156146ms step_avg:99.14ms
step:1586/1770 train_time:156254ms step_avg:99.15ms
step:1587/1770 train_time:156358ms step_avg:99.15ms
step:1588/1770 train_time:156462ms step_avg:99.15ms
step:1589/1770 train_time:156567ms step_avg:99.16ms
step:1590/1770 train_time:156669ms step_avg:99.16ms
step:1591/1770 train_time:156772ms step_avg:99.16ms
step:1592/1770 train_time:156877ms step_avg:99.16ms
step:1593/1770 train_time:156979ms step_avg:99.17ms
step:1594/1770 train_time:157083ms step_avg:99.17ms
step:1595/1770 train_time:157186ms step_avg:99.17ms
step:1596/1770 train_time:157291ms step_avg:99.17ms
step:1597/1770 train_time:157394ms step_avg:99.18ms
step:1598/1770 train_time:157498ms step_avg:99.18ms
step:1599/1770 train_time:157603ms step_avg:99.18ms
step:1600/1770 train_time:157709ms step_avg:99.19ms
step:1601/1770 train_time:157813ms step_avg:99.19ms
step:1602/1770 train_time:157917ms step_avg:99.19ms
step:1603/1770 train_time:158020ms step_avg:99.20ms
step:1604/1770 train_time:158123ms step_avg:99.20ms
step:1605/1770 train_time:158226ms step_avg:99.20ms
step:1606/1770 train_time:158331ms step_avg:99.20ms
step:1607/1770 train_time:158437ms step_avg:99.21ms
step:1608/1770 train_time:158541ms step_avg:99.21ms
step:1609/1770 train_time:158644ms step_avg:99.21ms
step:1610/1770 train_time:158750ms step_avg:99.22ms
step:1611/1770 train_time:158856ms step_avg:99.22ms
step:1612/1770 train_time:158960ms step_avg:99.23ms
step:1613/1770 train_time:159063ms step_avg:99.23ms
step:1614/1770 train_time:159167ms step_avg:99.23ms
step:1615/1770 train_time:159270ms step_avg:99.23ms
step:1616/1770 train_time:159373ms step_avg:99.24ms
step:1617/1770 train_time:159479ms step_avg:99.24ms
step:1618/1770 train_time:159583ms step_avg:99.24ms
step:1619/1770 train_time:159686ms step_avg:99.25ms
step:1620/1770 train_time:159790ms step_avg:99.25ms
step:1621/1770 train_time:159893ms step_avg:99.25ms
step:1622/1770 train_time:159997ms step_avg:99.25ms
step:1623/1770 train_time:160104ms step_avg:99.26ms
step:1624/1770 train_time:160207ms step_avg:99.26ms
step:1625/1770 train_time:160309ms step_avg:99.26ms
step:1625/1770 val_loss:3.3122 train_time:160412ms step_avg:99.33ms
step:1626/1770 train_time:160433ms step_avg:99.28ms
step:1627/1770 train_time:160522ms step_avg:99.27ms
step:1628/1770 train_time:160626ms step_avg:99.27ms
step:1629/1770 train_time:160728ms step_avg:99.28ms
step:1630/1770 train_time:160831ms step_avg:99.28ms
step:1631/1770 train_time:160934ms step_avg:99.28ms
step:1632/1770 train_time:161037ms step_avg:99.28ms
step:1633/1770 train_time:161140ms step_avg:99.29ms
step:1634/1770 train_time:161243ms step_avg:99.29ms
step:1635/1770 train_time:161347ms step_avg:99.29ms
step:1636/1770 train_time:161451ms step_avg:99.29ms
step:1637/1770 train_time:161557ms step_avg:99.30ms
step:1638/1770 train_time:161659ms step_avg:99.30ms
step:1639/1770 train_time:161763ms step_avg:99.30ms
step:1640/1770 train_time:161868ms step_avg:99.31ms
step:1641/1770 train_time:161971ms step_avg:99.31ms
step:1642/1770 train_time:162074ms step_avg:99.31ms
step:1643/1770 train_time:162178ms step_avg:99.31ms
step:1644/1770 train_time:162282ms step_avg:99.32ms
step:1645/1770 train_time:162385ms step_avg:99.32ms
step:1646/1770 train_time:162491ms step_avg:99.32ms
step:1647/1770 train_time:162596ms step_avg:99.33ms
step:1648/1770 train_time:162700ms step_avg:99.33ms
step:1649/1770 train_time:162803ms step_avg:99.33ms
step:1650/1770 train_time:162906ms step_avg:99.33ms
step:1651/1770 train_time:163009ms step_avg:99.34ms
step:1652/1770 train_time:163112ms step_avg:99.34ms
step:1653/1770 train_time:163217ms step_avg:99.34ms
step:1654/1770 train_time:163325ms step_avg:99.35ms
step:1655/1770 train_time:163430ms step_avg:99.35ms
step:1656/1770 train_time:163534ms step_avg:99.35ms
step:1657/1770 train_time:163639ms step_avg:99.36ms
step:1658/1770 train_time:163743ms step_avg:99.36ms
step:1659/1770 train_time:163847ms step_avg:99.36ms
step:1660/1770 train_time:163950ms step_avg:99.36ms
step:1661/1770 train_time:164055ms step_avg:99.37ms
step:1662/1770 train_time:164159ms step_avg:99.37ms
step:1663/1770 train_time:164261ms step_avg:99.37ms
step:1664/1770 train_time:164365ms step_avg:99.37ms
step:1665/1770 train_time:164469ms step_avg:99.38ms
step:1666/1770 train_time:164573ms step_avg:99.38ms
step:1667/1770 train_time:164677ms step_avg:99.38ms
step:1668/1770 train_time:164779ms step_avg:99.38ms
step:1669/1770 train_time:164881ms step_avg:99.39ms
step:1670/1770 train_time:164985ms step_avg:99.39ms
step:1671/1770 train_time:165089ms step_avg:99.39ms
step:1672/1770 train_time:165193ms step_avg:99.39ms
step:1673/1770 train_time:165300ms step_avg:99.40ms
step:1674/1770 train_time:165403ms step_avg:99.40ms
step:1675/1770 train_time:165506ms step_avg:99.40ms
step:1676/1770 train_time:165611ms step_avg:99.41ms
step:1677/1770 train_time:165718ms step_avg:99.41ms
step:1678/1770 train_time:165821ms step_avg:99.41ms
step:1679/1770 train_time:165924ms step_avg:99.42ms
step:1680/1770 train_time:166027ms step_avg:99.42ms
step:1681/1770 train_time:166131ms step_avg:99.42ms
step:1682/1770 train_time:166238ms step_avg:99.42ms
step:1683/1770 train_time:166340ms step_avg:99.43ms
step:1684/1770 train_time:166443ms step_avg:99.43ms
step:1685/1770 train_time:166547ms step_avg:99.43ms
step:1686/1770 train_time:166651ms step_avg:99.43ms
step:1687/1770 train_time:166756ms step_avg:99.44ms
step:1688/1770 train_time:166858ms step_avg:99.44ms
step:1689/1770 train_time:166962ms step_avg:99.44ms
step:1690/1770 train_time:167065ms step_avg:99.44ms
step:1691/1770 train_time:167169ms step_avg:99.45ms
step:1692/1770 train_time:167273ms step_avg:99.45ms
step:1693/1770 train_time:167379ms step_avg:99.45ms
step:1694/1770 train_time:167482ms step_avg:99.45ms
step:1695/1770 train_time:167586ms step_avg:99.46ms
step:1696/1770 train_time:167691ms step_avg:99.46ms
step:1697/1770 train_time:167797ms step_avg:99.46ms
step:1698/1770 train_time:167900ms step_avg:99.47ms
step:1699/1770 train_time:168004ms step_avg:99.47ms
step:1700/1770 train_time:168107ms step_avg:99.47ms
step:1701/1770 train_time:168209ms step_avg:99.47ms
step:1702/1770 train_time:168314ms step_avg:99.48ms
step:1703/1770 train_time:168416ms step_avg:99.48ms
step:1704/1770 train_time:168519ms step_avg:99.48ms
step:1705/1770 train_time:168621ms step_avg:99.48ms
step:1706/1770 train_time:168724ms step_avg:99.48ms
step:1707/1770 train_time:168830ms step_avg:99.49ms
step:1708/1770 train_time:168934ms step_avg:99.49ms
step:1709/1770 train_time:169038ms step_avg:99.49ms
step:1710/1770 train_time:169145ms step_avg:99.50ms
step:1711/1770 train_time:169251ms step_avg:99.50ms
step:1712/1770 train_time:169355ms step_avg:99.50ms
step:1713/1770 train_time:169459ms step_avg:99.51ms
step:1714/1770 train_time:169562ms step_avg:99.51ms
step:1715/1770 train_time:169666ms step_avg:99.51ms
step:1716/1770 train_time:169771ms step_avg:99.51ms
step:1717/1770 train_time:169875ms step_avg:99.52ms
step:1718/1770 train_time:169980ms step_avg:99.52ms
step:1719/1770 train_time:170085ms step_avg:99.52ms
step:1720/1770 train_time:170190ms step_avg:99.53ms
step:1721/1770 train_time:170293ms step_avg:99.53ms
step:1722/1770 train_time:170400ms step_avg:99.53ms
step:1723/1770 train_time:170506ms step_avg:99.54ms
step:1724/1770 train_time:170611ms step_avg:99.54ms
step:1725/1770 train_time:170717ms step_avg:99.54ms
step:1726/1770 train_time:170823ms step_avg:99.55ms
step:1727/1770 train_time:170928ms step_avg:99.55ms
step:1728/1770 train_time:171033ms step_avg:99.55ms
step:1729/1770 train_time:171136ms step_avg:99.56ms
step:1730/1770 train_time:171242ms step_avg:99.56ms
step:1731/1770 train_time:171347ms step_avg:99.56ms
step:1732/1770 train_time:171451ms step_avg:99.57ms
step:1733/1770 train_time:171557ms step_avg:99.57ms
step:1734/1770 train_time:171660ms step_avg:99.57ms
step:1735/1770 train_time:171765ms step_avg:99.57ms
step:1736/1770 train_time:171870ms step_avg:99.58ms
step:1737/1770 train_time:171974ms step_avg:99.58ms
step:1738/1770 train_time:172078ms step_avg:99.58ms
step:1739/1770 train_time:172183ms step_avg:99.59ms
step:1740/1770 train_time:172286ms step_avg:99.59ms
step:1741/1770 train_time:172393ms step_avg:99.59ms
step:1742/1770 train_time:172500ms step_avg:99.60ms
step:1743/1770 train_time:172606ms step_avg:99.60ms
step:1744/1770 train_time:172710ms step_avg:99.60ms
step:1745/1770 train_time:172814ms step_avg:99.60ms
step:1746/1770 train_time:172921ms step_avg:99.61ms
step:1747/1770 train_time:173024ms step_avg:99.61ms
step:1748/1770 train_time:173130ms step_avg:99.61ms
step:1749/1770 train_time:173234ms step_avg:99.62ms
step:1750/1770 train_time:173339ms step_avg:99.62ms
step:1750/1770 val_loss:3.2855 train_time:173441ms step_avg:99.68ms
step:1751/1770 train_time:173463ms step_avg:99.63ms
step:1752/1770 train_time:173556ms step_avg:99.63ms
step:1753/1770 train_time:173660ms step_avg:99.63ms
step:1754/1770 train_time:173765ms step_avg:99.64ms
step:1755/1770 train_time:173869ms step_avg:99.64ms
step:1756/1770 train_time:173973ms step_avg:99.64ms
step:1757/1770 train_time:174078ms step_avg:99.64ms
step:1758/1770 train_time:174181ms step_avg:99.65ms
step:1759/1770 train_time:174285ms step_avg:99.65ms
step:1760/1770 train_time:174390ms step_avg:99.65ms
step:1761/1770 train_time:174497ms step_avg:99.66ms
step:1762/1770 train_time:174606ms step_avg:99.66ms
step:1763/1770 train_time:174709ms step_avg:99.66ms
step:1764/1770 train_time:174813ms step_avg:99.67ms
step:1765/1770 train_time:174917ms step_avg:99.67ms
step:1766/1770 train_time:175026ms step_avg:99.67ms
step:1767/1770 train_time:175129ms step_avg:99.67ms
step:1768/1770 train_time:175233ms step_avg:99.68ms
step:1769/1770 train_time:175336ms step_avg:99.68ms
step:1770/1770 train_time:175440ms step_avg:99.68ms
step:1770/1770 val_loss:3.2823 train_time:175544ms step_avg:99.74ms
peak memory allocated: 28840 MiB reserved: 32232 MiB
