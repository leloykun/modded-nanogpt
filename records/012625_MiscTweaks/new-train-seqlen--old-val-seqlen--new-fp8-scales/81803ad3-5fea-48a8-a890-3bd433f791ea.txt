import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 15:45:34 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23411ms step_avg:nanms
step:2/1770 train_time:23826ms step_avg:nanms
step:3/1770 train_time:23921ms step_avg:nanms
step:4/1770 train_time:24013ms step_avg:nanms
step:5/1770 train_time:24107ms step_avg:nanms
step:6/1770 train_time:24200ms step_avg:nanms
step:7/1770 train_time:24294ms step_avg:nanms
step:8/1770 train_time:24387ms step_avg:nanms
step:9/1770 train_time:24481ms step_avg:nanms
step:10/1770 train_time:24574ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.21ms
step:14/1770 train_time:377ms step_avg:94.17ms
step:15/1770 train_time:471ms step_avg:94.21ms
step:16/1770 train_time:565ms step_avg:94.20ms
step:17/1770 train_time:658ms step_avg:94.03ms
step:18/1770 train_time:752ms step_avg:93.95ms
step:19/1770 train_time:845ms step_avg:93.92ms
step:20/1770 train_time:939ms step_avg:93.90ms
step:21/1770 train_time:1032ms step_avg:93.84ms
step:22/1770 train_time:1126ms step_avg:93.81ms
step:23/1770 train_time:1220ms step_avg:93.83ms
step:24/1770 train_time:1314ms step_avg:93.86ms
step:25/1770 train_time:1408ms step_avg:93.86ms
step:26/1770 train_time:1502ms step_avg:93.85ms
step:27/1770 train_time:1596ms step_avg:93.87ms
step:28/1770 train_time:1690ms step_avg:93.88ms
step:29/1770 train_time:1784ms step_avg:93.88ms
step:30/1770 train_time:1878ms step_avg:93.90ms
step:31/1770 train_time:1972ms step_avg:93.92ms
step:32/1770 train_time:2066ms step_avg:93.89ms
step:33/1770 train_time:2159ms step_avg:93.89ms
step:34/1770 train_time:2253ms step_avg:93.88ms
step:35/1770 train_time:2347ms step_avg:93.87ms
step:36/1770 train_time:2441ms step_avg:93.88ms
step:37/1770 train_time:2535ms step_avg:93.87ms
step:38/1770 train_time:2628ms step_avg:93.87ms
step:39/1770 train_time:2722ms step_avg:93.87ms
step:40/1770 train_time:2817ms step_avg:93.90ms
step:41/1770 train_time:2911ms step_avg:93.90ms
step:42/1770 train_time:3005ms step_avg:93.90ms
step:43/1770 train_time:3099ms step_avg:93.90ms
step:44/1770 train_time:3193ms step_avg:93.90ms
step:45/1770 train_time:3286ms step_avg:93.89ms
step:46/1770 train_time:3380ms step_avg:93.90ms
step:47/1770 train_time:3474ms step_avg:93.89ms
step:48/1770 train_time:3567ms step_avg:93.88ms
step:49/1770 train_time:3661ms step_avg:93.87ms
step:50/1770 train_time:3754ms step_avg:93.86ms
step:51/1770 train_time:3849ms step_avg:93.88ms
step:52/1770 train_time:3942ms step_avg:93.87ms
step:53/1770 train_time:4037ms step_avg:93.87ms
step:54/1770 train_time:4130ms step_avg:93.86ms
step:55/1770 train_time:4224ms step_avg:93.87ms
step:56/1770 train_time:4317ms step_avg:93.86ms
step:57/1770 train_time:4411ms step_avg:93.85ms
step:58/1770 train_time:4505ms step_avg:93.85ms
step:59/1770 train_time:4598ms step_avg:93.84ms
step:60/1770 train_time:4691ms step_avg:93.83ms
step:61/1770 train_time:4785ms step_avg:93.83ms
step:62/1770 train_time:4878ms step_avg:93.81ms
step:63/1770 train_time:4972ms step_avg:93.81ms
step:64/1770 train_time:5066ms step_avg:93.81ms
step:65/1770 train_time:5160ms step_avg:93.81ms
step:66/1770 train_time:5253ms step_avg:93.81ms
step:67/1770 train_time:5347ms step_avg:93.81ms
step:68/1770 train_time:5441ms step_avg:93.81ms
step:69/1770 train_time:5535ms step_avg:93.81ms
step:70/1770 train_time:5629ms step_avg:93.81ms
step:71/1770 train_time:5722ms step_avg:93.81ms
step:72/1770 train_time:5816ms step_avg:93.80ms
step:73/1770 train_time:5909ms step_avg:93.80ms
step:74/1770 train_time:6003ms step_avg:93.79ms
step:75/1770 train_time:6097ms step_avg:93.79ms
step:76/1770 train_time:6191ms step_avg:93.81ms
step:77/1770 train_time:6285ms step_avg:93.81ms
step:78/1770 train_time:6379ms step_avg:93.80ms
step:79/1770 train_time:6472ms step_avg:93.80ms
step:80/1770 train_time:6566ms step_avg:93.81ms
step:81/1770 train_time:6660ms step_avg:93.80ms
step:82/1770 train_time:6754ms step_avg:93.80ms
step:83/1770 train_time:6847ms step_avg:93.80ms
step:84/1770 train_time:6941ms step_avg:93.80ms
step:85/1770 train_time:7035ms step_avg:93.80ms
step:86/1770 train_time:7128ms step_avg:93.79ms
step:87/1770 train_time:7222ms step_avg:93.79ms
step:88/1770 train_time:7316ms step_avg:93.80ms
step:89/1770 train_time:7410ms step_avg:93.80ms
step:90/1770 train_time:7504ms step_avg:93.80ms
step:91/1770 train_time:7599ms step_avg:93.81ms
step:92/1770 train_time:7691ms step_avg:93.79ms
step:93/1770 train_time:7785ms step_avg:93.79ms
step:94/1770 train_time:7878ms step_avg:93.79ms
step:95/1770 train_time:7972ms step_avg:93.78ms
step:96/1770 train_time:8066ms step_avg:93.79ms
step:97/1770 train_time:8159ms step_avg:93.78ms
step:98/1770 train_time:8253ms step_avg:93.78ms
step:99/1770 train_time:8346ms step_avg:93.78ms
step:100/1770 train_time:8440ms step_avg:93.78ms
step:101/1770 train_time:8533ms step_avg:93.77ms
step:102/1770 train_time:8627ms step_avg:93.78ms
step:103/1770 train_time:8722ms step_avg:93.78ms
step:104/1770 train_time:8815ms step_avg:93.78ms
step:105/1770 train_time:8910ms step_avg:93.79ms
step:106/1770 train_time:9003ms step_avg:93.79ms
step:107/1770 train_time:9097ms step_avg:93.78ms
step:108/1770 train_time:9190ms step_avg:93.78ms
step:109/1770 train_time:9284ms step_avg:93.78ms
step:110/1770 train_time:9378ms step_avg:93.78ms
step:111/1770 train_time:9472ms step_avg:93.78ms
step:112/1770 train_time:9566ms step_avg:93.78ms
step:113/1770 train_time:9659ms step_avg:93.78ms
step:114/1770 train_time:9753ms step_avg:93.78ms
step:115/1770 train_time:9846ms step_avg:93.78ms
step:116/1770 train_time:9940ms step_avg:93.78ms
step:117/1770 train_time:10034ms step_avg:93.78ms
step:118/1770 train_time:10127ms step_avg:93.77ms
step:119/1770 train_time:10222ms step_avg:93.78ms
step:120/1770 train_time:10315ms step_avg:93.77ms
step:121/1770 train_time:10408ms step_avg:93.77ms
step:122/1770 train_time:10502ms step_avg:93.77ms
step:123/1770 train_time:10596ms step_avg:93.77ms
step:124/1770 train_time:10690ms step_avg:93.77ms
step:125/1770 train_time:10784ms step_avg:93.77ms
step:125/1770 val_loss:4.6497 train_time:10875ms step_avg:94.57ms
step:126/1770 train_time:10899ms step_avg:93.95ms
step:127/1770 train_time:10981ms step_avg:93.86ms
step:128/1770 train_time:11080ms step_avg:93.90ms
step:129/1770 train_time:11174ms step_avg:93.90ms
step:130/1770 train_time:11268ms step_avg:93.90ms
step:131/1770 train_time:11361ms step_avg:93.90ms
step:132/1770 train_time:11455ms step_avg:93.89ms
step:133/1770 train_time:11548ms step_avg:93.89ms
step:134/1770 train_time:11642ms step_avg:93.89ms
step:135/1770 train_time:11737ms step_avg:93.89ms
step:136/1770 train_time:11830ms step_avg:93.89ms
step:137/1770 train_time:11924ms step_avg:93.89ms
step:138/1770 train_time:12019ms step_avg:93.90ms
step:139/1770 train_time:12113ms step_avg:93.90ms
step:140/1770 train_time:12208ms step_avg:93.91ms
step:141/1770 train_time:12302ms step_avg:93.91ms
step:142/1770 train_time:12398ms step_avg:93.92ms
step:143/1770 train_time:12491ms step_avg:93.92ms
step:144/1770 train_time:12585ms step_avg:93.92ms
step:145/1770 train_time:12679ms step_avg:93.92ms
step:146/1770 train_time:12774ms step_avg:93.92ms
step:147/1770 train_time:12868ms step_avg:93.93ms
step:148/1770 train_time:12962ms step_avg:93.93ms
step:149/1770 train_time:13057ms step_avg:93.93ms
step:150/1770 train_time:13152ms step_avg:93.94ms
step:151/1770 train_time:13246ms step_avg:93.95ms
step:152/1770 train_time:13341ms step_avg:93.95ms
step:153/1770 train_time:13435ms step_avg:93.95ms
step:154/1770 train_time:13529ms step_avg:93.95ms
step:155/1770 train_time:13623ms step_avg:93.95ms
step:156/1770 train_time:13718ms step_avg:93.96ms
step:157/1770 train_time:13812ms step_avg:93.96ms
step:158/1770 train_time:13906ms step_avg:93.96ms
step:159/1770 train_time:14000ms step_avg:93.96ms
step:160/1770 train_time:14095ms step_avg:93.97ms
step:161/1770 train_time:14189ms step_avg:93.97ms
step:162/1770 train_time:14283ms step_avg:93.97ms
step:163/1770 train_time:14378ms step_avg:93.98ms
step:164/1770 train_time:14473ms step_avg:93.98ms
step:165/1770 train_time:14567ms step_avg:93.98ms
step:166/1770 train_time:14661ms step_avg:93.98ms
step:167/1770 train_time:14756ms step_avg:93.99ms
step:168/1770 train_time:14850ms step_avg:93.99ms
step:169/1770 train_time:14945ms step_avg:93.99ms
step:170/1770 train_time:15039ms step_avg:93.99ms
step:171/1770 train_time:15133ms step_avg:93.99ms
step:172/1770 train_time:15227ms step_avg:93.99ms
step:173/1770 train_time:15321ms step_avg:94.00ms
step:174/1770 train_time:15416ms step_avg:94.00ms
step:175/1770 train_time:15511ms step_avg:94.00ms
step:176/1770 train_time:15604ms step_avg:94.00ms
step:177/1770 train_time:15699ms step_avg:94.01ms
step:178/1770 train_time:15794ms step_avg:94.01ms
step:179/1770 train_time:15888ms step_avg:94.01ms
step:180/1770 train_time:15983ms step_avg:94.02ms
step:181/1770 train_time:16077ms step_avg:94.02ms
step:182/1770 train_time:16171ms step_avg:94.02ms
step:183/1770 train_time:16265ms step_avg:94.02ms
step:184/1770 train_time:16360ms step_avg:94.02ms
step:185/1770 train_time:16454ms step_avg:94.02ms
step:186/1770 train_time:16549ms step_avg:94.03ms
step:187/1770 train_time:16643ms step_avg:94.03ms
step:188/1770 train_time:16738ms step_avg:94.03ms
step:189/1770 train_time:16832ms step_avg:94.03ms
step:190/1770 train_time:16926ms step_avg:94.04ms
step:191/1770 train_time:17021ms step_avg:94.04ms
step:192/1770 train_time:17115ms step_avg:94.04ms
step:193/1770 train_time:17210ms step_avg:94.04ms
step:194/1770 train_time:17304ms step_avg:94.05ms
step:195/1770 train_time:17399ms step_avg:94.05ms
step:196/1770 train_time:17495ms step_avg:94.06ms
step:197/1770 train_time:17589ms step_avg:94.06ms
step:198/1770 train_time:17683ms step_avg:94.06ms
step:199/1770 train_time:17777ms step_avg:94.06ms
step:200/1770 train_time:17872ms step_avg:94.06ms
step:201/1770 train_time:17966ms step_avg:94.06ms
step:202/1770 train_time:18060ms step_avg:94.06ms
step:203/1770 train_time:18154ms step_avg:94.06ms
step:204/1770 train_time:18249ms step_avg:94.06ms
step:205/1770 train_time:18343ms step_avg:94.07ms
step:206/1770 train_time:18437ms step_avg:94.07ms
step:207/1770 train_time:18532ms step_avg:94.07ms
step:208/1770 train_time:18626ms step_avg:94.07ms
step:209/1770 train_time:18721ms step_avg:94.07ms
step:210/1770 train_time:18816ms step_avg:94.08ms
step:211/1770 train_time:18910ms step_avg:94.08ms
step:212/1770 train_time:19005ms step_avg:94.08ms
step:213/1770 train_time:19099ms step_avg:94.08ms
step:214/1770 train_time:19194ms step_avg:94.09ms
step:215/1770 train_time:19289ms step_avg:94.09ms
step:216/1770 train_time:19383ms step_avg:94.09ms
step:217/1770 train_time:19477ms step_avg:94.09ms
step:218/1770 train_time:19572ms step_avg:94.09ms
step:219/1770 train_time:19666ms step_avg:94.10ms
step:220/1770 train_time:19760ms step_avg:94.10ms
step:221/1770 train_time:19856ms step_avg:94.10ms
step:222/1770 train_time:19950ms step_avg:94.10ms
step:223/1770 train_time:20044ms step_avg:94.11ms
step:224/1770 train_time:20139ms step_avg:94.11ms
step:225/1770 train_time:20234ms step_avg:94.11ms
step:226/1770 train_time:20328ms step_avg:94.11ms
step:227/1770 train_time:20422ms step_avg:94.11ms
step:228/1770 train_time:20517ms step_avg:94.11ms
step:229/1770 train_time:20611ms step_avg:94.12ms
step:230/1770 train_time:20705ms step_avg:94.12ms
step:231/1770 train_time:20800ms step_avg:94.12ms
step:232/1770 train_time:20894ms step_avg:94.12ms
step:233/1770 train_time:20988ms step_avg:94.12ms
step:234/1770 train_time:21082ms step_avg:94.12ms
step:235/1770 train_time:21177ms step_avg:94.12ms
step:236/1770 train_time:21271ms step_avg:94.12ms
step:237/1770 train_time:21365ms step_avg:94.12ms
step:238/1770 train_time:21460ms step_avg:94.12ms
step:239/1770 train_time:21554ms step_avg:94.12ms
step:240/1770 train_time:21649ms step_avg:94.12ms
step:241/1770 train_time:21743ms step_avg:94.12ms
step:242/1770 train_time:21838ms step_avg:94.13ms
step:243/1770 train_time:21933ms step_avg:94.13ms
step:244/1770 train_time:22027ms step_avg:94.13ms
step:245/1770 train_time:22122ms step_avg:94.13ms
step:246/1770 train_time:22216ms step_avg:94.14ms
step:247/1770 train_time:22311ms step_avg:94.14ms
step:248/1770 train_time:22405ms step_avg:94.14ms
step:249/1770 train_time:22499ms step_avg:94.14ms
step:250/1770 train_time:22594ms step_avg:94.14ms
step:250/1770 val_loss:4.1119 train_time:22686ms step_avg:94.53ms
step:251/1770 train_time:22708ms step_avg:94.22ms
step:252/1770 train_time:22793ms step_avg:94.19ms
step:253/1770 train_time:22891ms step_avg:94.20ms
step:254/1770 train_time:22986ms step_avg:94.21ms
step:255/1770 train_time:23080ms step_avg:94.21ms
step:256/1770 train_time:23174ms step_avg:94.20ms
step:257/1770 train_time:23268ms step_avg:94.20ms
step:258/1770 train_time:23362ms step_avg:94.20ms
step:259/1770 train_time:23456ms step_avg:94.20ms
step:260/1770 train_time:23550ms step_avg:94.20ms
step:261/1770 train_time:23645ms step_avg:94.20ms
step:262/1770 train_time:23740ms step_avg:94.20ms
step:263/1770 train_time:23834ms step_avg:94.21ms
step:264/1770 train_time:23930ms step_avg:94.21ms
step:265/1770 train_time:24025ms step_avg:94.21ms
step:266/1770 train_time:24120ms step_avg:94.22ms
step:267/1770 train_time:24215ms step_avg:94.22ms
step:268/1770 train_time:24309ms step_avg:94.22ms
step:269/1770 train_time:24405ms step_avg:94.23ms
step:270/1770 train_time:24499ms step_avg:94.23ms
step:271/1770 train_time:24594ms step_avg:94.23ms
step:272/1770 train_time:24689ms step_avg:94.23ms
step:273/1770 train_time:24785ms step_avg:94.24ms
step:274/1770 train_time:24881ms step_avg:94.25ms
step:275/1770 train_time:24977ms step_avg:94.25ms
step:276/1770 train_time:25072ms step_avg:94.26ms
step:277/1770 train_time:25168ms step_avg:94.26ms
step:278/1770 train_time:25262ms step_avg:94.26ms
step:279/1770 train_time:25357ms step_avg:94.26ms
step:280/1770 train_time:25452ms step_avg:94.27ms
step:281/1770 train_time:25547ms step_avg:94.27ms
step:282/1770 train_time:25642ms step_avg:94.27ms
step:283/1770 train_time:25737ms step_avg:94.27ms
step:284/1770 train_time:25832ms step_avg:94.28ms
step:285/1770 train_time:25927ms step_avg:94.28ms
step:286/1770 train_time:26022ms step_avg:94.28ms
step:287/1770 train_time:26117ms step_avg:94.29ms
step:288/1770 train_time:26213ms step_avg:94.29ms
step:289/1770 train_time:26308ms step_avg:94.30ms
step:290/1770 train_time:26403ms step_avg:94.30ms
step:291/1770 train_time:26498ms step_avg:94.30ms
step:292/1770 train_time:26593ms step_avg:94.30ms
step:293/1770 train_time:26689ms step_avg:94.31ms
step:294/1770 train_time:26783ms step_avg:94.31ms
step:295/1770 train_time:26878ms step_avg:94.31ms
step:296/1770 train_time:26974ms step_avg:94.31ms
step:297/1770 train_time:27069ms step_avg:94.32ms
step:298/1770 train_time:27164ms step_avg:94.32ms
step:299/1770 train_time:27260ms step_avg:94.32ms
step:300/1770 train_time:27355ms step_avg:94.33ms
step:301/1770 train_time:27450ms step_avg:94.33ms
step:302/1770 train_time:27545ms step_avg:94.33ms
step:303/1770 train_time:27639ms step_avg:94.33ms
step:304/1770 train_time:27734ms step_avg:94.33ms
step:305/1770 train_time:27829ms step_avg:94.34ms
step:306/1770 train_time:27923ms step_avg:94.34ms
step:307/1770 train_time:28018ms step_avg:94.34ms
step:308/1770 train_time:28113ms step_avg:94.34ms
step:309/1770 train_time:28208ms step_avg:94.34ms
step:310/1770 train_time:28304ms step_avg:94.35ms
step:311/1770 train_time:28399ms step_avg:94.35ms
step:312/1770 train_time:28494ms step_avg:94.35ms
step:313/1770 train_time:28589ms step_avg:94.35ms
step:314/1770 train_time:28684ms step_avg:94.36ms
step:315/1770 train_time:28779ms step_avg:94.36ms
step:316/1770 train_time:28874ms step_avg:94.36ms
step:317/1770 train_time:28970ms step_avg:94.36ms
step:318/1770 train_time:29064ms step_avg:94.36ms
step:319/1770 train_time:29159ms step_avg:94.37ms
step:320/1770 train_time:29254ms step_avg:94.37ms
step:321/1770 train_time:29349ms step_avg:94.37ms
step:322/1770 train_time:29445ms step_avg:94.37ms
step:323/1770 train_time:29540ms step_avg:94.38ms
step:324/1770 train_time:29634ms step_avg:94.38ms
step:325/1770 train_time:29729ms step_avg:94.38ms
step:326/1770 train_time:29824ms step_avg:94.38ms
step:327/1770 train_time:29920ms step_avg:94.38ms
step:328/1770 train_time:30015ms step_avg:94.39ms
step:329/1770 train_time:30110ms step_avg:94.39ms
step:330/1770 train_time:30205ms step_avg:94.39ms
step:331/1770 train_time:30301ms step_avg:94.40ms
step:332/1770 train_time:30396ms step_avg:94.40ms
step:333/1770 train_time:30492ms step_avg:94.40ms
step:334/1770 train_time:30587ms step_avg:94.41ms
step:335/1770 train_time:30682ms step_avg:94.41ms
step:336/1770 train_time:30777ms step_avg:94.41ms
step:337/1770 train_time:30872ms step_avg:94.41ms
step:338/1770 train_time:30968ms step_avg:94.41ms
step:339/1770 train_time:31063ms step_avg:94.42ms
step:340/1770 train_time:31158ms step_avg:94.42ms
step:341/1770 train_time:31253ms step_avg:94.42ms
step:342/1770 train_time:31348ms step_avg:94.42ms
step:343/1770 train_time:31443ms step_avg:94.42ms
step:344/1770 train_time:31538ms step_avg:94.42ms
step:345/1770 train_time:31632ms step_avg:94.42ms
step:346/1770 train_time:31728ms step_avg:94.43ms
step:347/1770 train_time:31823ms step_avg:94.43ms
step:348/1770 train_time:31918ms step_avg:94.43ms
step:349/1770 train_time:32013ms step_avg:94.43ms
step:350/1770 train_time:32109ms step_avg:94.44ms
step:351/1770 train_time:32203ms step_avg:94.44ms
step:352/1770 train_time:32299ms step_avg:94.44ms
step:353/1770 train_time:32394ms step_avg:94.44ms
step:354/1770 train_time:32488ms step_avg:94.44ms
step:355/1770 train_time:32583ms step_avg:94.44ms
step:356/1770 train_time:32678ms step_avg:94.44ms
step:357/1770 train_time:32773ms step_avg:94.45ms
step:358/1770 train_time:32868ms step_avg:94.45ms
step:359/1770 train_time:32963ms step_avg:94.45ms
step:360/1770 train_time:33058ms step_avg:94.45ms
step:361/1770 train_time:33153ms step_avg:94.45ms
step:362/1770 train_time:33248ms step_avg:94.46ms
step:363/1770 train_time:33343ms step_avg:94.46ms
step:364/1770 train_time:33439ms step_avg:94.46ms
step:365/1770 train_time:33534ms step_avg:94.46ms
step:366/1770 train_time:33629ms step_avg:94.46ms
step:367/1770 train_time:33724ms step_avg:94.46ms
step:368/1770 train_time:33819ms step_avg:94.47ms
step:369/1770 train_time:33913ms step_avg:94.47ms
step:370/1770 train_time:34009ms step_avg:94.47ms
step:371/1770 train_time:34105ms step_avg:94.47ms
step:372/1770 train_time:34200ms step_avg:94.47ms
step:373/1770 train_time:34295ms step_avg:94.48ms
step:374/1770 train_time:34390ms step_avg:94.48ms
step:375/1770 train_time:34485ms step_avg:94.48ms
step:375/1770 val_loss:3.9098 train_time:34578ms step_avg:94.73ms
step:376/1770 train_time:34599ms step_avg:94.53ms
step:377/1770 train_time:34687ms step_avg:94.51ms
step:378/1770 train_time:34786ms step_avg:94.53ms
step:379/1770 train_time:34882ms step_avg:94.53ms
step:380/1770 train_time:34976ms step_avg:94.53ms
step:381/1770 train_time:35071ms step_avg:94.53ms
step:382/1770 train_time:35166ms step_avg:94.53ms
step:383/1770 train_time:35261ms step_avg:94.53ms
step:384/1770 train_time:35355ms step_avg:94.53ms
step:385/1770 train_time:35450ms step_avg:94.53ms
step:386/1770 train_time:35544ms step_avg:94.53ms
step:387/1770 train_time:35640ms step_avg:94.54ms
step:388/1770 train_time:35736ms step_avg:94.54ms
step:389/1770 train_time:35832ms step_avg:94.54ms
step:390/1770 train_time:35928ms step_avg:94.55ms
step:391/1770 train_time:36023ms step_avg:94.55ms
step:392/1770 train_time:36119ms step_avg:94.55ms
step:393/1770 train_time:36213ms step_avg:94.55ms
step:394/1770 train_time:36307ms step_avg:94.55ms
step:395/1770 train_time:36402ms step_avg:94.55ms
step:396/1770 train_time:36498ms step_avg:94.56ms
step:397/1770 train_time:36595ms step_avg:94.56ms
step:398/1770 train_time:36692ms step_avg:94.57ms
step:399/1770 train_time:36789ms step_avg:94.57ms
step:400/1770 train_time:36886ms step_avg:94.58ms
step:401/1770 train_time:36983ms step_avg:94.59ms
step:402/1770 train_time:37080ms step_avg:94.59ms
step:403/1770 train_time:37177ms step_avg:94.60ms
step:404/1770 train_time:37274ms step_avg:94.61ms
step:405/1770 train_time:37373ms step_avg:94.62ms
step:406/1770 train_time:37471ms step_avg:94.62ms
step:407/1770 train_time:37567ms step_avg:94.63ms
step:408/1770 train_time:37664ms step_avg:94.63ms
step:409/1770 train_time:37761ms step_avg:94.64ms
step:410/1770 train_time:37858ms step_avg:94.64ms
step:411/1770 train_time:37955ms step_avg:94.65ms
step:412/1770 train_time:38053ms step_avg:94.66ms
step:413/1770 train_time:38149ms step_avg:94.66ms
step:414/1770 train_time:38247ms step_avg:94.67ms
step:415/1770 train_time:38344ms step_avg:94.68ms
step:416/1770 train_time:38441ms step_avg:94.68ms
step:417/1770 train_time:38538ms step_avg:94.69ms
step:418/1770 train_time:38635ms step_avg:94.69ms
step:419/1770 train_time:38732ms step_avg:94.70ms
step:420/1770 train_time:38828ms step_avg:94.70ms
step:421/1770 train_time:38925ms step_avg:94.71ms
step:422/1770 train_time:39022ms step_avg:94.71ms
step:423/1770 train_time:39119ms step_avg:94.72ms
step:424/1770 train_time:39216ms step_avg:94.73ms
step:425/1770 train_time:39314ms step_avg:94.73ms
step:426/1770 train_time:39411ms step_avg:94.74ms
step:427/1770 train_time:39508ms step_avg:94.74ms
step:428/1770 train_time:39605ms step_avg:94.75ms
step:429/1770 train_time:39701ms step_avg:94.75ms
step:430/1770 train_time:39798ms step_avg:94.76ms
step:431/1770 train_time:39894ms step_avg:94.76ms
step:432/1770 train_time:39991ms step_avg:94.77ms
step:433/1770 train_time:40088ms step_avg:94.77ms
step:434/1770 train_time:40185ms step_avg:94.78ms
step:435/1770 train_time:40283ms step_avg:94.78ms
step:436/1770 train_time:40380ms step_avg:94.79ms
step:437/1770 train_time:40478ms step_avg:94.80ms
step:438/1770 train_time:40575ms step_avg:94.80ms
step:439/1770 train_time:40671ms step_avg:94.80ms
step:440/1770 train_time:40768ms step_avg:94.81ms
step:441/1770 train_time:40865ms step_avg:94.81ms
step:442/1770 train_time:40962ms step_avg:94.82ms
step:443/1770 train_time:41059ms step_avg:94.82ms
step:444/1770 train_time:41156ms step_avg:94.83ms
step:445/1770 train_time:41253ms step_avg:94.83ms
step:446/1770 train_time:41350ms step_avg:94.84ms
step:447/1770 train_time:41447ms step_avg:94.84ms
step:448/1770 train_time:41544ms step_avg:94.85ms
step:449/1770 train_time:41642ms step_avg:94.86ms
step:450/1770 train_time:41739ms step_avg:94.86ms
step:451/1770 train_time:41836ms step_avg:94.87ms
step:452/1770 train_time:41932ms step_avg:94.87ms
step:453/1770 train_time:42029ms step_avg:94.87ms
step:454/1770 train_time:42126ms step_avg:94.88ms
step:455/1770 train_time:42222ms step_avg:94.88ms
step:456/1770 train_time:42320ms step_avg:94.89ms
step:457/1770 train_time:42417ms step_avg:94.89ms
step:458/1770 train_time:42514ms step_avg:94.90ms
step:459/1770 train_time:42611ms step_avg:94.90ms
step:460/1770 train_time:42708ms step_avg:94.91ms
step:461/1770 train_time:42806ms step_avg:94.91ms
step:462/1770 train_time:42902ms step_avg:94.92ms
step:463/1770 train_time:42999ms step_avg:94.92ms
step:464/1770 train_time:43096ms step_avg:94.92ms
step:465/1770 train_time:43193ms step_avg:94.93ms
step:466/1770 train_time:43290ms step_avg:94.93ms
step:467/1770 train_time:43387ms step_avg:94.94ms
step:468/1770 train_time:43484ms step_avg:94.94ms
step:469/1770 train_time:43582ms step_avg:94.95ms
step:470/1770 train_time:43679ms step_avg:94.96ms
step:471/1770 train_time:43776ms step_avg:94.96ms
step:472/1770 train_time:43873ms step_avg:94.96ms
step:473/1770 train_time:43970ms step_avg:94.97ms
step:474/1770 train_time:44066ms step_avg:94.97ms
step:475/1770 train_time:44163ms step_avg:94.97ms
step:476/1770 train_time:44260ms step_avg:94.98ms
step:477/1770 train_time:44357ms step_avg:94.98ms
step:478/1770 train_time:44454ms step_avg:94.99ms
step:479/1770 train_time:44551ms step_avg:94.99ms
step:480/1770 train_time:44648ms step_avg:95.00ms
step:481/1770 train_time:44746ms step_avg:95.00ms
step:482/1770 train_time:44843ms step_avg:95.01ms
step:483/1770 train_time:44940ms step_avg:95.01ms
step:484/1770 train_time:45037ms step_avg:95.01ms
step:485/1770 train_time:45134ms step_avg:95.02ms
step:486/1770 train_time:45231ms step_avg:95.02ms
step:487/1770 train_time:45329ms step_avg:95.03ms
step:488/1770 train_time:45426ms step_avg:95.03ms
step:489/1770 train_time:45525ms step_avg:95.04ms
step:490/1770 train_time:45622ms step_avg:95.05ms
step:491/1770 train_time:45718ms step_avg:95.05ms
step:492/1770 train_time:45815ms step_avg:95.05ms
step:493/1770 train_time:45912ms step_avg:95.06ms
step:494/1770 train_time:46009ms step_avg:95.06ms
step:495/1770 train_time:46106ms step_avg:95.06ms
step:496/1770 train_time:46203ms step_avg:95.07ms
step:497/1770 train_time:46300ms step_avg:95.07ms
step:498/1770 train_time:46396ms step_avg:95.07ms
step:499/1770 train_time:46493ms step_avg:95.08ms
step:500/1770 train_time:46591ms step_avg:95.08ms
step:500/1770 val_loss:3.7580 train_time:46686ms step_avg:95.28ms
step:501/1770 train_time:46708ms step_avg:95.13ms
step:502/1770 train_time:46796ms step_avg:95.11ms
step:503/1770 train_time:46895ms step_avg:95.12ms
step:504/1770 train_time:46993ms step_avg:95.13ms
step:505/1770 train_time:47090ms step_avg:95.13ms
step:506/1770 train_time:47186ms step_avg:95.13ms
step:507/1770 train_time:47283ms step_avg:95.14ms
step:508/1770 train_time:47379ms step_avg:95.14ms
step:509/1770 train_time:47476ms step_avg:95.14ms
step:510/1770 train_time:47573ms step_avg:95.15ms
step:511/1770 train_time:47670ms step_avg:95.15ms
step:512/1770 train_time:47768ms step_avg:95.16ms
step:513/1770 train_time:47866ms step_avg:95.16ms
step:514/1770 train_time:47964ms step_avg:95.17ms
step:515/1770 train_time:48061ms step_avg:95.17ms
step:516/1770 train_time:48158ms step_avg:95.17ms
step:517/1770 train_time:48255ms step_avg:95.18ms
step:518/1770 train_time:48352ms step_avg:95.18ms
step:519/1770 train_time:48448ms step_avg:95.18ms
step:520/1770 train_time:48545ms step_avg:95.19ms
step:521/1770 train_time:48642ms step_avg:95.19ms
step:522/1770 train_time:48738ms step_avg:95.19ms
step:523/1770 train_time:48836ms step_avg:95.20ms
step:524/1770 train_time:48934ms step_avg:95.20ms
step:525/1770 train_time:49031ms step_avg:95.21ms
step:526/1770 train_time:49129ms step_avg:95.21ms
step:527/1770 train_time:49226ms step_avg:95.21ms
step:528/1770 train_time:49323ms step_avg:95.22ms
step:529/1770 train_time:49420ms step_avg:95.22ms
step:530/1770 train_time:49516ms step_avg:95.22ms
step:531/1770 train_time:49613ms step_avg:95.23ms
step:532/1770 train_time:49710ms step_avg:95.23ms
step:533/1770 train_time:49807ms step_avg:95.23ms
step:534/1770 train_time:49904ms step_avg:95.24ms
step:535/1770 train_time:50003ms step_avg:95.24ms
step:536/1770 train_time:50100ms step_avg:95.25ms
step:537/1770 train_time:50198ms step_avg:95.25ms
step:538/1770 train_time:50296ms step_avg:95.26ms
step:539/1770 train_time:50393ms step_avg:95.26ms
step:540/1770 train_time:50490ms step_avg:95.26ms
step:541/1770 train_time:50587ms step_avg:95.27ms
step:542/1770 train_time:50684ms step_avg:95.27ms
step:543/1770 train_time:50781ms step_avg:95.27ms
step:544/1770 train_time:50877ms step_avg:95.28ms
step:545/1770 train_time:50975ms step_avg:95.28ms
step:546/1770 train_time:51073ms step_avg:95.28ms
step:547/1770 train_time:51170ms step_avg:95.29ms
step:548/1770 train_time:51268ms step_avg:95.29ms
step:549/1770 train_time:51366ms step_avg:95.30ms
step:550/1770 train_time:51463ms step_avg:95.30ms
step:551/1770 train_time:51561ms step_avg:95.31ms
step:552/1770 train_time:51658ms step_avg:95.31ms
step:553/1770 train_time:51754ms step_avg:95.31ms
step:554/1770 train_time:51851ms step_avg:95.31ms
step:555/1770 train_time:51948ms step_avg:95.32ms
step:556/1770 train_time:52045ms step_avg:95.32ms
step:557/1770 train_time:52143ms step_avg:95.33ms
step:558/1770 train_time:52241ms step_avg:95.33ms
step:559/1770 train_time:52337ms step_avg:95.33ms
step:560/1770 train_time:52435ms step_avg:95.34ms
step:561/1770 train_time:52533ms step_avg:95.34ms
step:562/1770 train_time:52631ms step_avg:95.35ms
step:563/1770 train_time:52728ms step_avg:95.35ms
step:564/1770 train_time:52825ms step_avg:95.35ms
step:565/1770 train_time:52923ms step_avg:95.36ms
step:566/1770 train_time:53020ms step_avg:95.36ms
step:567/1770 train_time:53118ms step_avg:95.36ms
step:568/1770 train_time:53215ms step_avg:95.37ms
step:569/1770 train_time:53312ms step_avg:95.37ms
step:570/1770 train_time:53410ms step_avg:95.37ms
step:571/1770 train_time:53506ms step_avg:95.38ms
step:572/1770 train_time:53604ms step_avg:95.38ms
step:573/1770 train_time:53702ms step_avg:95.39ms
step:574/1770 train_time:53799ms step_avg:95.39ms
step:575/1770 train_time:53896ms step_avg:95.39ms
step:576/1770 train_time:53993ms step_avg:95.39ms
step:577/1770 train_time:54090ms step_avg:95.40ms
step:578/1770 train_time:54188ms step_avg:95.40ms
step:579/1770 train_time:54285ms step_avg:95.40ms
step:580/1770 train_time:54382ms step_avg:95.41ms
step:581/1770 train_time:54479ms step_avg:95.41ms
step:582/1770 train_time:54576ms step_avg:95.41ms
step:583/1770 train_time:54674ms step_avg:95.42ms
step:584/1770 train_time:54772ms step_avg:95.42ms
step:585/1770 train_time:54869ms step_avg:95.42ms
step:586/1770 train_time:54966ms step_avg:95.43ms
step:587/1770 train_time:55064ms step_avg:95.43ms
step:588/1770 train_time:55161ms step_avg:95.43ms
step:589/1770 train_time:55258ms step_avg:95.44ms
step:590/1770 train_time:55356ms step_avg:95.44ms
step:591/1770 train_time:55453ms step_avg:95.44ms
step:592/1770 train_time:55550ms step_avg:95.45ms
step:593/1770 train_time:55648ms step_avg:95.45ms
step:594/1770 train_time:55745ms step_avg:95.45ms
step:595/1770 train_time:55842ms step_avg:95.46ms
step:596/1770 train_time:55939ms step_avg:95.46ms
step:597/1770 train_time:56036ms step_avg:95.46ms
step:598/1770 train_time:56134ms step_avg:95.47ms
step:599/1770 train_time:56232ms step_avg:95.47ms
step:600/1770 train_time:56329ms step_avg:95.47ms
step:601/1770 train_time:56426ms step_avg:95.48ms
step:602/1770 train_time:56523ms step_avg:95.48ms
step:603/1770 train_time:56621ms step_avg:95.48ms
step:604/1770 train_time:56718ms step_avg:95.49ms
step:605/1770 train_time:56816ms step_avg:95.49ms
step:606/1770 train_time:56913ms step_avg:95.49ms
step:607/1770 train_time:57010ms step_avg:95.49ms
step:608/1770 train_time:57107ms step_avg:95.50ms
step:609/1770 train_time:57205ms step_avg:95.50ms
step:610/1770 train_time:57302ms step_avg:95.50ms
step:611/1770 train_time:57400ms step_avg:95.51ms
step:612/1770 train_time:57497ms step_avg:95.51ms
step:613/1770 train_time:57593ms step_avg:95.51ms
step:614/1770 train_time:57690ms step_avg:95.51ms
step:615/1770 train_time:57787ms step_avg:95.52ms
step:616/1770 train_time:57884ms step_avg:95.52ms
step:617/1770 train_time:57982ms step_avg:95.52ms
step:618/1770 train_time:58079ms step_avg:95.52ms
step:619/1770 train_time:58176ms step_avg:95.53ms
step:620/1770 train_time:58274ms step_avg:95.53ms
step:621/1770 train_time:58371ms step_avg:95.53ms
step:622/1770 train_time:58468ms step_avg:95.54ms
step:623/1770 train_time:58566ms step_avg:95.54ms
step:624/1770 train_time:58663ms step_avg:95.54ms
step:625/1770 train_time:58761ms step_avg:95.55ms
step:625/1770 val_loss:3.6734 train_time:58856ms step_avg:95.70ms
step:626/1770 train_time:58878ms step_avg:95.58ms
step:627/1770 train_time:58967ms step_avg:95.57ms
step:628/1770 train_time:59067ms step_avg:95.58ms
step:629/1770 train_time:59165ms step_avg:95.58ms
step:630/1770 train_time:59261ms step_avg:95.58ms
step:631/1770 train_time:59358ms step_avg:95.58ms
step:632/1770 train_time:59455ms step_avg:95.59ms
step:633/1770 train_time:59552ms step_avg:95.59ms
step:634/1770 train_time:59649ms step_avg:95.59ms
step:635/1770 train_time:59746ms step_avg:95.59ms
step:636/1770 train_time:59843ms step_avg:95.60ms
step:637/1770 train_time:59943ms step_avg:95.60ms
step:638/1770 train_time:60041ms step_avg:95.61ms
step:639/1770 train_time:60138ms step_avg:95.61ms
step:640/1770 train_time:60235ms step_avg:95.61ms
step:641/1770 train_time:60333ms step_avg:95.61ms
step:642/1770 train_time:60430ms step_avg:95.62ms
step:643/1770 train_time:60527ms step_avg:95.62ms
step:644/1770 train_time:60623ms step_avg:95.62ms
step:645/1770 train_time:60720ms step_avg:95.62ms
step:646/1770 train_time:60817ms step_avg:95.62ms
step:647/1770 train_time:60915ms step_avg:95.63ms
step:648/1770 train_time:61013ms step_avg:95.63ms
step:649/1770 train_time:61111ms step_avg:95.64ms
step:650/1770 train_time:61209ms step_avg:95.64ms
step:651/1770 train_time:61306ms step_avg:95.64ms
step:652/1770 train_time:61403ms step_avg:95.64ms
step:653/1770 train_time:61501ms step_avg:95.65ms
step:654/1770 train_time:61597ms step_avg:95.65ms
step:655/1770 train_time:61694ms step_avg:95.65ms
step:656/1770 train_time:61792ms step_avg:95.65ms
step:657/1770 train_time:61889ms step_avg:95.66ms
step:658/1770 train_time:61989ms step_avg:95.66ms
step:659/1770 train_time:62087ms step_avg:95.67ms
step:660/1770 train_time:62187ms step_avg:95.67ms
step:661/1770 train_time:62285ms step_avg:95.68ms
step:662/1770 train_time:62384ms step_avg:95.68ms
step:663/1770 train_time:62484ms step_avg:95.69ms
step:664/1770 train_time:62583ms step_avg:95.69ms
step:665/1770 train_time:62682ms step_avg:95.70ms
step:666/1770 train_time:62781ms step_avg:95.70ms
step:667/1770 train_time:62880ms step_avg:95.71ms
step:668/1770 train_time:62979ms step_avg:95.71ms
step:669/1770 train_time:63078ms step_avg:95.72ms
step:670/1770 train_time:63178ms step_avg:95.72ms
step:671/1770 train_time:63277ms step_avg:95.73ms
step:672/1770 train_time:63376ms step_avg:95.73ms
step:673/1770 train_time:63475ms step_avg:95.74ms
step:674/1770 train_time:63574ms step_avg:95.74ms
step:675/1770 train_time:63673ms step_avg:95.75ms
step:676/1770 train_time:63772ms step_avg:95.75ms
step:677/1770 train_time:63871ms step_avg:95.76ms
step:678/1770 train_time:63970ms step_avg:95.76ms
step:679/1770 train_time:64069ms step_avg:95.77ms
step:680/1770 train_time:64168ms step_avg:95.77ms
step:681/1770 train_time:64266ms step_avg:95.78ms
step:682/1770 train_time:64365ms step_avg:95.78ms
step:683/1770 train_time:64466ms step_avg:95.79ms
step:684/1770 train_time:64566ms step_avg:95.79ms
step:685/1770 train_time:64666ms step_avg:95.80ms
step:686/1770 train_time:64767ms step_avg:95.81ms
step:687/1770 train_time:64866ms step_avg:95.81ms
step:688/1770 train_time:64966ms step_avg:95.82ms
step:689/1770 train_time:65065ms step_avg:95.83ms
step:690/1770 train_time:65165ms step_avg:95.83ms
step:691/1770 train_time:65264ms step_avg:95.84ms
step:692/1770 train_time:65363ms step_avg:95.84ms
step:693/1770 train_time:65462ms step_avg:95.85ms
step:694/1770 train_time:65562ms step_avg:95.85ms
step:695/1770 train_time:65661ms step_avg:95.85ms
step:696/1770 train_time:65760ms step_avg:95.86ms
step:697/1770 train_time:65860ms step_avg:95.87ms
step:698/1770 train_time:65959ms step_avg:95.87ms
step:699/1770 train_time:66058ms step_avg:95.88ms
step:700/1770 train_time:66158ms step_avg:95.88ms
step:701/1770 train_time:66257ms step_avg:95.89ms
step:702/1770 train_time:66357ms step_avg:95.89ms
step:703/1770 train_time:66456ms step_avg:95.90ms
step:704/1770 train_time:66555ms step_avg:95.90ms
step:705/1770 train_time:66655ms step_avg:95.91ms
step:706/1770 train_time:66755ms step_avg:95.91ms
step:707/1770 train_time:66855ms step_avg:95.92ms
step:708/1770 train_time:66954ms step_avg:95.92ms
step:709/1770 train_time:67053ms step_avg:95.93ms
step:710/1770 train_time:67153ms step_avg:95.93ms
step:711/1770 train_time:67252ms step_avg:95.94ms
step:712/1770 train_time:67352ms step_avg:95.94ms
step:713/1770 train_time:67450ms step_avg:95.95ms
step:714/1770 train_time:67549ms step_avg:95.95ms
step:715/1770 train_time:67648ms step_avg:95.95ms
step:716/1770 train_time:67747ms step_avg:95.96ms
step:717/1770 train_time:67845ms step_avg:95.96ms
step:718/1770 train_time:67944ms step_avg:95.97ms
step:719/1770 train_time:68043ms step_avg:95.97ms
step:720/1770 train_time:68142ms step_avg:95.98ms
step:721/1770 train_time:68241ms step_avg:95.98ms
step:722/1770 train_time:68341ms step_avg:95.98ms
step:723/1770 train_time:68440ms step_avg:95.99ms
step:724/1770 train_time:68539ms step_avg:95.99ms
step:725/1770 train_time:68638ms step_avg:96.00ms
step:726/1770 train_time:68737ms step_avg:96.00ms
step:727/1770 train_time:68837ms step_avg:96.01ms
step:728/1770 train_time:68936ms step_avg:96.01ms
step:729/1770 train_time:69035ms step_avg:96.01ms
step:730/1770 train_time:69133ms step_avg:96.02ms
step:731/1770 train_time:69232ms step_avg:96.02ms
step:732/1770 train_time:69331ms step_avg:96.03ms
step:733/1770 train_time:69431ms step_avg:96.03ms
step:734/1770 train_time:69530ms step_avg:96.04ms
step:735/1770 train_time:69629ms step_avg:96.04ms
step:736/1770 train_time:69727ms step_avg:96.04ms
step:737/1770 train_time:69826ms step_avg:96.05ms
step:738/1770 train_time:69925ms step_avg:96.05ms
step:739/1770 train_time:70024ms step_avg:96.06ms
step:740/1770 train_time:70123ms step_avg:96.06ms
step:741/1770 train_time:70222ms step_avg:96.06ms
step:742/1770 train_time:70321ms step_avg:96.07ms
step:743/1770 train_time:70420ms step_avg:96.07ms
step:744/1770 train_time:70519ms step_avg:96.08ms
step:745/1770 train_time:70618ms step_avg:96.08ms
step:746/1770 train_time:70718ms step_avg:96.08ms
step:747/1770 train_time:70817ms step_avg:96.09ms
step:748/1770 train_time:70915ms step_avg:96.09ms
step:749/1770 train_time:71014ms step_avg:96.09ms
step:750/1770 train_time:71112ms step_avg:96.10ms
step:750/1770 val_loss:3.6063 train_time:71210ms step_avg:96.23ms
step:751/1770 train_time:71232ms step_avg:96.13ms
step:752/1770 train_time:71323ms step_avg:96.12ms
step:753/1770 train_time:71426ms step_avg:96.13ms
step:754/1770 train_time:71524ms step_avg:96.13ms
step:755/1770 train_time:71623ms step_avg:96.14ms
step:756/1770 train_time:71721ms step_avg:96.14ms
step:757/1770 train_time:71820ms step_avg:96.14ms
step:758/1770 train_time:71918ms step_avg:96.15ms
step:759/1770 train_time:72017ms step_avg:96.15ms
step:760/1770 train_time:72115ms step_avg:96.15ms
step:761/1770 train_time:72214ms step_avg:96.16ms
step:762/1770 train_time:72314ms step_avg:96.16ms
step:763/1770 train_time:72414ms step_avg:96.17ms
step:764/1770 train_time:72514ms step_avg:96.17ms
step:765/1770 train_time:72613ms step_avg:96.18ms
step:766/1770 train_time:72713ms step_avg:96.18ms
step:767/1770 train_time:72813ms step_avg:96.19ms
step:768/1770 train_time:72913ms step_avg:96.19ms
step:769/1770 train_time:73011ms step_avg:96.19ms
step:770/1770 train_time:73110ms step_avg:96.20ms
step:771/1770 train_time:73209ms step_avg:96.20ms
step:772/1770 train_time:73308ms step_avg:96.21ms
step:773/1770 train_time:73408ms step_avg:96.21ms
step:774/1770 train_time:73507ms step_avg:96.21ms
step:775/1770 train_time:73606ms step_avg:96.22ms
step:776/1770 train_time:73705ms step_avg:96.22ms
step:777/1770 train_time:73805ms step_avg:96.23ms
step:778/1770 train_time:73905ms step_avg:96.23ms
step:779/1770 train_time:74005ms step_avg:96.24ms
step:780/1770 train_time:74106ms step_avg:96.24ms
step:781/1770 train_time:74206ms step_avg:96.25ms
step:782/1770 train_time:74305ms step_avg:96.25ms
step:783/1770 train_time:74404ms step_avg:96.25ms
step:784/1770 train_time:74503ms step_avg:96.26ms
step:785/1770 train_time:74602ms step_avg:96.26ms
step:786/1770 train_time:74701ms step_avg:96.26ms
step:787/1770 train_time:74800ms step_avg:96.27ms
step:788/1770 train_time:74898ms step_avg:96.27ms
step:789/1770 train_time:74998ms step_avg:96.27ms
step:790/1770 train_time:75097ms step_avg:96.28ms
step:791/1770 train_time:75197ms step_avg:96.28ms
step:792/1770 train_time:75296ms step_avg:96.29ms
step:793/1770 train_time:75396ms step_avg:96.29ms
step:794/1770 train_time:75495ms step_avg:96.29ms
step:795/1770 train_time:75594ms step_avg:96.30ms
step:796/1770 train_time:75694ms step_avg:96.30ms
step:797/1770 train_time:75794ms step_avg:96.31ms
step:798/1770 train_time:75894ms step_avg:96.31ms
step:799/1770 train_time:75993ms step_avg:96.32ms
step:800/1770 train_time:76092ms step_avg:96.32ms
step:801/1770 train_time:76191ms step_avg:96.32ms
step:802/1770 train_time:76290ms step_avg:96.33ms
step:803/1770 train_time:76390ms step_avg:96.33ms
step:804/1770 train_time:76490ms step_avg:96.34ms
step:805/1770 train_time:76589ms step_avg:96.34ms
step:806/1770 train_time:76688ms step_avg:96.34ms
step:807/1770 train_time:76788ms step_avg:96.35ms
step:808/1770 train_time:76889ms step_avg:96.35ms
step:809/1770 train_time:76988ms step_avg:96.36ms
step:810/1770 train_time:77089ms step_avg:96.36ms
step:811/1770 train_time:77188ms step_avg:96.36ms
step:812/1770 train_time:77286ms step_avg:96.37ms
step:813/1770 train_time:77386ms step_avg:96.37ms
step:814/1770 train_time:77485ms step_avg:96.37ms
step:815/1770 train_time:77584ms step_avg:96.38ms
step:816/1770 train_time:77683ms step_avg:96.38ms
step:817/1770 train_time:77783ms step_avg:96.39ms
step:818/1770 train_time:77883ms step_avg:96.39ms
step:819/1770 train_time:77983ms step_avg:96.39ms
step:820/1770 train_time:78082ms step_avg:96.40ms
step:821/1770 train_time:78182ms step_avg:96.40ms
step:822/1770 train_time:78281ms step_avg:96.41ms
step:823/1770 train_time:78381ms step_avg:96.41ms
step:824/1770 train_time:78481ms step_avg:96.41ms
step:825/1770 train_time:78580ms step_avg:96.42ms
step:826/1770 train_time:78679ms step_avg:96.42ms
step:827/1770 train_time:78778ms step_avg:96.42ms
step:828/1770 train_time:78877ms step_avg:96.43ms
step:829/1770 train_time:78976ms step_avg:96.43ms
step:830/1770 train_time:79075ms step_avg:96.43ms
step:831/1770 train_time:79175ms step_avg:96.44ms
step:832/1770 train_time:79275ms step_avg:96.44ms
step:833/1770 train_time:79373ms step_avg:96.44ms
step:834/1770 train_time:79474ms step_avg:96.45ms
step:835/1770 train_time:79573ms step_avg:96.45ms
step:836/1770 train_time:79673ms step_avg:96.46ms
step:837/1770 train_time:79773ms step_avg:96.46ms
step:838/1770 train_time:79872ms step_avg:96.46ms
step:839/1770 train_time:79972ms step_avg:96.47ms
step:840/1770 train_time:80072ms step_avg:96.47ms
step:841/1770 train_time:80171ms step_avg:96.48ms
step:842/1770 train_time:80270ms step_avg:96.48ms
step:843/1770 train_time:80370ms step_avg:96.48ms
step:844/1770 train_time:80469ms step_avg:96.49ms
step:845/1770 train_time:80569ms step_avg:96.49ms
step:846/1770 train_time:80669ms step_avg:96.49ms
step:847/1770 train_time:80769ms step_avg:96.50ms
step:848/1770 train_time:80868ms step_avg:96.50ms
step:849/1770 train_time:80967ms step_avg:96.50ms
step:850/1770 train_time:81066ms step_avg:96.51ms
step:851/1770 train_time:81165ms step_avg:96.51ms
step:852/1770 train_time:81265ms step_avg:96.51ms
step:853/1770 train_time:81364ms step_avg:96.52ms
step:854/1770 train_time:81463ms step_avg:96.52ms
step:855/1770 train_time:81564ms step_avg:96.52ms
step:856/1770 train_time:81663ms step_avg:96.53ms
step:857/1770 train_time:81763ms step_avg:96.53ms
step:858/1770 train_time:81863ms step_avg:96.54ms
step:859/1770 train_time:81963ms step_avg:96.54ms
step:860/1770 train_time:82062ms step_avg:96.54ms
step:861/1770 train_time:82162ms step_avg:96.55ms
step:862/1770 train_time:82262ms step_avg:96.55ms
step:863/1770 train_time:82362ms step_avg:96.56ms
step:864/1770 train_time:82461ms step_avg:96.56ms
step:865/1770 train_time:82560ms step_avg:96.56ms
step:866/1770 train_time:82660ms step_avg:96.57ms
step:867/1770 train_time:82760ms step_avg:96.57ms
step:868/1770 train_time:82860ms step_avg:96.57ms
step:869/1770 train_time:82959ms step_avg:96.58ms
step:870/1770 train_time:83058ms step_avg:96.58ms
step:871/1770 train_time:83158ms step_avg:96.58ms
step:872/1770 train_time:83257ms step_avg:96.59ms
step:873/1770 train_time:83357ms step_avg:96.59ms
step:874/1770 train_time:83457ms step_avg:96.59ms
step:875/1770 train_time:83557ms step_avg:96.60ms
step:875/1770 val_loss:3.5542 train_time:83656ms step_avg:96.71ms
step:876/1770 train_time:83677ms step_avg:96.62ms
step:877/1770 train_time:83769ms step_avg:96.62ms
step:878/1770 train_time:83869ms step_avg:96.62ms
step:879/1770 train_time:83968ms step_avg:96.63ms
step:880/1770 train_time:84067ms step_avg:96.63ms
step:881/1770 train_time:84166ms step_avg:96.63ms
step:882/1770 train_time:84264ms step_avg:96.63ms
step:883/1770 train_time:84363ms step_avg:96.64ms
step:884/1770 train_time:84462ms step_avg:96.64ms
step:885/1770 train_time:84560ms step_avg:96.64ms
step:886/1770 train_time:84659ms step_avg:96.64ms
step:887/1770 train_time:84760ms step_avg:96.65ms
step:888/1770 train_time:84861ms step_avg:96.65ms
step:889/1770 train_time:84961ms step_avg:96.66ms
step:890/1770 train_time:85062ms step_avg:96.66ms
step:891/1770 train_time:85161ms step_avg:96.66ms
step:892/1770 train_time:85260ms step_avg:96.67ms
step:893/1770 train_time:85359ms step_avg:96.67ms
step:894/1770 train_time:85457ms step_avg:96.67ms
step:895/1770 train_time:85557ms step_avg:96.67ms
step:896/1770 train_time:85656ms step_avg:96.68ms
step:897/1770 train_time:85755ms step_avg:96.68ms
step:898/1770 train_time:85854ms step_avg:96.68ms
step:899/1770 train_time:85954ms step_avg:96.69ms
step:900/1770 train_time:86053ms step_avg:96.69ms
step:901/1770 train_time:86153ms step_avg:96.69ms
step:902/1770 train_time:86254ms step_avg:96.70ms
step:903/1770 train_time:86353ms step_avg:96.70ms
step:904/1770 train_time:86453ms step_avg:96.70ms
step:905/1770 train_time:86552ms step_avg:96.71ms
step:906/1770 train_time:86651ms step_avg:96.71ms
step:907/1770 train_time:86750ms step_avg:96.71ms
step:908/1770 train_time:86850ms step_avg:96.71ms
step:909/1770 train_time:86949ms step_avg:96.72ms
step:910/1770 train_time:87049ms step_avg:96.72ms
step:911/1770 train_time:87148ms step_avg:96.72ms
step:912/1770 train_time:87248ms step_avg:96.73ms
step:913/1770 train_time:87347ms step_avg:96.73ms
step:914/1770 train_time:87447ms step_avg:96.73ms
step:915/1770 train_time:87547ms step_avg:96.74ms
step:916/1770 train_time:87647ms step_avg:96.74ms
step:917/1770 train_time:87746ms step_avg:96.74ms
step:918/1770 train_time:87844ms step_avg:96.74ms
step:919/1770 train_time:87943ms step_avg:96.75ms
step:920/1770 train_time:88044ms step_avg:96.75ms
step:921/1770 train_time:88145ms step_avg:96.76ms
step:922/1770 train_time:88246ms step_avg:96.76ms
step:923/1770 train_time:88346ms step_avg:96.76ms
step:924/1770 train_time:88448ms step_avg:96.77ms
step:925/1770 train_time:88548ms step_avg:96.77ms
step:926/1770 train_time:88649ms step_avg:96.78ms
step:927/1770 train_time:88750ms step_avg:96.78ms
step:928/1770 train_time:88850ms step_avg:96.79ms
step:929/1770 train_time:88950ms step_avg:96.79ms
step:930/1770 train_time:89051ms step_avg:96.79ms
step:931/1770 train_time:89152ms step_avg:96.80ms
step:932/1770 train_time:89253ms step_avg:96.80ms
step:933/1770 train_time:89353ms step_avg:96.81ms
step:934/1770 train_time:89453ms step_avg:96.81ms
step:935/1770 train_time:89554ms step_avg:96.82ms
step:936/1770 train_time:89656ms step_avg:96.82ms
step:937/1770 train_time:89756ms step_avg:96.82ms
step:938/1770 train_time:89858ms step_avg:96.83ms
step:939/1770 train_time:89959ms step_avg:96.83ms
step:940/1770 train_time:90061ms step_avg:96.84ms
step:941/1770 train_time:90161ms step_avg:96.84ms
step:942/1770 train_time:90261ms step_avg:96.85ms
step:943/1770 train_time:90362ms step_avg:96.85ms
step:944/1770 train_time:90463ms step_avg:96.86ms
step:945/1770 train_time:90564ms step_avg:96.86ms
step:946/1770 train_time:90665ms step_avg:96.86ms
step:947/1770 train_time:90766ms step_avg:96.87ms
step:948/1770 train_time:90866ms step_avg:96.87ms
step:949/1770 train_time:90968ms step_avg:96.88ms
step:950/1770 train_time:91069ms step_avg:96.88ms
step:951/1770 train_time:91171ms step_avg:96.89ms
step:952/1770 train_time:91271ms step_avg:96.89ms
step:953/1770 train_time:91372ms step_avg:96.90ms
step:954/1770 train_time:91472ms step_avg:96.90ms
step:955/1770 train_time:91572ms step_avg:96.90ms
step:956/1770 train_time:91673ms step_avg:96.91ms
step:957/1770 train_time:91774ms step_avg:96.91ms
step:958/1770 train_time:91875ms step_avg:96.91ms
step:959/1770 train_time:91977ms step_avg:96.92ms
step:960/1770 train_time:92078ms step_avg:96.92ms
step:961/1770 train_time:92179ms step_avg:96.93ms
step:962/1770 train_time:92279ms step_avg:96.93ms
step:963/1770 train_time:92380ms step_avg:96.94ms
step:964/1770 train_time:92480ms step_avg:96.94ms
step:965/1770 train_time:92581ms step_avg:96.94ms
step:966/1770 train_time:92682ms step_avg:96.95ms
step:967/1770 train_time:92782ms step_avg:96.95ms
step:968/1770 train_time:92883ms step_avg:96.96ms
step:969/1770 train_time:92984ms step_avg:96.96ms
step:970/1770 train_time:93084ms step_avg:96.96ms
step:971/1770 train_time:93185ms step_avg:96.97ms
step:972/1770 train_time:93287ms step_avg:96.97ms
step:973/1770 train_time:93389ms step_avg:96.98ms
step:974/1770 train_time:93489ms step_avg:96.98ms
step:975/1770 train_time:93590ms step_avg:96.98ms
step:976/1770 train_time:93690ms step_avg:96.99ms
step:977/1770 train_time:93791ms step_avg:96.99ms
step:978/1770 train_time:93892ms step_avg:97.00ms
step:979/1770 train_time:93992ms step_avg:97.00ms
step:980/1770 train_time:94093ms step_avg:97.00ms
step:981/1770 train_time:94194ms step_avg:97.01ms
step:982/1770 train_time:94295ms step_avg:97.01ms
step:983/1770 train_time:94397ms step_avg:97.02ms
step:984/1770 train_time:94500ms step_avg:97.02ms
step:985/1770 train_time:94601ms step_avg:97.03ms
step:986/1770 train_time:94702ms step_avg:97.03ms
step:987/1770 train_time:94803ms step_avg:97.04ms
step:988/1770 train_time:94903ms step_avg:97.04ms
step:989/1770 train_time:95005ms step_avg:97.04ms
step:990/1770 train_time:95106ms step_avg:97.05ms
step:991/1770 train_time:95206ms step_avg:97.05ms
step:992/1770 train_time:95307ms step_avg:97.05ms
step:993/1770 train_time:95408ms step_avg:97.06ms
step:994/1770 train_time:95510ms step_avg:97.06ms
step:995/1770 train_time:95612ms step_avg:97.07ms
step:996/1770 train_time:95713ms step_avg:97.07ms
step:997/1770 train_time:95814ms step_avg:97.08ms
step:998/1770 train_time:95914ms step_avg:97.08ms
step:999/1770 train_time:96014ms step_avg:97.08ms
step:1000/1770 train_time:96115ms step_avg:97.09ms
step:1000/1770 val_loss:3.5162 train_time:96214ms step_avg:97.19ms
step:1001/1770 train_time:96236ms step_avg:97.11ms
step:1002/1770 train_time:96327ms step_avg:97.10ms
step:1003/1770 train_time:96430ms step_avg:97.11ms
step:1004/1770 train_time:96530ms step_avg:97.11ms
step:1005/1770 train_time:96630ms step_avg:97.12ms
step:1006/1770 train_time:96730ms step_avg:97.12ms
step:1007/1770 train_time:96830ms step_avg:97.12ms
step:1008/1770 train_time:96930ms step_avg:97.12ms
step:1009/1770 train_time:97030ms step_avg:97.13ms
step:1010/1770 train_time:97130ms step_avg:97.13ms
step:1011/1770 train_time:97233ms step_avg:97.14ms
step:1012/1770 train_time:97336ms step_avg:97.14ms
step:1013/1770 train_time:97438ms step_avg:97.15ms
step:1014/1770 train_time:97540ms step_avg:97.15ms
step:1015/1770 train_time:97640ms step_avg:97.15ms
step:1016/1770 train_time:97740ms step_avg:97.16ms
step:1017/1770 train_time:97841ms step_avg:97.16ms
step:1018/1770 train_time:97940ms step_avg:97.16ms
step:1019/1770 train_time:98040ms step_avg:97.17ms
step:1020/1770 train_time:98141ms step_avg:97.17ms
step:1021/1770 train_time:98243ms step_avg:97.17ms
step:1022/1770 train_time:98344ms step_avg:97.18ms
step:1023/1770 train_time:98445ms step_avg:97.18ms
step:1024/1770 train_time:98547ms step_avg:97.19ms
step:1025/1770 train_time:98648ms step_avg:97.19ms
step:1026/1770 train_time:98748ms step_avg:97.19ms
step:1027/1770 train_time:98849ms step_avg:97.20ms
step:1028/1770 train_time:98950ms step_avg:97.20ms
step:1029/1770 train_time:99051ms step_avg:97.20ms
step:1030/1770 train_time:99151ms step_avg:97.21ms
step:1031/1770 train_time:99252ms step_avg:97.21ms
step:1032/1770 train_time:99352ms step_avg:97.21ms
step:1033/1770 train_time:99453ms step_avg:97.22ms
step:1034/1770 train_time:99554ms step_avg:97.22ms
step:1035/1770 train_time:99656ms step_avg:97.23ms
step:1036/1770 train_time:99757ms step_avg:97.23ms
step:1037/1770 train_time:99859ms step_avg:97.23ms
step:1038/1770 train_time:99960ms step_avg:97.24ms
step:1039/1770 train_time:100061ms step_avg:97.24ms
step:1040/1770 train_time:100161ms step_avg:97.24ms
step:1041/1770 train_time:100262ms step_avg:97.25ms
step:1042/1770 train_time:100362ms step_avg:97.25ms
step:1043/1770 train_time:100463ms step_avg:97.25ms
step:1044/1770 train_time:100564ms step_avg:97.26ms
step:1045/1770 train_time:100666ms step_avg:97.26ms
step:1046/1770 train_time:100767ms step_avg:97.27ms
step:1047/1770 train_time:100868ms step_avg:97.27ms
step:1048/1770 train_time:100969ms step_avg:97.27ms
step:1049/1770 train_time:101070ms step_avg:97.28ms
step:1050/1770 train_time:101170ms step_avg:97.28ms
step:1051/1770 train_time:101272ms step_avg:97.28ms
step:1052/1770 train_time:101372ms step_avg:97.29ms
step:1053/1770 train_time:101473ms step_avg:97.29ms
step:1054/1770 train_time:101574ms step_avg:97.29ms
step:1055/1770 train_time:101675ms step_avg:97.30ms
step:1056/1770 train_time:101776ms step_avg:97.30ms
step:1057/1770 train_time:101878ms step_avg:97.30ms
step:1058/1770 train_time:101980ms step_avg:97.31ms
step:1059/1770 train_time:102082ms step_avg:97.31ms
step:1060/1770 train_time:102183ms step_avg:97.32ms
step:1061/1770 train_time:102284ms step_avg:97.32ms
step:1062/1770 train_time:102388ms step_avg:97.33ms
step:1063/1770 train_time:102490ms step_avg:97.33ms
step:1064/1770 train_time:102591ms step_avg:97.34ms
step:1065/1770 train_time:102692ms step_avg:97.34ms
step:1066/1770 train_time:102792ms step_avg:97.34ms
step:1067/1770 train_time:102893ms step_avg:97.34ms
step:1068/1770 train_time:102995ms step_avg:97.35ms
step:1069/1770 train_time:103096ms step_avg:97.35ms
step:1070/1770 train_time:103198ms step_avg:97.36ms
step:1071/1770 train_time:103299ms step_avg:97.36ms
step:1072/1770 train_time:103401ms step_avg:97.36ms
step:1073/1770 train_time:103501ms step_avg:97.37ms
step:1074/1770 train_time:103602ms step_avg:97.37ms
step:1075/1770 train_time:103703ms step_avg:97.37ms
step:1076/1770 train_time:103805ms step_avg:97.38ms
step:1077/1770 train_time:103906ms step_avg:97.38ms
step:1078/1770 train_time:104008ms step_avg:97.39ms
step:1079/1770 train_time:104109ms step_avg:97.39ms
step:1080/1770 train_time:104210ms step_avg:97.39ms
step:1081/1770 train_time:104312ms step_avg:97.40ms
step:1082/1770 train_time:104414ms step_avg:97.40ms
step:1083/1770 train_time:104515ms step_avg:97.40ms
step:1084/1770 train_time:104615ms step_avg:97.41ms
step:1085/1770 train_time:104717ms step_avg:97.41ms
step:1086/1770 train_time:104818ms step_avg:97.41ms
step:1087/1770 train_time:104919ms step_avg:97.42ms
step:1088/1770 train_time:105019ms step_avg:97.42ms
step:1089/1770 train_time:105120ms step_avg:97.42ms
step:1090/1770 train_time:105220ms step_avg:97.43ms
step:1091/1770 train_time:105321ms step_avg:97.43ms
step:1092/1770 train_time:105421ms step_avg:97.43ms
step:1093/1770 train_time:105522ms step_avg:97.43ms
step:1094/1770 train_time:105623ms step_avg:97.44ms
step:1095/1770 train_time:105725ms step_avg:97.44ms
step:1096/1770 train_time:105827ms step_avg:97.45ms
step:1097/1770 train_time:105927ms step_avg:97.45ms
step:1098/1770 train_time:106028ms step_avg:97.45ms
step:1099/1770 train_time:106128ms step_avg:97.45ms
step:1100/1770 train_time:106229ms step_avg:97.46ms
step:1101/1770 train_time:106330ms step_avg:97.46ms
step:1102/1770 train_time:106432ms step_avg:97.47ms
step:1103/1770 train_time:106532ms step_avg:97.47ms
step:1104/1770 train_time:106633ms step_avg:97.47ms
step:1105/1770 train_time:106734ms step_avg:97.47ms
step:1106/1770 train_time:106836ms step_avg:97.48ms
step:1107/1770 train_time:106937ms step_avg:97.48ms
step:1108/1770 train_time:107038ms step_avg:97.48ms
step:1109/1770 train_time:107138ms step_avg:97.49ms
step:1110/1770 train_time:107240ms step_avg:97.49ms
step:1111/1770 train_time:107340ms step_avg:97.49ms
step:1112/1770 train_time:107441ms step_avg:97.50ms
step:1113/1770 train_time:107541ms step_avg:97.50ms
step:1114/1770 train_time:107642ms step_avg:97.50ms
step:1115/1770 train_time:107745ms step_avg:97.51ms
step:1116/1770 train_time:107846ms step_avg:97.51ms
step:1117/1770 train_time:107947ms step_avg:97.51ms
step:1118/1770 train_time:108048ms step_avg:97.52ms
step:1119/1770 train_time:108149ms step_avg:97.52ms
step:1120/1770 train_time:108250ms step_avg:97.52ms
step:1121/1770 train_time:108351ms step_avg:97.53ms
step:1122/1770 train_time:108453ms step_avg:97.53ms
step:1123/1770 train_time:108554ms step_avg:97.53ms
step:1124/1770 train_time:108655ms step_avg:97.54ms
step:1125/1770 train_time:108757ms step_avg:97.54ms
step:1125/1770 val_loss:3.4735 train_time:108857ms step_avg:97.63ms
step:1126/1770 train_time:108878ms step_avg:97.56ms
step:1127/1770 train_time:108970ms step_avg:97.56ms
step:1128/1770 train_time:109072ms step_avg:97.56ms
step:1129/1770 train_time:109172ms step_avg:97.56ms
step:1130/1770 train_time:109274ms step_avg:97.57ms
step:1131/1770 train_time:109375ms step_avg:97.57ms
step:1132/1770 train_time:109476ms step_avg:97.57ms
step:1133/1770 train_time:109576ms step_avg:97.57ms
step:1134/1770 train_time:109677ms step_avg:97.58ms
step:1135/1770 train_time:109778ms step_avg:97.58ms
step:1136/1770 train_time:109878ms step_avg:97.58ms
step:1137/1770 train_time:109980ms step_avg:97.59ms
step:1138/1770 train_time:110081ms step_avg:97.59ms
step:1139/1770 train_time:110183ms step_avg:97.59ms
step:1140/1770 train_time:110285ms step_avg:97.60ms
step:1141/1770 train_time:110385ms step_avg:97.60ms
step:1142/1770 train_time:110486ms step_avg:97.60ms
step:1143/1770 train_time:110587ms step_avg:97.61ms
step:1144/1770 train_time:110687ms step_avg:97.61ms
step:1145/1770 train_time:110788ms step_avg:97.61ms
step:1146/1770 train_time:110891ms step_avg:97.61ms
step:1147/1770 train_time:110992ms step_avg:97.62ms
step:1148/1770 train_time:111093ms step_avg:97.62ms
step:1149/1770 train_time:111194ms step_avg:97.62ms
step:1150/1770 train_time:111295ms step_avg:97.63ms
step:1151/1770 train_time:111397ms step_avg:97.63ms
step:1152/1770 train_time:111499ms step_avg:97.63ms
step:1153/1770 train_time:111599ms step_avg:97.64ms
step:1154/1770 train_time:111700ms step_avg:97.64ms
step:1155/1770 train_time:111800ms step_avg:97.64ms
step:1156/1770 train_time:111901ms step_avg:97.64ms
step:1157/1770 train_time:112004ms step_avg:97.65ms
step:1158/1770 train_time:112106ms step_avg:97.65ms
step:1159/1770 train_time:112207ms step_avg:97.66ms
step:1160/1770 train_time:112309ms step_avg:97.66ms
step:1161/1770 train_time:112410ms step_avg:97.66ms
step:1162/1770 train_time:112513ms step_avg:97.67ms
step:1163/1770 train_time:112613ms step_avg:97.67ms
step:1164/1770 train_time:112714ms step_avg:97.67ms
step:1165/1770 train_time:112816ms step_avg:97.68ms
step:1166/1770 train_time:112919ms step_avg:97.68ms
step:1167/1770 train_time:113020ms step_avg:97.68ms
step:1168/1770 train_time:113120ms step_avg:97.69ms
step:1169/1770 train_time:113221ms step_avg:97.69ms
step:1170/1770 train_time:113323ms step_avg:97.69ms
step:1171/1770 train_time:113424ms step_avg:97.70ms
step:1172/1770 train_time:113526ms step_avg:97.70ms
step:1173/1770 train_time:113626ms step_avg:97.70ms
step:1174/1770 train_time:113728ms step_avg:97.70ms
step:1175/1770 train_time:113829ms step_avg:97.71ms
step:1176/1770 train_time:113931ms step_avg:97.71ms
step:1177/1770 train_time:114032ms step_avg:97.71ms
step:1178/1770 train_time:114132ms step_avg:97.72ms
step:1179/1770 train_time:114234ms step_avg:97.72ms
step:1180/1770 train_time:114335ms step_avg:97.72ms
step:1181/1770 train_time:114437ms step_avg:97.73ms
step:1182/1770 train_time:114538ms step_avg:97.73ms
step:1183/1770 train_time:114640ms step_avg:97.73ms
step:1184/1770 train_time:114743ms step_avg:97.74ms
step:1185/1770 train_time:114844ms step_avg:97.74ms
step:1186/1770 train_time:114947ms step_avg:97.74ms
step:1187/1770 train_time:115051ms step_avg:97.75ms
step:1188/1770 train_time:115153ms step_avg:97.75ms
step:1189/1770 train_time:115254ms step_avg:97.76ms
step:1190/1770 train_time:115356ms step_avg:97.76ms
step:1191/1770 train_time:115459ms step_avg:97.76ms
step:1192/1770 train_time:115560ms step_avg:97.77ms
step:1193/1770 train_time:115662ms step_avg:97.77ms
step:1194/1770 train_time:115765ms step_avg:97.77ms
step:1195/1770 train_time:115867ms step_avg:97.78ms
step:1196/1770 train_time:115971ms step_avg:97.78ms
step:1197/1770 train_time:116073ms step_avg:97.79ms
step:1198/1770 train_time:116174ms step_avg:97.79ms
step:1199/1770 train_time:116276ms step_avg:97.79ms
step:1200/1770 train_time:116379ms step_avg:97.80ms
step:1201/1770 train_time:116482ms step_avg:97.80ms
step:1202/1770 train_time:116583ms step_avg:97.80ms
step:1203/1770 train_time:116686ms step_avg:97.81ms
step:1204/1770 train_time:116788ms step_avg:97.81ms
step:1205/1770 train_time:116890ms step_avg:97.82ms
step:1206/1770 train_time:116992ms step_avg:97.82ms
step:1207/1770 train_time:117094ms step_avg:97.82ms
step:1208/1770 train_time:117196ms step_avg:97.83ms
step:1209/1770 train_time:117297ms step_avg:97.83ms
step:1210/1770 train_time:117400ms step_avg:97.83ms
step:1211/1770 train_time:117502ms step_avg:97.84ms
step:1212/1770 train_time:117605ms step_avg:97.84ms
step:1213/1770 train_time:117707ms step_avg:97.84ms
step:1214/1770 train_time:117808ms step_avg:97.85ms
step:1215/1770 train_time:117910ms step_avg:97.85ms
step:1216/1770 train_time:118014ms step_avg:97.86ms
step:1217/1770 train_time:118116ms step_avg:97.86ms
step:1218/1770 train_time:118218ms step_avg:97.86ms
step:1219/1770 train_time:118320ms step_avg:97.87ms
step:1220/1770 train_time:118423ms step_avg:97.87ms
step:1221/1770 train_time:118524ms step_avg:97.87ms
step:1222/1770 train_time:118628ms step_avg:97.88ms
step:1223/1770 train_time:118729ms step_avg:97.88ms
step:1224/1770 train_time:118832ms step_avg:97.88ms
step:1225/1770 train_time:118934ms step_avg:97.89ms
step:1226/1770 train_time:119036ms step_avg:97.89ms
step:1227/1770 train_time:119140ms step_avg:97.90ms
step:1228/1770 train_time:119244ms step_avg:97.90ms
step:1229/1770 train_time:119346ms step_avg:97.90ms
step:1230/1770 train_time:119448ms step_avg:97.91ms
step:1231/1770 train_time:119550ms step_avg:97.91ms
step:1232/1770 train_time:119652ms step_avg:97.92ms
step:1233/1770 train_time:119755ms step_avg:97.92ms
step:1234/1770 train_time:119856ms step_avg:97.92ms
step:1235/1770 train_time:119958ms step_avg:97.93ms
step:1236/1770 train_time:120060ms step_avg:97.93ms
step:1237/1770 train_time:120162ms step_avg:97.93ms
step:1238/1770 train_time:120264ms step_avg:97.94ms
step:1239/1770 train_time:120367ms step_avg:97.94ms
step:1240/1770 train_time:120469ms step_avg:97.94ms
step:1241/1770 train_time:120571ms step_avg:97.95ms
step:1242/1770 train_time:120673ms step_avg:97.95ms
step:1243/1770 train_time:120776ms step_avg:97.95ms
step:1244/1770 train_time:120877ms step_avg:97.96ms
step:1245/1770 train_time:120979ms step_avg:97.96ms
step:1246/1770 train_time:121081ms step_avg:97.96ms
step:1247/1770 train_time:121183ms step_avg:97.96ms
step:1248/1770 train_time:121286ms step_avg:97.97ms
step:1249/1770 train_time:121387ms step_avg:97.97ms
step:1250/1770 train_time:121489ms step_avg:97.98ms
step:1250/1770 val_loss:3.4268 train_time:121591ms step_avg:98.06ms
step:1251/1770 train_time:121614ms step_avg:98.00ms
step:1252/1770 train_time:121702ms step_avg:97.99ms
step:1253/1770 train_time:121805ms step_avg:97.99ms
step:1254/1770 train_time:121907ms step_avg:98.00ms
step:1255/1770 train_time:122011ms step_avg:98.00ms
step:1256/1770 train_time:122111ms step_avg:98.00ms
step:1257/1770 train_time:122212ms step_avg:98.01ms
step:1258/1770 train_time:122315ms step_avg:98.01ms
step:1259/1770 train_time:122417ms step_avg:98.01ms
step:1260/1770 train_time:122518ms step_avg:98.01ms
step:1261/1770 train_time:122622ms step_avg:98.02ms
step:1262/1770 train_time:122725ms step_avg:98.02ms
step:1263/1770 train_time:122827ms step_avg:98.03ms
step:1264/1770 train_time:122931ms step_avg:98.03ms
step:1265/1770 train_time:123032ms step_avg:98.03ms
step:1266/1770 train_time:123134ms step_avg:98.04ms
step:1267/1770 train_time:123236ms step_avg:98.04ms
step:1268/1770 train_time:123338ms step_avg:98.04ms
step:1269/1770 train_time:123440ms step_avg:98.05ms
step:1270/1770 train_time:123542ms step_avg:98.05ms
step:1271/1770 train_time:123643ms step_avg:98.05ms
step:1272/1770 train_time:123746ms step_avg:98.06ms
step:1273/1770 train_time:123849ms step_avg:98.06ms
step:1274/1770 train_time:123951ms step_avg:98.06ms
step:1275/1770 train_time:124053ms step_avg:98.07ms
step:1276/1770 train_time:124155ms step_avg:98.07ms
step:1277/1770 train_time:124257ms step_avg:98.07ms
step:1278/1770 train_time:124360ms step_avg:98.08ms
step:1279/1770 train_time:124463ms step_avg:98.08ms
step:1280/1770 train_time:124566ms step_avg:98.08ms
step:1281/1770 train_time:124667ms step_avg:98.09ms
step:1282/1770 train_time:124770ms step_avg:98.09ms
step:1283/1770 train_time:124873ms step_avg:98.09ms
step:1284/1770 train_time:124976ms step_avg:98.10ms
step:1285/1770 train_time:125078ms step_avg:98.10ms
step:1286/1770 train_time:125182ms step_avg:98.11ms
step:1287/1770 train_time:125286ms step_avg:98.11ms
step:1288/1770 train_time:125389ms step_avg:98.11ms
step:1289/1770 train_time:125491ms step_avg:98.12ms
step:1290/1770 train_time:125593ms step_avg:98.12ms
step:1291/1770 train_time:125694ms step_avg:98.12ms
step:1292/1770 train_time:125796ms step_avg:98.12ms
step:1293/1770 train_time:125899ms step_avg:98.13ms
step:1294/1770 train_time:126001ms step_avg:98.13ms
step:1295/1770 train_time:126103ms step_avg:98.13ms
step:1296/1770 train_time:126205ms step_avg:98.14ms
step:1297/1770 train_time:126306ms step_avg:98.14ms
step:1298/1770 train_time:126408ms step_avg:98.14ms
step:1299/1770 train_time:126511ms step_avg:98.15ms
step:1300/1770 train_time:126612ms step_avg:98.15ms
step:1301/1770 train_time:126715ms step_avg:98.15ms
step:1302/1770 train_time:126816ms step_avg:98.16ms
step:1303/1770 train_time:126918ms step_avg:98.16ms
step:1304/1770 train_time:127021ms step_avg:98.16ms
step:1305/1770 train_time:127124ms step_avg:98.17ms
step:1306/1770 train_time:127226ms step_avg:98.17ms
step:1307/1770 train_time:127329ms step_avg:98.17ms
step:1308/1770 train_time:127431ms step_avg:98.17ms
step:1309/1770 train_time:127533ms step_avg:98.18ms
step:1310/1770 train_time:127634ms step_avg:98.18ms
step:1311/1770 train_time:127736ms step_avg:98.18ms
step:1312/1770 train_time:127838ms step_avg:98.19ms
step:1313/1770 train_time:127940ms step_avg:98.19ms
step:1314/1770 train_time:128042ms step_avg:98.19ms
step:1315/1770 train_time:128143ms step_avg:98.19ms
step:1316/1770 train_time:128246ms step_avg:98.20ms
step:1317/1770 train_time:128349ms step_avg:98.20ms
step:1318/1770 train_time:128453ms step_avg:98.21ms
step:1319/1770 train_time:128556ms step_avg:98.21ms
step:1320/1770 train_time:128658ms step_avg:98.21ms
step:1321/1770 train_time:128760ms step_avg:98.21ms
step:1322/1770 train_time:128862ms step_avg:98.22ms
step:1323/1770 train_time:128965ms step_avg:98.22ms
step:1324/1770 train_time:129068ms step_avg:98.23ms
step:1325/1770 train_time:129171ms step_avg:98.23ms
step:1326/1770 train_time:129273ms step_avg:98.23ms
step:1327/1770 train_time:129378ms step_avg:98.24ms
step:1328/1770 train_time:129480ms step_avg:98.24ms
step:1329/1770 train_time:129583ms step_avg:98.24ms
step:1330/1770 train_time:129684ms step_avg:98.25ms
step:1331/1770 train_time:129786ms step_avg:98.25ms
step:1332/1770 train_time:129887ms step_avg:98.25ms
step:1333/1770 train_time:129989ms step_avg:98.25ms
step:1334/1770 train_time:130091ms step_avg:98.26ms
step:1335/1770 train_time:130193ms step_avg:98.26ms
step:1336/1770 train_time:130295ms step_avg:98.26ms
step:1337/1770 train_time:130397ms step_avg:98.26ms
step:1338/1770 train_time:130498ms step_avg:98.27ms
step:1339/1770 train_time:130602ms step_avg:98.27ms
step:1340/1770 train_time:130705ms step_avg:98.27ms
step:1341/1770 train_time:130807ms step_avg:98.28ms
step:1342/1770 train_time:130909ms step_avg:98.28ms
step:1343/1770 train_time:131012ms step_avg:98.28ms
step:1344/1770 train_time:131114ms step_avg:98.29ms
step:1345/1770 train_time:131216ms step_avg:98.29ms
step:1346/1770 train_time:131319ms step_avg:98.29ms
step:1347/1770 train_time:131420ms step_avg:98.30ms
step:1348/1770 train_time:131524ms step_avg:98.30ms
step:1349/1770 train_time:131626ms step_avg:98.30ms
step:1350/1770 train_time:131728ms step_avg:98.30ms
step:1351/1770 train_time:131830ms step_avg:98.31ms
step:1352/1770 train_time:131932ms step_avg:98.31ms
step:1353/1770 train_time:132035ms step_avg:98.31ms
step:1354/1770 train_time:132136ms step_avg:98.32ms
step:1355/1770 train_time:132239ms step_avg:98.32ms
step:1356/1770 train_time:132340ms step_avg:98.32ms
step:1357/1770 train_time:132443ms step_avg:98.32ms
step:1358/1770 train_time:132545ms step_avg:98.33ms
step:1359/1770 train_time:132648ms step_avg:98.33ms
step:1360/1770 train_time:132750ms step_avg:98.33ms
step:1361/1770 train_time:132853ms step_avg:98.34ms
step:1362/1770 train_time:132956ms step_avg:98.34ms
step:1363/1770 train_time:133058ms step_avg:98.34ms
step:1364/1770 train_time:133162ms step_avg:98.35ms
step:1365/1770 train_time:133263ms step_avg:98.35ms
step:1366/1770 train_time:133365ms step_avg:98.35ms
step:1367/1770 train_time:133467ms step_avg:98.35ms
step:1368/1770 train_time:133569ms step_avg:98.36ms
step:1369/1770 train_time:133671ms step_avg:98.36ms
step:1370/1770 train_time:133774ms step_avg:98.36ms
step:1371/1770 train_time:133876ms step_avg:98.37ms
step:1372/1770 train_time:133978ms step_avg:98.37ms
step:1373/1770 train_time:134081ms step_avg:98.37ms
step:1374/1770 train_time:134183ms step_avg:98.37ms
step:1375/1770 train_time:134285ms step_avg:98.38ms
step:1375/1770 val_loss:3.3827 train_time:134386ms step_avg:98.45ms
step:1376/1770 train_time:134407ms step_avg:98.39ms
step:1377/1770 train_time:134500ms step_avg:98.39ms
step:1378/1770 train_time:134603ms step_avg:98.39ms
step:1379/1770 train_time:134706ms step_avg:98.40ms
step:1380/1770 train_time:134808ms step_avg:98.40ms
step:1381/1770 train_time:134910ms step_avg:98.40ms
step:1382/1770 train_time:135012ms step_avg:98.41ms
step:1383/1770 train_time:135115ms step_avg:98.41ms
step:1384/1770 train_time:135217ms step_avg:98.41ms
step:1385/1770 train_time:135319ms step_avg:98.41ms
step:1386/1770 train_time:135422ms step_avg:98.42ms
step:1387/1770 train_time:135526ms step_avg:98.42ms
step:1388/1770 train_time:135627ms step_avg:98.42ms
step:1389/1770 train_time:135729ms step_avg:98.43ms
step:1390/1770 train_time:135831ms step_avg:98.43ms
step:1391/1770 train_time:135932ms step_avg:98.43ms
step:1392/1770 train_time:136035ms step_avg:98.43ms
step:1393/1770 train_time:136137ms step_avg:98.44ms
step:1394/1770 train_time:136239ms step_avg:98.44ms
step:1395/1770 train_time:136342ms step_avg:98.44ms
step:1396/1770 train_time:136445ms step_avg:98.45ms
step:1397/1770 train_time:136548ms step_avg:98.45ms
step:1398/1770 train_time:136650ms step_avg:98.45ms
step:1399/1770 train_time:136752ms step_avg:98.45ms
step:1400/1770 train_time:136855ms step_avg:98.46ms
step:1401/1770 train_time:136957ms step_avg:98.46ms
step:1402/1770 train_time:137059ms step_avg:98.46ms
step:1403/1770 train_time:137161ms step_avg:98.46ms
step:1404/1770 train_time:137263ms step_avg:98.47ms
step:1405/1770 train_time:137365ms step_avg:98.47ms
step:1406/1770 train_time:137467ms step_avg:98.47ms
step:1407/1770 train_time:137569ms step_avg:98.47ms
step:1408/1770 train_time:137672ms step_avg:98.48ms
step:1409/1770 train_time:137774ms step_avg:98.48ms
step:1410/1770 train_time:137876ms step_avg:98.48ms
step:1411/1770 train_time:137977ms step_avg:98.48ms
step:1412/1770 train_time:138080ms step_avg:98.49ms
step:1413/1770 train_time:138181ms step_avg:98.49ms
step:1414/1770 train_time:138284ms step_avg:98.49ms
step:1415/1770 train_time:138387ms step_avg:98.50ms
step:1416/1770 train_time:138490ms step_avg:98.50ms
step:1417/1770 train_time:138592ms step_avg:98.50ms
step:1418/1770 train_time:138694ms step_avg:98.50ms
step:1419/1770 train_time:138797ms step_avg:98.51ms
step:1420/1770 train_time:138899ms step_avg:98.51ms
step:1421/1770 train_time:139001ms step_avg:98.51ms
step:1422/1770 train_time:139102ms step_avg:98.51ms
step:1423/1770 train_time:139204ms step_avg:98.52ms
step:1424/1770 train_time:139307ms step_avg:98.52ms
step:1425/1770 train_time:139409ms step_avg:98.52ms
step:1426/1770 train_time:139511ms step_avg:98.52ms
step:1427/1770 train_time:139613ms step_avg:98.53ms
step:1428/1770 train_time:139717ms step_avg:98.53ms
step:1429/1770 train_time:139821ms step_avg:98.53ms
step:1430/1770 train_time:139922ms step_avg:98.54ms
step:1431/1770 train_time:140025ms step_avg:98.54ms
step:1432/1770 train_time:140126ms step_avg:98.54ms
step:1433/1770 train_time:140228ms step_avg:98.54ms
step:1434/1770 train_time:140330ms step_avg:98.55ms
step:1435/1770 train_time:140432ms step_avg:98.55ms
step:1436/1770 train_time:140536ms step_avg:98.55ms
step:1437/1770 train_time:140639ms step_avg:98.56ms
step:1438/1770 train_time:140741ms step_avg:98.56ms
step:1439/1770 train_time:140844ms step_avg:98.56ms
step:1440/1770 train_time:140945ms step_avg:98.56ms
step:1441/1770 train_time:141050ms step_avg:98.57ms
step:1442/1770 train_time:141152ms step_avg:98.57ms
step:1443/1770 train_time:141254ms step_avg:98.57ms
step:1444/1770 train_time:141356ms step_avg:98.57ms
step:1445/1770 train_time:141459ms step_avg:98.58ms
step:1446/1770 train_time:141562ms step_avg:98.58ms
step:1447/1770 train_time:141666ms step_avg:98.58ms
step:1448/1770 train_time:141770ms step_avg:98.59ms
step:1449/1770 train_time:141874ms step_avg:98.59ms
step:1450/1770 train_time:141977ms step_avg:98.60ms
step:1451/1770 train_time:142081ms step_avg:98.60ms
step:1452/1770 train_time:142186ms step_avg:98.60ms
step:1453/1770 train_time:142289ms step_avg:98.61ms
step:1454/1770 train_time:142393ms step_avg:98.61ms
step:1455/1770 train_time:142497ms step_avg:98.61ms
step:1456/1770 train_time:142600ms step_avg:98.62ms
step:1457/1770 train_time:142704ms step_avg:98.62ms
step:1458/1770 train_time:142808ms step_avg:98.62ms
step:1459/1770 train_time:142912ms step_avg:98.63ms
step:1460/1770 train_time:143015ms step_avg:98.63ms
step:1461/1770 train_time:143118ms step_avg:98.63ms
step:1462/1770 train_time:143221ms step_avg:98.64ms
step:1463/1770 train_time:143325ms step_avg:98.64ms
step:1464/1770 train_time:143429ms step_avg:98.64ms
step:1465/1770 train_time:143532ms step_avg:98.65ms
step:1466/1770 train_time:143636ms step_avg:98.65ms
step:1467/1770 train_time:143740ms step_avg:98.65ms
step:1468/1770 train_time:143843ms step_avg:98.66ms
step:1469/1770 train_time:143946ms step_avg:98.66ms
step:1470/1770 train_time:144050ms step_avg:98.66ms
step:1471/1770 train_time:144154ms step_avg:98.67ms
step:1472/1770 train_time:144257ms step_avg:98.67ms
step:1473/1770 train_time:144360ms step_avg:98.67ms
step:1474/1770 train_time:144465ms step_avg:98.68ms
step:1475/1770 train_time:144568ms step_avg:98.68ms
step:1476/1770 train_time:144671ms step_avg:98.68ms
step:1477/1770 train_time:144776ms step_avg:98.69ms
step:1478/1770 train_time:144880ms step_avg:98.69ms
step:1479/1770 train_time:144983ms step_avg:98.69ms
step:1480/1770 train_time:145087ms step_avg:98.70ms
step:1481/1770 train_time:145194ms step_avg:98.70ms
step:1482/1770 train_time:145297ms step_avg:98.71ms
step:1483/1770 train_time:145400ms step_avg:98.71ms
step:1484/1770 train_time:145503ms step_avg:98.71ms
step:1485/1770 train_time:145607ms step_avg:98.72ms
step:1486/1770 train_time:145710ms step_avg:98.72ms
step:1487/1770 train_time:145814ms step_avg:98.72ms
step:1488/1770 train_time:145918ms step_avg:98.73ms
step:1489/1770 train_time:146022ms step_avg:98.73ms
step:1490/1770 train_time:146125ms step_avg:98.73ms
step:1491/1770 train_time:146227ms step_avg:98.74ms
step:1492/1770 train_time:146331ms step_avg:98.74ms
step:1493/1770 train_time:146437ms step_avg:98.74ms
step:1494/1770 train_time:146544ms step_avg:98.75ms
step:1495/1770 train_time:146646ms step_avg:98.75ms
step:1496/1770 train_time:146749ms step_avg:98.75ms
step:1497/1770 train_time:146853ms step_avg:98.76ms
step:1498/1770 train_time:146956ms step_avg:98.76ms
step:1499/1770 train_time:147058ms step_avg:98.76ms
step:1500/1770 train_time:147160ms step_avg:98.77ms
step:1500/1770 val_loss:3.3444 train_time:147261ms step_avg:98.83ms
step:1501/1770 train_time:147283ms step_avg:98.78ms
step:1502/1770 train_time:147375ms step_avg:98.78ms
step:1503/1770 train_time:147477ms step_avg:98.78ms
step:1504/1770 train_time:147580ms step_avg:98.78ms
step:1505/1770 train_time:147686ms step_avg:98.79ms
step:1506/1770 train_time:147789ms step_avg:98.79ms
step:1507/1770 train_time:147892ms step_avg:98.79ms
step:1508/1770 train_time:147997ms step_avg:98.80ms
step:1509/1770 train_time:148099ms step_avg:98.80ms
step:1510/1770 train_time:148201ms step_avg:98.80ms
step:1511/1770 train_time:148307ms step_avg:98.81ms
step:1512/1770 train_time:148411ms step_avg:98.81ms
step:1513/1770 train_time:148515ms step_avg:98.81ms
step:1514/1770 train_time:148618ms step_avg:98.82ms
step:1515/1770 train_time:148722ms step_avg:98.82ms
step:1516/1770 train_time:148826ms step_avg:98.82ms
step:1517/1770 train_time:148928ms step_avg:98.82ms
step:1518/1770 train_time:149033ms step_avg:98.83ms
step:1519/1770 train_time:149135ms step_avg:98.83ms
step:1520/1770 train_time:149240ms step_avg:98.83ms
step:1521/1770 train_time:149343ms step_avg:98.84ms
step:1522/1770 train_time:149446ms step_avg:98.84ms
step:1523/1770 train_time:149550ms step_avg:98.84ms
step:1524/1770 train_time:149653ms step_avg:98.85ms
step:1525/1770 train_time:149757ms step_avg:98.85ms
step:1526/1770 train_time:149860ms step_avg:98.85ms
step:1527/1770 train_time:149964ms step_avg:98.86ms
step:1528/1770 train_time:150068ms step_avg:98.86ms
step:1529/1770 train_time:150170ms step_avg:98.86ms
step:1530/1770 train_time:150273ms step_avg:98.86ms
step:1531/1770 train_time:150376ms step_avg:98.87ms
step:1532/1770 train_time:150482ms step_avg:98.87ms
step:1533/1770 train_time:150585ms step_avg:98.87ms
step:1534/1770 train_time:150690ms step_avg:98.88ms
step:1535/1770 train_time:150792ms step_avg:98.88ms
step:1536/1770 train_time:150895ms step_avg:98.88ms
step:1537/1770 train_time:150998ms step_avg:98.89ms
step:1538/1770 train_time:151103ms step_avg:98.89ms
step:1539/1770 train_time:151206ms step_avg:98.89ms
step:1540/1770 train_time:151312ms step_avg:98.90ms
step:1541/1770 train_time:151417ms step_avg:98.90ms
step:1542/1770 train_time:151520ms step_avg:98.90ms
step:1543/1770 train_time:151623ms step_avg:98.91ms
step:1544/1770 train_time:151728ms step_avg:98.91ms
step:1545/1770 train_time:151831ms step_avg:98.91ms
step:1546/1770 train_time:151934ms step_avg:98.92ms
step:1547/1770 train_time:152037ms step_avg:98.92ms
step:1548/1770 train_time:152141ms step_avg:98.92ms
step:1549/1770 train_time:152245ms step_avg:98.92ms
step:1550/1770 train_time:152348ms step_avg:98.93ms
step:1551/1770 train_time:152452ms step_avg:98.93ms
step:1552/1770 train_time:152558ms step_avg:98.94ms
step:1553/1770 train_time:152662ms step_avg:98.94ms
step:1554/1770 train_time:152764ms step_avg:98.94ms
step:1555/1770 train_time:152868ms step_avg:98.94ms
step:1556/1770 train_time:152971ms step_avg:98.95ms
step:1557/1770 train_time:153074ms step_avg:98.95ms
step:1558/1770 train_time:153177ms step_avg:98.95ms
step:1559/1770 train_time:153281ms step_avg:98.95ms
step:1560/1770 train_time:153383ms step_avg:98.96ms
step:1561/1770 train_time:153487ms step_avg:98.96ms
step:1562/1770 train_time:153590ms step_avg:98.96ms
step:1563/1770 train_time:153693ms step_avg:98.97ms
step:1564/1770 train_time:153796ms step_avg:98.97ms
step:1565/1770 train_time:153900ms step_avg:98.97ms
step:1566/1770 train_time:154004ms step_avg:98.97ms
step:1567/1770 train_time:154107ms step_avg:98.98ms
step:1568/1770 train_time:154210ms step_avg:98.98ms
step:1569/1770 train_time:154316ms step_avg:98.98ms
step:1570/1770 train_time:154419ms step_avg:98.99ms
step:1571/1770 train_time:154522ms step_avg:98.99ms
step:1572/1770 train_time:154626ms step_avg:98.99ms
step:1573/1770 train_time:154731ms step_avg:99.00ms
step:1574/1770 train_time:154834ms step_avg:99.00ms
step:1575/1770 train_time:154936ms step_avg:99.00ms
step:1576/1770 train_time:155039ms step_avg:99.00ms
step:1577/1770 train_time:155144ms step_avg:99.01ms
step:1578/1770 train_time:155249ms step_avg:99.01ms
step:1579/1770 train_time:155352ms step_avg:99.01ms
step:1580/1770 train_time:155456ms step_avg:99.02ms
step:1581/1770 train_time:155561ms step_avg:99.02ms
step:1582/1770 train_time:155666ms step_avg:99.02ms
step:1583/1770 train_time:155770ms step_avg:99.03ms
step:1584/1770 train_time:155874ms step_avg:99.03ms
step:1585/1770 train_time:155977ms step_avg:99.03ms
step:1586/1770 train_time:156084ms step_avg:99.04ms
step:1587/1770 train_time:156188ms step_avg:99.04ms
step:1588/1770 train_time:156291ms step_avg:99.04ms
step:1589/1770 train_time:156396ms step_avg:99.05ms
step:1590/1770 train_time:156499ms step_avg:99.05ms
step:1591/1770 train_time:156603ms step_avg:99.05ms
step:1592/1770 train_time:156707ms step_avg:99.06ms
step:1593/1770 train_time:156810ms step_avg:99.06ms
step:1594/1770 train_time:156913ms step_avg:99.06ms
step:1595/1770 train_time:157016ms step_avg:99.06ms
step:1596/1770 train_time:157121ms step_avg:99.07ms
step:1597/1770 train_time:157224ms step_avg:99.07ms
step:1598/1770 train_time:157328ms step_avg:99.07ms
step:1599/1770 train_time:157434ms step_avg:99.08ms
step:1600/1770 train_time:157539ms step_avg:99.08ms
step:1601/1770 train_time:157643ms step_avg:99.08ms
step:1602/1770 train_time:157748ms step_avg:99.09ms
step:1603/1770 train_time:157851ms step_avg:99.09ms
step:1604/1770 train_time:157953ms step_avg:99.09ms
step:1605/1770 train_time:158056ms step_avg:99.09ms
step:1606/1770 train_time:158160ms step_avg:99.10ms
step:1607/1770 train_time:158267ms step_avg:99.10ms
step:1608/1770 train_time:158370ms step_avg:99.11ms
step:1609/1770 train_time:158473ms step_avg:99.11ms
step:1610/1770 train_time:158578ms step_avg:99.11ms
step:1611/1770 train_time:158684ms step_avg:99.12ms
step:1612/1770 train_time:158789ms step_avg:99.12ms
step:1613/1770 train_time:158892ms step_avg:99.12ms
step:1614/1770 train_time:158995ms step_avg:99.12ms
step:1615/1770 train_time:159098ms step_avg:99.13ms
step:1616/1770 train_time:159201ms step_avg:99.13ms
step:1617/1770 train_time:159307ms step_avg:99.13ms
step:1618/1770 train_time:159412ms step_avg:99.14ms
step:1619/1770 train_time:159516ms step_avg:99.14ms
step:1620/1770 train_time:159619ms step_avg:99.14ms
step:1621/1770 train_time:159723ms step_avg:99.15ms
step:1622/1770 train_time:159827ms step_avg:99.15ms
step:1623/1770 train_time:159933ms step_avg:99.15ms
step:1624/1770 train_time:160036ms step_avg:99.15ms
step:1625/1770 train_time:160138ms step_avg:99.16ms
step:1625/1770 val_loss:3.3100 train_time:160240ms step_avg:99.22ms
step:1626/1770 train_time:160262ms step_avg:99.17ms
step:1627/1770 train_time:160353ms step_avg:99.17ms
step:1628/1770 train_time:160457ms step_avg:99.17ms
step:1629/1770 train_time:160560ms step_avg:99.17ms
step:1630/1770 train_time:160663ms step_avg:99.17ms
step:1631/1770 train_time:160766ms step_avg:99.18ms
step:1632/1770 train_time:160868ms step_avg:99.18ms
step:1633/1770 train_time:160971ms step_avg:99.18ms
step:1634/1770 train_time:161074ms step_avg:99.18ms
step:1635/1770 train_time:161177ms step_avg:99.19ms
step:1636/1770 train_time:161283ms step_avg:99.19ms
step:1637/1770 train_time:161387ms step_avg:99.19ms
step:1638/1770 train_time:161491ms step_avg:99.20ms
step:1639/1770 train_time:161595ms step_avg:99.20ms
step:1640/1770 train_time:161700ms step_avg:99.20ms
step:1641/1770 train_time:161804ms step_avg:99.21ms
step:1642/1770 train_time:161907ms step_avg:99.21ms
step:1643/1770 train_time:162009ms step_avg:99.21ms
step:1644/1770 train_time:162114ms step_avg:99.21ms
step:1645/1770 train_time:162217ms step_avg:99.22ms
step:1646/1770 train_time:162322ms step_avg:99.22ms
step:1647/1770 train_time:162427ms step_avg:99.22ms
step:1648/1770 train_time:162530ms step_avg:99.22ms
step:1649/1770 train_time:162636ms step_avg:99.23ms
step:1650/1770 train_time:162739ms step_avg:99.23ms
step:1651/1770 train_time:162842ms step_avg:99.23ms
step:1652/1770 train_time:162946ms step_avg:99.24ms
step:1653/1770 train_time:163049ms step_avg:99.24ms
step:1654/1770 train_time:163156ms step_avg:99.24ms
step:1655/1770 train_time:163262ms step_avg:99.25ms
step:1656/1770 train_time:163365ms step_avg:99.25ms
step:1657/1770 train_time:163470ms step_avg:99.25ms
step:1658/1770 train_time:163574ms step_avg:99.26ms
step:1659/1770 train_time:163680ms step_avg:99.26ms
step:1660/1770 train_time:163784ms step_avg:99.26ms
step:1661/1770 train_time:163888ms step_avg:99.27ms
step:1662/1770 train_time:163992ms step_avg:99.27ms
step:1663/1770 train_time:164094ms step_avg:99.27ms
step:1664/1770 train_time:164198ms step_avg:99.27ms
step:1665/1770 train_time:164301ms step_avg:99.28ms
step:1666/1770 train_time:164404ms step_avg:99.28ms
step:1667/1770 train_time:164507ms step_avg:99.28ms
step:1668/1770 train_time:164609ms step_avg:99.28ms
step:1669/1770 train_time:164712ms step_avg:99.28ms
step:1670/1770 train_time:164816ms step_avg:99.29ms
step:1671/1770 train_time:164920ms step_avg:99.29ms
step:1672/1770 train_time:165024ms step_avg:99.29ms
step:1673/1770 train_time:165128ms step_avg:99.30ms
step:1674/1770 train_time:165231ms step_avg:99.30ms
step:1675/1770 train_time:165333ms step_avg:99.30ms
step:1676/1770 train_time:165438ms step_avg:99.30ms
step:1677/1770 train_time:165545ms step_avg:99.31ms
step:1678/1770 train_time:165647ms step_avg:99.31ms
step:1679/1770 train_time:165751ms step_avg:99.31ms
step:1680/1770 train_time:165855ms step_avg:99.31ms
step:1681/1770 train_time:165960ms step_avg:99.32ms
step:1682/1770 train_time:166065ms step_avg:99.32ms
step:1683/1770 train_time:166168ms step_avg:99.32ms
step:1684/1770 train_time:166270ms step_avg:99.33ms
step:1685/1770 train_time:166375ms step_avg:99.33ms
step:1686/1770 train_time:166479ms step_avg:99.33ms
step:1687/1770 train_time:166584ms step_avg:99.33ms
step:1688/1770 train_time:166687ms step_avg:99.34ms
step:1689/1770 train_time:166790ms step_avg:99.34ms
step:1690/1770 train_time:166893ms step_avg:99.34ms
step:1691/1770 train_time:166998ms step_avg:99.34ms
step:1692/1770 train_time:167101ms step_avg:99.35ms
step:1693/1770 train_time:167205ms step_avg:99.35ms
step:1694/1770 train_time:167309ms step_avg:99.35ms
step:1695/1770 train_time:167412ms step_avg:99.35ms
step:1696/1770 train_time:167518ms step_avg:99.36ms
step:1697/1770 train_time:167623ms step_avg:99.36ms
step:1698/1770 train_time:167727ms step_avg:99.36ms
step:1699/1770 train_time:167830ms step_avg:99.37ms
step:1700/1770 train_time:167933ms step_avg:99.37ms
step:1701/1770 train_time:168036ms step_avg:99.37ms
step:1702/1770 train_time:168140ms step_avg:99.37ms
step:1703/1770 train_time:168243ms step_avg:99.38ms
step:1704/1770 train_time:168347ms step_avg:99.38ms
step:1705/1770 train_time:168450ms step_avg:99.38ms
step:1706/1770 train_time:168552ms step_avg:99.38ms
step:1707/1770 train_time:168657ms step_avg:99.39ms
step:1708/1770 train_time:168760ms step_avg:99.39ms
step:1709/1770 train_time:168865ms step_avg:99.39ms
step:1710/1770 train_time:168972ms step_avg:99.40ms
step:1711/1770 train_time:169078ms step_avg:99.40ms
step:1712/1770 train_time:169183ms step_avg:99.40ms
step:1713/1770 train_time:169286ms step_avg:99.40ms
step:1714/1770 train_time:169390ms step_avg:99.41ms
step:1715/1770 train_time:169493ms step_avg:99.41ms
step:1716/1770 train_time:169599ms step_avg:99.41ms
step:1717/1770 train_time:169703ms step_avg:99.42ms
step:1718/1770 train_time:169807ms step_avg:99.42ms
step:1719/1770 train_time:169912ms step_avg:99.42ms
step:1720/1770 train_time:170018ms step_avg:99.43ms
step:1721/1770 train_time:170121ms step_avg:99.43ms
step:1722/1770 train_time:170228ms step_avg:99.43ms
step:1723/1770 train_time:170334ms step_avg:99.44ms
step:1724/1770 train_time:170440ms step_avg:99.44ms
step:1725/1770 train_time:170546ms step_avg:99.44ms
step:1726/1770 train_time:170652ms step_avg:99.45ms
step:1727/1770 train_time:170755ms step_avg:99.45ms
step:1728/1770 train_time:170861ms step_avg:99.45ms
step:1729/1770 train_time:170965ms step_avg:99.46ms
step:1730/1770 train_time:171071ms step_avg:99.46ms
step:1731/1770 train_time:171177ms step_avg:99.46ms
step:1732/1770 train_time:171280ms step_avg:99.47ms
step:1733/1770 train_time:171387ms step_avg:99.47ms
step:1734/1770 train_time:171490ms step_avg:99.47ms
step:1735/1770 train_time:171595ms step_avg:99.48ms
step:1736/1770 train_time:171699ms step_avg:99.48ms
step:1737/1770 train_time:171803ms step_avg:99.48ms
step:1738/1770 train_time:171907ms step_avg:99.48ms
step:1739/1770 train_time:172013ms step_avg:99.49ms
step:1740/1770 train_time:172118ms step_avg:99.49ms
step:1741/1770 train_time:172224ms step_avg:99.49ms
step:1742/1770 train_time:172331ms step_avg:99.50ms
step:1743/1770 train_time:172437ms step_avg:99.50ms
step:1744/1770 train_time:172541ms step_avg:99.50ms
step:1745/1770 train_time:172645ms step_avg:99.51ms
step:1746/1770 train_time:172752ms step_avg:99.51ms
step:1747/1770 train_time:172855ms step_avg:99.51ms
step:1748/1770 train_time:172962ms step_avg:99.52ms
step:1749/1770 train_time:173066ms step_avg:99.52ms
step:1750/1770 train_time:173170ms step_avg:99.52ms
step:1750/1770 val_loss:3.2837 train_time:173273ms step_avg:99.58ms
step:1751/1770 train_time:173295ms step_avg:99.54ms
step:1752/1770 train_time:173387ms step_avg:99.53ms
step:1753/1770 train_time:173490ms step_avg:99.54ms
step:1754/1770 train_time:173595ms step_avg:99.54ms
step:1755/1770 train_time:173699ms step_avg:99.54ms
step:1756/1770 train_time:173804ms step_avg:99.54ms
step:1757/1770 train_time:173908ms step_avg:99.55ms
step:1758/1770 train_time:174013ms step_avg:99.55ms
step:1759/1770 train_time:174117ms step_avg:99.55ms
step:1760/1770 train_time:174221ms step_avg:99.55ms
step:1761/1770 train_time:174329ms step_avg:99.56ms
step:1762/1770 train_time:174436ms step_avg:99.56ms
step:1763/1770 train_time:174540ms step_avg:99.57ms
step:1764/1770 train_time:174645ms step_avg:99.57ms
step:1765/1770 train_time:174751ms step_avg:99.57ms
step:1766/1770 train_time:174859ms step_avg:99.58ms
step:1767/1770 train_time:174962ms step_avg:99.58ms
step:1768/1770 train_time:175066ms step_avg:99.58ms
step:1769/1770 train_time:175170ms step_avg:99.59ms
step:1770/1770 train_time:175274ms step_avg:99.59ms
step:1770/1770 val_loss:3.2807 train_time:175379ms step_avg:99.65ms
peak memory allocated: 28840 MiB reserved: 32252 MiB
