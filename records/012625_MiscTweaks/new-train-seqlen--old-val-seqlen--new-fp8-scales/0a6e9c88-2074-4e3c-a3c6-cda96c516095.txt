import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 14:56:29 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   37C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             112W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24386ms step_avg:nanms
step:2/1770 train_time:24898ms step_avg:nanms
step:3/1770 train_time:24992ms step_avg:nanms
step:4/1770 train_time:25085ms step_avg:nanms
step:5/1770 train_time:25179ms step_avg:nanms
step:6/1770 train_time:25273ms step_avg:nanms
step:7/1770 train_time:25368ms step_avg:nanms
step:8/1770 train_time:25461ms step_avg:nanms
step:9/1770 train_time:25555ms step_avg:nanms
step:10/1770 train_time:25649ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:93.84ms
step:14/1770 train_time:377ms step_avg:94.22ms
step:15/1770 train_time:470ms step_avg:93.95ms
step:16/1770 train_time:564ms step_avg:94.05ms
step:17/1770 train_time:658ms step_avg:94.00ms
step:18/1770 train_time:752ms step_avg:93.96ms
step:19/1770 train_time:846ms step_avg:93.95ms
step:20/1770 train_time:940ms step_avg:93.96ms
step:21/1770 train_time:1034ms step_avg:94.00ms
step:22/1770 train_time:1128ms step_avg:93.99ms
step:23/1770 train_time:1222ms step_avg:93.99ms
step:24/1770 train_time:1316ms step_avg:94.01ms
step:25/1770 train_time:1410ms step_avg:94.00ms
step:26/1770 train_time:1504ms step_avg:94.02ms
step:27/1770 train_time:1599ms step_avg:94.03ms
step:28/1770 train_time:1692ms step_avg:94.02ms
step:29/1770 train_time:1786ms step_avg:94.00ms
step:30/1770 train_time:1880ms step_avg:93.99ms
step:31/1770 train_time:1974ms step_avg:93.98ms
step:32/1770 train_time:2067ms step_avg:93.96ms
step:33/1770 train_time:2161ms step_avg:93.95ms
step:34/1770 train_time:2255ms step_avg:93.95ms
step:35/1770 train_time:2348ms step_avg:93.93ms
step:36/1770 train_time:2443ms step_avg:93.96ms
step:37/1770 train_time:2537ms step_avg:93.97ms
step:38/1770 train_time:2631ms step_avg:93.96ms
step:39/1770 train_time:2725ms step_avg:93.97ms
step:40/1770 train_time:2819ms step_avg:93.97ms
step:41/1770 train_time:2913ms step_avg:93.97ms
step:42/1770 train_time:3007ms step_avg:93.96ms
step:43/1770 train_time:3101ms step_avg:93.97ms
step:44/1770 train_time:3195ms step_avg:93.98ms
step:45/1770 train_time:3289ms step_avg:93.97ms
step:46/1770 train_time:3383ms step_avg:93.97ms
step:47/1770 train_time:3477ms step_avg:93.98ms
step:48/1770 train_time:3571ms step_avg:93.97ms
step:49/1770 train_time:3665ms step_avg:93.97ms
step:50/1770 train_time:3759ms step_avg:93.98ms
step:51/1770 train_time:3853ms step_avg:93.98ms
step:52/1770 train_time:3947ms step_avg:93.98ms
step:53/1770 train_time:4041ms step_avg:93.97ms
step:54/1770 train_time:4135ms step_avg:93.98ms
step:55/1770 train_time:4229ms step_avg:93.98ms
step:56/1770 train_time:4323ms step_avg:93.98ms
step:57/1770 train_time:4417ms step_avg:93.99ms
step:58/1770 train_time:4511ms step_avg:93.98ms
step:59/1770 train_time:4604ms step_avg:93.97ms
step:60/1770 train_time:4698ms step_avg:93.97ms
step:61/1770 train_time:4792ms step_avg:93.97ms
step:62/1770 train_time:4887ms step_avg:93.98ms
step:63/1770 train_time:4981ms step_avg:93.97ms
step:64/1770 train_time:5075ms step_avg:93.98ms
step:65/1770 train_time:5169ms step_avg:93.98ms
step:66/1770 train_time:5263ms step_avg:93.98ms
step:67/1770 train_time:5357ms step_avg:93.98ms
step:68/1770 train_time:5451ms step_avg:93.97ms
step:69/1770 train_time:5545ms step_avg:93.98ms
step:70/1770 train_time:5639ms step_avg:93.98ms
step:71/1770 train_time:5733ms step_avg:93.98ms
step:72/1770 train_time:5827ms step_avg:93.98ms
step:73/1770 train_time:5921ms step_avg:93.98ms
step:74/1770 train_time:6015ms step_avg:93.98ms
step:75/1770 train_time:6109ms step_avg:93.98ms
step:76/1770 train_time:6203ms step_avg:93.98ms
step:77/1770 train_time:6297ms step_avg:93.99ms
step:78/1770 train_time:6391ms step_avg:93.99ms
step:79/1770 train_time:6485ms step_avg:93.99ms
step:80/1770 train_time:6579ms step_avg:93.98ms
step:81/1770 train_time:6673ms step_avg:93.99ms
step:82/1770 train_time:6767ms step_avg:93.98ms
step:83/1770 train_time:6861ms step_avg:93.98ms
step:84/1770 train_time:6955ms step_avg:93.99ms
step:85/1770 train_time:7049ms step_avg:93.99ms
step:86/1770 train_time:7143ms step_avg:93.99ms
step:87/1770 train_time:7238ms step_avg:93.99ms
step:88/1770 train_time:7332ms step_avg:94.00ms
step:89/1770 train_time:7426ms step_avg:93.99ms
step:90/1770 train_time:7520ms step_avg:94.00ms
step:91/1770 train_time:7614ms step_avg:94.00ms
step:92/1770 train_time:7707ms step_avg:93.99ms
step:93/1770 train_time:7803ms step_avg:94.01ms
step:94/1770 train_time:7896ms step_avg:93.99ms
step:95/1770 train_time:7989ms step_avg:93.99ms
step:96/1770 train_time:8083ms step_avg:93.99ms
step:97/1770 train_time:8177ms step_avg:93.99ms
step:98/1770 train_time:8271ms step_avg:93.99ms
step:99/1770 train_time:8365ms step_avg:93.98ms
step:100/1770 train_time:8459ms step_avg:93.98ms
step:101/1770 train_time:8553ms step_avg:93.99ms
step:102/1770 train_time:8646ms step_avg:93.98ms
step:103/1770 train_time:8741ms step_avg:93.99ms
step:104/1770 train_time:8835ms step_avg:93.99ms
step:105/1770 train_time:8929ms step_avg:93.98ms
step:106/1770 train_time:9022ms step_avg:93.98ms
step:107/1770 train_time:9116ms step_avg:93.98ms
step:108/1770 train_time:9210ms step_avg:93.98ms
step:109/1770 train_time:9304ms step_avg:93.98ms
step:110/1770 train_time:9398ms step_avg:93.98ms
step:111/1770 train_time:9492ms step_avg:93.98ms
step:112/1770 train_time:9586ms step_avg:93.98ms
step:113/1770 train_time:9681ms step_avg:93.99ms
step:114/1770 train_time:9775ms step_avg:93.99ms
step:115/1770 train_time:9868ms step_avg:93.98ms
step:116/1770 train_time:9962ms step_avg:93.98ms
step:117/1770 train_time:10057ms step_avg:93.99ms
step:118/1770 train_time:10150ms step_avg:93.98ms
step:119/1770 train_time:10244ms step_avg:93.98ms
step:120/1770 train_time:10337ms step_avg:93.98ms
step:121/1770 train_time:10431ms step_avg:93.98ms
step:122/1770 train_time:10525ms step_avg:93.97ms
step:123/1770 train_time:10620ms step_avg:93.98ms
step:124/1770 train_time:10714ms step_avg:93.98ms
step:125/1770 train_time:10807ms step_avg:93.98ms
step:125/1770 val_loss:4.6478 train_time:10900ms step_avg:94.78ms
step:126/1770 train_time:10922ms step_avg:94.15ms
step:127/1770 train_time:11006ms step_avg:94.07ms
step:128/1770 train_time:11105ms step_avg:94.11ms
step:129/1770 train_time:11199ms step_avg:94.11ms
step:130/1770 train_time:11293ms step_avg:94.11ms
step:131/1770 train_time:11387ms step_avg:94.11ms
step:132/1770 train_time:11480ms step_avg:94.10ms
step:133/1770 train_time:11574ms step_avg:94.10ms
step:134/1770 train_time:11668ms step_avg:94.10ms
step:135/1770 train_time:11762ms step_avg:94.10ms
step:136/1770 train_time:11856ms step_avg:94.10ms
step:137/1770 train_time:11951ms step_avg:94.10ms
step:138/1770 train_time:12046ms step_avg:94.11ms
step:139/1770 train_time:12141ms step_avg:94.12ms
step:140/1770 train_time:12236ms step_avg:94.12ms
step:141/1770 train_time:12330ms step_avg:94.12ms
step:142/1770 train_time:12425ms step_avg:94.13ms
step:143/1770 train_time:12519ms step_avg:94.12ms
step:144/1770 train_time:12613ms step_avg:94.12ms
step:145/1770 train_time:12707ms step_avg:94.13ms
step:146/1770 train_time:12801ms step_avg:94.13ms
step:147/1770 train_time:12896ms step_avg:94.13ms
step:148/1770 train_time:12990ms step_avg:94.13ms
step:149/1770 train_time:13086ms step_avg:94.14ms
step:150/1770 train_time:13181ms step_avg:94.15ms
step:151/1770 train_time:13276ms step_avg:94.15ms
step:152/1770 train_time:13371ms step_avg:94.16ms
step:153/1770 train_time:13466ms step_avg:94.17ms
step:154/1770 train_time:13560ms step_avg:94.17ms
step:155/1770 train_time:13654ms step_avg:94.16ms
step:156/1770 train_time:13748ms step_avg:94.17ms
step:157/1770 train_time:13842ms step_avg:94.17ms
step:158/1770 train_time:13937ms step_avg:94.17ms
step:159/1770 train_time:14031ms step_avg:94.17ms
step:160/1770 train_time:14126ms step_avg:94.17ms
step:161/1770 train_time:14220ms step_avg:94.17ms
step:162/1770 train_time:14315ms step_avg:94.18ms
step:163/1770 train_time:14410ms step_avg:94.18ms
step:164/1770 train_time:14505ms step_avg:94.19ms
step:165/1770 train_time:14600ms step_avg:94.19ms
step:166/1770 train_time:14695ms step_avg:94.20ms
step:167/1770 train_time:14789ms step_avg:94.20ms
step:168/1770 train_time:14884ms step_avg:94.20ms
step:169/1770 train_time:14978ms step_avg:94.20ms
step:170/1770 train_time:15072ms step_avg:94.20ms
step:171/1770 train_time:15167ms step_avg:94.21ms
step:172/1770 train_time:15262ms step_avg:94.21ms
step:173/1770 train_time:15357ms step_avg:94.21ms
step:174/1770 train_time:15451ms step_avg:94.22ms
step:175/1770 train_time:15547ms step_avg:94.22ms
step:176/1770 train_time:15641ms step_avg:94.22ms
step:177/1770 train_time:15735ms step_avg:94.22ms
step:178/1770 train_time:15831ms step_avg:94.23ms
step:179/1770 train_time:15926ms step_avg:94.23ms
step:180/1770 train_time:16020ms step_avg:94.24ms
step:181/1770 train_time:16115ms step_avg:94.24ms
step:182/1770 train_time:16210ms step_avg:94.24ms
step:183/1770 train_time:16304ms step_avg:94.24ms
step:184/1770 train_time:16398ms step_avg:94.24ms
step:185/1770 train_time:16492ms step_avg:94.24ms
step:186/1770 train_time:16588ms step_avg:94.25ms
step:187/1770 train_time:16682ms step_avg:94.25ms
step:188/1770 train_time:16777ms step_avg:94.25ms
step:189/1770 train_time:16871ms step_avg:94.25ms
step:190/1770 train_time:16966ms step_avg:94.26ms
step:191/1770 train_time:17060ms step_avg:94.26ms
step:192/1770 train_time:17155ms step_avg:94.26ms
step:193/1770 train_time:17249ms step_avg:94.26ms
step:194/1770 train_time:17344ms step_avg:94.26ms
step:195/1770 train_time:17438ms step_avg:94.26ms
step:196/1770 train_time:17532ms step_avg:94.26ms
step:197/1770 train_time:17627ms step_avg:94.26ms
step:198/1770 train_time:17722ms step_avg:94.27ms
step:199/1770 train_time:17816ms step_avg:94.27ms
step:200/1770 train_time:17911ms step_avg:94.27ms
step:201/1770 train_time:18006ms step_avg:94.27ms
step:202/1770 train_time:18101ms step_avg:94.28ms
step:203/1770 train_time:18195ms step_avg:94.27ms
step:204/1770 train_time:18290ms step_avg:94.28ms
step:205/1770 train_time:18385ms step_avg:94.28ms
step:206/1770 train_time:18479ms step_avg:94.28ms
step:207/1770 train_time:18573ms step_avg:94.28ms
step:208/1770 train_time:18668ms step_avg:94.28ms
step:209/1770 train_time:18763ms step_avg:94.28ms
step:210/1770 train_time:18857ms step_avg:94.28ms
step:211/1770 train_time:18951ms step_avg:94.29ms
step:212/1770 train_time:19047ms step_avg:94.29ms
step:213/1770 train_time:19141ms step_avg:94.29ms
step:214/1770 train_time:19235ms step_avg:94.29ms
step:215/1770 train_time:19330ms step_avg:94.29ms
step:216/1770 train_time:19425ms step_avg:94.30ms
step:217/1770 train_time:19519ms step_avg:94.29ms
step:218/1770 train_time:19613ms step_avg:94.29ms
step:219/1770 train_time:19709ms step_avg:94.30ms
step:220/1770 train_time:19803ms step_avg:94.30ms
step:221/1770 train_time:19897ms step_avg:94.30ms
step:222/1770 train_time:19992ms step_avg:94.30ms
step:223/1770 train_time:20088ms step_avg:94.31ms
step:224/1770 train_time:20182ms step_avg:94.31ms
step:225/1770 train_time:20276ms step_avg:94.31ms
step:226/1770 train_time:20371ms step_avg:94.31ms
step:227/1770 train_time:20466ms step_avg:94.31ms
step:228/1770 train_time:20561ms step_avg:94.31ms
step:229/1770 train_time:20655ms step_avg:94.31ms
step:230/1770 train_time:20750ms step_avg:94.32ms
step:231/1770 train_time:20846ms step_avg:94.32ms
step:232/1770 train_time:20940ms step_avg:94.32ms
step:233/1770 train_time:21035ms step_avg:94.33ms
step:234/1770 train_time:21129ms step_avg:94.33ms
step:235/1770 train_time:21225ms step_avg:94.33ms
step:236/1770 train_time:21319ms step_avg:94.33ms
step:237/1770 train_time:21414ms step_avg:94.33ms
step:238/1770 train_time:21509ms step_avg:94.34ms
step:239/1770 train_time:21603ms step_avg:94.34ms
step:240/1770 train_time:21697ms step_avg:94.34ms
step:241/1770 train_time:21792ms step_avg:94.34ms
step:242/1770 train_time:21887ms step_avg:94.34ms
step:243/1770 train_time:21982ms step_avg:94.34ms
step:244/1770 train_time:22076ms step_avg:94.34ms
step:245/1770 train_time:22171ms step_avg:94.34ms
step:246/1770 train_time:22266ms step_avg:94.35ms
step:247/1770 train_time:22360ms step_avg:94.35ms
step:248/1770 train_time:22455ms step_avg:94.35ms
step:249/1770 train_time:22550ms step_avg:94.35ms
step:250/1770 train_time:22645ms step_avg:94.36ms
step:250/1770 val_loss:4.1024 train_time:22738ms step_avg:94.74ms
step:251/1770 train_time:22760ms step_avg:94.44ms
step:252/1770 train_time:22846ms step_avg:94.41ms
step:253/1770 train_time:22945ms step_avg:94.42ms
step:254/1770 train_time:23041ms step_avg:94.43ms
step:255/1770 train_time:23136ms step_avg:94.43ms
step:256/1770 train_time:23230ms step_avg:94.43ms
step:257/1770 train_time:23324ms step_avg:94.43ms
step:258/1770 train_time:23419ms step_avg:94.43ms
step:259/1770 train_time:23513ms step_avg:94.43ms
step:260/1770 train_time:23607ms step_avg:94.43ms
step:261/1770 train_time:23702ms step_avg:94.43ms
step:262/1770 train_time:23797ms step_avg:94.43ms
step:263/1770 train_time:23892ms step_avg:94.44ms
step:264/1770 train_time:23987ms step_avg:94.44ms
step:265/1770 train_time:24083ms step_avg:94.44ms
step:266/1770 train_time:24178ms step_avg:94.45ms
step:267/1770 train_time:24273ms step_avg:94.45ms
step:268/1770 train_time:24367ms step_avg:94.45ms
step:269/1770 train_time:24462ms step_avg:94.45ms
step:270/1770 train_time:24558ms step_avg:94.45ms
step:271/1770 train_time:24652ms step_avg:94.45ms
step:272/1770 train_time:24746ms step_avg:94.45ms
step:273/1770 train_time:24842ms step_avg:94.46ms
step:274/1770 train_time:24938ms step_avg:94.46ms
step:275/1770 train_time:25033ms step_avg:94.47ms
step:276/1770 train_time:25128ms step_avg:94.47ms
step:277/1770 train_time:25224ms step_avg:94.47ms
step:278/1770 train_time:25319ms step_avg:94.47ms
step:279/1770 train_time:25414ms step_avg:94.47ms
step:280/1770 train_time:25509ms step_avg:94.48ms
step:281/1770 train_time:25603ms step_avg:94.48ms
step:282/1770 train_time:25699ms step_avg:94.48ms
step:283/1770 train_time:25794ms step_avg:94.48ms
step:284/1770 train_time:25889ms step_avg:94.49ms
step:285/1770 train_time:25984ms step_avg:94.49ms
step:286/1770 train_time:26080ms step_avg:94.49ms
step:287/1770 train_time:26176ms step_avg:94.50ms
step:288/1770 train_time:26271ms step_avg:94.50ms
step:289/1770 train_time:26366ms step_avg:94.50ms
step:290/1770 train_time:26462ms step_avg:94.51ms
step:291/1770 train_time:26557ms step_avg:94.51ms
step:292/1770 train_time:26651ms step_avg:94.51ms
step:293/1770 train_time:26746ms step_avg:94.51ms
step:294/1770 train_time:26841ms step_avg:94.51ms
step:295/1770 train_time:26937ms step_avg:94.52ms
step:296/1770 train_time:27033ms step_avg:94.52ms
step:297/1770 train_time:27128ms step_avg:94.52ms
step:298/1770 train_time:27223ms step_avg:94.53ms
step:299/1770 train_time:27318ms step_avg:94.53ms
step:300/1770 train_time:27414ms step_avg:94.53ms
step:301/1770 train_time:27509ms step_avg:94.53ms
step:302/1770 train_time:27603ms step_avg:94.53ms
step:303/1770 train_time:27699ms step_avg:94.54ms
step:304/1770 train_time:27794ms step_avg:94.54ms
step:305/1770 train_time:27889ms step_avg:94.54ms
step:306/1770 train_time:27984ms step_avg:94.54ms
step:307/1770 train_time:28080ms step_avg:94.55ms
step:308/1770 train_time:28175ms step_avg:94.55ms
step:309/1770 train_time:28271ms step_avg:94.55ms
step:310/1770 train_time:28366ms step_avg:94.55ms
step:311/1770 train_time:28462ms step_avg:94.56ms
step:312/1770 train_time:28558ms step_avg:94.56ms
step:313/1770 train_time:28652ms step_avg:94.56ms
step:314/1770 train_time:28747ms step_avg:94.56ms
step:315/1770 train_time:28843ms step_avg:94.57ms
step:316/1770 train_time:28938ms step_avg:94.57ms
step:317/1770 train_time:29033ms step_avg:94.57ms
step:318/1770 train_time:29128ms step_avg:94.57ms
step:319/1770 train_time:29223ms step_avg:94.57ms
step:320/1770 train_time:29318ms step_avg:94.58ms
step:321/1770 train_time:29413ms step_avg:94.58ms
step:322/1770 train_time:29508ms step_avg:94.58ms
step:323/1770 train_time:29603ms step_avg:94.58ms
step:324/1770 train_time:29699ms step_avg:94.58ms
step:325/1770 train_time:29794ms step_avg:94.58ms
step:326/1770 train_time:29889ms step_avg:94.59ms
step:327/1770 train_time:29984ms step_avg:94.59ms
step:328/1770 train_time:30079ms step_avg:94.59ms
step:329/1770 train_time:30175ms step_avg:94.59ms
step:330/1770 train_time:30269ms step_avg:94.59ms
step:331/1770 train_time:30364ms step_avg:94.59ms
step:332/1770 train_time:30460ms step_avg:94.59ms
step:333/1770 train_time:30555ms step_avg:94.60ms
step:334/1770 train_time:30650ms step_avg:94.60ms
step:335/1770 train_time:30745ms step_avg:94.60ms
step:336/1770 train_time:30841ms step_avg:94.60ms
step:337/1770 train_time:30936ms step_avg:94.61ms
step:338/1770 train_time:31032ms step_avg:94.61ms
step:339/1770 train_time:31127ms step_avg:94.61ms
step:340/1770 train_time:31222ms step_avg:94.61ms
step:341/1770 train_time:31317ms step_avg:94.61ms
step:342/1770 train_time:31412ms step_avg:94.61ms
step:343/1770 train_time:31506ms step_avg:94.61ms
step:344/1770 train_time:31602ms step_avg:94.62ms
step:345/1770 train_time:31697ms step_avg:94.62ms
step:346/1770 train_time:31792ms step_avg:94.62ms
step:347/1770 train_time:31887ms step_avg:94.62ms
step:348/1770 train_time:31983ms step_avg:94.62ms
step:349/1770 train_time:32079ms step_avg:94.63ms
step:350/1770 train_time:32174ms step_avg:94.63ms
step:351/1770 train_time:32268ms step_avg:94.63ms
step:352/1770 train_time:32364ms step_avg:94.63ms
step:353/1770 train_time:32459ms step_avg:94.63ms
step:354/1770 train_time:32555ms step_avg:94.64ms
step:355/1770 train_time:32649ms step_avg:94.64ms
step:356/1770 train_time:32744ms step_avg:94.64ms
step:357/1770 train_time:32840ms step_avg:94.64ms
step:358/1770 train_time:32936ms step_avg:94.64ms
step:359/1770 train_time:33031ms step_avg:94.64ms
step:360/1770 train_time:33125ms step_avg:94.64ms
step:361/1770 train_time:33220ms step_avg:94.65ms
step:362/1770 train_time:33316ms step_avg:94.65ms
step:363/1770 train_time:33411ms step_avg:94.65ms
step:364/1770 train_time:33506ms step_avg:94.65ms
step:365/1770 train_time:33601ms step_avg:94.65ms
step:366/1770 train_time:33697ms step_avg:94.65ms
step:367/1770 train_time:33792ms step_avg:94.65ms
step:368/1770 train_time:33886ms step_avg:94.65ms
step:369/1770 train_time:33982ms step_avg:94.66ms
step:370/1770 train_time:34077ms step_avg:94.66ms
step:371/1770 train_time:34172ms step_avg:94.66ms
step:372/1770 train_time:34267ms step_avg:94.66ms
step:373/1770 train_time:34362ms step_avg:94.66ms
step:374/1770 train_time:34457ms step_avg:94.66ms
step:375/1770 train_time:34552ms step_avg:94.66ms
step:375/1770 val_loss:3.8939 train_time:34645ms step_avg:94.92ms
step:376/1770 train_time:34667ms step_avg:94.72ms
step:377/1770 train_time:34753ms step_avg:94.69ms
step:378/1770 train_time:34851ms step_avg:94.70ms
step:379/1770 train_time:34947ms step_avg:94.71ms
step:380/1770 train_time:35042ms step_avg:94.71ms
step:381/1770 train_time:35137ms step_avg:94.71ms
step:382/1770 train_time:35231ms step_avg:94.71ms
step:383/1770 train_time:35326ms step_avg:94.71ms
step:384/1770 train_time:35421ms step_avg:94.71ms
step:385/1770 train_time:35515ms step_avg:94.71ms
step:386/1770 train_time:35609ms step_avg:94.71ms
step:387/1770 train_time:35706ms step_avg:94.71ms
step:388/1770 train_time:35803ms step_avg:94.72ms
step:389/1770 train_time:35899ms step_avg:94.72ms
step:390/1770 train_time:35994ms step_avg:94.72ms
step:391/1770 train_time:36089ms step_avg:94.72ms
step:392/1770 train_time:36185ms step_avg:94.72ms
step:393/1770 train_time:36279ms step_avg:94.72ms
step:394/1770 train_time:36374ms step_avg:94.72ms
step:395/1770 train_time:36468ms step_avg:94.72ms
step:396/1770 train_time:36565ms step_avg:94.73ms
step:397/1770 train_time:36663ms step_avg:94.74ms
step:398/1770 train_time:36761ms step_avg:94.74ms
step:399/1770 train_time:36859ms step_avg:94.75ms
step:400/1770 train_time:36957ms step_avg:94.76ms
step:401/1770 train_time:37054ms step_avg:94.77ms
step:402/1770 train_time:37151ms step_avg:94.77ms
step:403/1770 train_time:37247ms step_avg:94.78ms
step:404/1770 train_time:37344ms step_avg:94.78ms
step:405/1770 train_time:37441ms step_avg:94.79ms
step:406/1770 train_time:37538ms step_avg:94.79ms
step:407/1770 train_time:37635ms step_avg:94.80ms
step:408/1770 train_time:37731ms step_avg:94.80ms
step:409/1770 train_time:37827ms step_avg:94.80ms
step:410/1770 train_time:37924ms step_avg:94.81ms
step:411/1770 train_time:38023ms step_avg:94.82ms
step:412/1770 train_time:38121ms step_avg:94.83ms
step:413/1770 train_time:38218ms step_avg:94.83ms
step:414/1770 train_time:38315ms step_avg:94.84ms
step:415/1770 train_time:38411ms step_avg:94.84ms
step:416/1770 train_time:38508ms step_avg:94.85ms
step:417/1770 train_time:38605ms step_avg:94.85ms
step:418/1770 train_time:38702ms step_avg:94.86ms
step:419/1770 train_time:38799ms step_avg:94.86ms
step:420/1770 train_time:38896ms step_avg:94.87ms
step:421/1770 train_time:38992ms step_avg:94.87ms
step:422/1770 train_time:39089ms step_avg:94.88ms
step:423/1770 train_time:39186ms step_avg:94.88ms
step:424/1770 train_time:39284ms step_avg:94.89ms
step:425/1770 train_time:39381ms step_avg:94.89ms
step:426/1770 train_time:39478ms step_avg:94.90ms
step:427/1770 train_time:39575ms step_avg:94.90ms
step:428/1770 train_time:39671ms step_avg:94.91ms
step:429/1770 train_time:39768ms step_avg:94.91ms
step:430/1770 train_time:39865ms step_avg:94.92ms
step:431/1770 train_time:39962ms step_avg:94.92ms
step:432/1770 train_time:40060ms step_avg:94.93ms
step:433/1770 train_time:40156ms step_avg:94.93ms
step:434/1770 train_time:40253ms step_avg:94.94ms
step:435/1770 train_time:40349ms step_avg:94.94ms
step:436/1770 train_time:40447ms step_avg:94.95ms
step:437/1770 train_time:40545ms step_avg:94.95ms
step:438/1770 train_time:40642ms step_avg:94.96ms
step:439/1770 train_time:40740ms step_avg:94.97ms
step:440/1770 train_time:40837ms step_avg:94.97ms
step:441/1770 train_time:40933ms step_avg:94.97ms
step:442/1770 train_time:41030ms step_avg:94.98ms
step:443/1770 train_time:41126ms step_avg:94.98ms
step:444/1770 train_time:41223ms step_avg:94.98ms
step:445/1770 train_time:41320ms step_avg:94.99ms
step:446/1770 train_time:41417ms step_avg:94.99ms
step:447/1770 train_time:41514ms step_avg:95.00ms
step:448/1770 train_time:41611ms step_avg:95.00ms
step:449/1770 train_time:41708ms step_avg:95.01ms
step:450/1770 train_time:41806ms step_avg:95.01ms
step:451/1770 train_time:41904ms step_avg:95.02ms
step:452/1770 train_time:42001ms step_avg:95.03ms
step:453/1770 train_time:42098ms step_avg:95.03ms
step:454/1770 train_time:42195ms step_avg:95.03ms
step:455/1770 train_time:42292ms step_avg:95.04ms
step:456/1770 train_time:42388ms step_avg:95.04ms
step:457/1770 train_time:42486ms step_avg:95.05ms
step:458/1770 train_time:42583ms step_avg:95.05ms
step:459/1770 train_time:42680ms step_avg:95.06ms
step:460/1770 train_time:42777ms step_avg:95.06ms
step:461/1770 train_time:42874ms step_avg:95.06ms
step:462/1770 train_time:42971ms step_avg:95.07ms
step:463/1770 train_time:43068ms step_avg:95.07ms
step:464/1770 train_time:43165ms step_avg:95.08ms
step:465/1770 train_time:43262ms step_avg:95.08ms
step:466/1770 train_time:43359ms step_avg:95.09ms
step:467/1770 train_time:43456ms step_avg:95.09ms
step:468/1770 train_time:43553ms step_avg:95.09ms
step:469/1770 train_time:43650ms step_avg:95.10ms
step:470/1770 train_time:43746ms step_avg:95.10ms
step:471/1770 train_time:43843ms step_avg:95.10ms
step:472/1770 train_time:43940ms step_avg:95.11ms
step:473/1770 train_time:44037ms step_avg:95.11ms
step:474/1770 train_time:44134ms step_avg:95.12ms
step:475/1770 train_time:44230ms step_avg:95.12ms
step:476/1770 train_time:44328ms step_avg:95.12ms
step:477/1770 train_time:44425ms step_avg:95.13ms
step:478/1770 train_time:44523ms step_avg:95.13ms
step:479/1770 train_time:44620ms step_avg:95.14ms
step:480/1770 train_time:44717ms step_avg:95.14ms
step:481/1770 train_time:44813ms step_avg:95.15ms
step:482/1770 train_time:44910ms step_avg:95.15ms
step:483/1770 train_time:45007ms step_avg:95.15ms
step:484/1770 train_time:45105ms step_avg:95.16ms
step:485/1770 train_time:45202ms step_avg:95.16ms
step:486/1770 train_time:45300ms step_avg:95.17ms
step:487/1770 train_time:45397ms step_avg:95.17ms
step:488/1770 train_time:45494ms step_avg:95.18ms
step:489/1770 train_time:45591ms step_avg:95.18ms
step:490/1770 train_time:45689ms step_avg:95.18ms
step:491/1770 train_time:45786ms step_avg:95.19ms
step:492/1770 train_time:45884ms step_avg:95.19ms
step:493/1770 train_time:45981ms step_avg:95.20ms
step:494/1770 train_time:46078ms step_avg:95.20ms
step:495/1770 train_time:46175ms step_avg:95.21ms
step:496/1770 train_time:46271ms step_avg:95.21ms
step:497/1770 train_time:46368ms step_avg:95.21ms
step:498/1770 train_time:46465ms step_avg:95.22ms
step:499/1770 train_time:46563ms step_avg:95.22ms
step:500/1770 train_time:46660ms step_avg:95.22ms
step:500/1770 val_loss:3.7485 train_time:46755ms step_avg:95.42ms
step:501/1770 train_time:46779ms step_avg:95.27ms
step:502/1770 train_time:46865ms step_avg:95.25ms
step:503/1770 train_time:46965ms step_avg:95.26ms
step:504/1770 train_time:47061ms step_avg:95.27ms
step:505/1770 train_time:47158ms step_avg:95.27ms
step:506/1770 train_time:47255ms step_avg:95.27ms
step:507/1770 train_time:47351ms step_avg:95.27ms
step:508/1770 train_time:47447ms step_avg:95.28ms
step:509/1770 train_time:47543ms step_avg:95.28ms
step:510/1770 train_time:47640ms step_avg:95.28ms
step:511/1770 train_time:47737ms step_avg:95.28ms
step:512/1770 train_time:47835ms step_avg:95.29ms
step:513/1770 train_time:47933ms step_avg:95.29ms
step:514/1770 train_time:48031ms step_avg:95.30ms
step:515/1770 train_time:48128ms step_avg:95.30ms
step:516/1770 train_time:48224ms step_avg:95.31ms
step:517/1770 train_time:48321ms step_avg:95.31ms
step:518/1770 train_time:48418ms step_avg:95.31ms
step:519/1770 train_time:48515ms step_avg:95.32ms
step:520/1770 train_time:48612ms step_avg:95.32ms
step:521/1770 train_time:48709ms step_avg:95.32ms
step:522/1770 train_time:48805ms step_avg:95.32ms
step:523/1770 train_time:48902ms step_avg:95.33ms
step:524/1770 train_time:48999ms step_avg:95.33ms
step:525/1770 train_time:49096ms step_avg:95.33ms
step:526/1770 train_time:49194ms step_avg:95.34ms
step:527/1770 train_time:49292ms step_avg:95.34ms
step:528/1770 train_time:49390ms step_avg:95.35ms
step:529/1770 train_time:49486ms step_avg:95.35ms
step:530/1770 train_time:49584ms step_avg:95.35ms
step:531/1770 train_time:49680ms step_avg:95.36ms
step:532/1770 train_time:49778ms step_avg:95.36ms
step:533/1770 train_time:49876ms step_avg:95.36ms
step:534/1770 train_time:49974ms step_avg:95.37ms
step:535/1770 train_time:50071ms step_avg:95.37ms
step:536/1770 train_time:50168ms step_avg:95.38ms
step:537/1770 train_time:50266ms step_avg:95.38ms
step:538/1770 train_time:50363ms step_avg:95.38ms
step:539/1770 train_time:50460ms step_avg:95.39ms
step:540/1770 train_time:50558ms step_avg:95.39ms
step:541/1770 train_time:50655ms step_avg:95.40ms
step:542/1770 train_time:50753ms step_avg:95.40ms
step:543/1770 train_time:50851ms step_avg:95.41ms
step:544/1770 train_time:50948ms step_avg:95.41ms
step:545/1770 train_time:51045ms step_avg:95.41ms
step:546/1770 train_time:51141ms step_avg:95.41ms
step:547/1770 train_time:51238ms step_avg:95.42ms
step:548/1770 train_time:51336ms step_avg:95.42ms
step:549/1770 train_time:51434ms step_avg:95.42ms
step:550/1770 train_time:51531ms step_avg:95.43ms
step:551/1770 train_time:51629ms step_avg:95.43ms
step:552/1770 train_time:51726ms step_avg:95.44ms
step:553/1770 train_time:51823ms step_avg:95.44ms
step:554/1770 train_time:51920ms step_avg:95.44ms
step:555/1770 train_time:52017ms step_avg:95.44ms
step:556/1770 train_time:52115ms step_avg:95.45ms
step:557/1770 train_time:52213ms step_avg:95.45ms
step:558/1770 train_time:52310ms step_avg:95.46ms
step:559/1770 train_time:52408ms step_avg:95.46ms
step:560/1770 train_time:52505ms step_avg:95.46ms
step:561/1770 train_time:52601ms step_avg:95.47ms
step:562/1770 train_time:52699ms step_avg:95.47ms
step:563/1770 train_time:52797ms step_avg:95.47ms
step:564/1770 train_time:52894ms step_avg:95.48ms
step:565/1770 train_time:52992ms step_avg:95.48ms
step:566/1770 train_time:53089ms step_avg:95.48ms
step:567/1770 train_time:53186ms step_avg:95.49ms
step:568/1770 train_time:53283ms step_avg:95.49ms
step:569/1770 train_time:53380ms step_avg:95.49ms
step:570/1770 train_time:53478ms step_avg:95.50ms
step:571/1770 train_time:53576ms step_avg:95.50ms
step:572/1770 train_time:53673ms step_avg:95.50ms
step:573/1770 train_time:53770ms step_avg:95.51ms
step:574/1770 train_time:53867ms step_avg:95.51ms
step:575/1770 train_time:53964ms step_avg:95.51ms
step:576/1770 train_time:54061ms step_avg:95.51ms
step:577/1770 train_time:54158ms step_avg:95.52ms
step:578/1770 train_time:54256ms step_avg:95.52ms
step:579/1770 train_time:54354ms step_avg:95.53ms
step:580/1770 train_time:54451ms step_avg:95.53ms
step:581/1770 train_time:54548ms step_avg:95.53ms
step:582/1770 train_time:54645ms step_avg:95.53ms
step:583/1770 train_time:54742ms step_avg:95.54ms
step:584/1770 train_time:54839ms step_avg:95.54ms
step:585/1770 train_time:54937ms step_avg:95.54ms
step:586/1770 train_time:55035ms step_avg:95.55ms
step:587/1770 train_time:55132ms step_avg:95.55ms
step:588/1770 train_time:55229ms step_avg:95.55ms
step:589/1770 train_time:55327ms step_avg:95.56ms
step:590/1770 train_time:55424ms step_avg:95.56ms
step:591/1770 train_time:55521ms step_avg:95.56ms
step:592/1770 train_time:55619ms step_avg:95.56ms
step:593/1770 train_time:55716ms step_avg:95.57ms
step:594/1770 train_time:55814ms step_avg:95.57ms
step:595/1770 train_time:55911ms step_avg:95.57ms
step:596/1770 train_time:56008ms step_avg:95.58ms
step:597/1770 train_time:56105ms step_avg:95.58ms
step:598/1770 train_time:56202ms step_avg:95.58ms
step:599/1770 train_time:56299ms step_avg:95.58ms
step:600/1770 train_time:56397ms step_avg:95.59ms
step:601/1770 train_time:56495ms step_avg:95.59ms
step:602/1770 train_time:56593ms step_avg:95.60ms
step:603/1770 train_time:56691ms step_avg:95.60ms
step:604/1770 train_time:56788ms step_avg:95.60ms
step:605/1770 train_time:56885ms step_avg:95.60ms
step:606/1770 train_time:56981ms step_avg:95.61ms
step:607/1770 train_time:57079ms step_avg:95.61ms
step:608/1770 train_time:57177ms step_avg:95.61ms
step:609/1770 train_time:57275ms step_avg:95.62ms
step:610/1770 train_time:57372ms step_avg:95.62ms
step:611/1770 train_time:57469ms step_avg:95.62ms
step:612/1770 train_time:57566ms step_avg:95.63ms
step:613/1770 train_time:57663ms step_avg:95.63ms
step:614/1770 train_time:57761ms step_avg:95.63ms
step:615/1770 train_time:57857ms step_avg:95.63ms
step:616/1770 train_time:57954ms step_avg:95.63ms
step:617/1770 train_time:58051ms step_avg:95.64ms
step:618/1770 train_time:58149ms step_avg:95.64ms
step:619/1770 train_time:58246ms step_avg:95.64ms
step:620/1770 train_time:58342ms step_avg:95.64ms
step:621/1770 train_time:58439ms step_avg:95.64ms
step:622/1770 train_time:58536ms step_avg:95.65ms
step:623/1770 train_time:58634ms step_avg:95.65ms
step:624/1770 train_time:58733ms step_avg:95.66ms
step:625/1770 train_time:58831ms step_avg:95.66ms
step:625/1770 val_loss:3.6607 train_time:58927ms step_avg:95.82ms
step:626/1770 train_time:58949ms step_avg:95.70ms
step:627/1770 train_time:59036ms step_avg:95.68ms
step:628/1770 train_time:59136ms step_avg:95.69ms
step:629/1770 train_time:59233ms step_avg:95.69ms
step:630/1770 train_time:59330ms step_avg:95.69ms
step:631/1770 train_time:59427ms step_avg:95.70ms
step:632/1770 train_time:59525ms step_avg:95.70ms
step:633/1770 train_time:59622ms step_avg:95.70ms
step:634/1770 train_time:59718ms step_avg:95.70ms
step:635/1770 train_time:59815ms step_avg:95.70ms
step:636/1770 train_time:59912ms step_avg:95.71ms
step:637/1770 train_time:60009ms step_avg:95.71ms
step:638/1770 train_time:60107ms step_avg:95.71ms
step:639/1770 train_time:60206ms step_avg:95.72ms
step:640/1770 train_time:60304ms step_avg:95.72ms
step:641/1770 train_time:60401ms step_avg:95.72ms
step:642/1770 train_time:60499ms step_avg:95.73ms
step:643/1770 train_time:60596ms step_avg:95.73ms
step:644/1770 train_time:60693ms step_avg:95.73ms
step:645/1770 train_time:60790ms step_avg:95.73ms
step:646/1770 train_time:60887ms step_avg:95.73ms
step:647/1770 train_time:60985ms step_avg:95.74ms
step:648/1770 train_time:61082ms step_avg:95.74ms
step:649/1770 train_time:61180ms step_avg:95.74ms
step:650/1770 train_time:61277ms step_avg:95.75ms
step:651/1770 train_time:61374ms step_avg:95.75ms
step:652/1770 train_time:61471ms step_avg:95.75ms
step:653/1770 train_time:61568ms step_avg:95.75ms
step:654/1770 train_time:61666ms step_avg:95.75ms
step:655/1770 train_time:61763ms step_avg:95.76ms
step:656/1770 train_time:61861ms step_avg:95.76ms
step:657/1770 train_time:61958ms step_avg:95.76ms
step:658/1770 train_time:62056ms step_avg:95.77ms
step:659/1770 train_time:62154ms step_avg:95.77ms
step:660/1770 train_time:62253ms step_avg:95.77ms
step:661/1770 train_time:62352ms step_avg:95.78ms
step:662/1770 train_time:62451ms step_avg:95.78ms
step:663/1770 train_time:62550ms step_avg:95.79ms
step:664/1770 train_time:62649ms step_avg:95.79ms
step:665/1770 train_time:62748ms step_avg:95.80ms
step:666/1770 train_time:62848ms step_avg:95.80ms
step:667/1770 train_time:62947ms step_avg:95.81ms
step:668/1770 train_time:63047ms step_avg:95.82ms
step:669/1770 train_time:63147ms step_avg:95.82ms
step:670/1770 train_time:63246ms step_avg:95.83ms
step:671/1770 train_time:63346ms step_avg:95.83ms
step:672/1770 train_time:63447ms step_avg:95.84ms
step:673/1770 train_time:63546ms step_avg:95.85ms
step:674/1770 train_time:63646ms step_avg:95.85ms
step:675/1770 train_time:63745ms step_avg:95.86ms
step:676/1770 train_time:63845ms step_avg:95.86ms
step:677/1770 train_time:63943ms step_avg:95.87ms
step:678/1770 train_time:64043ms step_avg:95.87ms
step:679/1770 train_time:64142ms step_avg:95.88ms
step:680/1770 train_time:64241ms step_avg:95.88ms
step:681/1770 train_time:64340ms step_avg:95.89ms
step:682/1770 train_time:64440ms step_avg:95.89ms
step:683/1770 train_time:64539ms step_avg:95.90ms
step:684/1770 train_time:64639ms step_avg:95.90ms
step:685/1770 train_time:64737ms step_avg:95.91ms
step:686/1770 train_time:64835ms step_avg:95.91ms
step:687/1770 train_time:64934ms step_avg:95.91ms
step:688/1770 train_time:65033ms step_avg:95.92ms
step:689/1770 train_time:65132ms step_avg:95.92ms
step:690/1770 train_time:65230ms step_avg:95.93ms
step:691/1770 train_time:65329ms step_avg:95.93ms
step:692/1770 train_time:65429ms step_avg:95.94ms
step:693/1770 train_time:65528ms step_avg:95.94ms
step:694/1770 train_time:65628ms step_avg:95.95ms
step:695/1770 train_time:65727ms step_avg:95.95ms
step:696/1770 train_time:65826ms step_avg:95.96ms
step:697/1770 train_time:65926ms step_avg:95.96ms
step:698/1770 train_time:66026ms step_avg:95.97ms
step:699/1770 train_time:66126ms step_avg:95.97ms
step:700/1770 train_time:66227ms step_avg:95.98ms
step:701/1770 train_time:66326ms step_avg:95.99ms
step:702/1770 train_time:66425ms step_avg:95.99ms
step:703/1770 train_time:66525ms step_avg:96.00ms
step:704/1770 train_time:66625ms step_avg:96.00ms
step:705/1770 train_time:66725ms step_avg:96.01ms
step:706/1770 train_time:66826ms step_avg:96.01ms
step:707/1770 train_time:66926ms step_avg:96.02ms
step:708/1770 train_time:67026ms step_avg:96.03ms
step:709/1770 train_time:67126ms step_avg:96.03ms
step:710/1770 train_time:67226ms step_avg:96.04ms
step:711/1770 train_time:67326ms step_avg:96.04ms
step:712/1770 train_time:67426ms step_avg:96.05ms
step:713/1770 train_time:67525ms step_avg:96.05ms
step:714/1770 train_time:67624ms step_avg:96.06ms
step:715/1770 train_time:67724ms step_avg:96.06ms
step:716/1770 train_time:67823ms step_avg:96.07ms
step:717/1770 train_time:67922ms step_avg:96.07ms
step:718/1770 train_time:68022ms step_avg:96.08ms
step:719/1770 train_time:68121ms step_avg:96.08ms
step:720/1770 train_time:68220ms step_avg:96.08ms
step:721/1770 train_time:68319ms step_avg:96.09ms
step:722/1770 train_time:68419ms step_avg:96.09ms
step:723/1770 train_time:68518ms step_avg:96.10ms
step:724/1770 train_time:68617ms step_avg:96.10ms
step:725/1770 train_time:68716ms step_avg:96.11ms
step:726/1770 train_time:68814ms step_avg:96.11ms
step:727/1770 train_time:68913ms step_avg:96.11ms
step:728/1770 train_time:69012ms step_avg:96.12ms
step:729/1770 train_time:69111ms step_avg:96.12ms
step:730/1770 train_time:69210ms step_avg:96.13ms
step:731/1770 train_time:69309ms step_avg:96.13ms
step:732/1770 train_time:69408ms step_avg:96.13ms
step:733/1770 train_time:69508ms step_avg:96.14ms
step:734/1770 train_time:69607ms step_avg:96.14ms
step:735/1770 train_time:69707ms step_avg:96.15ms
step:736/1770 train_time:69807ms step_avg:96.15ms
step:737/1770 train_time:69906ms step_avg:96.16ms
step:738/1770 train_time:70006ms step_avg:96.16ms
step:739/1770 train_time:70106ms step_avg:96.17ms
step:740/1770 train_time:70205ms step_avg:96.17ms
step:741/1770 train_time:70305ms step_avg:96.18ms
step:742/1770 train_time:70405ms step_avg:96.18ms
step:743/1770 train_time:70505ms step_avg:96.19ms
step:744/1770 train_time:70604ms step_avg:96.19ms
step:745/1770 train_time:70703ms step_avg:96.19ms
step:746/1770 train_time:70802ms step_avg:96.20ms
step:747/1770 train_time:70902ms step_avg:96.20ms
step:748/1770 train_time:71001ms step_avg:96.21ms
step:749/1770 train_time:71100ms step_avg:96.21ms
step:750/1770 train_time:71199ms step_avg:96.21ms
step:750/1770 val_loss:3.5974 train_time:71296ms step_avg:96.35ms
step:751/1770 train_time:71320ms step_avg:96.25ms
step:752/1770 train_time:71406ms step_avg:96.23ms
step:753/1770 train_time:71506ms step_avg:96.24ms
step:754/1770 train_time:71605ms step_avg:96.24ms
step:755/1770 train_time:71704ms step_avg:96.25ms
step:756/1770 train_time:71802ms step_avg:96.25ms
step:757/1770 train_time:71900ms step_avg:96.25ms
step:758/1770 train_time:71999ms step_avg:96.26ms
step:759/1770 train_time:72098ms step_avg:96.26ms
step:760/1770 train_time:72197ms step_avg:96.26ms
step:761/1770 train_time:72296ms step_avg:96.27ms
step:762/1770 train_time:72397ms step_avg:96.27ms
step:763/1770 train_time:72497ms step_avg:96.28ms
step:764/1770 train_time:72598ms step_avg:96.28ms
step:765/1770 train_time:72698ms step_avg:96.29ms
step:766/1770 train_time:72798ms step_avg:96.29ms
step:767/1770 train_time:72898ms step_avg:96.30ms
step:768/1770 train_time:72998ms step_avg:96.30ms
step:769/1770 train_time:73097ms step_avg:96.31ms
step:770/1770 train_time:73196ms step_avg:96.31ms
step:771/1770 train_time:73295ms step_avg:96.31ms
step:772/1770 train_time:73395ms step_avg:96.32ms
step:773/1770 train_time:73495ms step_avg:96.32ms
step:774/1770 train_time:73595ms step_avg:96.33ms
step:775/1770 train_time:73694ms step_avg:96.33ms
step:776/1770 train_time:73793ms step_avg:96.33ms
step:777/1770 train_time:73892ms step_avg:96.34ms
step:778/1770 train_time:73991ms step_avg:96.34ms
step:779/1770 train_time:74090ms step_avg:96.35ms
step:780/1770 train_time:74187ms step_avg:96.35ms
step:781/1770 train_time:74286ms step_avg:96.35ms
step:782/1770 train_time:74384ms step_avg:96.35ms
step:783/1770 train_time:74483ms step_avg:96.36ms
step:784/1770 train_time:74582ms step_avg:96.36ms
step:785/1770 train_time:74681ms step_avg:96.36ms
step:786/1770 train_time:74781ms step_avg:96.37ms
step:787/1770 train_time:74880ms step_avg:96.37ms
step:788/1770 train_time:74980ms step_avg:96.38ms
step:789/1770 train_time:75081ms step_avg:96.38ms
step:790/1770 train_time:75181ms step_avg:96.39ms
step:791/1770 train_time:75282ms step_avg:96.39ms
step:792/1770 train_time:75382ms step_avg:96.40ms
step:793/1770 train_time:75481ms step_avg:96.40ms
step:794/1770 train_time:75580ms step_avg:96.40ms
step:795/1770 train_time:75680ms step_avg:96.41ms
step:796/1770 train_time:75779ms step_avg:96.41ms
step:797/1770 train_time:75879ms step_avg:96.42ms
step:798/1770 train_time:75979ms step_avg:96.42ms
step:799/1770 train_time:76078ms step_avg:96.42ms
step:800/1770 train_time:76178ms step_avg:96.43ms
step:801/1770 train_time:76279ms step_avg:96.43ms
step:802/1770 train_time:76379ms step_avg:96.44ms
step:803/1770 train_time:76479ms step_avg:96.44ms
step:804/1770 train_time:76579ms step_avg:96.45ms
step:805/1770 train_time:76678ms step_avg:96.45ms
step:806/1770 train_time:76778ms step_avg:96.45ms
step:807/1770 train_time:76878ms step_avg:96.46ms
step:808/1770 train_time:76977ms step_avg:96.46ms
step:809/1770 train_time:77077ms step_avg:96.47ms
step:810/1770 train_time:77176ms step_avg:96.47ms
step:811/1770 train_time:77276ms step_avg:96.47ms
step:812/1770 train_time:77376ms step_avg:96.48ms
step:813/1770 train_time:77477ms step_avg:96.48ms
step:814/1770 train_time:77576ms step_avg:96.49ms
step:815/1770 train_time:77676ms step_avg:96.49ms
step:816/1770 train_time:77775ms step_avg:96.50ms
step:817/1770 train_time:77875ms step_avg:96.50ms
step:818/1770 train_time:77974ms step_avg:96.50ms
step:819/1770 train_time:78073ms step_avg:96.51ms
step:820/1770 train_time:78172ms step_avg:96.51ms
step:821/1770 train_time:78272ms step_avg:96.51ms
step:822/1770 train_time:78372ms step_avg:96.52ms
step:823/1770 train_time:78472ms step_avg:96.52ms
step:824/1770 train_time:78571ms step_avg:96.52ms
step:825/1770 train_time:78670ms step_avg:96.53ms
step:826/1770 train_time:78768ms step_avg:96.53ms
step:827/1770 train_time:78867ms step_avg:96.53ms
step:828/1770 train_time:78966ms step_avg:96.54ms
step:829/1770 train_time:79064ms step_avg:96.54ms
step:830/1770 train_time:79163ms step_avg:96.54ms
step:831/1770 train_time:79262ms step_avg:96.54ms
step:832/1770 train_time:79362ms step_avg:96.55ms
step:833/1770 train_time:79461ms step_avg:96.55ms
step:834/1770 train_time:79561ms step_avg:96.55ms
step:835/1770 train_time:79661ms step_avg:96.56ms
step:836/1770 train_time:79761ms step_avg:96.56ms
step:837/1770 train_time:79862ms step_avg:96.57ms
step:838/1770 train_time:79961ms step_avg:96.57ms
step:839/1770 train_time:80061ms step_avg:96.57ms
step:840/1770 train_time:80160ms step_avg:96.58ms
step:841/1770 train_time:80259ms step_avg:96.58ms
step:842/1770 train_time:80359ms step_avg:96.59ms
step:843/1770 train_time:80458ms step_avg:96.59ms
step:844/1770 train_time:80558ms step_avg:96.59ms
step:845/1770 train_time:80659ms step_avg:96.60ms
step:846/1770 train_time:80759ms step_avg:96.60ms
step:847/1770 train_time:80859ms step_avg:96.61ms
step:848/1770 train_time:80960ms step_avg:96.61ms
step:849/1770 train_time:81060ms step_avg:96.61ms
step:850/1770 train_time:81159ms step_avg:96.62ms
step:851/1770 train_time:81260ms step_avg:96.62ms
step:852/1770 train_time:81359ms step_avg:96.63ms
step:853/1770 train_time:81458ms step_avg:96.63ms
step:854/1770 train_time:81558ms step_avg:96.63ms
step:855/1770 train_time:81658ms step_avg:96.64ms
step:856/1770 train_time:81758ms step_avg:96.64ms
step:857/1770 train_time:81858ms step_avg:96.64ms
step:858/1770 train_time:81959ms step_avg:96.65ms
step:859/1770 train_time:82059ms step_avg:96.65ms
step:860/1770 train_time:82159ms step_avg:96.66ms
step:861/1770 train_time:82258ms step_avg:96.66ms
step:862/1770 train_time:82358ms step_avg:96.66ms
step:863/1770 train_time:82457ms step_avg:96.67ms
step:864/1770 train_time:82556ms step_avg:96.67ms
step:865/1770 train_time:82655ms step_avg:96.67ms
step:866/1770 train_time:82757ms step_avg:96.68ms
step:867/1770 train_time:82857ms step_avg:96.68ms
step:868/1770 train_time:82957ms step_avg:96.69ms
step:869/1770 train_time:83057ms step_avg:96.69ms
step:870/1770 train_time:83157ms step_avg:96.69ms
step:871/1770 train_time:83258ms step_avg:96.70ms
step:872/1770 train_time:83356ms step_avg:96.70ms
step:873/1770 train_time:83456ms step_avg:96.70ms
step:874/1770 train_time:83555ms step_avg:96.71ms
step:875/1770 train_time:83654ms step_avg:96.71ms
step:875/1770 val_loss:3.5507 train_time:83752ms step_avg:96.82ms
step:876/1770 train_time:83777ms step_avg:96.74ms
step:877/1770 train_time:83863ms step_avg:96.73ms
step:878/1770 train_time:83964ms step_avg:96.73ms
step:879/1770 train_time:84063ms step_avg:96.74ms
step:880/1770 train_time:84162ms step_avg:96.74ms
step:881/1770 train_time:84260ms step_avg:96.74ms
step:882/1770 train_time:84359ms step_avg:96.74ms
step:883/1770 train_time:84457ms step_avg:96.74ms
step:884/1770 train_time:84556ms step_avg:96.75ms
step:885/1770 train_time:84655ms step_avg:96.75ms
step:886/1770 train_time:84754ms step_avg:96.75ms
step:887/1770 train_time:84854ms step_avg:96.75ms
step:888/1770 train_time:84954ms step_avg:96.76ms
step:889/1770 train_time:85054ms step_avg:96.76ms
step:890/1770 train_time:85154ms step_avg:96.77ms
step:891/1770 train_time:85253ms step_avg:96.77ms
step:892/1770 train_time:85354ms step_avg:96.77ms
step:893/1770 train_time:85456ms step_avg:96.78ms
step:894/1770 train_time:85556ms step_avg:96.78ms
step:895/1770 train_time:85655ms step_avg:96.79ms
step:896/1770 train_time:85754ms step_avg:96.79ms
step:897/1770 train_time:85853ms step_avg:96.79ms
step:898/1770 train_time:85952ms step_avg:96.79ms
step:899/1770 train_time:86051ms step_avg:96.80ms
step:900/1770 train_time:86151ms step_avg:96.80ms
step:901/1770 train_time:86251ms step_avg:96.80ms
step:902/1770 train_time:86351ms step_avg:96.81ms
step:903/1770 train_time:86452ms step_avg:96.81ms
step:904/1770 train_time:86552ms step_avg:96.81ms
step:905/1770 train_time:86653ms step_avg:96.82ms
step:906/1770 train_time:86752ms step_avg:96.82ms
step:907/1770 train_time:86852ms step_avg:96.82ms
step:908/1770 train_time:86952ms step_avg:96.83ms
step:909/1770 train_time:87051ms step_avg:96.83ms
step:910/1770 train_time:87152ms step_avg:96.84ms
step:911/1770 train_time:87251ms step_avg:96.84ms
step:912/1770 train_time:87351ms step_avg:96.84ms
step:913/1770 train_time:87451ms step_avg:96.84ms
step:914/1770 train_time:87551ms step_avg:96.85ms
step:915/1770 train_time:87651ms step_avg:96.85ms
step:916/1770 train_time:87751ms step_avg:96.86ms
step:917/1770 train_time:87850ms step_avg:96.86ms
step:918/1770 train_time:87950ms step_avg:96.86ms
step:919/1770 train_time:88049ms step_avg:96.86ms
step:920/1770 train_time:88152ms step_avg:96.87ms
step:921/1770 train_time:88253ms step_avg:96.88ms
step:922/1770 train_time:88354ms step_avg:96.88ms
step:923/1770 train_time:88456ms step_avg:96.88ms
step:924/1770 train_time:88556ms step_avg:96.89ms
step:925/1770 train_time:88656ms step_avg:96.89ms
step:926/1770 train_time:88756ms step_avg:96.90ms
step:927/1770 train_time:88856ms step_avg:96.90ms
step:928/1770 train_time:88956ms step_avg:96.90ms
step:929/1770 train_time:89056ms step_avg:96.90ms
step:930/1770 train_time:89156ms step_avg:96.91ms
step:931/1770 train_time:89257ms step_avg:96.91ms
step:932/1770 train_time:89357ms step_avg:96.92ms
step:933/1770 train_time:89456ms step_avg:96.92ms
step:934/1770 train_time:89556ms step_avg:96.92ms
step:935/1770 train_time:89657ms step_avg:96.93ms
step:936/1770 train_time:89757ms step_avg:96.93ms
step:937/1770 train_time:89857ms step_avg:96.93ms
step:938/1770 train_time:89958ms step_avg:96.94ms
step:939/1770 train_time:90058ms step_avg:96.94ms
step:940/1770 train_time:90158ms step_avg:96.94ms
step:941/1770 train_time:90258ms step_avg:96.95ms
step:942/1770 train_time:90358ms step_avg:96.95ms
step:943/1770 train_time:90458ms step_avg:96.95ms
step:944/1770 train_time:90558ms step_avg:96.96ms
step:945/1770 train_time:90657ms step_avg:96.96ms
step:946/1770 train_time:90758ms step_avg:96.96ms
step:947/1770 train_time:90858ms step_avg:96.97ms
step:948/1770 train_time:90958ms step_avg:96.97ms
step:949/1770 train_time:91058ms step_avg:96.97ms
step:950/1770 train_time:91159ms step_avg:96.98ms
step:951/1770 train_time:91259ms step_avg:96.98ms
step:952/1770 train_time:91359ms step_avg:96.98ms
step:953/1770 train_time:91459ms step_avg:96.99ms
step:954/1770 train_time:91559ms step_avg:96.99ms
step:955/1770 train_time:91659ms step_avg:96.99ms
step:956/1770 train_time:91759ms step_avg:97.00ms
step:957/1770 train_time:91859ms step_avg:97.00ms
step:958/1770 train_time:91959ms step_avg:97.00ms
step:959/1770 train_time:92059ms step_avg:97.01ms
step:960/1770 train_time:92158ms step_avg:97.01ms
step:961/1770 train_time:92258ms step_avg:97.01ms
step:962/1770 train_time:92359ms step_avg:97.02ms
step:963/1770 train_time:92459ms step_avg:97.02ms
step:964/1770 train_time:92560ms step_avg:97.02ms
step:965/1770 train_time:92659ms step_avg:97.03ms
step:966/1770 train_time:92759ms step_avg:97.03ms
step:967/1770 train_time:92859ms step_avg:97.03ms
step:968/1770 train_time:92960ms step_avg:97.04ms
step:969/1770 train_time:93059ms step_avg:97.04ms
step:970/1770 train_time:93159ms step_avg:97.04ms
step:971/1770 train_time:93259ms step_avg:97.04ms
step:972/1770 train_time:93359ms step_avg:97.05ms
step:973/1770 train_time:93459ms step_avg:97.05ms
step:974/1770 train_time:93559ms step_avg:97.05ms
step:975/1770 train_time:93659ms step_avg:97.06ms
step:976/1770 train_time:93759ms step_avg:97.06ms
step:977/1770 train_time:93859ms step_avg:97.06ms
step:978/1770 train_time:93959ms step_avg:97.07ms
step:979/1770 train_time:94059ms step_avg:97.07ms
step:980/1770 train_time:94159ms step_avg:97.07ms
step:981/1770 train_time:94259ms step_avg:97.07ms
step:982/1770 train_time:94359ms step_avg:97.08ms
step:983/1770 train_time:94459ms step_avg:97.08ms
step:984/1770 train_time:94560ms step_avg:97.08ms
step:985/1770 train_time:94660ms step_avg:97.09ms
step:986/1770 train_time:94759ms step_avg:97.09ms
step:987/1770 train_time:94860ms step_avg:97.09ms
step:988/1770 train_time:94959ms step_avg:97.10ms
step:989/1770 train_time:95060ms step_avg:97.10ms
step:990/1770 train_time:95160ms step_avg:97.10ms
step:991/1770 train_time:95260ms step_avg:97.11ms
step:992/1770 train_time:95361ms step_avg:97.11ms
step:993/1770 train_time:95461ms step_avg:97.11ms
step:994/1770 train_time:95561ms step_avg:97.11ms
step:995/1770 train_time:95661ms step_avg:97.12ms
step:996/1770 train_time:95761ms step_avg:97.12ms
step:997/1770 train_time:95861ms step_avg:97.12ms
step:998/1770 train_time:95961ms step_avg:97.13ms
step:999/1770 train_time:96061ms step_avg:97.13ms
step:1000/1770 train_time:96161ms step_avg:97.13ms
step:1000/1770 val_loss:3.5117 train_time:96259ms step_avg:97.23ms
step:1001/1770 train_time:96281ms step_avg:97.15ms
step:1002/1770 train_time:96371ms step_avg:97.15ms
step:1003/1770 train_time:96474ms step_avg:97.15ms
step:1004/1770 train_time:96575ms step_avg:97.16ms
step:1005/1770 train_time:96675ms step_avg:97.16ms
step:1006/1770 train_time:96775ms step_avg:97.16ms
step:1007/1770 train_time:96876ms step_avg:97.17ms
step:1008/1770 train_time:96975ms step_avg:97.17ms
step:1009/1770 train_time:97075ms step_avg:97.17ms
step:1010/1770 train_time:97175ms step_avg:97.18ms
step:1011/1770 train_time:97276ms step_avg:97.18ms
step:1012/1770 train_time:97377ms step_avg:97.18ms
step:1013/1770 train_time:97478ms step_avg:97.19ms
step:1014/1770 train_time:97578ms step_avg:97.19ms
step:1015/1770 train_time:97678ms step_avg:97.19ms
step:1016/1770 train_time:97778ms step_avg:97.19ms
step:1017/1770 train_time:97878ms step_avg:97.20ms
step:1018/1770 train_time:97977ms step_avg:97.20ms
step:1019/1770 train_time:98078ms step_avg:97.20ms
step:1020/1770 train_time:98178ms step_avg:97.21ms
step:1021/1770 train_time:98279ms step_avg:97.21ms
step:1022/1770 train_time:98379ms step_avg:97.21ms
step:1023/1770 train_time:98479ms step_avg:97.22ms
step:1024/1770 train_time:98579ms step_avg:97.22ms
step:1025/1770 train_time:98678ms step_avg:97.22ms
step:1026/1770 train_time:98778ms step_avg:97.22ms
step:1027/1770 train_time:98878ms step_avg:97.23ms
step:1028/1770 train_time:98978ms step_avg:97.23ms
step:1029/1770 train_time:99078ms step_avg:97.23ms
step:1030/1770 train_time:99178ms step_avg:97.23ms
step:1031/1770 train_time:99278ms step_avg:97.24ms
step:1032/1770 train_time:99379ms step_avg:97.24ms
step:1033/1770 train_time:99479ms step_avg:97.24ms
step:1034/1770 train_time:99578ms step_avg:97.24ms
step:1035/1770 train_time:99678ms step_avg:97.25ms
step:1036/1770 train_time:99778ms step_avg:97.25ms
step:1037/1770 train_time:99878ms step_avg:97.25ms
step:1038/1770 train_time:99978ms step_avg:97.25ms
step:1039/1770 train_time:100079ms step_avg:97.26ms
step:1040/1770 train_time:100178ms step_avg:97.26ms
step:1041/1770 train_time:100278ms step_avg:97.26ms
step:1042/1770 train_time:100378ms step_avg:97.27ms
step:1043/1770 train_time:100478ms step_avg:97.27ms
step:1044/1770 train_time:100578ms step_avg:97.27ms
step:1045/1770 train_time:100678ms step_avg:97.27ms
step:1046/1770 train_time:100778ms step_avg:97.28ms
step:1047/1770 train_time:100878ms step_avg:97.28ms
step:1048/1770 train_time:100978ms step_avg:97.28ms
step:1049/1770 train_time:101079ms step_avg:97.28ms
step:1050/1770 train_time:101179ms step_avg:97.29ms
step:1051/1770 train_time:101279ms step_avg:97.29ms
step:1052/1770 train_time:101380ms step_avg:97.29ms
step:1053/1770 train_time:101480ms step_avg:97.30ms
step:1054/1770 train_time:101579ms step_avg:97.30ms
step:1055/1770 train_time:101679ms step_avg:97.30ms
step:1056/1770 train_time:101779ms step_avg:97.30ms
step:1057/1770 train_time:101880ms step_avg:97.31ms
step:1058/1770 train_time:101981ms step_avg:97.31ms
step:1059/1770 train_time:102081ms step_avg:97.31ms
step:1060/1770 train_time:102181ms step_avg:97.32ms
step:1061/1770 train_time:102282ms step_avg:97.32ms
step:1062/1770 train_time:102383ms step_avg:97.32ms
step:1063/1770 train_time:102485ms step_avg:97.33ms
step:1064/1770 train_time:102586ms step_avg:97.33ms
step:1065/1770 train_time:102688ms step_avg:97.33ms
step:1066/1770 train_time:102789ms step_avg:97.34ms
step:1067/1770 train_time:102891ms step_avg:97.34ms
step:1068/1770 train_time:102994ms step_avg:97.35ms
step:1069/1770 train_time:103095ms step_avg:97.35ms
step:1070/1770 train_time:103197ms step_avg:97.36ms
step:1071/1770 train_time:103298ms step_avg:97.36ms
step:1072/1770 train_time:103398ms step_avg:97.36ms
step:1073/1770 train_time:103498ms step_avg:97.36ms
step:1074/1770 train_time:103598ms step_avg:97.37ms
step:1075/1770 train_time:103699ms step_avg:97.37ms
step:1076/1770 train_time:103800ms step_avg:97.37ms
step:1077/1770 train_time:103899ms step_avg:97.38ms
step:1078/1770 train_time:104000ms step_avg:97.38ms
step:1079/1770 train_time:104101ms step_avg:97.38ms
step:1080/1770 train_time:104201ms step_avg:97.38ms
step:1081/1770 train_time:104301ms step_avg:97.39ms
step:1082/1770 train_time:104402ms step_avg:97.39ms
step:1083/1770 train_time:104502ms step_avg:97.39ms
step:1084/1770 train_time:104603ms step_avg:97.40ms
step:1085/1770 train_time:104705ms step_avg:97.40ms
step:1086/1770 train_time:104805ms step_avg:97.40ms
step:1087/1770 train_time:104906ms step_avg:97.41ms
step:1088/1770 train_time:105007ms step_avg:97.41ms
step:1089/1770 train_time:105109ms step_avg:97.41ms
step:1090/1770 train_time:105211ms step_avg:97.42ms
step:1091/1770 train_time:105313ms step_avg:97.42ms
step:1092/1770 train_time:105413ms step_avg:97.42ms
step:1093/1770 train_time:105514ms step_avg:97.43ms
step:1094/1770 train_time:105615ms step_avg:97.43ms
step:1095/1770 train_time:105716ms step_avg:97.43ms
step:1096/1770 train_time:105817ms step_avg:97.44ms
step:1097/1770 train_time:105917ms step_avg:97.44ms
step:1098/1770 train_time:106017ms step_avg:97.44ms
step:1099/1770 train_time:106118ms step_avg:97.45ms
step:1100/1770 train_time:106219ms step_avg:97.45ms
step:1101/1770 train_time:106319ms step_avg:97.45ms
step:1102/1770 train_time:106419ms step_avg:97.45ms
step:1103/1770 train_time:106520ms step_avg:97.46ms
step:1104/1770 train_time:106619ms step_avg:97.46ms
step:1105/1770 train_time:106719ms step_avg:97.46ms
step:1106/1770 train_time:106820ms step_avg:97.46ms
step:1107/1770 train_time:106920ms step_avg:97.47ms
step:1108/1770 train_time:107020ms step_avg:97.47ms
step:1109/1770 train_time:107120ms step_avg:97.47ms
step:1110/1770 train_time:107220ms step_avg:97.47ms
step:1111/1770 train_time:107321ms step_avg:97.48ms
step:1112/1770 train_time:107422ms step_avg:97.48ms
step:1113/1770 train_time:107522ms step_avg:97.48ms
step:1114/1770 train_time:107622ms step_avg:97.48ms
step:1115/1770 train_time:107723ms step_avg:97.49ms
step:1116/1770 train_time:107824ms step_avg:97.49ms
step:1117/1770 train_time:107925ms step_avg:97.49ms
step:1118/1770 train_time:108026ms step_avg:97.50ms
step:1119/1770 train_time:108128ms step_avg:97.50ms
step:1120/1770 train_time:108230ms step_avg:97.50ms
step:1121/1770 train_time:108331ms step_avg:97.51ms
step:1122/1770 train_time:108433ms step_avg:97.51ms
step:1123/1770 train_time:108534ms step_avg:97.51ms
step:1124/1770 train_time:108635ms step_avg:97.52ms
step:1125/1770 train_time:108736ms step_avg:97.52ms
step:1125/1770 val_loss:3.4708 train_time:108835ms step_avg:97.61ms
step:1126/1770 train_time:108858ms step_avg:97.54ms
step:1127/1770 train_time:108949ms step_avg:97.54ms
step:1128/1770 train_time:109051ms step_avg:97.54ms
step:1129/1770 train_time:109152ms step_avg:97.54ms
step:1130/1770 train_time:109253ms step_avg:97.55ms
step:1131/1770 train_time:109354ms step_avg:97.55ms
step:1132/1770 train_time:109455ms step_avg:97.55ms
step:1133/1770 train_time:109556ms step_avg:97.56ms
step:1134/1770 train_time:109657ms step_avg:97.56ms
step:1135/1770 train_time:109757ms step_avg:97.56ms
step:1136/1770 train_time:109857ms step_avg:97.56ms
step:1137/1770 train_time:109958ms step_avg:97.57ms
step:1138/1770 train_time:110058ms step_avg:97.57ms
step:1139/1770 train_time:110159ms step_avg:97.57ms
step:1140/1770 train_time:110259ms step_avg:97.57ms
step:1141/1770 train_time:110359ms step_avg:97.58ms
step:1142/1770 train_time:110460ms step_avg:97.58ms
step:1143/1770 train_time:110560ms step_avg:97.58ms
step:1144/1770 train_time:110662ms step_avg:97.59ms
step:1145/1770 train_time:110762ms step_avg:97.59ms
step:1146/1770 train_time:110864ms step_avg:97.59ms
step:1147/1770 train_time:110964ms step_avg:97.59ms
step:1148/1770 train_time:111065ms step_avg:97.60ms
step:1149/1770 train_time:111167ms step_avg:97.60ms
step:1150/1770 train_time:111268ms step_avg:97.60ms
step:1151/1770 train_time:111370ms step_avg:97.61ms
step:1152/1770 train_time:111472ms step_avg:97.61ms
step:1153/1770 train_time:111573ms step_avg:97.61ms
step:1154/1770 train_time:111674ms step_avg:97.62ms
step:1155/1770 train_time:111774ms step_avg:97.62ms
step:1156/1770 train_time:111875ms step_avg:97.62ms
step:1157/1770 train_time:111978ms step_avg:97.63ms
step:1158/1770 train_time:112078ms step_avg:97.63ms
step:1159/1770 train_time:112178ms step_avg:97.63ms
step:1160/1770 train_time:112278ms step_avg:97.63ms
step:1161/1770 train_time:112379ms step_avg:97.64ms
step:1162/1770 train_time:112479ms step_avg:97.64ms
step:1163/1770 train_time:112580ms step_avg:97.64ms
step:1164/1770 train_time:112680ms step_avg:97.64ms
step:1165/1770 train_time:112781ms step_avg:97.65ms
step:1166/1770 train_time:112882ms step_avg:97.65ms
step:1167/1770 train_time:112983ms step_avg:97.65ms
step:1168/1770 train_time:113084ms step_avg:97.65ms
step:1169/1770 train_time:113186ms step_avg:97.66ms
step:1170/1770 train_time:113287ms step_avg:97.66ms
step:1171/1770 train_time:113389ms step_avg:97.67ms
step:1172/1770 train_time:113492ms step_avg:97.67ms
step:1173/1770 train_time:113594ms step_avg:97.67ms
step:1174/1770 train_time:113694ms step_avg:97.68ms
step:1175/1770 train_time:113795ms step_avg:97.68ms
step:1176/1770 train_time:113896ms step_avg:97.68ms
step:1177/1770 train_time:113996ms step_avg:97.68ms
step:1178/1770 train_time:114097ms step_avg:97.69ms
step:1179/1770 train_time:114197ms step_avg:97.69ms
step:1180/1770 train_time:114297ms step_avg:97.69ms
step:1181/1770 train_time:114398ms step_avg:97.69ms
step:1182/1770 train_time:114498ms step_avg:97.69ms
step:1183/1770 train_time:114599ms step_avg:97.70ms
step:1184/1770 train_time:114702ms step_avg:97.70ms
step:1185/1770 train_time:114803ms step_avg:97.70ms
step:1186/1770 train_time:114905ms step_avg:97.71ms
step:1187/1770 train_time:115010ms step_avg:97.71ms
step:1188/1770 train_time:115112ms step_avg:97.72ms
step:1189/1770 train_time:115214ms step_avg:97.72ms
step:1190/1770 train_time:115316ms step_avg:97.73ms
step:1191/1770 train_time:115418ms step_avg:97.73ms
step:1192/1770 train_time:115520ms step_avg:97.73ms
step:1193/1770 train_time:115622ms step_avg:97.74ms
step:1194/1770 train_time:115723ms step_avg:97.74ms
step:1195/1770 train_time:115826ms step_avg:97.74ms
step:1196/1770 train_time:115929ms step_avg:97.75ms
step:1197/1770 train_time:116030ms step_avg:97.75ms
step:1198/1770 train_time:116132ms step_avg:97.75ms
step:1199/1770 train_time:116234ms step_avg:97.76ms
step:1200/1770 train_time:116337ms step_avg:97.76ms
step:1201/1770 train_time:116438ms step_avg:97.77ms
step:1202/1770 train_time:116539ms step_avg:97.77ms
step:1203/1770 train_time:116640ms step_avg:97.77ms
step:1204/1770 train_time:116743ms step_avg:97.77ms
step:1205/1770 train_time:116843ms step_avg:97.78ms
step:1206/1770 train_time:116946ms step_avg:97.78ms
step:1207/1770 train_time:117049ms step_avg:97.79ms
step:1208/1770 train_time:117152ms step_avg:97.79ms
step:1209/1770 train_time:117255ms step_avg:97.79ms
step:1210/1770 train_time:117357ms step_avg:97.80ms
step:1211/1770 train_time:117460ms step_avg:97.80ms
step:1212/1770 train_time:117563ms step_avg:97.81ms
step:1213/1770 train_time:117665ms step_avg:97.81ms
step:1214/1770 train_time:117767ms step_avg:97.81ms
step:1215/1770 train_time:117870ms step_avg:97.82ms
step:1216/1770 train_time:117974ms step_avg:97.82ms
step:1217/1770 train_time:118076ms step_avg:97.83ms
step:1218/1770 train_time:118177ms step_avg:97.83ms
step:1219/1770 train_time:118279ms step_avg:97.83ms
step:1220/1770 train_time:118383ms step_avg:97.84ms
step:1221/1770 train_time:118484ms step_avg:97.84ms
step:1222/1770 train_time:118588ms step_avg:97.85ms
step:1223/1770 train_time:118690ms step_avg:97.85ms
step:1224/1770 train_time:118793ms step_avg:97.85ms
step:1225/1770 train_time:118896ms step_avg:97.86ms
step:1226/1770 train_time:118998ms step_avg:97.86ms
step:1227/1770 train_time:119102ms step_avg:97.87ms
step:1228/1770 train_time:119207ms step_avg:97.87ms
step:1229/1770 train_time:119309ms step_avg:97.87ms
step:1230/1770 train_time:119411ms step_avg:97.88ms
step:1231/1770 train_time:119514ms step_avg:97.88ms
step:1232/1770 train_time:119616ms step_avg:97.89ms
step:1233/1770 train_time:119717ms step_avg:97.89ms
step:1234/1770 train_time:119820ms step_avg:97.89ms
step:1235/1770 train_time:119921ms step_avg:97.89ms
step:1236/1770 train_time:120023ms step_avg:97.90ms
step:1237/1770 train_time:120125ms step_avg:97.90ms
step:1238/1770 train_time:120228ms step_avg:97.91ms
step:1239/1770 train_time:120330ms step_avg:97.91ms
step:1240/1770 train_time:120432ms step_avg:97.91ms
step:1241/1770 train_time:120536ms step_avg:97.92ms
step:1242/1770 train_time:120638ms step_avg:97.92ms
step:1243/1770 train_time:120740ms step_avg:97.92ms
step:1244/1770 train_time:120841ms step_avg:97.93ms
step:1245/1770 train_time:120943ms step_avg:97.93ms
step:1246/1770 train_time:121046ms step_avg:97.93ms
step:1247/1770 train_time:121149ms step_avg:97.94ms
step:1248/1770 train_time:121251ms step_avg:97.94ms
step:1249/1770 train_time:121354ms step_avg:97.94ms
step:1250/1770 train_time:121456ms step_avg:97.95ms
step:1250/1770 val_loss:3.4235 train_time:121557ms step_avg:98.03ms
step:1251/1770 train_time:121578ms step_avg:97.97ms
step:1252/1770 train_time:121671ms step_avg:97.96ms
step:1253/1770 train_time:121773ms step_avg:97.97ms
step:1254/1770 train_time:121875ms step_avg:97.97ms
step:1255/1770 train_time:121980ms step_avg:97.98ms
step:1256/1770 train_time:122081ms step_avg:97.98ms
step:1257/1770 train_time:122183ms step_avg:97.98ms
step:1258/1770 train_time:122285ms step_avg:97.98ms
step:1259/1770 train_time:122387ms step_avg:97.99ms
step:1260/1770 train_time:122488ms step_avg:97.99ms
step:1261/1770 train_time:122591ms step_avg:97.99ms
step:1262/1770 train_time:122695ms step_avg:98.00ms
step:1263/1770 train_time:122796ms step_avg:98.00ms
step:1264/1770 train_time:122901ms step_avg:98.01ms
step:1265/1770 train_time:123004ms step_avg:98.01ms
step:1266/1770 train_time:123106ms step_avg:98.01ms
step:1267/1770 train_time:123208ms step_avg:98.02ms
step:1268/1770 train_time:123310ms step_avg:98.02ms
step:1269/1770 train_time:123411ms step_avg:98.02ms
step:1270/1770 train_time:123513ms step_avg:98.03ms
step:1271/1770 train_time:123616ms step_avg:98.03ms
step:1272/1770 train_time:123717ms step_avg:98.03ms
step:1273/1770 train_time:123820ms step_avg:98.04ms
step:1274/1770 train_time:123923ms step_avg:98.04ms
step:1275/1770 train_time:124025ms step_avg:98.04ms
step:1276/1770 train_time:124128ms step_avg:98.05ms
step:1277/1770 train_time:124230ms step_avg:98.05ms
step:1278/1770 train_time:124333ms step_avg:98.05ms
step:1279/1770 train_time:124435ms step_avg:98.06ms
step:1280/1770 train_time:124540ms step_avg:98.06ms
step:1281/1770 train_time:124641ms step_avg:98.07ms
step:1282/1770 train_time:124743ms step_avg:98.07ms
step:1283/1770 train_time:124846ms step_avg:98.07ms
step:1284/1770 train_time:124948ms step_avg:98.08ms
step:1285/1770 train_time:125050ms step_avg:98.08ms
step:1286/1770 train_time:125152ms step_avg:98.08ms
step:1287/1770 train_time:125256ms step_avg:98.09ms
step:1288/1770 train_time:125360ms step_avg:98.09ms
step:1289/1770 train_time:125463ms step_avg:98.09ms
step:1290/1770 train_time:125564ms step_avg:98.10ms
step:1291/1770 train_time:125666ms step_avg:98.10ms
step:1292/1770 train_time:125768ms step_avg:98.10ms
step:1293/1770 train_time:125870ms step_avg:98.11ms
step:1294/1770 train_time:125972ms step_avg:98.11ms
step:1295/1770 train_time:126074ms step_avg:98.11ms
step:1296/1770 train_time:126176ms step_avg:98.12ms
step:1297/1770 train_time:126278ms step_avg:98.12ms
step:1298/1770 train_time:126380ms step_avg:98.12ms
step:1299/1770 train_time:126483ms step_avg:98.12ms
step:1300/1770 train_time:126585ms step_avg:98.13ms
step:1301/1770 train_time:126687ms step_avg:98.13ms
step:1302/1770 train_time:126789ms step_avg:98.13ms
step:1303/1770 train_time:126890ms step_avg:98.14ms
step:1304/1770 train_time:126992ms step_avg:98.14ms
step:1305/1770 train_time:127094ms step_avg:98.14ms
step:1306/1770 train_time:127195ms step_avg:98.14ms
step:1307/1770 train_time:127297ms step_avg:98.15ms
step:1308/1770 train_time:127400ms step_avg:98.15ms
step:1309/1770 train_time:127503ms step_avg:98.15ms
step:1310/1770 train_time:127605ms step_avg:98.16ms
step:1311/1770 train_time:127707ms step_avg:98.16ms
step:1312/1770 train_time:127809ms step_avg:98.16ms
step:1313/1770 train_time:127910ms step_avg:98.17ms
step:1314/1770 train_time:128012ms step_avg:98.17ms
step:1315/1770 train_time:128113ms step_avg:98.17ms
step:1316/1770 train_time:128216ms step_avg:98.17ms
step:1317/1770 train_time:128318ms step_avg:98.18ms
step:1318/1770 train_time:128423ms step_avg:98.18ms
step:1319/1770 train_time:128526ms step_avg:98.19ms
step:1320/1770 train_time:128627ms step_avg:98.19ms
step:1321/1770 train_time:128730ms step_avg:98.19ms
step:1322/1770 train_time:128832ms step_avg:98.20ms
step:1323/1770 train_time:128934ms step_avg:98.20ms
step:1324/1770 train_time:129037ms step_avg:98.20ms
step:1325/1770 train_time:129140ms step_avg:98.21ms
step:1326/1770 train_time:129242ms step_avg:98.21ms
step:1327/1770 train_time:129347ms step_avg:98.21ms
step:1328/1770 train_time:129450ms step_avg:98.22ms
step:1329/1770 train_time:129552ms step_avg:98.22ms
step:1330/1770 train_time:129653ms step_avg:98.22ms
step:1331/1770 train_time:129754ms step_avg:98.22ms
step:1332/1770 train_time:129856ms step_avg:98.23ms
step:1333/1770 train_time:129959ms step_avg:98.23ms
step:1334/1770 train_time:130061ms step_avg:98.23ms
step:1335/1770 train_time:130164ms step_avg:98.24ms
step:1336/1770 train_time:130266ms step_avg:98.24ms
step:1337/1770 train_time:130368ms step_avg:98.24ms
step:1338/1770 train_time:130469ms step_avg:98.24ms
step:1339/1770 train_time:130571ms step_avg:98.25ms
step:1340/1770 train_time:130674ms step_avg:98.25ms
step:1341/1770 train_time:130777ms step_avg:98.25ms
step:1342/1770 train_time:130879ms step_avg:98.26ms
step:1343/1770 train_time:130982ms step_avg:98.26ms
step:1344/1770 train_time:131086ms step_avg:98.27ms
step:1345/1770 train_time:131188ms step_avg:98.27ms
step:1346/1770 train_time:131289ms step_avg:98.27ms
step:1347/1770 train_time:131391ms step_avg:98.27ms
step:1348/1770 train_time:131495ms step_avg:98.28ms
step:1349/1770 train_time:131597ms step_avg:98.28ms
step:1350/1770 train_time:131699ms step_avg:98.28ms
step:1351/1770 train_time:131802ms step_avg:98.29ms
step:1352/1770 train_time:131904ms step_avg:98.29ms
step:1353/1770 train_time:132007ms step_avg:98.29ms
step:1354/1770 train_time:132109ms step_avg:98.30ms
step:1355/1770 train_time:132210ms step_avg:98.30ms
step:1356/1770 train_time:132312ms step_avg:98.30ms
step:1357/1770 train_time:132415ms step_avg:98.30ms
step:1358/1770 train_time:132517ms step_avg:98.31ms
step:1359/1770 train_time:132619ms step_avg:98.31ms
step:1360/1770 train_time:132723ms step_avg:98.31ms
step:1361/1770 train_time:132826ms step_avg:98.32ms
step:1362/1770 train_time:132929ms step_avg:98.32ms
step:1363/1770 train_time:133032ms step_avg:98.32ms
step:1364/1770 train_time:133134ms step_avg:98.33ms
step:1365/1770 train_time:133236ms step_avg:98.33ms
step:1366/1770 train_time:133338ms step_avg:98.33ms
step:1367/1770 train_time:133441ms step_avg:98.34ms
step:1368/1770 train_time:133543ms step_avg:98.34ms
step:1369/1770 train_time:133646ms step_avg:98.34ms
step:1370/1770 train_time:133749ms step_avg:98.34ms
step:1371/1770 train_time:133851ms step_avg:98.35ms
step:1372/1770 train_time:133952ms step_avg:98.35ms
step:1373/1770 train_time:134055ms step_avg:98.35ms
step:1374/1770 train_time:134158ms step_avg:98.36ms
step:1375/1770 train_time:134260ms step_avg:98.36ms
step:1375/1770 val_loss:3.3803 train_time:134362ms step_avg:98.43ms
step:1376/1770 train_time:134383ms step_avg:98.38ms
step:1377/1770 train_time:134472ms step_avg:98.37ms
step:1378/1770 train_time:134574ms step_avg:98.37ms
step:1379/1770 train_time:134676ms step_avg:98.38ms
step:1380/1770 train_time:134777ms step_avg:98.38ms
step:1381/1770 train_time:134880ms step_avg:98.38ms
step:1382/1770 train_time:134982ms step_avg:98.38ms
step:1383/1770 train_time:135085ms step_avg:98.39ms
step:1384/1770 train_time:135187ms step_avg:98.39ms
step:1385/1770 train_time:135288ms step_avg:98.39ms
step:1386/1770 train_time:135391ms step_avg:98.39ms
step:1387/1770 train_time:135495ms step_avg:98.40ms
step:1388/1770 train_time:135597ms step_avg:98.40ms
step:1389/1770 train_time:135699ms step_avg:98.40ms
step:1390/1770 train_time:135801ms step_avg:98.41ms
step:1391/1770 train_time:135904ms step_avg:98.41ms
step:1392/1770 train_time:136006ms step_avg:98.41ms
step:1393/1770 train_time:136108ms step_avg:98.42ms
step:1394/1770 train_time:136210ms step_avg:98.42ms
step:1395/1770 train_time:136313ms step_avg:98.42ms
step:1396/1770 train_time:136417ms step_avg:98.42ms
step:1397/1770 train_time:136519ms step_avg:98.43ms
step:1398/1770 train_time:136621ms step_avg:98.43ms
step:1399/1770 train_time:136723ms step_avg:98.43ms
step:1400/1770 train_time:136826ms step_avg:98.44ms
step:1401/1770 train_time:136928ms step_avg:98.44ms
step:1402/1770 train_time:137030ms step_avg:98.44ms
step:1403/1770 train_time:137132ms step_avg:98.44ms
step:1404/1770 train_time:137235ms step_avg:98.45ms
step:1405/1770 train_time:137336ms step_avg:98.45ms
step:1406/1770 train_time:137439ms step_avg:98.45ms
step:1407/1770 train_time:137540ms step_avg:98.45ms
step:1408/1770 train_time:137642ms step_avg:98.46ms
step:1409/1770 train_time:137745ms step_avg:98.46ms
step:1410/1770 train_time:137847ms step_avg:98.46ms
step:1411/1770 train_time:137950ms step_avg:98.47ms
step:1412/1770 train_time:138052ms step_avg:98.47ms
step:1413/1770 train_time:138154ms step_avg:98.47ms
step:1414/1770 train_time:138257ms step_avg:98.47ms
step:1415/1770 train_time:138360ms step_avg:98.48ms
step:1416/1770 train_time:138463ms step_avg:98.48ms
step:1417/1770 train_time:138566ms step_avg:98.48ms
step:1418/1770 train_time:138668ms step_avg:98.49ms
step:1419/1770 train_time:138771ms step_avg:98.49ms
step:1420/1770 train_time:138873ms step_avg:98.49ms
step:1421/1770 train_time:138975ms step_avg:98.49ms
step:1422/1770 train_time:139078ms step_avg:98.50ms
step:1423/1770 train_time:139181ms step_avg:98.50ms
step:1424/1770 train_time:139284ms step_avg:98.50ms
step:1425/1770 train_time:139386ms step_avg:98.51ms
step:1426/1770 train_time:139488ms step_avg:98.51ms
step:1427/1770 train_time:139589ms step_avg:98.51ms
step:1428/1770 train_time:139693ms step_avg:98.51ms
step:1429/1770 train_time:139795ms step_avg:98.52ms
step:1430/1770 train_time:139897ms step_avg:98.52ms
step:1431/1770 train_time:140000ms step_avg:98.52ms
step:1432/1770 train_time:140101ms step_avg:98.52ms
step:1433/1770 train_time:140203ms step_avg:98.53ms
step:1434/1770 train_time:140305ms step_avg:98.53ms
step:1435/1770 train_time:140407ms step_avg:98.53ms
step:1436/1770 train_time:140511ms step_avg:98.54ms
step:1437/1770 train_time:140613ms step_avg:98.54ms
step:1438/1770 train_time:140715ms step_avg:98.54ms
step:1439/1770 train_time:140817ms step_avg:98.54ms
step:1440/1770 train_time:140919ms step_avg:98.54ms
step:1441/1770 train_time:141024ms step_avg:98.55ms
step:1442/1770 train_time:141126ms step_avg:98.55ms
step:1443/1770 train_time:141228ms step_avg:98.55ms
step:1444/1770 train_time:141331ms step_avg:98.56ms
step:1445/1770 train_time:141434ms step_avg:98.56ms
step:1446/1770 train_time:141536ms step_avg:98.56ms
step:1447/1770 train_time:141640ms step_avg:98.57ms
step:1448/1770 train_time:141744ms step_avg:98.57ms
step:1449/1770 train_time:141847ms step_avg:98.57ms
step:1450/1770 train_time:141950ms step_avg:98.58ms
step:1451/1770 train_time:142052ms step_avg:98.58ms
step:1452/1770 train_time:142156ms step_avg:98.58ms
step:1453/1770 train_time:142260ms step_avg:98.59ms
step:1454/1770 train_time:142363ms step_avg:98.59ms
step:1455/1770 train_time:142467ms step_avg:98.59ms
step:1456/1770 train_time:142571ms step_avg:98.60ms
step:1457/1770 train_time:142675ms step_avg:98.60ms
step:1458/1770 train_time:142778ms step_avg:98.60ms
step:1459/1770 train_time:142883ms step_avg:98.61ms
step:1460/1770 train_time:142987ms step_avg:98.61ms
step:1461/1770 train_time:143091ms step_avg:98.62ms
step:1462/1770 train_time:143193ms step_avg:98.62ms
step:1463/1770 train_time:143296ms step_avg:98.62ms
step:1464/1770 train_time:143401ms step_avg:98.62ms
step:1465/1770 train_time:143504ms step_avg:98.63ms
step:1466/1770 train_time:143608ms step_avg:98.63ms
step:1467/1770 train_time:143712ms step_avg:98.64ms
step:1468/1770 train_time:143815ms step_avg:98.64ms
step:1469/1770 train_time:143917ms step_avg:98.64ms
step:1470/1770 train_time:144020ms step_avg:98.64ms
step:1471/1770 train_time:144124ms step_avg:98.65ms
step:1472/1770 train_time:144228ms step_avg:98.65ms
step:1473/1770 train_time:144331ms step_avg:98.65ms
step:1474/1770 train_time:144435ms step_avg:98.66ms
step:1475/1770 train_time:144538ms step_avg:98.66ms
step:1476/1770 train_time:144642ms step_avg:98.66ms
step:1477/1770 train_time:144748ms step_avg:98.67ms
step:1478/1770 train_time:144852ms step_avg:98.67ms
step:1479/1770 train_time:144954ms step_avg:98.68ms
step:1480/1770 train_time:145057ms step_avg:98.68ms
step:1481/1770 train_time:145164ms step_avg:98.68ms
step:1482/1770 train_time:145268ms step_avg:98.69ms
step:1483/1770 train_time:145371ms step_avg:98.69ms
step:1484/1770 train_time:145474ms step_avg:98.69ms
step:1485/1770 train_time:145576ms step_avg:98.70ms
step:1486/1770 train_time:145679ms step_avg:98.70ms
step:1487/1770 train_time:145782ms step_avg:98.70ms
step:1488/1770 train_time:145888ms step_avg:98.71ms
step:1489/1770 train_time:145992ms step_avg:98.71ms
step:1490/1770 train_time:146096ms step_avg:98.71ms
step:1491/1770 train_time:146198ms step_avg:98.72ms
step:1492/1770 train_time:146302ms step_avg:98.72ms
step:1493/1770 train_time:146407ms step_avg:98.72ms
step:1494/1770 train_time:146514ms step_avg:98.73ms
step:1495/1770 train_time:146616ms step_avg:98.73ms
step:1496/1770 train_time:146719ms step_avg:98.73ms
step:1497/1770 train_time:146822ms step_avg:98.74ms
step:1498/1770 train_time:146926ms step_avg:98.74ms
step:1499/1770 train_time:147029ms step_avg:98.74ms
step:1500/1770 train_time:147131ms step_avg:98.75ms
step:1500/1770 val_loss:3.3432 train_time:147232ms step_avg:98.81ms
step:1501/1770 train_time:147253ms step_avg:98.76ms
step:1502/1770 train_time:147347ms step_avg:98.76ms
step:1503/1770 train_time:147450ms step_avg:98.76ms
step:1504/1770 train_time:147553ms step_avg:98.76ms
step:1505/1770 train_time:147659ms step_avg:98.77ms
step:1506/1770 train_time:147763ms step_avg:98.77ms
step:1507/1770 train_time:147866ms step_avg:98.78ms
step:1508/1770 train_time:147971ms step_avg:98.78ms
step:1509/1770 train_time:148074ms step_avg:98.78ms
step:1510/1770 train_time:148176ms step_avg:98.78ms
step:1511/1770 train_time:148281ms step_avg:98.79ms
step:1512/1770 train_time:148386ms step_avg:98.79ms
step:1513/1770 train_time:148491ms step_avg:98.80ms
step:1514/1770 train_time:148593ms step_avg:98.80ms
step:1515/1770 train_time:148696ms step_avg:98.80ms
step:1516/1770 train_time:148800ms step_avg:98.80ms
step:1517/1770 train_time:148904ms step_avg:98.81ms
step:1518/1770 train_time:149009ms step_avg:98.81ms
step:1519/1770 train_time:149111ms step_avg:98.81ms
step:1520/1770 train_time:149215ms step_avg:98.82ms
step:1521/1770 train_time:149318ms step_avg:98.82ms
step:1522/1770 train_time:149422ms step_avg:98.82ms
step:1523/1770 train_time:149527ms step_avg:98.83ms
step:1524/1770 train_time:149629ms step_avg:98.83ms
step:1525/1770 train_time:149731ms step_avg:98.83ms
step:1526/1770 train_time:149834ms step_avg:98.83ms
step:1527/1770 train_time:149938ms step_avg:98.84ms
step:1528/1770 train_time:150044ms step_avg:98.84ms
step:1529/1770 train_time:150146ms step_avg:98.85ms
step:1530/1770 train_time:150249ms step_avg:98.85ms
step:1531/1770 train_time:150352ms step_avg:98.85ms
step:1532/1770 train_time:150456ms step_avg:98.85ms
step:1533/1770 train_time:150560ms step_avg:98.86ms
step:1534/1770 train_time:150664ms step_avg:98.86ms
step:1535/1770 train_time:150767ms step_avg:98.86ms
step:1536/1770 train_time:150869ms step_avg:98.87ms
step:1537/1770 train_time:150972ms step_avg:98.87ms
step:1538/1770 train_time:151077ms step_avg:98.87ms
step:1539/1770 train_time:151180ms step_avg:98.87ms
step:1540/1770 train_time:151287ms step_avg:98.88ms
step:1541/1770 train_time:151391ms step_avg:98.88ms
step:1542/1770 train_time:151495ms step_avg:98.89ms
step:1543/1770 train_time:151598ms step_avg:98.89ms
step:1544/1770 train_time:151704ms step_avg:98.89ms
step:1545/1770 train_time:151808ms step_avg:98.90ms
step:1546/1770 train_time:151911ms step_avg:98.90ms
step:1547/1770 train_time:152013ms step_avg:98.90ms
step:1548/1770 train_time:152116ms step_avg:98.91ms
step:1549/1770 train_time:152220ms step_avg:98.91ms
step:1550/1770 train_time:152324ms step_avg:98.91ms
step:1551/1770 train_time:152427ms step_avg:98.91ms
step:1552/1770 train_time:152531ms step_avg:98.92ms
step:1553/1770 train_time:152635ms step_avg:98.92ms
step:1554/1770 train_time:152737ms step_avg:98.92ms
step:1555/1770 train_time:152841ms step_avg:98.93ms
step:1556/1770 train_time:152945ms step_avg:98.93ms
step:1557/1770 train_time:153048ms step_avg:98.93ms
step:1558/1770 train_time:153151ms step_avg:98.93ms
step:1559/1770 train_time:153254ms step_avg:98.94ms
step:1560/1770 train_time:153356ms step_avg:98.94ms
step:1561/1770 train_time:153462ms step_avg:98.94ms
step:1562/1770 train_time:153565ms step_avg:98.95ms
step:1563/1770 train_time:153668ms step_avg:98.95ms
step:1564/1770 train_time:153770ms step_avg:98.95ms
step:1565/1770 train_time:153873ms step_avg:98.95ms
step:1566/1770 train_time:153976ms step_avg:98.96ms
step:1567/1770 train_time:154080ms step_avg:98.96ms
step:1568/1770 train_time:154183ms step_avg:98.96ms
step:1569/1770 train_time:154290ms step_avg:98.97ms
step:1570/1770 train_time:154392ms step_avg:98.97ms
step:1571/1770 train_time:154495ms step_avg:98.97ms
step:1572/1770 train_time:154600ms step_avg:98.98ms
step:1573/1770 train_time:154705ms step_avg:98.98ms
step:1574/1770 train_time:154809ms step_avg:98.98ms
step:1575/1770 train_time:154910ms step_avg:98.98ms
step:1576/1770 train_time:155013ms step_avg:98.99ms
step:1577/1770 train_time:155118ms step_avg:98.99ms
step:1578/1770 train_time:155223ms step_avg:98.99ms
step:1579/1770 train_time:155328ms step_avg:99.00ms
step:1580/1770 train_time:155430ms step_avg:99.00ms
step:1581/1770 train_time:155536ms step_avg:99.00ms
step:1582/1770 train_time:155642ms step_avg:99.01ms
step:1583/1770 train_time:155746ms step_avg:99.01ms
step:1584/1770 train_time:155850ms step_avg:99.02ms
step:1585/1770 train_time:155954ms step_avg:99.02ms
step:1586/1770 train_time:156061ms step_avg:99.02ms
step:1587/1770 train_time:156165ms step_avg:99.03ms
step:1588/1770 train_time:156268ms step_avg:99.03ms
step:1589/1770 train_time:156373ms step_avg:99.03ms
step:1590/1770 train_time:156475ms step_avg:99.03ms
step:1591/1770 train_time:156578ms step_avg:99.04ms
step:1592/1770 train_time:156682ms step_avg:99.04ms
step:1593/1770 train_time:156785ms step_avg:99.04ms
step:1594/1770 train_time:156888ms step_avg:99.05ms
step:1595/1770 train_time:156991ms step_avg:99.05ms
step:1596/1770 train_time:157095ms step_avg:99.05ms
step:1597/1770 train_time:157199ms step_avg:99.05ms
step:1598/1770 train_time:157302ms step_avg:99.06ms
step:1599/1770 train_time:157406ms step_avg:99.06ms
step:1600/1770 train_time:157512ms step_avg:99.06ms
step:1601/1770 train_time:157615ms step_avg:99.07ms
step:1602/1770 train_time:157721ms step_avg:99.07ms
step:1603/1770 train_time:157825ms step_avg:99.07ms
step:1604/1770 train_time:157928ms step_avg:99.08ms
step:1605/1770 train_time:158030ms step_avg:99.08ms
step:1606/1770 train_time:158132ms step_avg:99.08ms
step:1607/1770 train_time:158239ms step_avg:99.09ms
step:1608/1770 train_time:158343ms step_avg:99.09ms
step:1609/1770 train_time:158447ms step_avg:99.09ms
step:1610/1770 train_time:158551ms step_avg:99.09ms
step:1611/1770 train_time:158657ms step_avg:99.10ms
step:1612/1770 train_time:158761ms step_avg:99.10ms
step:1613/1770 train_time:158866ms step_avg:99.11ms
step:1614/1770 train_time:158969ms step_avg:99.11ms
step:1615/1770 train_time:159072ms step_avg:99.11ms
step:1616/1770 train_time:159174ms step_avg:99.11ms
step:1617/1770 train_time:159280ms step_avg:99.12ms
step:1618/1770 train_time:159385ms step_avg:99.12ms
step:1619/1770 train_time:159489ms step_avg:99.12ms
step:1620/1770 train_time:159592ms step_avg:99.13ms
step:1621/1770 train_time:159696ms step_avg:99.13ms
step:1622/1770 train_time:159800ms step_avg:99.13ms
step:1623/1770 train_time:159907ms step_avg:99.14ms
step:1624/1770 train_time:160010ms step_avg:99.14ms
step:1625/1770 train_time:160112ms step_avg:99.14ms
step:1625/1770 val_loss:3.3087 train_time:160213ms step_avg:99.20ms
step:1626/1770 train_time:160236ms step_avg:99.16ms
step:1627/1770 train_time:160328ms step_avg:99.15ms
step:1628/1770 train_time:160430ms step_avg:99.15ms
step:1629/1770 train_time:160533ms step_avg:99.16ms
step:1630/1770 train_time:160635ms step_avg:99.16ms
step:1631/1770 train_time:160738ms step_avg:99.16ms
step:1632/1770 train_time:160841ms step_avg:99.16ms
step:1633/1770 train_time:160944ms step_avg:99.16ms
step:1634/1770 train_time:161047ms step_avg:99.17ms
step:1635/1770 train_time:161150ms step_avg:99.17ms
step:1636/1770 train_time:161255ms step_avg:99.17ms
step:1637/1770 train_time:161361ms step_avg:99.18ms
step:1638/1770 train_time:161464ms step_avg:99.18ms
step:1639/1770 train_time:161567ms step_avg:99.18ms
step:1640/1770 train_time:161671ms step_avg:99.18ms
step:1641/1770 train_time:161774ms step_avg:99.19ms
step:1642/1770 train_time:161876ms step_avg:99.19ms
step:1643/1770 train_time:161980ms step_avg:99.19ms
step:1644/1770 train_time:162085ms step_avg:99.20ms
step:1645/1770 train_time:162187ms step_avg:99.20ms
step:1646/1770 train_time:162293ms step_avg:99.20ms
step:1647/1770 train_time:162398ms step_avg:99.20ms
step:1648/1770 train_time:162501ms step_avg:99.21ms
step:1649/1770 train_time:162604ms step_avg:99.21ms
step:1650/1770 train_time:162707ms step_avg:99.21ms
step:1651/1770 train_time:162809ms step_avg:99.21ms
step:1652/1770 train_time:162912ms step_avg:99.22ms
step:1653/1770 train_time:163016ms step_avg:99.22ms
step:1654/1770 train_time:163123ms step_avg:99.22ms
step:1655/1770 train_time:163230ms step_avg:99.23ms
step:1656/1770 train_time:163333ms step_avg:99.23ms
step:1657/1770 train_time:163439ms step_avg:99.23ms
step:1658/1770 train_time:163542ms step_avg:99.24ms
step:1659/1770 train_time:163647ms step_avg:99.24ms
step:1660/1770 train_time:163749ms step_avg:99.24ms
step:1661/1770 train_time:163854ms step_avg:99.25ms
step:1662/1770 train_time:163958ms step_avg:99.25ms
step:1663/1770 train_time:164061ms step_avg:99.25ms
step:1664/1770 train_time:164164ms step_avg:99.25ms
step:1665/1770 train_time:164266ms step_avg:99.25ms
step:1666/1770 train_time:164370ms step_avg:99.26ms
step:1667/1770 train_time:164474ms step_avg:99.26ms
step:1668/1770 train_time:164578ms step_avg:99.26ms
step:1669/1770 train_time:164681ms step_avg:99.27ms
step:1670/1770 train_time:164784ms step_avg:99.27ms
step:1671/1770 train_time:164888ms step_avg:99.27ms
step:1672/1770 train_time:164993ms step_avg:99.27ms
step:1673/1770 train_time:165099ms step_avg:99.28ms
step:1674/1770 train_time:165202ms step_avg:99.28ms
step:1675/1770 train_time:165304ms step_avg:99.28ms
step:1676/1770 train_time:165409ms step_avg:99.28ms
step:1677/1770 train_time:165516ms step_avg:99.29ms
step:1678/1770 train_time:165619ms step_avg:99.29ms
step:1679/1770 train_time:165722ms step_avg:99.29ms
step:1680/1770 train_time:165825ms step_avg:99.30ms
step:1681/1770 train_time:165929ms step_avg:99.30ms
step:1682/1770 train_time:166033ms step_avg:99.30ms
step:1683/1770 train_time:166137ms step_avg:99.30ms
step:1684/1770 train_time:166241ms step_avg:99.31ms
step:1685/1770 train_time:166345ms step_avg:99.31ms
step:1686/1770 train_time:166449ms step_avg:99.31ms
step:1687/1770 train_time:166554ms step_avg:99.32ms
step:1688/1770 train_time:166657ms step_avg:99.32ms
step:1689/1770 train_time:166761ms step_avg:99.32ms
step:1690/1770 train_time:166863ms step_avg:99.32ms
step:1691/1770 train_time:166966ms step_avg:99.33ms
step:1692/1770 train_time:167070ms step_avg:99.33ms
step:1693/1770 train_time:167175ms step_avg:99.33ms
step:1694/1770 train_time:167279ms step_avg:99.33ms
step:1695/1770 train_time:167383ms step_avg:99.34ms
step:1696/1770 train_time:167488ms step_avg:99.34ms
step:1697/1770 train_time:167593ms step_avg:99.34ms
step:1698/1770 train_time:167698ms step_avg:99.35ms
step:1699/1770 train_time:167801ms step_avg:99.35ms
step:1700/1770 train_time:167904ms step_avg:99.35ms
step:1701/1770 train_time:168007ms step_avg:99.35ms
step:1702/1770 train_time:168111ms step_avg:99.36ms
step:1703/1770 train_time:168214ms step_avg:99.36ms
step:1704/1770 train_time:168318ms step_avg:99.36ms
step:1705/1770 train_time:168421ms step_avg:99.36ms
step:1706/1770 train_time:168523ms step_avg:99.37ms
step:1707/1770 train_time:168627ms step_avg:99.37ms
step:1708/1770 train_time:168731ms step_avg:99.37ms
step:1709/1770 train_time:168837ms step_avg:99.37ms
step:1710/1770 train_time:168944ms step_avg:99.38ms
step:1711/1770 train_time:169050ms step_avg:99.38ms
step:1712/1770 train_time:169155ms step_avg:99.39ms
step:1713/1770 train_time:169259ms step_avg:99.39ms
step:1714/1770 train_time:169362ms step_avg:99.39ms
step:1715/1770 train_time:169466ms step_avg:99.39ms
step:1716/1770 train_time:169570ms step_avg:99.40ms
step:1717/1770 train_time:169674ms step_avg:99.40ms
step:1718/1770 train_time:169779ms step_avg:99.40ms
step:1719/1770 train_time:169884ms step_avg:99.41ms
step:1720/1770 train_time:169989ms step_avg:99.41ms
step:1721/1770 train_time:170092ms step_avg:99.41ms
step:1722/1770 train_time:170199ms step_avg:99.42ms
step:1723/1770 train_time:170305ms step_avg:99.42ms
step:1724/1770 train_time:170410ms step_avg:99.42ms
step:1725/1770 train_time:170516ms step_avg:99.43ms
step:1726/1770 train_time:170623ms step_avg:99.43ms
step:1727/1770 train_time:170727ms step_avg:99.43ms
step:1728/1770 train_time:170832ms step_avg:99.44ms
step:1729/1770 train_time:170935ms step_avg:99.44ms
step:1730/1770 train_time:171041ms step_avg:99.44ms
step:1731/1770 train_time:171147ms step_avg:99.45ms
step:1732/1770 train_time:171250ms step_avg:99.45ms
step:1733/1770 train_time:171356ms step_avg:99.45ms
step:1734/1770 train_time:171460ms step_avg:99.45ms
step:1735/1770 train_time:171565ms step_avg:99.46ms
step:1736/1770 train_time:171669ms step_avg:99.46ms
step:1737/1770 train_time:171773ms step_avg:99.46ms
step:1738/1770 train_time:171877ms step_avg:99.47ms
step:1739/1770 train_time:171982ms step_avg:99.47ms
step:1740/1770 train_time:172086ms step_avg:99.47ms
step:1741/1770 train_time:172192ms step_avg:99.48ms
step:1742/1770 train_time:172300ms step_avg:99.48ms
step:1743/1770 train_time:172405ms step_avg:99.48ms
step:1744/1770 train_time:172508ms step_avg:99.49ms
step:1745/1770 train_time:172612ms step_avg:99.49ms
step:1746/1770 train_time:172719ms step_avg:99.49ms
step:1747/1770 train_time:172823ms step_avg:99.49ms
step:1748/1770 train_time:172929ms step_avg:99.50ms
step:1749/1770 train_time:173034ms step_avg:99.50ms
step:1750/1770 train_time:173138ms step_avg:99.50ms
step:1750/1770 val_loss:3.2817 train_time:173241ms step_avg:99.56ms
step:1751/1770 train_time:173265ms step_avg:99.52ms
step:1752/1770 train_time:173353ms step_avg:99.51ms
step:1753/1770 train_time:173457ms step_avg:99.52ms
step:1754/1770 train_time:173562ms step_avg:99.52ms
step:1755/1770 train_time:173666ms step_avg:99.52ms
step:1756/1770 train_time:173771ms step_avg:99.53ms
step:1757/1770 train_time:173876ms step_avg:99.53ms
step:1758/1770 train_time:173979ms step_avg:99.53ms
step:1759/1770 train_time:174084ms step_avg:99.53ms
step:1760/1770 train_time:174188ms step_avg:99.54ms
step:1761/1770 train_time:174295ms step_avg:99.54ms
step:1762/1770 train_time:174403ms step_avg:99.55ms
step:1763/1770 train_time:174507ms step_avg:99.55ms
step:1764/1770 train_time:174611ms step_avg:99.55ms
step:1765/1770 train_time:174715ms step_avg:99.55ms
step:1766/1770 train_time:174824ms step_avg:99.56ms
step:1767/1770 train_time:174927ms step_avg:99.56ms
step:1768/1770 train_time:175030ms step_avg:99.56ms
step:1769/1770 train_time:175134ms step_avg:99.56ms
step:1770/1770 train_time:175237ms step_avg:99.57ms
step:1770/1770 val_loss:3.2789 train_time:175342ms step_avg:99.63ms
peak memory allocated: 28840 MiB reserved: 32192 MiB
