import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 16:11:46 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23724ms step_avg:nanms
step:2/1770 train_time:24195ms step_avg:nanms
step:3/1770 train_time:24291ms step_avg:nanms
step:4/1770 train_time:24384ms step_avg:nanms
step:5/1770 train_time:24477ms step_avg:nanms
step:6/1770 train_time:24571ms step_avg:nanms
step:7/1770 train_time:24665ms step_avg:nanms
step:8/1770 train_time:24759ms step_avg:nanms
step:9/1770 train_time:24853ms step_avg:nanms
step:10/1770 train_time:24947ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.30ms
step:14/1770 train_time:378ms step_avg:94.43ms
step:15/1770 train_time:472ms step_avg:94.35ms
step:16/1770 train_time:566ms step_avg:94.32ms
step:17/1770 train_time:660ms step_avg:94.22ms
step:18/1770 train_time:753ms step_avg:94.16ms
step:19/1770 train_time:848ms step_avg:94.17ms
step:20/1770 train_time:942ms step_avg:94.18ms
step:21/1770 train_time:1036ms step_avg:94.15ms
step:22/1770 train_time:1130ms step_avg:94.17ms
step:23/1770 train_time:1224ms step_avg:94.15ms
step:24/1770 train_time:1318ms step_avg:94.13ms
step:25/1770 train_time:1412ms step_avg:94.13ms
step:26/1770 train_time:1506ms step_avg:94.16ms
step:27/1770 train_time:1600ms step_avg:94.13ms
step:28/1770 train_time:1695ms step_avg:94.15ms
step:29/1770 train_time:1789ms step_avg:94.17ms
step:30/1770 train_time:1884ms step_avg:94.18ms
step:31/1770 train_time:1978ms step_avg:94.18ms
step:32/1770 train_time:2073ms step_avg:94.21ms
step:33/1770 train_time:2166ms step_avg:94.19ms
step:34/1770 train_time:2261ms step_avg:94.19ms
step:35/1770 train_time:2355ms step_avg:94.20ms
step:36/1770 train_time:2449ms step_avg:94.19ms
step:37/1770 train_time:2543ms step_avg:94.18ms
step:38/1770 train_time:2638ms step_avg:94.20ms
step:39/1770 train_time:2731ms step_avg:94.18ms
step:40/1770 train_time:2825ms step_avg:94.18ms
step:41/1770 train_time:2919ms step_avg:94.17ms
step:42/1770 train_time:3013ms step_avg:94.17ms
step:43/1770 train_time:3107ms step_avg:94.16ms
step:44/1770 train_time:3202ms step_avg:94.16ms
step:45/1770 train_time:3295ms step_avg:94.15ms
step:46/1770 train_time:3389ms step_avg:94.14ms
step:47/1770 train_time:3483ms step_avg:94.14ms
step:48/1770 train_time:3577ms step_avg:94.13ms
step:49/1770 train_time:3671ms step_avg:94.13ms
step:50/1770 train_time:3765ms step_avg:94.12ms
step:51/1770 train_time:3859ms step_avg:94.13ms
step:52/1770 train_time:3954ms step_avg:94.14ms
step:53/1770 train_time:4048ms step_avg:94.14ms
step:54/1770 train_time:4142ms step_avg:94.14ms
step:55/1770 train_time:4236ms step_avg:94.13ms
step:56/1770 train_time:4330ms step_avg:94.13ms
step:57/1770 train_time:4424ms step_avg:94.13ms
step:58/1770 train_time:4518ms step_avg:94.13ms
step:59/1770 train_time:4612ms step_avg:94.13ms
step:60/1770 train_time:4706ms step_avg:94.12ms
step:61/1770 train_time:4800ms step_avg:94.12ms
step:62/1770 train_time:4894ms step_avg:94.12ms
step:63/1770 train_time:4988ms step_avg:94.12ms
step:64/1770 train_time:5082ms step_avg:94.11ms
step:65/1770 train_time:5176ms step_avg:94.10ms
step:66/1770 train_time:5270ms step_avg:94.11ms
step:67/1770 train_time:5364ms step_avg:94.10ms
step:68/1770 train_time:5458ms step_avg:94.10ms
step:69/1770 train_time:5552ms step_avg:94.09ms
step:70/1770 train_time:5646ms step_avg:94.10ms
step:71/1770 train_time:5740ms step_avg:94.09ms
step:72/1770 train_time:5833ms step_avg:94.09ms
step:73/1770 train_time:5928ms step_avg:94.09ms
step:74/1770 train_time:6022ms step_avg:94.09ms
step:75/1770 train_time:6116ms step_avg:94.09ms
step:76/1770 train_time:6210ms step_avg:94.09ms
step:77/1770 train_time:6304ms step_avg:94.09ms
step:78/1770 train_time:6398ms step_avg:94.09ms
step:79/1770 train_time:6492ms step_avg:94.08ms
step:80/1770 train_time:6585ms step_avg:94.07ms
step:81/1770 train_time:6679ms step_avg:94.07ms
step:82/1770 train_time:6773ms step_avg:94.07ms
step:83/1770 train_time:6867ms step_avg:94.07ms
step:84/1770 train_time:6961ms step_avg:94.07ms
step:85/1770 train_time:7055ms step_avg:94.07ms
step:86/1770 train_time:7149ms step_avg:94.07ms
step:87/1770 train_time:7244ms step_avg:94.07ms
step:88/1770 train_time:7338ms step_avg:94.07ms
step:89/1770 train_time:7431ms step_avg:94.06ms
step:90/1770 train_time:7525ms step_avg:94.06ms
step:91/1770 train_time:7618ms step_avg:94.05ms
step:92/1770 train_time:7712ms step_avg:94.05ms
step:93/1770 train_time:7806ms step_avg:94.05ms
step:94/1770 train_time:7900ms step_avg:94.05ms
step:95/1770 train_time:7994ms step_avg:94.04ms
step:96/1770 train_time:8088ms step_avg:94.04ms
step:97/1770 train_time:8182ms step_avg:94.04ms
step:98/1770 train_time:8276ms step_avg:94.05ms
step:99/1770 train_time:8370ms step_avg:94.05ms
step:100/1770 train_time:8464ms step_avg:94.05ms
step:101/1770 train_time:8559ms step_avg:94.05ms
step:102/1770 train_time:8653ms step_avg:94.05ms
step:103/1770 train_time:8747ms step_avg:94.05ms
step:104/1770 train_time:8841ms step_avg:94.05ms
step:105/1770 train_time:8935ms step_avg:94.05ms
step:106/1770 train_time:9028ms step_avg:94.05ms
step:107/1770 train_time:9123ms step_avg:94.05ms
step:108/1770 train_time:9217ms step_avg:94.05ms
step:109/1770 train_time:9311ms step_avg:94.05ms
step:110/1770 train_time:9406ms step_avg:94.06ms
step:111/1770 train_time:9500ms step_avg:94.06ms
step:112/1770 train_time:9594ms step_avg:94.06ms
step:113/1770 train_time:9688ms step_avg:94.05ms
step:114/1770 train_time:9781ms step_avg:94.05ms
step:115/1770 train_time:9875ms step_avg:94.05ms
step:116/1770 train_time:9970ms step_avg:94.05ms
step:117/1770 train_time:10065ms step_avg:94.06ms
step:118/1770 train_time:10159ms step_avg:94.06ms
step:119/1770 train_time:10252ms step_avg:94.06ms
step:120/1770 train_time:10347ms step_avg:94.06ms
step:121/1770 train_time:10441ms step_avg:94.06ms
step:122/1770 train_time:10535ms step_avg:94.06ms
step:123/1770 train_time:10629ms step_avg:94.06ms
step:124/1770 train_time:10724ms step_avg:94.07ms
step:125/1770 train_time:10817ms step_avg:94.06ms
step:125/1770 val_loss:4.6522 train_time:10910ms step_avg:94.87ms
step:126/1770 train_time:10932ms step_avg:94.24ms
step:127/1770 train_time:11017ms step_avg:94.17ms
step:128/1770 train_time:11115ms step_avg:94.20ms
step:129/1770 train_time:11210ms step_avg:94.20ms
step:130/1770 train_time:11304ms step_avg:94.20ms
step:131/1770 train_time:11398ms step_avg:94.20ms
step:132/1770 train_time:11491ms step_avg:94.19ms
step:133/1770 train_time:11585ms step_avg:94.19ms
step:134/1770 train_time:11679ms step_avg:94.19ms
step:135/1770 train_time:11773ms step_avg:94.19ms
step:136/1770 train_time:11867ms step_avg:94.19ms
step:137/1770 train_time:11963ms step_avg:94.20ms
step:138/1770 train_time:12059ms step_avg:94.21ms
step:139/1770 train_time:12155ms step_avg:94.22ms
step:140/1770 train_time:12249ms step_avg:94.22ms
step:141/1770 train_time:12344ms step_avg:94.23ms
step:142/1770 train_time:12439ms step_avg:94.23ms
step:143/1770 train_time:12533ms step_avg:94.23ms
step:144/1770 train_time:12628ms step_avg:94.24ms
step:145/1770 train_time:12722ms step_avg:94.24ms
step:146/1770 train_time:12816ms step_avg:94.24ms
step:147/1770 train_time:12910ms step_avg:94.23ms
step:148/1770 train_time:13005ms step_avg:94.24ms
step:149/1770 train_time:13100ms step_avg:94.25ms
step:150/1770 train_time:13195ms step_avg:94.25ms
step:151/1770 train_time:13290ms step_avg:94.26ms
step:152/1770 train_time:13385ms step_avg:94.26ms
step:153/1770 train_time:13479ms step_avg:94.26ms
step:154/1770 train_time:13574ms step_avg:94.26ms
step:155/1770 train_time:13668ms step_avg:94.26ms
step:156/1770 train_time:13763ms step_avg:94.27ms
step:157/1770 train_time:13857ms step_avg:94.27ms
step:158/1770 train_time:13952ms step_avg:94.27ms
step:159/1770 train_time:14047ms step_avg:94.27ms
step:160/1770 train_time:14142ms step_avg:94.28ms
step:161/1770 train_time:14236ms step_avg:94.28ms
step:162/1770 train_time:14331ms step_avg:94.28ms
step:163/1770 train_time:14426ms step_avg:94.29ms
step:164/1770 train_time:14521ms step_avg:94.29ms
step:165/1770 train_time:14616ms step_avg:94.30ms
step:166/1770 train_time:14711ms step_avg:94.30ms
step:167/1770 train_time:14805ms step_avg:94.30ms
step:168/1770 train_time:14900ms step_avg:94.31ms
step:169/1770 train_time:14995ms step_avg:94.31ms
step:170/1770 train_time:15089ms step_avg:94.31ms
step:171/1770 train_time:15184ms step_avg:94.31ms
step:172/1770 train_time:15280ms step_avg:94.32ms
step:173/1770 train_time:15375ms step_avg:94.32ms
step:174/1770 train_time:15469ms step_avg:94.33ms
step:175/1770 train_time:15565ms step_avg:94.33ms
step:176/1770 train_time:15660ms step_avg:94.34ms
step:177/1770 train_time:15754ms step_avg:94.34ms
step:178/1770 train_time:15849ms step_avg:94.34ms
step:179/1770 train_time:15945ms step_avg:94.35ms
step:180/1770 train_time:16040ms step_avg:94.35ms
step:181/1770 train_time:16134ms step_avg:94.35ms
step:182/1770 train_time:16229ms step_avg:94.35ms
step:183/1770 train_time:16324ms step_avg:94.36ms
step:184/1770 train_time:16419ms step_avg:94.36ms
step:185/1770 train_time:16513ms step_avg:94.36ms
step:186/1770 train_time:16608ms step_avg:94.36ms
step:187/1770 train_time:16703ms step_avg:94.37ms
step:188/1770 train_time:16797ms step_avg:94.37ms
step:189/1770 train_time:16892ms step_avg:94.37ms
step:190/1770 train_time:16987ms step_avg:94.37ms
step:191/1770 train_time:17081ms step_avg:94.37ms
step:192/1770 train_time:17175ms step_avg:94.37ms
step:193/1770 train_time:17269ms step_avg:94.37ms
step:194/1770 train_time:17364ms step_avg:94.37ms
step:195/1770 train_time:17459ms step_avg:94.37ms
step:196/1770 train_time:17553ms step_avg:94.37ms
step:197/1770 train_time:17648ms step_avg:94.37ms
step:198/1770 train_time:17742ms step_avg:94.37ms
step:199/1770 train_time:17837ms step_avg:94.37ms
step:200/1770 train_time:17931ms step_avg:94.37ms
step:201/1770 train_time:18026ms step_avg:94.38ms
step:202/1770 train_time:18121ms step_avg:94.38ms
step:203/1770 train_time:18215ms step_avg:94.38ms
step:204/1770 train_time:18309ms step_avg:94.38ms
step:205/1770 train_time:18403ms step_avg:94.38ms
step:206/1770 train_time:18498ms step_avg:94.38ms
step:207/1770 train_time:18592ms step_avg:94.38ms
step:208/1770 train_time:18687ms step_avg:94.38ms
step:209/1770 train_time:18781ms step_avg:94.38ms
step:210/1770 train_time:18876ms step_avg:94.38ms
step:211/1770 train_time:18970ms step_avg:94.38ms
step:212/1770 train_time:19065ms step_avg:94.38ms
step:213/1770 train_time:19160ms step_avg:94.38ms
step:214/1770 train_time:19254ms step_avg:94.38ms
step:215/1770 train_time:19348ms step_avg:94.38ms
step:216/1770 train_time:19443ms step_avg:94.38ms
step:217/1770 train_time:19538ms step_avg:94.39ms
step:218/1770 train_time:19633ms step_avg:94.39ms
step:219/1770 train_time:19727ms step_avg:94.39ms
step:220/1770 train_time:19822ms step_avg:94.39ms
step:221/1770 train_time:19917ms step_avg:94.39ms
step:222/1770 train_time:20011ms step_avg:94.39ms
step:223/1770 train_time:20106ms step_avg:94.39ms
step:224/1770 train_time:20201ms step_avg:94.40ms
step:225/1770 train_time:20296ms step_avg:94.40ms
step:226/1770 train_time:20391ms step_avg:94.40ms
step:227/1770 train_time:20485ms step_avg:94.40ms
step:228/1770 train_time:20580ms step_avg:94.40ms
step:229/1770 train_time:20673ms step_avg:94.40ms
step:230/1770 train_time:20768ms step_avg:94.40ms
step:231/1770 train_time:20863ms step_avg:94.40ms
step:232/1770 train_time:20957ms step_avg:94.40ms
step:233/1770 train_time:21051ms step_avg:94.40ms
step:234/1770 train_time:21146ms step_avg:94.40ms
step:235/1770 train_time:21241ms step_avg:94.41ms
step:236/1770 train_time:21336ms step_avg:94.41ms
step:237/1770 train_time:21431ms step_avg:94.41ms
step:238/1770 train_time:21526ms step_avg:94.41ms
step:239/1770 train_time:21620ms step_avg:94.41ms
step:240/1770 train_time:21714ms step_avg:94.41ms
step:241/1770 train_time:21809ms step_avg:94.41ms
step:242/1770 train_time:21903ms step_avg:94.41ms
step:243/1770 train_time:21997ms step_avg:94.41ms
step:244/1770 train_time:22093ms step_avg:94.41ms
step:245/1770 train_time:22189ms step_avg:94.42ms
step:246/1770 train_time:22284ms step_avg:94.42ms
step:247/1770 train_time:22379ms step_avg:94.42ms
step:248/1770 train_time:22473ms step_avg:94.42ms
step:249/1770 train_time:22567ms step_avg:94.42ms
step:250/1770 train_time:22662ms step_avg:94.43ms
step:250/1770 val_loss:4.1048 train_time:22755ms step_avg:94.81ms
step:251/1770 train_time:22776ms step_avg:94.51ms
step:252/1770 train_time:22864ms step_avg:94.48ms
step:253/1770 train_time:22961ms step_avg:94.49ms
step:254/1770 train_time:23056ms step_avg:94.49ms
step:255/1770 train_time:23150ms step_avg:94.49ms
step:256/1770 train_time:23245ms step_avg:94.49ms
step:257/1770 train_time:23339ms step_avg:94.49ms
step:258/1770 train_time:23433ms step_avg:94.49ms
step:259/1770 train_time:23527ms step_avg:94.49ms
step:260/1770 train_time:23622ms step_avg:94.49ms
step:261/1770 train_time:23716ms step_avg:94.49ms
step:262/1770 train_time:23811ms step_avg:94.49ms
step:263/1770 train_time:23907ms step_avg:94.49ms
step:264/1770 train_time:24002ms step_avg:94.50ms
step:265/1770 train_time:24098ms step_avg:94.50ms
step:266/1770 train_time:24193ms step_avg:94.50ms
step:267/1770 train_time:24288ms step_avg:94.50ms
step:268/1770 train_time:24383ms step_avg:94.51ms
step:269/1770 train_time:24478ms step_avg:94.51ms
step:270/1770 train_time:24573ms step_avg:94.51ms
step:271/1770 train_time:24668ms step_avg:94.51ms
step:272/1770 train_time:24763ms step_avg:94.51ms
step:273/1770 train_time:24859ms step_avg:94.52ms
step:274/1770 train_time:24954ms step_avg:94.52ms
step:275/1770 train_time:25049ms step_avg:94.53ms
step:276/1770 train_time:25145ms step_avg:94.53ms
step:277/1770 train_time:25240ms step_avg:94.53ms
step:278/1770 train_time:25335ms step_avg:94.53ms
step:279/1770 train_time:25430ms step_avg:94.53ms
step:280/1770 train_time:25524ms step_avg:94.53ms
step:281/1770 train_time:25620ms step_avg:94.54ms
step:282/1770 train_time:25714ms step_avg:94.54ms
step:283/1770 train_time:25809ms step_avg:94.54ms
step:284/1770 train_time:25904ms step_avg:94.54ms
step:285/1770 train_time:26000ms step_avg:94.54ms
step:286/1770 train_time:26095ms step_avg:94.55ms
step:287/1770 train_time:26190ms step_avg:94.55ms
step:288/1770 train_time:26286ms step_avg:94.56ms
step:289/1770 train_time:26382ms step_avg:94.56ms
step:290/1770 train_time:26477ms step_avg:94.56ms
step:291/1770 train_time:26572ms step_avg:94.56ms
step:292/1770 train_time:26668ms step_avg:94.57ms
step:293/1770 train_time:26763ms step_avg:94.57ms
step:294/1770 train_time:26858ms step_avg:94.57ms
step:295/1770 train_time:26953ms step_avg:94.57ms
step:296/1770 train_time:27048ms step_avg:94.57ms
step:297/1770 train_time:27143ms step_avg:94.58ms
step:298/1770 train_time:27239ms step_avg:94.58ms
step:299/1770 train_time:27334ms step_avg:94.58ms
step:300/1770 train_time:27429ms step_avg:94.58ms
step:301/1770 train_time:27524ms step_avg:94.58ms
step:302/1770 train_time:27619ms step_avg:94.59ms
step:303/1770 train_time:27714ms step_avg:94.59ms
step:304/1770 train_time:27809ms step_avg:94.59ms
step:305/1770 train_time:27904ms step_avg:94.59ms
step:306/1770 train_time:27999ms step_avg:94.59ms
step:307/1770 train_time:28094ms step_avg:94.59ms
step:308/1770 train_time:28188ms step_avg:94.59ms
step:309/1770 train_time:28283ms step_avg:94.59ms
step:310/1770 train_time:28379ms step_avg:94.60ms
step:311/1770 train_time:28474ms step_avg:94.60ms
step:312/1770 train_time:28569ms step_avg:94.60ms
step:313/1770 train_time:28665ms step_avg:94.60ms
step:314/1770 train_time:28761ms step_avg:94.61ms
step:315/1770 train_time:28856ms step_avg:94.61ms
step:316/1770 train_time:28951ms step_avg:94.61ms
step:317/1770 train_time:29046ms step_avg:94.61ms
step:318/1770 train_time:29141ms step_avg:94.61ms
step:319/1770 train_time:29236ms step_avg:94.62ms
step:320/1770 train_time:29331ms step_avg:94.62ms
step:321/1770 train_time:29427ms step_avg:94.62ms
step:322/1770 train_time:29522ms step_avg:94.62ms
step:323/1770 train_time:29618ms step_avg:94.63ms
step:324/1770 train_time:29713ms step_avg:94.63ms
step:325/1770 train_time:29808ms step_avg:94.63ms
step:326/1770 train_time:29903ms step_avg:94.63ms
step:327/1770 train_time:29998ms step_avg:94.63ms
step:328/1770 train_time:30094ms step_avg:94.63ms
step:329/1770 train_time:30189ms step_avg:94.64ms
step:330/1770 train_time:30284ms step_avg:94.64ms
step:331/1770 train_time:30379ms step_avg:94.64ms
step:332/1770 train_time:30475ms step_avg:94.64ms
step:333/1770 train_time:30569ms step_avg:94.64ms
step:334/1770 train_time:30664ms step_avg:94.64ms
step:335/1770 train_time:30760ms step_avg:94.65ms
step:336/1770 train_time:30856ms step_avg:94.65ms
step:337/1770 train_time:30950ms step_avg:94.65ms
step:338/1770 train_time:31045ms step_avg:94.65ms
step:339/1770 train_time:31140ms step_avg:94.65ms
step:340/1770 train_time:31235ms step_avg:94.65ms
step:341/1770 train_time:31330ms step_avg:94.65ms
step:342/1770 train_time:31425ms step_avg:94.65ms
step:343/1770 train_time:31520ms step_avg:94.66ms
step:344/1770 train_time:31616ms step_avg:94.66ms
step:345/1770 train_time:31711ms step_avg:94.66ms
step:346/1770 train_time:31807ms step_avg:94.66ms
step:347/1770 train_time:31902ms step_avg:94.66ms
step:348/1770 train_time:31997ms step_avg:94.67ms
step:349/1770 train_time:32092ms step_avg:94.67ms
step:350/1770 train_time:32187ms step_avg:94.67ms
step:351/1770 train_time:32282ms step_avg:94.67ms
step:352/1770 train_time:32377ms step_avg:94.67ms
step:353/1770 train_time:32472ms step_avg:94.67ms
step:354/1770 train_time:32567ms step_avg:94.67ms
step:355/1770 train_time:32663ms step_avg:94.67ms
step:356/1770 train_time:32758ms step_avg:94.68ms
step:357/1770 train_time:32853ms step_avg:94.68ms
step:358/1770 train_time:32948ms step_avg:94.68ms
step:359/1770 train_time:33043ms step_avg:94.68ms
step:360/1770 train_time:33139ms step_avg:94.68ms
step:361/1770 train_time:33234ms step_avg:94.68ms
step:362/1770 train_time:33329ms step_avg:94.69ms
step:363/1770 train_time:33424ms step_avg:94.69ms
step:364/1770 train_time:33519ms step_avg:94.69ms
step:365/1770 train_time:33614ms step_avg:94.69ms
step:366/1770 train_time:33709ms step_avg:94.69ms
step:367/1770 train_time:33804ms step_avg:94.69ms
step:368/1770 train_time:33899ms step_avg:94.69ms
step:369/1770 train_time:33994ms step_avg:94.69ms
step:370/1770 train_time:34089ms step_avg:94.69ms
step:371/1770 train_time:34184ms step_avg:94.69ms
step:372/1770 train_time:34280ms step_avg:94.70ms
step:373/1770 train_time:34376ms step_avg:94.70ms
step:374/1770 train_time:34471ms step_avg:94.70ms
step:375/1770 train_time:34566ms step_avg:94.70ms
step:375/1770 val_loss:3.8988 train_time:34659ms step_avg:94.96ms
step:376/1770 train_time:34687ms step_avg:94.77ms
step:377/1770 train_time:34767ms step_avg:94.73ms
step:378/1770 train_time:34865ms step_avg:94.74ms
step:379/1770 train_time:34962ms step_avg:94.75ms
step:380/1770 train_time:35057ms step_avg:94.75ms
step:381/1770 train_time:35151ms step_avg:94.75ms
step:382/1770 train_time:35247ms step_avg:94.75ms
step:383/1770 train_time:35341ms step_avg:94.75ms
step:384/1770 train_time:35436ms step_avg:94.75ms
step:385/1770 train_time:35530ms step_avg:94.75ms
step:386/1770 train_time:35625ms step_avg:94.75ms
step:387/1770 train_time:35720ms step_avg:94.75ms
step:388/1770 train_time:35816ms step_avg:94.75ms
step:389/1770 train_time:35912ms step_avg:94.75ms
step:390/1770 train_time:36009ms step_avg:94.76ms
step:391/1770 train_time:36104ms step_avg:94.76ms
step:392/1770 train_time:36199ms step_avg:94.76ms
step:393/1770 train_time:36294ms step_avg:94.76ms
step:394/1770 train_time:36388ms step_avg:94.76ms
step:395/1770 train_time:36483ms step_avg:94.76ms
step:396/1770 train_time:36580ms step_avg:94.77ms
step:397/1770 train_time:36676ms step_avg:94.77ms
step:398/1770 train_time:36773ms step_avg:94.78ms
step:399/1770 train_time:36870ms step_avg:94.78ms
step:400/1770 train_time:36968ms step_avg:94.79ms
step:401/1770 train_time:37066ms step_avg:94.80ms
step:402/1770 train_time:37163ms step_avg:94.80ms
step:403/1770 train_time:37260ms step_avg:94.81ms
step:404/1770 train_time:37357ms step_avg:94.82ms
step:405/1770 train_time:37455ms step_avg:94.82ms
step:406/1770 train_time:37552ms step_avg:94.83ms
step:407/1770 train_time:37650ms step_avg:94.84ms
step:408/1770 train_time:37747ms step_avg:94.84ms
step:409/1770 train_time:37845ms step_avg:94.85ms
step:410/1770 train_time:37941ms step_avg:94.85ms
step:411/1770 train_time:38038ms step_avg:94.86ms
step:412/1770 train_time:38136ms step_avg:94.86ms
step:413/1770 train_time:38232ms step_avg:94.87ms
step:414/1770 train_time:38330ms step_avg:94.88ms
step:415/1770 train_time:38428ms step_avg:94.88ms
step:416/1770 train_time:38524ms step_avg:94.89ms
step:417/1770 train_time:38621ms step_avg:94.89ms
step:418/1770 train_time:38718ms step_avg:94.90ms
step:419/1770 train_time:38815ms step_avg:94.90ms
step:420/1770 train_time:38912ms step_avg:94.91ms
step:421/1770 train_time:39010ms step_avg:94.91ms
step:422/1770 train_time:39107ms step_avg:94.92ms
step:423/1770 train_time:39205ms step_avg:94.93ms
step:424/1770 train_time:39301ms step_avg:94.93ms
step:425/1770 train_time:39398ms step_avg:94.94ms
step:426/1770 train_time:39495ms step_avg:94.94ms
step:427/1770 train_time:39591ms step_avg:94.94ms
step:428/1770 train_time:39689ms step_avg:94.95ms
step:429/1770 train_time:39786ms step_avg:94.95ms
step:430/1770 train_time:39883ms step_avg:94.96ms
step:431/1770 train_time:39980ms step_avg:94.96ms
step:432/1770 train_time:40078ms step_avg:94.97ms
step:433/1770 train_time:40175ms step_avg:94.98ms
step:434/1770 train_time:40272ms step_avg:94.98ms
step:435/1770 train_time:40369ms step_avg:94.99ms
step:436/1770 train_time:40466ms step_avg:94.99ms
step:437/1770 train_time:40563ms step_avg:94.99ms
step:438/1770 train_time:40660ms step_avg:95.00ms
step:439/1770 train_time:40758ms step_avg:95.01ms
step:440/1770 train_time:40854ms step_avg:95.01ms
step:441/1770 train_time:40952ms step_avg:95.02ms
step:442/1770 train_time:41049ms step_avg:95.02ms
step:443/1770 train_time:41147ms step_avg:95.03ms
step:444/1770 train_time:41244ms step_avg:95.03ms
step:445/1770 train_time:41340ms step_avg:95.04ms
step:446/1770 train_time:41437ms step_avg:95.04ms
step:447/1770 train_time:41534ms step_avg:95.04ms
step:448/1770 train_time:41632ms step_avg:95.05ms
step:449/1770 train_time:41730ms step_avg:95.06ms
step:450/1770 train_time:41827ms step_avg:95.06ms
step:451/1770 train_time:41924ms step_avg:95.07ms
step:452/1770 train_time:42021ms step_avg:95.07ms
step:453/1770 train_time:42119ms step_avg:95.08ms
step:454/1770 train_time:42217ms step_avg:95.08ms
step:455/1770 train_time:42313ms step_avg:95.09ms
step:456/1770 train_time:42411ms step_avg:95.09ms
step:457/1770 train_time:42508ms step_avg:95.10ms
step:458/1770 train_time:42606ms step_avg:95.10ms
step:459/1770 train_time:42702ms step_avg:95.11ms
step:460/1770 train_time:42799ms step_avg:95.11ms
step:461/1770 train_time:42897ms step_avg:95.11ms
step:462/1770 train_time:42994ms step_avg:95.12ms
step:463/1770 train_time:43091ms step_avg:95.12ms
step:464/1770 train_time:43189ms step_avg:95.13ms
step:465/1770 train_time:43286ms step_avg:95.13ms
step:466/1770 train_time:43383ms step_avg:95.14ms
step:467/1770 train_time:43480ms step_avg:95.14ms
step:468/1770 train_time:43577ms step_avg:95.15ms
step:469/1770 train_time:43675ms step_avg:95.15ms
step:470/1770 train_time:43772ms step_avg:95.16ms
step:471/1770 train_time:43869ms step_avg:95.16ms
step:472/1770 train_time:43966ms step_avg:95.16ms
step:473/1770 train_time:44063ms step_avg:95.17ms
step:474/1770 train_time:44160ms step_avg:95.17ms
step:475/1770 train_time:44258ms step_avg:95.18ms
step:476/1770 train_time:44355ms step_avg:95.18ms
step:477/1770 train_time:44452ms step_avg:95.19ms
step:478/1770 train_time:44550ms step_avg:95.19ms
step:479/1770 train_time:44648ms step_avg:95.20ms
step:480/1770 train_time:44745ms step_avg:95.20ms
step:481/1770 train_time:44842ms step_avg:95.21ms
step:482/1770 train_time:44939ms step_avg:95.21ms
step:483/1770 train_time:45035ms step_avg:95.21ms
step:484/1770 train_time:45132ms step_avg:95.22ms
step:485/1770 train_time:45229ms step_avg:95.22ms
step:486/1770 train_time:45327ms step_avg:95.22ms
step:487/1770 train_time:45424ms step_avg:95.23ms
step:488/1770 train_time:45521ms step_avg:95.23ms
step:489/1770 train_time:45619ms step_avg:95.24ms
step:490/1770 train_time:45716ms step_avg:95.24ms
step:491/1770 train_time:45812ms step_avg:95.24ms
step:492/1770 train_time:45909ms step_avg:95.25ms
step:493/1770 train_time:46006ms step_avg:95.25ms
step:494/1770 train_time:46103ms step_avg:95.25ms
step:495/1770 train_time:46200ms step_avg:95.26ms
step:496/1770 train_time:46297ms step_avg:95.26ms
step:497/1770 train_time:46394ms step_avg:95.26ms
step:498/1770 train_time:46491ms step_avg:95.27ms
step:499/1770 train_time:46589ms step_avg:95.27ms
step:500/1770 train_time:46686ms step_avg:95.28ms
step:500/1770 val_loss:3.7514 train_time:46781ms step_avg:95.47ms
step:501/1770 train_time:46803ms step_avg:95.32ms
step:502/1770 train_time:46893ms step_avg:95.31ms
step:503/1770 train_time:46993ms step_avg:95.32ms
step:504/1770 train_time:47090ms step_avg:95.32ms
step:505/1770 train_time:47187ms step_avg:95.33ms
step:506/1770 train_time:47283ms step_avg:95.33ms
step:507/1770 train_time:47381ms step_avg:95.33ms
step:508/1770 train_time:47477ms step_avg:95.34ms
step:509/1770 train_time:47574ms step_avg:95.34ms
step:510/1770 train_time:47670ms step_avg:95.34ms
step:511/1770 train_time:47767ms step_avg:95.34ms
step:512/1770 train_time:47865ms step_avg:95.35ms
step:513/1770 train_time:47963ms step_avg:95.35ms
step:514/1770 train_time:48060ms step_avg:95.36ms
step:515/1770 train_time:48158ms step_avg:95.36ms
step:516/1770 train_time:48254ms step_avg:95.36ms
step:517/1770 train_time:48351ms step_avg:95.37ms
step:518/1770 train_time:48448ms step_avg:95.37ms
step:519/1770 train_time:48545ms step_avg:95.37ms
step:520/1770 train_time:48642ms step_avg:95.38ms
step:521/1770 train_time:48739ms step_avg:95.38ms
step:522/1770 train_time:48836ms step_avg:95.38ms
step:523/1770 train_time:48932ms step_avg:95.38ms
step:524/1770 train_time:49030ms step_avg:95.39ms
step:525/1770 train_time:49127ms step_avg:95.39ms
step:526/1770 train_time:49225ms step_avg:95.40ms
step:527/1770 train_time:49324ms step_avg:95.40ms
step:528/1770 train_time:49422ms step_avg:95.41ms
step:529/1770 train_time:49519ms step_avg:95.41ms
step:530/1770 train_time:49616ms step_avg:95.42ms
step:531/1770 train_time:49713ms step_avg:95.42ms
step:532/1770 train_time:49810ms step_avg:95.42ms
step:533/1770 train_time:49907ms step_avg:95.42ms
step:534/1770 train_time:50005ms step_avg:95.43ms
step:535/1770 train_time:50103ms step_avg:95.43ms
step:536/1770 train_time:50201ms step_avg:95.44ms
step:537/1770 train_time:50298ms step_avg:95.44ms
step:538/1770 train_time:50396ms step_avg:95.45ms
step:539/1770 train_time:50493ms step_avg:95.45ms
step:540/1770 train_time:50591ms step_avg:95.45ms
step:541/1770 train_time:50688ms step_avg:95.46ms
step:542/1770 train_time:50785ms step_avg:95.46ms
step:543/1770 train_time:50883ms step_avg:95.47ms
step:544/1770 train_time:50980ms step_avg:95.47ms
step:545/1770 train_time:51078ms step_avg:95.47ms
step:546/1770 train_time:51175ms step_avg:95.48ms
step:547/1770 train_time:51272ms step_avg:95.48ms
step:548/1770 train_time:51370ms step_avg:95.48ms
step:549/1770 train_time:51467ms step_avg:95.49ms
step:550/1770 train_time:51565ms step_avg:95.49ms
step:551/1770 train_time:51663ms step_avg:95.50ms
step:552/1770 train_time:51761ms step_avg:95.50ms
step:553/1770 train_time:51858ms step_avg:95.50ms
step:554/1770 train_time:51956ms step_avg:95.51ms
step:555/1770 train_time:52053ms step_avg:95.51ms
step:556/1770 train_time:52150ms step_avg:95.51ms
step:557/1770 train_time:52247ms step_avg:95.52ms
step:558/1770 train_time:52345ms step_avg:95.52ms
step:559/1770 train_time:52443ms step_avg:95.52ms
step:560/1770 train_time:52540ms step_avg:95.53ms
step:561/1770 train_time:52637ms step_avg:95.53ms
step:562/1770 train_time:52734ms step_avg:95.53ms
step:563/1770 train_time:52832ms step_avg:95.54ms
step:564/1770 train_time:52929ms step_avg:95.54ms
step:565/1770 train_time:53027ms step_avg:95.54ms
step:566/1770 train_time:53125ms step_avg:95.55ms
step:567/1770 train_time:53223ms step_avg:95.55ms
step:568/1770 train_time:53321ms step_avg:95.56ms
step:569/1770 train_time:53418ms step_avg:95.56ms
step:570/1770 train_time:53515ms step_avg:95.56ms
step:571/1770 train_time:53612ms step_avg:95.57ms
step:572/1770 train_time:53710ms step_avg:95.57ms
step:573/1770 train_time:53807ms step_avg:95.57ms
step:574/1770 train_time:53905ms step_avg:95.58ms
step:575/1770 train_time:54003ms step_avg:95.58ms
step:576/1770 train_time:54101ms step_avg:95.59ms
step:577/1770 train_time:54199ms step_avg:95.59ms
step:578/1770 train_time:54296ms step_avg:95.59ms
step:579/1770 train_time:54393ms step_avg:95.59ms
step:580/1770 train_time:54490ms step_avg:95.60ms
step:581/1770 train_time:54587ms step_avg:95.60ms
step:582/1770 train_time:54685ms step_avg:95.60ms
step:583/1770 train_time:54783ms step_avg:95.61ms
step:584/1770 train_time:54880ms step_avg:95.61ms
step:585/1770 train_time:54977ms step_avg:95.61ms
step:586/1770 train_time:55074ms step_avg:95.61ms
step:587/1770 train_time:55172ms step_avg:95.62ms
step:588/1770 train_time:55270ms step_avg:95.62ms
step:589/1770 train_time:55367ms step_avg:95.63ms
step:590/1770 train_time:55465ms step_avg:95.63ms
step:591/1770 train_time:55563ms step_avg:95.63ms
step:592/1770 train_time:55661ms step_avg:95.64ms
step:593/1770 train_time:55758ms step_avg:95.64ms
step:594/1770 train_time:55855ms step_avg:95.64ms
step:595/1770 train_time:55952ms step_avg:95.64ms
step:596/1770 train_time:56049ms step_avg:95.65ms
step:597/1770 train_time:56146ms step_avg:95.65ms
step:598/1770 train_time:56244ms step_avg:95.65ms
step:599/1770 train_time:56342ms step_avg:95.66ms
step:600/1770 train_time:56440ms step_avg:95.66ms
step:601/1770 train_time:56537ms step_avg:95.66ms
step:602/1770 train_time:56634ms step_avg:95.67ms
step:603/1770 train_time:56732ms step_avg:95.67ms
step:604/1770 train_time:56830ms step_avg:95.67ms
step:605/1770 train_time:56928ms step_avg:95.68ms
step:606/1770 train_time:57025ms step_avg:95.68ms
step:607/1770 train_time:57123ms step_avg:95.68ms
step:608/1770 train_time:57220ms step_avg:95.69ms
step:609/1770 train_time:57318ms step_avg:95.69ms
step:610/1770 train_time:57415ms step_avg:95.69ms
step:611/1770 train_time:57511ms step_avg:95.69ms
step:612/1770 train_time:57608ms step_avg:95.69ms
step:613/1770 train_time:57706ms step_avg:95.70ms
step:614/1770 train_time:57804ms step_avg:95.70ms
step:615/1770 train_time:57902ms step_avg:95.71ms
step:616/1770 train_time:58000ms step_avg:95.71ms
step:617/1770 train_time:58097ms step_avg:95.71ms
step:618/1770 train_time:58194ms step_avg:95.71ms
step:619/1770 train_time:58292ms step_avg:95.72ms
step:620/1770 train_time:58390ms step_avg:95.72ms
step:621/1770 train_time:58487ms step_avg:95.72ms
step:622/1770 train_time:58585ms step_avg:95.73ms
step:623/1770 train_time:58682ms step_avg:95.73ms
step:624/1770 train_time:58779ms step_avg:95.73ms
step:625/1770 train_time:58876ms step_avg:95.73ms
step:625/1770 val_loss:3.6626 train_time:58972ms step_avg:95.89ms
step:626/1770 train_time:58994ms step_avg:95.77ms
step:627/1770 train_time:59083ms step_avg:95.76ms
step:628/1770 train_time:59184ms step_avg:95.77ms
step:629/1770 train_time:59281ms step_avg:95.77ms
step:630/1770 train_time:59378ms step_avg:95.77ms
step:631/1770 train_time:59474ms step_avg:95.77ms
step:632/1770 train_time:59572ms step_avg:95.77ms
step:633/1770 train_time:59669ms step_avg:95.78ms
step:634/1770 train_time:59766ms step_avg:95.78ms
step:635/1770 train_time:59863ms step_avg:95.78ms
step:636/1770 train_time:59960ms step_avg:95.78ms
step:637/1770 train_time:60058ms step_avg:95.79ms
step:638/1770 train_time:60157ms step_avg:95.79ms
step:639/1770 train_time:60255ms step_avg:95.79ms
step:640/1770 train_time:60353ms step_avg:95.80ms
step:641/1770 train_time:60451ms step_avg:95.80ms
step:642/1770 train_time:60548ms step_avg:95.80ms
step:643/1770 train_time:60645ms step_avg:95.81ms
step:644/1770 train_time:60742ms step_avg:95.81ms
step:645/1770 train_time:60838ms step_avg:95.81ms
step:646/1770 train_time:60935ms step_avg:95.81ms
step:647/1770 train_time:61034ms step_avg:95.81ms
step:648/1770 train_time:61132ms step_avg:95.82ms
step:649/1770 train_time:61230ms step_avg:95.82ms
step:650/1770 train_time:61328ms step_avg:95.82ms
step:651/1770 train_time:61425ms step_avg:95.83ms
step:652/1770 train_time:61522ms step_avg:95.83ms
step:653/1770 train_time:61619ms step_avg:95.83ms
step:654/1770 train_time:61716ms step_avg:95.83ms
step:655/1770 train_time:61814ms step_avg:95.84ms
step:656/1770 train_time:61912ms step_avg:95.84ms
step:657/1770 train_time:62009ms step_avg:95.84ms
step:658/1770 train_time:62108ms step_avg:95.85ms
step:659/1770 train_time:62208ms step_avg:95.85ms
step:660/1770 train_time:62307ms step_avg:95.86ms
step:661/1770 train_time:62407ms step_avg:95.86ms
step:662/1770 train_time:62506ms step_avg:95.87ms
step:663/1770 train_time:62605ms step_avg:95.87ms
step:664/1770 train_time:62704ms step_avg:95.88ms
step:665/1770 train_time:62803ms step_avg:95.88ms
step:666/1770 train_time:62902ms step_avg:95.89ms
step:667/1770 train_time:63001ms step_avg:95.89ms
step:668/1770 train_time:63101ms step_avg:95.90ms
step:669/1770 train_time:63201ms step_avg:95.90ms
step:670/1770 train_time:63300ms step_avg:95.91ms
step:671/1770 train_time:63401ms step_avg:95.92ms
step:672/1770 train_time:63500ms step_avg:95.92ms
step:673/1770 train_time:63600ms step_avg:95.93ms
step:674/1770 train_time:63699ms step_avg:95.93ms
step:675/1770 train_time:63798ms step_avg:95.94ms
step:676/1770 train_time:63897ms step_avg:95.94ms
step:677/1770 train_time:63997ms step_avg:95.95ms
step:678/1770 train_time:64096ms step_avg:95.95ms
step:679/1770 train_time:64196ms step_avg:95.96ms
step:680/1770 train_time:64295ms step_avg:95.96ms
step:681/1770 train_time:64395ms step_avg:95.97ms
step:682/1770 train_time:64495ms step_avg:95.97ms
step:683/1770 train_time:64595ms step_avg:95.98ms
step:684/1770 train_time:64694ms step_avg:95.99ms
step:685/1770 train_time:64794ms step_avg:95.99ms
step:686/1770 train_time:64894ms step_avg:96.00ms
step:687/1770 train_time:64994ms step_avg:96.00ms
step:688/1770 train_time:65094ms step_avg:96.01ms
step:689/1770 train_time:65193ms step_avg:96.01ms
step:690/1770 train_time:65292ms step_avg:96.02ms
step:691/1770 train_time:65391ms step_avg:96.02ms
step:692/1770 train_time:65491ms step_avg:96.03ms
step:693/1770 train_time:65590ms step_avg:96.03ms
step:694/1770 train_time:65690ms step_avg:96.04ms
step:695/1770 train_time:65789ms step_avg:96.04ms
step:696/1770 train_time:65888ms step_avg:96.05ms
step:697/1770 train_time:65987ms step_avg:96.05ms
step:698/1770 train_time:66087ms step_avg:96.06ms
step:699/1770 train_time:66185ms step_avg:96.06ms
step:700/1770 train_time:66284ms step_avg:96.06ms
step:701/1770 train_time:66383ms step_avg:96.07ms
step:702/1770 train_time:66482ms step_avg:96.07ms
step:703/1770 train_time:66581ms step_avg:96.08ms
step:704/1770 train_time:66680ms step_avg:96.08ms
step:705/1770 train_time:66780ms step_avg:96.09ms
step:706/1770 train_time:66880ms step_avg:96.09ms
step:707/1770 train_time:66978ms step_avg:96.10ms
step:708/1770 train_time:67077ms step_avg:96.10ms
step:709/1770 train_time:67177ms step_avg:96.11ms
step:710/1770 train_time:67278ms step_avg:96.11ms
step:711/1770 train_time:67377ms step_avg:96.12ms
step:712/1770 train_time:67477ms step_avg:96.12ms
step:713/1770 train_time:67576ms step_avg:96.13ms
step:714/1770 train_time:67676ms step_avg:96.13ms
step:715/1770 train_time:67776ms step_avg:96.14ms
step:716/1770 train_time:67876ms step_avg:96.14ms
step:717/1770 train_time:67976ms step_avg:96.15ms
step:718/1770 train_time:68075ms step_avg:96.15ms
step:719/1770 train_time:68175ms step_avg:96.16ms
step:720/1770 train_time:68276ms step_avg:96.16ms
step:721/1770 train_time:68376ms step_avg:96.17ms
step:722/1770 train_time:68476ms step_avg:96.17ms
step:723/1770 train_time:68575ms step_avg:96.18ms
step:724/1770 train_time:68674ms step_avg:96.18ms
step:725/1770 train_time:68773ms step_avg:96.19ms
step:726/1770 train_time:68873ms step_avg:96.19ms
step:727/1770 train_time:68972ms step_avg:96.20ms
step:728/1770 train_time:69071ms step_avg:96.20ms
step:729/1770 train_time:69171ms step_avg:96.20ms
step:730/1770 train_time:69270ms step_avg:96.21ms
step:731/1770 train_time:69369ms step_avg:96.21ms
step:732/1770 train_time:69469ms step_avg:96.22ms
step:733/1770 train_time:69569ms step_avg:96.22ms
step:734/1770 train_time:69668ms step_avg:96.23ms
step:735/1770 train_time:69767ms step_avg:96.23ms
step:736/1770 train_time:69866ms step_avg:96.23ms
step:737/1770 train_time:69965ms step_avg:96.24ms
step:738/1770 train_time:70064ms step_avg:96.24ms
step:739/1770 train_time:70163ms step_avg:96.25ms
step:740/1770 train_time:70262ms step_avg:96.25ms
step:741/1770 train_time:70361ms step_avg:96.25ms
step:742/1770 train_time:70461ms step_avg:96.26ms
step:743/1770 train_time:70561ms step_avg:96.26ms
step:744/1770 train_time:70660ms step_avg:96.27ms
step:745/1770 train_time:70759ms step_avg:96.27ms
step:746/1770 train_time:70858ms step_avg:96.27ms
step:747/1770 train_time:70957ms step_avg:96.28ms
step:748/1770 train_time:71057ms step_avg:96.28ms
step:749/1770 train_time:71156ms step_avg:96.29ms
step:750/1770 train_time:71255ms step_avg:96.29ms
step:750/1770 val_loss:3.5998 train_time:71352ms step_avg:96.42ms
step:751/1770 train_time:71374ms step_avg:96.32ms
step:752/1770 train_time:71466ms step_avg:96.32ms
step:753/1770 train_time:71566ms step_avg:96.32ms
step:754/1770 train_time:71665ms step_avg:96.32ms
step:755/1770 train_time:71764ms step_avg:96.33ms
step:756/1770 train_time:71862ms step_avg:96.33ms
step:757/1770 train_time:71961ms step_avg:96.33ms
step:758/1770 train_time:72059ms step_avg:96.34ms
step:759/1770 train_time:72157ms step_avg:96.34ms
step:760/1770 train_time:72256ms step_avg:96.34ms
step:761/1770 train_time:72356ms step_avg:96.35ms
step:762/1770 train_time:72456ms step_avg:96.35ms
step:763/1770 train_time:72557ms step_avg:96.36ms
step:764/1770 train_time:72657ms step_avg:96.36ms
step:765/1770 train_time:72756ms step_avg:96.37ms
step:766/1770 train_time:72855ms step_avg:96.37ms
step:767/1770 train_time:72955ms step_avg:96.37ms
step:768/1770 train_time:73054ms step_avg:96.38ms
step:769/1770 train_time:73154ms step_avg:96.38ms
step:770/1770 train_time:73253ms step_avg:96.39ms
step:771/1770 train_time:73353ms step_avg:96.39ms
step:772/1770 train_time:73452ms step_avg:96.39ms
step:773/1770 train_time:73552ms step_avg:96.40ms
step:774/1770 train_time:73653ms step_avg:96.40ms
step:775/1770 train_time:73752ms step_avg:96.41ms
step:776/1770 train_time:73852ms step_avg:96.41ms
step:777/1770 train_time:73952ms step_avg:96.42ms
step:778/1770 train_time:74051ms step_avg:96.42ms
step:779/1770 train_time:74151ms step_avg:96.42ms
step:780/1770 train_time:74249ms step_avg:96.43ms
step:781/1770 train_time:74349ms step_avg:96.43ms
step:782/1770 train_time:74448ms step_avg:96.44ms
step:783/1770 train_time:74547ms step_avg:96.44ms
step:784/1770 train_time:74646ms step_avg:96.44ms
step:785/1770 train_time:74746ms step_avg:96.45ms
step:786/1770 train_time:74845ms step_avg:96.45ms
step:787/1770 train_time:74945ms step_avg:96.45ms
step:788/1770 train_time:75044ms step_avg:96.46ms
step:789/1770 train_time:75144ms step_avg:96.46ms
step:790/1770 train_time:75243ms step_avg:96.47ms
step:791/1770 train_time:75343ms step_avg:96.47ms
step:792/1770 train_time:75443ms step_avg:96.47ms
step:793/1770 train_time:75544ms step_avg:96.48ms
step:794/1770 train_time:75643ms step_avg:96.48ms
step:795/1770 train_time:75743ms step_avg:96.49ms
step:796/1770 train_time:75843ms step_avg:96.49ms
step:797/1770 train_time:75942ms step_avg:96.50ms
step:798/1770 train_time:76041ms step_avg:96.50ms
step:799/1770 train_time:76140ms step_avg:96.50ms
step:800/1770 train_time:76239ms step_avg:96.51ms
step:801/1770 train_time:76339ms step_avg:96.51ms
step:802/1770 train_time:76438ms step_avg:96.51ms
step:803/1770 train_time:76538ms step_avg:96.52ms
step:804/1770 train_time:76637ms step_avg:96.52ms
step:805/1770 train_time:76737ms step_avg:96.52ms
step:806/1770 train_time:76837ms step_avg:96.53ms
step:807/1770 train_time:76936ms step_avg:96.53ms
step:808/1770 train_time:77036ms step_avg:96.54ms
step:809/1770 train_time:77135ms step_avg:96.54ms
step:810/1770 train_time:77235ms step_avg:96.54ms
step:811/1770 train_time:77334ms step_avg:96.55ms
step:812/1770 train_time:77434ms step_avg:96.55ms
step:813/1770 train_time:77534ms step_avg:96.56ms
step:814/1770 train_time:77634ms step_avg:96.56ms
step:815/1770 train_time:77733ms step_avg:96.56ms
step:816/1770 train_time:77833ms step_avg:96.57ms
step:817/1770 train_time:77933ms step_avg:96.57ms
step:818/1770 train_time:78033ms step_avg:96.58ms
step:819/1770 train_time:78133ms step_avg:96.58ms
step:820/1770 train_time:78234ms step_avg:96.58ms
step:821/1770 train_time:78334ms step_avg:96.59ms
step:822/1770 train_time:78434ms step_avg:96.59ms
step:823/1770 train_time:78533ms step_avg:96.60ms
step:824/1770 train_time:78633ms step_avg:96.60ms
step:825/1770 train_time:78733ms step_avg:96.61ms
step:826/1770 train_time:78833ms step_avg:96.61ms
step:827/1770 train_time:78933ms step_avg:96.61ms
step:828/1770 train_time:79032ms step_avg:96.62ms
step:829/1770 train_time:79133ms step_avg:96.62ms
step:830/1770 train_time:79233ms step_avg:96.63ms
step:831/1770 train_time:79333ms step_avg:96.63ms
step:832/1770 train_time:79433ms step_avg:96.63ms
step:833/1770 train_time:79533ms step_avg:96.64ms
step:834/1770 train_time:79633ms step_avg:96.64ms
step:835/1770 train_time:79732ms step_avg:96.65ms
step:836/1770 train_time:79833ms step_avg:96.65ms
step:837/1770 train_time:79932ms step_avg:96.65ms
step:838/1770 train_time:80032ms step_avg:96.66ms
step:839/1770 train_time:80132ms step_avg:96.66ms
step:840/1770 train_time:80232ms step_avg:96.66ms
step:841/1770 train_time:80332ms step_avg:96.67ms
step:842/1770 train_time:80431ms step_avg:96.67ms
step:843/1770 train_time:80531ms step_avg:96.68ms
step:844/1770 train_time:80631ms step_avg:96.68ms
step:845/1770 train_time:80731ms step_avg:96.68ms
step:846/1770 train_time:80830ms step_avg:96.69ms
step:847/1770 train_time:80930ms step_avg:96.69ms
step:848/1770 train_time:81029ms step_avg:96.69ms
step:849/1770 train_time:81129ms step_avg:96.70ms
step:850/1770 train_time:81228ms step_avg:96.70ms
step:851/1770 train_time:81328ms step_avg:96.70ms
step:852/1770 train_time:81428ms step_avg:96.71ms
step:853/1770 train_time:81527ms step_avg:96.71ms
step:854/1770 train_time:81626ms step_avg:96.71ms
step:855/1770 train_time:81725ms step_avg:96.72ms
step:856/1770 train_time:81823ms step_avg:96.72ms
step:857/1770 train_time:81923ms step_avg:96.72ms
step:858/1770 train_time:82022ms step_avg:96.72ms
step:859/1770 train_time:82123ms step_avg:96.73ms
step:860/1770 train_time:82223ms step_avg:96.73ms
step:861/1770 train_time:82323ms step_avg:96.74ms
step:862/1770 train_time:82424ms step_avg:96.74ms
step:863/1770 train_time:82523ms step_avg:96.74ms
step:864/1770 train_time:82623ms step_avg:96.75ms
step:865/1770 train_time:82722ms step_avg:96.75ms
step:866/1770 train_time:82822ms step_avg:96.75ms
step:867/1770 train_time:82922ms step_avg:96.76ms
step:868/1770 train_time:83022ms step_avg:96.76ms
step:869/1770 train_time:83122ms step_avg:96.77ms
step:870/1770 train_time:83221ms step_avg:96.77ms
step:871/1770 train_time:83321ms step_avg:96.77ms
step:872/1770 train_time:83421ms step_avg:96.78ms
step:873/1770 train_time:83520ms step_avg:96.78ms
step:874/1770 train_time:83620ms step_avg:96.78ms
step:875/1770 train_time:83720ms step_avg:96.79ms
step:875/1770 val_loss:3.5532 train_time:83817ms step_avg:96.90ms
step:876/1770 train_time:83839ms step_avg:96.81ms
step:877/1770 train_time:83929ms step_avg:96.80ms
step:878/1770 train_time:84030ms step_avg:96.81ms
step:879/1770 train_time:84130ms step_avg:96.81ms
step:880/1770 train_time:84229ms step_avg:96.81ms
step:881/1770 train_time:84328ms step_avg:96.82ms
step:882/1770 train_time:84427ms step_avg:96.82ms
step:883/1770 train_time:84526ms step_avg:96.82ms
step:884/1770 train_time:84625ms step_avg:96.82ms
step:885/1770 train_time:84723ms step_avg:96.83ms
step:886/1770 train_time:84822ms step_avg:96.83ms
step:887/1770 train_time:84923ms step_avg:96.83ms
step:888/1770 train_time:85023ms step_avg:96.84ms
step:889/1770 train_time:85123ms step_avg:96.84ms
step:890/1770 train_time:85222ms step_avg:96.84ms
step:891/1770 train_time:85322ms step_avg:96.85ms
step:892/1770 train_time:85422ms step_avg:96.85ms
step:893/1770 train_time:85521ms step_avg:96.85ms
step:894/1770 train_time:85620ms step_avg:96.85ms
step:895/1770 train_time:85719ms step_avg:96.86ms
step:896/1770 train_time:85818ms step_avg:96.86ms
step:897/1770 train_time:85917ms step_avg:96.86ms
step:898/1770 train_time:86017ms step_avg:96.87ms
step:899/1770 train_time:86117ms step_avg:96.87ms
step:900/1770 train_time:86217ms step_avg:96.87ms
step:901/1770 train_time:86317ms step_avg:96.88ms
step:902/1770 train_time:86417ms step_avg:96.88ms
step:903/1770 train_time:86517ms step_avg:96.88ms
step:904/1770 train_time:86616ms step_avg:96.89ms
step:905/1770 train_time:86716ms step_avg:96.89ms
step:906/1770 train_time:86815ms step_avg:96.89ms
step:907/1770 train_time:86915ms step_avg:96.89ms
step:908/1770 train_time:87015ms step_avg:96.90ms
step:909/1770 train_time:87114ms step_avg:96.90ms
step:910/1770 train_time:87214ms step_avg:96.90ms
step:911/1770 train_time:87314ms step_avg:96.91ms
step:912/1770 train_time:87414ms step_avg:96.91ms
step:913/1770 train_time:87514ms step_avg:96.91ms
step:914/1770 train_time:87614ms step_avg:96.92ms
step:915/1770 train_time:87714ms step_avg:96.92ms
step:916/1770 train_time:87813ms step_avg:96.92ms
step:917/1770 train_time:87912ms step_avg:96.93ms
step:918/1770 train_time:88011ms step_avg:96.93ms
step:919/1770 train_time:88111ms step_avg:96.93ms
step:920/1770 train_time:88212ms step_avg:96.94ms
step:921/1770 train_time:88313ms step_avg:96.94ms
step:922/1770 train_time:88415ms step_avg:96.95ms
step:923/1770 train_time:88515ms step_avg:96.95ms
step:924/1770 train_time:88616ms step_avg:96.95ms
step:925/1770 train_time:88716ms step_avg:96.96ms
step:926/1770 train_time:88817ms step_avg:96.96ms
step:927/1770 train_time:88918ms step_avg:96.97ms
step:928/1770 train_time:89019ms step_avg:96.97ms
step:929/1770 train_time:89119ms step_avg:96.97ms
step:930/1770 train_time:89220ms step_avg:96.98ms
step:931/1770 train_time:89321ms step_avg:96.98ms
step:932/1770 train_time:89422ms step_avg:96.99ms
step:933/1770 train_time:89522ms step_avg:96.99ms
step:934/1770 train_time:89622ms step_avg:96.99ms
step:935/1770 train_time:89723ms step_avg:97.00ms
step:936/1770 train_time:89824ms step_avg:97.00ms
step:937/1770 train_time:89926ms step_avg:97.01ms
step:938/1770 train_time:90028ms step_avg:97.01ms
step:939/1770 train_time:90130ms step_avg:97.02ms
step:940/1770 train_time:90232ms step_avg:97.02ms
step:941/1770 train_time:90333ms step_avg:97.03ms
step:942/1770 train_time:90434ms step_avg:97.03ms
step:943/1770 train_time:90535ms step_avg:97.04ms
step:944/1770 train_time:90636ms step_avg:97.04ms
step:945/1770 train_time:90737ms step_avg:97.05ms
step:946/1770 train_time:90838ms step_avg:97.05ms
step:947/1770 train_time:90940ms step_avg:97.05ms
step:948/1770 train_time:91041ms step_avg:97.06ms
step:949/1770 train_time:91142ms step_avg:97.06ms
step:950/1770 train_time:91243ms step_avg:97.07ms
step:951/1770 train_time:91343ms step_avg:97.07ms
step:952/1770 train_time:91444ms step_avg:97.07ms
step:953/1770 train_time:91545ms step_avg:97.08ms
step:954/1770 train_time:91645ms step_avg:97.08ms
step:955/1770 train_time:91746ms step_avg:97.09ms
step:956/1770 train_time:91847ms step_avg:97.09ms
step:957/1770 train_time:91949ms step_avg:97.10ms
step:958/1770 train_time:92050ms step_avg:97.10ms
step:959/1770 train_time:92152ms step_avg:97.10ms
step:960/1770 train_time:92252ms step_avg:97.11ms
step:961/1770 train_time:92353ms step_avg:97.11ms
step:962/1770 train_time:92454ms step_avg:97.12ms
step:963/1770 train_time:92555ms step_avg:97.12ms
step:964/1770 train_time:92657ms step_avg:97.12ms
step:965/1770 train_time:92759ms step_avg:97.13ms
step:966/1770 train_time:92860ms step_avg:97.13ms
step:967/1770 train_time:92962ms step_avg:97.14ms
step:968/1770 train_time:93062ms step_avg:97.14ms
step:969/1770 train_time:93163ms step_avg:97.15ms
step:970/1770 train_time:93263ms step_avg:97.15ms
step:971/1770 train_time:93363ms step_avg:97.15ms
step:972/1770 train_time:93464ms step_avg:97.16ms
step:973/1770 train_time:93565ms step_avg:97.16ms
step:974/1770 train_time:93666ms step_avg:97.16ms
step:975/1770 train_time:93769ms step_avg:97.17ms
step:976/1770 train_time:93870ms step_avg:97.17ms
step:977/1770 train_time:93971ms step_avg:97.18ms
step:978/1770 train_time:94072ms step_avg:97.18ms
step:979/1770 train_time:94173ms step_avg:97.19ms
step:980/1770 train_time:94274ms step_avg:97.19ms
step:981/1770 train_time:94375ms step_avg:97.19ms
step:982/1770 train_time:94477ms step_avg:97.20ms
step:983/1770 train_time:94579ms step_avg:97.20ms
step:984/1770 train_time:94681ms step_avg:97.21ms
step:985/1770 train_time:94782ms step_avg:97.21ms
step:986/1770 train_time:94883ms step_avg:97.22ms
step:987/1770 train_time:94983ms step_avg:97.22ms
step:988/1770 train_time:95083ms step_avg:97.22ms
step:989/1770 train_time:95186ms step_avg:97.23ms
step:990/1770 train_time:95288ms step_avg:97.23ms
step:991/1770 train_time:95389ms step_avg:97.24ms
step:992/1770 train_time:95492ms step_avg:97.24ms
step:993/1770 train_time:95594ms step_avg:97.25ms
step:994/1770 train_time:95694ms step_avg:97.25ms
step:995/1770 train_time:95795ms step_avg:97.25ms
step:996/1770 train_time:95896ms step_avg:97.26ms
step:997/1770 train_time:95998ms step_avg:97.26ms
step:998/1770 train_time:96099ms step_avg:97.27ms
step:999/1770 train_time:96201ms step_avg:97.27ms
step:1000/1770 train_time:96302ms step_avg:97.28ms
step:1000/1770 val_loss:3.5128 train_time:96401ms step_avg:97.38ms
step:1001/1770 train_time:96424ms step_avg:97.30ms
step:1002/1770 train_time:96515ms step_avg:97.29ms
step:1003/1770 train_time:96616ms step_avg:97.30ms
step:1004/1770 train_time:96718ms step_avg:97.30ms
step:1005/1770 train_time:96818ms step_avg:97.30ms
step:1006/1770 train_time:96919ms step_avg:97.31ms
step:1007/1770 train_time:97019ms step_avg:97.31ms
step:1008/1770 train_time:97120ms step_avg:97.31ms
step:1009/1770 train_time:97220ms step_avg:97.32ms
step:1010/1770 train_time:97320ms step_avg:97.32ms
step:1011/1770 train_time:97422ms step_avg:97.32ms
step:1012/1770 train_time:97524ms step_avg:97.33ms
step:1013/1770 train_time:97627ms step_avg:97.34ms
step:1014/1770 train_time:97729ms step_avg:97.34ms
step:1015/1770 train_time:97829ms step_avg:97.34ms
step:1016/1770 train_time:97930ms step_avg:97.35ms
step:1017/1770 train_time:98030ms step_avg:97.35ms
step:1018/1770 train_time:98130ms step_avg:97.35ms
step:1019/1770 train_time:98231ms step_avg:97.35ms
step:1020/1770 train_time:98332ms step_avg:97.36ms
step:1021/1770 train_time:98432ms step_avg:97.36ms
step:1022/1770 train_time:98535ms step_avg:97.37ms
step:1023/1770 train_time:98637ms step_avg:97.37ms
step:1024/1770 train_time:98738ms step_avg:97.37ms
step:1025/1770 train_time:98839ms step_avg:97.38ms
step:1026/1770 train_time:98940ms step_avg:97.38ms
step:1027/1770 train_time:99042ms step_avg:97.39ms
step:1028/1770 train_time:99142ms step_avg:97.39ms
step:1029/1770 train_time:99243ms step_avg:97.39ms
step:1030/1770 train_time:99343ms step_avg:97.40ms
step:1031/1770 train_time:99444ms step_avg:97.40ms
step:1032/1770 train_time:99545ms step_avg:97.40ms
step:1033/1770 train_time:99647ms step_avg:97.41ms
step:1034/1770 train_time:99747ms step_avg:97.41ms
step:1035/1770 train_time:99848ms step_avg:97.41ms
step:1036/1770 train_time:99949ms step_avg:97.42ms
step:1037/1770 train_time:100050ms step_avg:97.42ms
step:1038/1770 train_time:100150ms step_avg:97.42ms
step:1039/1770 train_time:100251ms step_avg:97.43ms
step:1040/1770 train_time:100351ms step_avg:97.43ms
step:1041/1770 train_time:100452ms step_avg:97.43ms
step:1042/1770 train_time:100552ms step_avg:97.43ms
step:1043/1770 train_time:100654ms step_avg:97.44ms
step:1044/1770 train_time:100755ms step_avg:97.44ms
step:1045/1770 train_time:100857ms step_avg:97.45ms
step:1046/1770 train_time:100958ms step_avg:97.45ms
step:1047/1770 train_time:101059ms step_avg:97.45ms
step:1048/1770 train_time:101161ms step_avg:97.46ms
step:1049/1770 train_time:101262ms step_avg:97.46ms
step:1050/1770 train_time:101363ms step_avg:97.46ms
step:1051/1770 train_time:101464ms step_avg:97.47ms
step:1052/1770 train_time:101565ms step_avg:97.47ms
step:1053/1770 train_time:101666ms step_avg:97.47ms
step:1054/1770 train_time:101767ms step_avg:97.48ms
step:1055/1770 train_time:101868ms step_avg:97.48ms
step:1056/1770 train_time:101969ms step_avg:97.48ms
step:1057/1770 train_time:102070ms step_avg:97.49ms
step:1058/1770 train_time:102171ms step_avg:97.49ms
step:1059/1770 train_time:102271ms step_avg:97.49ms
step:1060/1770 train_time:102372ms step_avg:97.50ms
step:1061/1770 train_time:102474ms step_avg:97.50ms
step:1062/1770 train_time:102576ms step_avg:97.51ms
step:1063/1770 train_time:102678ms step_avg:97.51ms
step:1064/1770 train_time:102781ms step_avg:97.52ms
step:1065/1770 train_time:102882ms step_avg:97.52ms
step:1066/1770 train_time:102983ms step_avg:97.52ms
step:1067/1770 train_time:103084ms step_avg:97.52ms
step:1068/1770 train_time:103187ms step_avg:97.53ms
step:1069/1770 train_time:103288ms step_avg:97.53ms
step:1070/1770 train_time:103389ms step_avg:97.54ms
step:1071/1770 train_time:103490ms step_avg:97.54ms
step:1072/1770 train_time:103591ms step_avg:97.54ms
step:1073/1770 train_time:103691ms step_avg:97.55ms
step:1074/1770 train_time:103793ms step_avg:97.55ms
step:1075/1770 train_time:103894ms step_avg:97.55ms
step:1076/1770 train_time:103997ms step_avg:97.56ms
step:1077/1770 train_time:104099ms step_avg:97.56ms
step:1078/1770 train_time:104200ms step_avg:97.57ms
step:1079/1770 train_time:104301ms step_avg:97.57ms
step:1080/1770 train_time:104402ms step_avg:97.57ms
step:1081/1770 train_time:104502ms step_avg:97.57ms
step:1082/1770 train_time:104604ms step_avg:97.58ms
step:1083/1770 train_time:104706ms step_avg:97.58ms
step:1084/1770 train_time:104807ms step_avg:97.59ms
step:1085/1770 train_time:104909ms step_avg:97.59ms
step:1086/1770 train_time:105010ms step_avg:97.59ms
step:1087/1770 train_time:105110ms step_avg:97.60ms
step:1088/1770 train_time:105210ms step_avg:97.60ms
step:1089/1770 train_time:105311ms step_avg:97.60ms
step:1090/1770 train_time:105412ms step_avg:97.60ms
step:1091/1770 train_time:105513ms step_avg:97.61ms
step:1092/1770 train_time:105614ms step_avg:97.61ms
step:1093/1770 train_time:105716ms step_avg:97.61ms
step:1094/1770 train_time:105819ms step_avg:97.62ms
step:1095/1770 train_time:105920ms step_avg:97.62ms
step:1096/1770 train_time:106021ms step_avg:97.62ms
step:1097/1770 train_time:106121ms step_avg:97.63ms
step:1098/1770 train_time:106222ms step_avg:97.63ms
step:1099/1770 train_time:106323ms step_avg:97.63ms
step:1100/1770 train_time:106425ms step_avg:97.64ms
step:1101/1770 train_time:106526ms step_avg:97.64ms
step:1102/1770 train_time:106627ms step_avg:97.64ms
step:1103/1770 train_time:106729ms step_avg:97.65ms
step:1104/1770 train_time:106830ms step_avg:97.65ms
step:1105/1770 train_time:106930ms step_avg:97.65ms
step:1106/1770 train_time:107031ms step_avg:97.66ms
step:1107/1770 train_time:107131ms step_avg:97.66ms
step:1108/1770 train_time:107233ms step_avg:97.66ms
step:1109/1770 train_time:107335ms step_avg:97.67ms
step:1110/1770 train_time:107437ms step_avg:97.67ms
step:1111/1770 train_time:107539ms step_avg:97.67ms
step:1112/1770 train_time:107640ms step_avg:97.68ms
step:1113/1770 train_time:107741ms step_avg:97.68ms
step:1114/1770 train_time:107842ms step_avg:97.68ms
step:1115/1770 train_time:107944ms step_avg:97.69ms
step:1116/1770 train_time:108046ms step_avg:97.69ms
step:1117/1770 train_time:108148ms step_avg:97.69ms
step:1118/1770 train_time:108248ms step_avg:97.70ms
step:1119/1770 train_time:108349ms step_avg:97.70ms
step:1120/1770 train_time:108450ms step_avg:97.70ms
step:1121/1770 train_time:108551ms step_avg:97.71ms
step:1122/1770 train_time:108651ms step_avg:97.71ms
step:1123/1770 train_time:108752ms step_avg:97.71ms
step:1124/1770 train_time:108854ms step_avg:97.71ms
step:1125/1770 train_time:108956ms step_avg:97.72ms
step:1125/1770 val_loss:3.4717 train_time:109056ms step_avg:97.81ms
step:1126/1770 train_time:109077ms step_avg:97.74ms
step:1127/1770 train_time:109168ms step_avg:97.73ms
step:1128/1770 train_time:109271ms step_avg:97.74ms
step:1129/1770 train_time:109372ms step_avg:97.74ms
step:1130/1770 train_time:109472ms step_avg:97.74ms
step:1131/1770 train_time:109573ms step_avg:97.75ms
step:1132/1770 train_time:109673ms step_avg:97.75ms
step:1133/1770 train_time:109774ms step_avg:97.75ms
step:1134/1770 train_time:109874ms step_avg:97.75ms
step:1135/1770 train_time:109975ms step_avg:97.76ms
step:1136/1770 train_time:110076ms step_avg:97.76ms
step:1137/1770 train_time:110178ms step_avg:97.76ms
step:1138/1770 train_time:110280ms step_avg:97.77ms
step:1139/1770 train_time:110383ms step_avg:97.77ms
step:1140/1770 train_time:110483ms step_avg:97.77ms
step:1141/1770 train_time:110584ms step_avg:97.78ms
step:1142/1770 train_time:110685ms step_avg:97.78ms
step:1143/1770 train_time:110786ms step_avg:97.78ms
step:1144/1770 train_time:110887ms step_avg:97.78ms
step:1145/1770 train_time:110989ms step_avg:97.79ms
step:1146/1770 train_time:111090ms step_avg:97.79ms
step:1147/1770 train_time:111192ms step_avg:97.79ms
step:1148/1770 train_time:111293ms step_avg:97.80ms
step:1149/1770 train_time:111395ms step_avg:97.80ms
step:1150/1770 train_time:111495ms step_avg:97.80ms
step:1151/1770 train_time:111597ms step_avg:97.81ms
step:1152/1770 train_time:111697ms step_avg:97.81ms
step:1153/1770 train_time:111798ms step_avg:97.81ms
step:1154/1770 train_time:111900ms step_avg:97.81ms
step:1155/1770 train_time:112003ms step_avg:97.82ms
step:1156/1770 train_time:112104ms step_avg:97.82ms
step:1157/1770 train_time:112206ms step_avg:97.83ms
step:1158/1770 train_time:112307ms step_avg:97.83ms
step:1159/1770 train_time:112408ms step_avg:97.83ms
step:1160/1770 train_time:112509ms step_avg:97.83ms
step:1161/1770 train_time:112610ms step_avg:97.84ms
step:1162/1770 train_time:112712ms step_avg:97.84ms
step:1163/1770 train_time:112813ms step_avg:97.84ms
step:1164/1770 train_time:112914ms step_avg:97.85ms
step:1165/1770 train_time:113015ms step_avg:97.85ms
step:1166/1770 train_time:113116ms step_avg:97.85ms
step:1167/1770 train_time:113217ms step_avg:97.85ms
step:1168/1770 train_time:113319ms step_avg:97.86ms
step:1169/1770 train_time:113420ms step_avg:97.86ms
step:1170/1770 train_time:113521ms step_avg:97.86ms
step:1171/1770 train_time:113623ms step_avg:97.87ms
step:1172/1770 train_time:113725ms step_avg:97.87ms
step:1173/1770 train_time:113825ms step_avg:97.87ms
step:1174/1770 train_time:113927ms step_avg:97.88ms
step:1175/1770 train_time:114028ms step_avg:97.88ms
step:1176/1770 train_time:114130ms step_avg:97.88ms
step:1177/1770 train_time:114231ms step_avg:97.88ms
step:1178/1770 train_time:114333ms step_avg:97.89ms
step:1179/1770 train_time:114434ms step_avg:97.89ms
step:1180/1770 train_time:114535ms step_avg:97.89ms
step:1181/1770 train_time:114635ms step_avg:97.89ms
step:1182/1770 train_time:114736ms step_avg:97.90ms
step:1183/1770 train_time:114838ms step_avg:97.90ms
step:1184/1770 train_time:114941ms step_avg:97.91ms
step:1185/1770 train_time:115044ms step_avg:97.91ms
step:1186/1770 train_time:115147ms step_avg:97.91ms
step:1187/1770 train_time:115251ms step_avg:97.92ms
step:1188/1770 train_time:115354ms step_avg:97.92ms
step:1189/1770 train_time:115457ms step_avg:97.93ms
step:1190/1770 train_time:115558ms step_avg:97.93ms
step:1191/1770 train_time:115661ms step_avg:97.93ms
step:1192/1770 train_time:115764ms step_avg:97.94ms
step:1193/1770 train_time:115866ms step_avg:97.94ms
step:1194/1770 train_time:115967ms step_avg:97.95ms
step:1195/1770 train_time:116070ms step_avg:97.95ms
step:1196/1770 train_time:116173ms step_avg:97.95ms
step:1197/1770 train_time:116275ms step_avg:97.96ms
step:1198/1770 train_time:116377ms step_avg:97.96ms
step:1199/1770 train_time:116478ms step_avg:97.96ms
step:1200/1770 train_time:116581ms step_avg:97.97ms
step:1201/1770 train_time:116683ms step_avg:97.97ms
step:1202/1770 train_time:116784ms step_avg:97.97ms
step:1203/1770 train_time:116887ms step_avg:97.98ms
step:1204/1770 train_time:116989ms step_avg:97.98ms
step:1205/1770 train_time:117090ms step_avg:97.98ms
step:1206/1770 train_time:117194ms step_avg:97.99ms
step:1207/1770 train_time:117295ms step_avg:97.99ms
step:1208/1770 train_time:117397ms step_avg:97.99ms
step:1209/1770 train_time:117499ms step_avg:98.00ms
step:1210/1770 train_time:117600ms step_avg:98.00ms
step:1211/1770 train_time:117704ms step_avg:98.01ms
step:1212/1770 train_time:117808ms step_avg:98.01ms
step:1213/1770 train_time:117910ms step_avg:98.01ms
step:1214/1770 train_time:118011ms step_avg:98.02ms
step:1215/1770 train_time:118114ms step_avg:98.02ms
step:1216/1770 train_time:118219ms step_avg:98.03ms
step:1217/1770 train_time:118321ms step_avg:98.03ms
step:1218/1770 train_time:118424ms step_avg:98.03ms
step:1219/1770 train_time:118526ms step_avg:98.04ms
step:1220/1770 train_time:118628ms step_avg:98.04ms
step:1221/1770 train_time:118730ms step_avg:98.04ms
step:1222/1770 train_time:118834ms step_avg:98.05ms
step:1223/1770 train_time:118935ms step_avg:98.05ms
step:1224/1770 train_time:119038ms step_avg:98.05ms
step:1225/1770 train_time:119140ms step_avg:98.06ms
step:1226/1770 train_time:119243ms step_avg:98.06ms
step:1227/1770 train_time:119347ms step_avg:98.07ms
step:1228/1770 train_time:119451ms step_avg:98.07ms
step:1229/1770 train_time:119553ms step_avg:98.07ms
step:1230/1770 train_time:119656ms step_avg:98.08ms
step:1231/1770 train_time:119758ms step_avg:98.08ms
step:1232/1770 train_time:119860ms step_avg:98.09ms
step:1233/1770 train_time:119963ms step_avg:98.09ms
step:1234/1770 train_time:120064ms step_avg:98.09ms
step:1235/1770 train_time:120167ms step_avg:98.10ms
step:1236/1770 train_time:120269ms step_avg:98.10ms
step:1237/1770 train_time:120371ms step_avg:98.10ms
step:1238/1770 train_time:120474ms step_avg:98.11ms
step:1239/1770 train_time:120576ms step_avg:98.11ms
step:1240/1770 train_time:120678ms step_avg:98.11ms
step:1241/1770 train_time:120781ms step_avg:98.12ms
step:1242/1770 train_time:120884ms step_avg:98.12ms
step:1243/1770 train_time:120986ms step_avg:98.12ms
step:1244/1770 train_time:121088ms step_avg:98.13ms
step:1245/1770 train_time:121190ms step_avg:98.13ms
step:1246/1770 train_time:121292ms step_avg:98.13ms
step:1247/1770 train_time:121394ms step_avg:98.14ms
step:1248/1770 train_time:121497ms step_avg:98.14ms
step:1249/1770 train_time:121598ms step_avg:98.14ms
step:1250/1770 train_time:121700ms step_avg:98.15ms
step:1250/1770 val_loss:3.4240 train_time:121803ms step_avg:98.23ms
step:1251/1770 train_time:121824ms step_avg:98.17ms
step:1252/1770 train_time:121917ms step_avg:98.16ms
step:1253/1770 train_time:122020ms step_avg:98.17ms
step:1254/1770 train_time:122121ms step_avg:98.17ms
step:1255/1770 train_time:122225ms step_avg:98.17ms
step:1256/1770 train_time:122327ms step_avg:98.18ms
step:1257/1770 train_time:122428ms step_avg:98.18ms
step:1258/1770 train_time:122531ms step_avg:98.18ms
step:1259/1770 train_time:122633ms step_avg:98.18ms
step:1260/1770 train_time:122735ms step_avg:98.19ms
step:1261/1770 train_time:122838ms step_avg:98.19ms
step:1262/1770 train_time:122941ms step_avg:98.20ms
step:1263/1770 train_time:123043ms step_avg:98.20ms
step:1264/1770 train_time:123146ms step_avg:98.20ms
step:1265/1770 train_time:123248ms step_avg:98.21ms
step:1266/1770 train_time:123351ms step_avg:98.21ms
step:1267/1770 train_time:123454ms step_avg:98.21ms
step:1268/1770 train_time:123557ms step_avg:98.22ms
step:1269/1770 train_time:123659ms step_avg:98.22ms
step:1270/1770 train_time:123762ms step_avg:98.22ms
step:1271/1770 train_time:123864ms step_avg:98.23ms
step:1272/1770 train_time:123966ms step_avg:98.23ms
step:1273/1770 train_time:124069ms step_avg:98.23ms
step:1274/1770 train_time:124171ms step_avg:98.24ms
step:1275/1770 train_time:124273ms step_avg:98.24ms
step:1276/1770 train_time:124376ms step_avg:98.24ms
step:1277/1770 train_time:124478ms step_avg:98.25ms
step:1278/1770 train_time:124581ms step_avg:98.25ms
step:1279/1770 train_time:124684ms step_avg:98.25ms
step:1280/1770 train_time:124787ms step_avg:98.26ms
step:1281/1770 train_time:124889ms step_avg:98.26ms
step:1282/1770 train_time:124992ms step_avg:98.26ms
step:1283/1770 train_time:125095ms step_avg:98.27ms
step:1284/1770 train_time:125199ms step_avg:98.27ms
step:1285/1770 train_time:125301ms step_avg:98.28ms
step:1286/1770 train_time:125404ms step_avg:98.28ms
step:1287/1770 train_time:125508ms step_avg:98.28ms
step:1288/1770 train_time:125610ms step_avg:98.29ms
step:1289/1770 train_time:125713ms step_avg:98.29ms
step:1290/1770 train_time:125815ms step_avg:98.29ms
step:1291/1770 train_time:125916ms step_avg:98.30ms
step:1292/1770 train_time:126018ms step_avg:98.30ms
step:1293/1770 train_time:126122ms step_avg:98.30ms
step:1294/1770 train_time:126223ms step_avg:98.30ms
step:1295/1770 train_time:126326ms step_avg:98.31ms
step:1296/1770 train_time:126428ms step_avg:98.31ms
step:1297/1770 train_time:126530ms step_avg:98.31ms
step:1298/1770 train_time:126632ms step_avg:98.32ms
step:1299/1770 train_time:126735ms step_avg:98.32ms
step:1300/1770 train_time:126837ms step_avg:98.32ms
step:1301/1770 train_time:126939ms step_avg:98.33ms
step:1302/1770 train_time:127041ms step_avg:98.33ms
step:1303/1770 train_time:127143ms step_avg:98.33ms
step:1304/1770 train_time:127245ms step_avg:98.33ms
step:1305/1770 train_time:127347ms step_avg:98.34ms
step:1306/1770 train_time:127448ms step_avg:98.34ms
step:1307/1770 train_time:127550ms step_avg:98.34ms
step:1308/1770 train_time:127652ms step_avg:98.34ms
step:1309/1770 train_time:127755ms step_avg:98.35ms
step:1310/1770 train_time:127856ms step_avg:98.35ms
step:1311/1770 train_time:127958ms step_avg:98.35ms
step:1312/1770 train_time:128060ms step_avg:98.36ms
step:1313/1770 train_time:128161ms step_avg:98.36ms
step:1314/1770 train_time:128265ms step_avg:98.36ms
step:1315/1770 train_time:128368ms step_avg:98.37ms
step:1316/1770 train_time:128470ms step_avg:98.37ms
step:1317/1770 train_time:128572ms step_avg:98.37ms
step:1318/1770 train_time:128679ms step_avg:98.38ms
step:1319/1770 train_time:128781ms step_avg:98.38ms
step:1320/1770 train_time:128883ms step_avg:98.38ms
step:1321/1770 train_time:128985ms step_avg:98.39ms
step:1322/1770 train_time:129088ms step_avg:98.39ms
step:1323/1770 train_time:129191ms step_avg:98.39ms
step:1324/1770 train_time:129294ms step_avg:98.40ms
step:1325/1770 train_time:129397ms step_avg:98.40ms
step:1326/1770 train_time:129500ms step_avg:98.40ms
step:1327/1770 train_time:129605ms step_avg:98.41ms
step:1328/1770 train_time:129707ms step_avg:98.41ms
step:1329/1770 train_time:129810ms step_avg:98.42ms
step:1330/1770 train_time:129913ms step_avg:98.42ms
step:1331/1770 train_time:130016ms step_avg:98.42ms
step:1332/1770 train_time:130117ms step_avg:98.42ms
step:1333/1770 train_time:130218ms step_avg:98.43ms
step:1334/1770 train_time:130320ms step_avg:98.43ms
step:1335/1770 train_time:130422ms step_avg:98.43ms
step:1336/1770 train_time:130524ms step_avg:98.43ms
step:1337/1770 train_time:130627ms step_avg:98.44ms
step:1338/1770 train_time:130728ms step_avg:98.44ms
step:1339/1770 train_time:130831ms step_avg:98.44ms
step:1340/1770 train_time:130935ms step_avg:98.45ms
step:1341/1770 train_time:131037ms step_avg:98.45ms
step:1342/1770 train_time:131140ms step_avg:98.45ms
step:1343/1770 train_time:131243ms step_avg:98.46ms
step:1344/1770 train_time:131347ms step_avg:98.46ms
step:1345/1770 train_time:131449ms step_avg:98.46ms
step:1346/1770 train_time:131551ms step_avg:98.47ms
step:1347/1770 train_time:131654ms step_avg:98.47ms
step:1348/1770 train_time:131759ms step_avg:98.47ms
step:1349/1770 train_time:131861ms step_avg:98.48ms
step:1350/1770 train_time:131963ms step_avg:98.48ms
step:1351/1770 train_time:132066ms step_avg:98.48ms
step:1352/1770 train_time:132169ms step_avg:98.49ms
step:1353/1770 train_time:132272ms step_avg:98.49ms
step:1354/1770 train_time:132374ms step_avg:98.49ms
step:1355/1770 train_time:132476ms step_avg:98.50ms
step:1356/1770 train_time:132578ms step_avg:98.50ms
step:1357/1770 train_time:132679ms step_avg:98.50ms
step:1358/1770 train_time:132782ms step_avg:98.50ms
step:1359/1770 train_time:132885ms step_avg:98.51ms
step:1360/1770 train_time:132987ms step_avg:98.51ms
step:1361/1770 train_time:133090ms step_avg:98.51ms
step:1362/1770 train_time:133192ms step_avg:98.51ms
step:1363/1770 train_time:133295ms step_avg:98.52ms
step:1364/1770 train_time:133398ms step_avg:98.52ms
step:1365/1770 train_time:133500ms step_avg:98.52ms
step:1366/1770 train_time:133602ms step_avg:98.53ms
step:1367/1770 train_time:133704ms step_avg:98.53ms
step:1368/1770 train_time:133806ms step_avg:98.53ms
step:1369/1770 train_time:133909ms step_avg:98.53ms
step:1370/1770 train_time:134011ms step_avg:98.54ms
step:1371/1770 train_time:134114ms step_avg:98.54ms
step:1372/1770 train_time:134216ms step_avg:98.54ms
step:1373/1770 train_time:134318ms step_avg:98.55ms
step:1374/1770 train_time:134421ms step_avg:98.55ms
step:1375/1770 train_time:134523ms step_avg:98.55ms
step:1375/1770 val_loss:3.3808 train_time:134624ms step_avg:98.63ms
step:1376/1770 train_time:134645ms step_avg:98.57ms
step:1377/1770 train_time:134736ms step_avg:98.56ms
step:1378/1770 train_time:134839ms step_avg:98.57ms
step:1379/1770 train_time:134942ms step_avg:98.57ms
step:1380/1770 train_time:135043ms step_avg:98.57ms
step:1381/1770 train_time:135145ms step_avg:98.57ms
step:1382/1770 train_time:135246ms step_avg:98.58ms
step:1383/1770 train_time:135349ms step_avg:98.58ms
step:1384/1770 train_time:135452ms step_avg:98.58ms
step:1385/1770 train_time:135553ms step_avg:98.58ms
step:1386/1770 train_time:135658ms step_avg:98.59ms
step:1387/1770 train_time:135761ms step_avg:98.59ms
step:1388/1770 train_time:135863ms step_avg:98.59ms
step:1389/1770 train_time:135965ms step_avg:98.60ms
step:1390/1770 train_time:136067ms step_avg:98.60ms
step:1391/1770 train_time:136169ms step_avg:98.60ms
step:1392/1770 train_time:136271ms step_avg:98.60ms
step:1393/1770 train_time:136373ms step_avg:98.61ms
step:1394/1770 train_time:136475ms step_avg:98.61ms
step:1395/1770 train_time:136578ms step_avg:98.61ms
step:1396/1770 train_time:136682ms step_avg:98.62ms
step:1397/1770 train_time:136784ms step_avg:98.62ms
step:1398/1770 train_time:136887ms step_avg:98.62ms
step:1399/1770 train_time:136989ms step_avg:98.62ms
step:1400/1770 train_time:137091ms step_avg:98.63ms
step:1401/1770 train_time:137193ms step_avg:98.63ms
step:1402/1770 train_time:137295ms step_avg:98.63ms
step:1403/1770 train_time:137397ms step_avg:98.63ms
step:1404/1770 train_time:137500ms step_avg:98.64ms
step:1405/1770 train_time:137603ms step_avg:98.64ms
step:1406/1770 train_time:137706ms step_avg:98.64ms
step:1407/1770 train_time:137808ms step_avg:98.65ms
step:1408/1770 train_time:137910ms step_avg:98.65ms
step:1409/1770 train_time:138013ms step_avg:98.65ms
step:1410/1770 train_time:138115ms step_avg:98.65ms
step:1411/1770 train_time:138218ms step_avg:98.66ms
step:1412/1770 train_time:138320ms step_avg:98.66ms
step:1413/1770 train_time:138422ms step_avg:98.66ms
step:1414/1770 train_time:138525ms step_avg:98.66ms
step:1415/1770 train_time:138627ms step_avg:98.67ms
step:1416/1770 train_time:138730ms step_avg:98.67ms
step:1417/1770 train_time:138832ms step_avg:98.67ms
step:1418/1770 train_time:138934ms step_avg:98.67ms
step:1419/1770 train_time:139036ms step_avg:98.68ms
step:1420/1770 train_time:139139ms step_avg:98.68ms
step:1421/1770 train_time:139241ms step_avg:98.68ms
step:1422/1770 train_time:139343ms step_avg:98.68ms
step:1423/1770 train_time:139445ms step_avg:98.69ms
step:1424/1770 train_time:139547ms step_avg:98.69ms
step:1425/1770 train_time:139649ms step_avg:98.69ms
step:1426/1770 train_time:139752ms step_avg:98.69ms
step:1427/1770 train_time:139854ms step_avg:98.70ms
step:1428/1770 train_time:139957ms step_avg:98.70ms
step:1429/1770 train_time:140059ms step_avg:98.70ms
step:1430/1770 train_time:140160ms step_avg:98.70ms
step:1431/1770 train_time:140263ms step_avg:98.71ms
step:1432/1770 train_time:140366ms step_avg:98.71ms
step:1433/1770 train_time:140468ms step_avg:98.71ms
step:1434/1770 train_time:140569ms step_avg:98.71ms
step:1435/1770 train_time:140671ms step_avg:98.72ms
step:1436/1770 train_time:140775ms step_avg:98.72ms
step:1437/1770 train_time:140877ms step_avg:98.72ms
step:1438/1770 train_time:140978ms step_avg:98.72ms
step:1439/1770 train_time:141080ms step_avg:98.73ms
step:1440/1770 train_time:141183ms step_avg:98.73ms
step:1441/1770 train_time:141288ms step_avg:98.73ms
step:1442/1770 train_time:141390ms step_avg:98.74ms
step:1443/1770 train_time:141492ms step_avg:98.74ms
step:1444/1770 train_time:141594ms step_avg:98.74ms
step:1445/1770 train_time:141696ms step_avg:98.74ms
step:1446/1770 train_time:141800ms step_avg:98.75ms
step:1447/1770 train_time:141904ms step_avg:98.75ms
step:1448/1770 train_time:142007ms step_avg:98.75ms
step:1449/1770 train_time:142111ms step_avg:98.76ms
step:1450/1770 train_time:142213ms step_avg:98.76ms
step:1451/1770 train_time:142317ms step_avg:98.76ms
step:1452/1770 train_time:142421ms step_avg:98.77ms
step:1453/1770 train_time:142523ms step_avg:98.77ms
step:1454/1770 train_time:142626ms step_avg:98.77ms
step:1455/1770 train_time:142730ms step_avg:98.78ms
step:1456/1770 train_time:142835ms step_avg:98.78ms
step:1457/1770 train_time:142939ms step_avg:98.78ms
step:1458/1770 train_time:143042ms step_avg:98.79ms
step:1459/1770 train_time:143148ms step_avg:98.79ms
step:1460/1770 train_time:143251ms step_avg:98.79ms
step:1461/1770 train_time:143355ms step_avg:98.80ms
step:1462/1770 train_time:143458ms step_avg:98.80ms
step:1463/1770 train_time:143562ms step_avg:98.80ms
step:1464/1770 train_time:143667ms step_avg:98.81ms
step:1465/1770 train_time:143769ms step_avg:98.81ms
step:1466/1770 train_time:143873ms step_avg:98.81ms
step:1467/1770 train_time:143978ms step_avg:98.82ms
step:1468/1770 train_time:144081ms step_avg:98.82ms
step:1469/1770 train_time:144184ms step_avg:98.82ms
step:1470/1770 train_time:144287ms step_avg:98.83ms
step:1471/1770 train_time:144390ms step_avg:98.83ms
step:1472/1770 train_time:144493ms step_avg:98.83ms
step:1473/1770 train_time:144596ms step_avg:98.84ms
step:1474/1770 train_time:144701ms step_avg:98.84ms
step:1475/1770 train_time:144804ms step_avg:98.84ms
step:1476/1770 train_time:144908ms step_avg:98.85ms
step:1477/1770 train_time:145013ms step_avg:98.85ms
step:1478/1770 train_time:145117ms step_avg:98.85ms
step:1479/1770 train_time:145221ms step_avg:98.86ms
step:1480/1770 train_time:145324ms step_avg:98.86ms
step:1481/1770 train_time:145432ms step_avg:98.87ms
step:1482/1770 train_time:145535ms step_avg:98.87ms
step:1483/1770 train_time:145638ms step_avg:98.87ms
step:1484/1770 train_time:145742ms step_avg:98.87ms
step:1485/1770 train_time:145844ms step_avg:98.88ms
step:1486/1770 train_time:145948ms step_avg:98.88ms
step:1487/1770 train_time:146051ms step_avg:98.88ms
step:1488/1770 train_time:146155ms step_avg:98.89ms
step:1489/1770 train_time:146260ms step_avg:98.89ms
step:1490/1770 train_time:146364ms step_avg:98.89ms
step:1491/1770 train_time:146466ms step_avg:98.90ms
step:1492/1770 train_time:146571ms step_avg:98.90ms
step:1493/1770 train_time:146678ms step_avg:98.91ms
step:1494/1770 train_time:146785ms step_avg:98.91ms
step:1495/1770 train_time:146887ms step_avg:98.91ms
step:1496/1770 train_time:146990ms step_avg:98.92ms
step:1497/1770 train_time:147093ms step_avg:98.92ms
step:1498/1770 train_time:147195ms step_avg:98.92ms
step:1499/1770 train_time:147298ms step_avg:98.92ms
step:1500/1770 train_time:147402ms step_avg:98.93ms
step:1500/1770 val_loss:3.3431 train_time:147502ms step_avg:98.99ms
step:1501/1770 train_time:147524ms step_avg:98.94ms
step:1502/1770 train_time:147614ms step_avg:98.94ms
step:1503/1770 train_time:147718ms step_avg:98.94ms
step:1504/1770 train_time:147822ms step_avg:98.94ms
step:1505/1770 train_time:147927ms step_avg:98.95ms
step:1506/1770 train_time:148030ms step_avg:98.95ms
step:1507/1770 train_time:148134ms step_avg:98.95ms
step:1508/1770 train_time:148239ms step_avg:98.96ms
step:1509/1770 train_time:148342ms step_avg:98.96ms
step:1510/1770 train_time:148444ms step_avg:98.96ms
step:1511/1770 train_time:148549ms step_avg:98.97ms
step:1512/1770 train_time:148654ms step_avg:98.97ms
step:1513/1770 train_time:148759ms step_avg:98.97ms
step:1514/1770 train_time:148862ms step_avg:98.98ms
step:1515/1770 train_time:148965ms step_avg:98.98ms
step:1516/1770 train_time:149069ms step_avg:98.98ms
step:1517/1770 train_time:149172ms step_avg:98.99ms
step:1518/1770 train_time:149278ms step_avg:98.99ms
step:1519/1770 train_time:149380ms step_avg:98.99ms
step:1520/1770 train_time:149485ms step_avg:99.00ms
step:1521/1770 train_time:149588ms step_avg:99.00ms
step:1522/1770 train_time:149691ms step_avg:99.00ms
step:1523/1770 train_time:149795ms step_avg:99.01ms
step:1524/1770 train_time:149898ms step_avg:99.01ms
step:1525/1770 train_time:150001ms step_avg:99.01ms
step:1526/1770 train_time:150104ms step_avg:99.01ms
step:1527/1770 train_time:150207ms step_avg:99.02ms
step:1528/1770 train_time:150313ms step_avg:99.02ms
step:1529/1770 train_time:150415ms step_avg:99.02ms
step:1530/1770 train_time:150520ms step_avg:99.03ms
step:1531/1770 train_time:150622ms step_avg:99.03ms
step:1532/1770 train_time:150726ms step_avg:99.03ms
step:1533/1770 train_time:150829ms step_avg:99.03ms
step:1534/1770 train_time:150934ms step_avg:99.04ms
step:1535/1770 train_time:151037ms step_avg:99.04ms
step:1536/1770 train_time:151140ms step_avg:99.04ms
step:1537/1770 train_time:151244ms step_avg:99.05ms
step:1538/1770 train_time:151348ms step_avg:99.05ms
step:1539/1770 train_time:151452ms step_avg:99.05ms
step:1540/1770 train_time:151558ms step_avg:99.06ms
step:1541/1770 train_time:151663ms step_avg:99.06ms
step:1542/1770 train_time:151766ms step_avg:99.06ms
step:1543/1770 train_time:151869ms step_avg:99.07ms
step:1544/1770 train_time:151974ms step_avg:99.07ms
step:1545/1770 train_time:152078ms step_avg:99.07ms
step:1546/1770 train_time:152181ms step_avg:99.08ms
step:1547/1770 train_time:152284ms step_avg:99.08ms
step:1548/1770 train_time:152387ms step_avg:99.08ms
step:1549/1770 train_time:152492ms step_avg:99.08ms
step:1550/1770 train_time:152596ms step_avg:99.09ms
step:1551/1770 train_time:152700ms step_avg:99.09ms
step:1552/1770 train_time:152805ms step_avg:99.10ms
step:1553/1770 train_time:152909ms step_avg:99.10ms
step:1554/1770 train_time:153011ms step_avg:99.10ms
step:1555/1770 train_time:153115ms step_avg:99.10ms
step:1556/1770 train_time:153218ms step_avg:99.11ms
step:1557/1770 train_time:153321ms step_avg:99.11ms
step:1558/1770 train_time:153425ms step_avg:99.11ms
step:1559/1770 train_time:153528ms step_avg:99.11ms
step:1560/1770 train_time:153631ms step_avg:99.12ms
step:1561/1770 train_time:153736ms step_avg:99.12ms
step:1562/1770 train_time:153839ms step_avg:99.12ms
step:1563/1770 train_time:153942ms step_avg:99.13ms
step:1564/1770 train_time:154045ms step_avg:99.13ms
step:1565/1770 train_time:154148ms step_avg:99.13ms
step:1566/1770 train_time:154252ms step_avg:99.13ms
step:1567/1770 train_time:154356ms step_avg:99.14ms
step:1568/1770 train_time:154458ms step_avg:99.14ms
step:1569/1770 train_time:154565ms step_avg:99.14ms
step:1570/1770 train_time:154668ms step_avg:99.15ms
step:1571/1770 train_time:154772ms step_avg:99.15ms
step:1572/1770 train_time:154877ms step_avg:99.15ms
step:1573/1770 train_time:154982ms step_avg:99.16ms
step:1574/1770 train_time:155085ms step_avg:99.16ms
step:1575/1770 train_time:155187ms step_avg:99.16ms
step:1576/1770 train_time:155290ms step_avg:99.16ms
step:1577/1770 train_time:155395ms step_avg:99.17ms
step:1578/1770 train_time:155502ms step_avg:99.17ms
step:1579/1770 train_time:155605ms step_avg:99.17ms
step:1580/1770 train_time:155707ms step_avg:99.18ms
step:1581/1770 train_time:155813ms step_avg:99.18ms
step:1582/1770 train_time:155917ms step_avg:99.18ms
step:1583/1770 train_time:156021ms step_avg:99.19ms
step:1584/1770 train_time:156125ms step_avg:99.19ms
step:1585/1770 train_time:156228ms step_avg:99.19ms
step:1586/1770 train_time:156337ms step_avg:99.20ms
step:1587/1770 train_time:156440ms step_avg:99.20ms
step:1588/1770 train_time:156544ms step_avg:99.20ms
step:1589/1770 train_time:156648ms step_avg:99.21ms
step:1590/1770 train_time:156752ms step_avg:99.21ms
step:1591/1770 train_time:156855ms step_avg:99.21ms
step:1592/1770 train_time:156959ms step_avg:99.22ms
step:1593/1770 train_time:157063ms step_avg:99.22ms
step:1594/1770 train_time:157166ms step_avg:99.22ms
step:1595/1770 train_time:157270ms step_avg:99.22ms
step:1596/1770 train_time:157374ms step_avg:99.23ms
step:1597/1770 train_time:157477ms step_avg:99.23ms
step:1598/1770 train_time:157579ms step_avg:99.23ms
step:1599/1770 train_time:157684ms step_avg:99.23ms
step:1600/1770 train_time:157790ms step_avg:99.24ms
step:1601/1770 train_time:157895ms step_avg:99.24ms
step:1602/1770 train_time:158000ms step_avg:99.25ms
step:1603/1770 train_time:158103ms step_avg:99.25ms
step:1604/1770 train_time:158205ms step_avg:99.25ms
step:1605/1770 train_time:158308ms step_avg:99.25ms
step:1606/1770 train_time:158411ms step_avg:99.26ms
step:1607/1770 train_time:158519ms step_avg:99.26ms
step:1608/1770 train_time:158622ms step_avg:99.26ms
step:1609/1770 train_time:158725ms step_avg:99.27ms
step:1610/1770 train_time:158830ms step_avg:99.27ms
step:1611/1770 train_time:158936ms step_avg:99.27ms
step:1612/1770 train_time:159040ms step_avg:99.28ms
step:1613/1770 train_time:159144ms step_avg:99.28ms
step:1614/1770 train_time:159247ms step_avg:99.28ms
step:1615/1770 train_time:159350ms step_avg:99.28ms
step:1616/1770 train_time:159454ms step_avg:99.29ms
step:1617/1770 train_time:159559ms step_avg:99.29ms
step:1618/1770 train_time:159663ms step_avg:99.29ms
step:1619/1770 train_time:159767ms step_avg:99.30ms
step:1620/1770 train_time:159871ms step_avg:99.30ms
step:1621/1770 train_time:159975ms step_avg:99.30ms
step:1622/1770 train_time:160080ms step_avg:99.31ms
step:1623/1770 train_time:160186ms step_avg:99.31ms
step:1624/1770 train_time:160289ms step_avg:99.31ms
step:1625/1770 train_time:160391ms step_avg:99.31ms
step:1625/1770 val_loss:3.3089 train_time:160494ms step_avg:99.38ms
step:1626/1770 train_time:160515ms step_avg:99.33ms
step:1627/1770 train_time:160608ms step_avg:99.32ms
step:1628/1770 train_time:160712ms step_avg:99.33ms
step:1629/1770 train_time:160815ms step_avg:99.33ms
step:1630/1770 train_time:160917ms step_avg:99.33ms
step:1631/1770 train_time:161020ms step_avg:99.33ms
step:1632/1770 train_time:161123ms step_avg:99.34ms
step:1633/1770 train_time:161226ms step_avg:99.34ms
step:1634/1770 train_time:161329ms step_avg:99.34ms
step:1635/1770 train_time:161432ms step_avg:99.34ms
step:1636/1770 train_time:161536ms step_avg:99.35ms
step:1637/1770 train_time:161643ms step_avg:99.35ms
step:1638/1770 train_time:161747ms step_avg:99.35ms
step:1639/1770 train_time:161851ms step_avg:99.36ms
step:1640/1770 train_time:161955ms step_avg:99.36ms
step:1641/1770 train_time:162058ms step_avg:99.36ms
step:1642/1770 train_time:162160ms step_avg:99.36ms
step:1643/1770 train_time:162264ms step_avg:99.37ms
step:1644/1770 train_time:162369ms step_avg:99.37ms
step:1645/1770 train_time:162471ms step_avg:99.37ms
step:1646/1770 train_time:162576ms step_avg:99.37ms
step:1647/1770 train_time:162683ms step_avg:99.38ms
step:1648/1770 train_time:162786ms step_avg:99.38ms
step:1649/1770 train_time:162889ms step_avg:99.38ms
step:1650/1770 train_time:162994ms step_avg:99.39ms
step:1651/1770 train_time:163096ms step_avg:99.39ms
step:1652/1770 train_time:163199ms step_avg:99.39ms
step:1653/1770 train_time:163304ms step_avg:99.39ms
step:1654/1770 train_time:163411ms step_avg:99.40ms
step:1655/1770 train_time:163517ms step_avg:99.40ms
step:1656/1770 train_time:163620ms step_avg:99.40ms
step:1657/1770 train_time:163725ms step_avg:99.41ms
step:1658/1770 train_time:163829ms step_avg:99.41ms
step:1659/1770 train_time:163934ms step_avg:99.41ms
step:1660/1770 train_time:164038ms step_avg:99.42ms
step:1661/1770 train_time:164143ms step_avg:99.42ms
step:1662/1770 train_time:164247ms step_avg:99.42ms
step:1663/1770 train_time:164351ms step_avg:99.43ms
step:1664/1770 train_time:164454ms step_avg:99.43ms
step:1665/1770 train_time:164558ms step_avg:99.43ms
step:1666/1770 train_time:164662ms step_avg:99.43ms
step:1667/1770 train_time:164765ms step_avg:99.44ms
step:1668/1770 train_time:164868ms step_avg:99.44ms
step:1669/1770 train_time:164971ms step_avg:99.44ms
step:1670/1770 train_time:165074ms step_avg:99.44ms
step:1671/1770 train_time:165178ms step_avg:99.45ms
step:1672/1770 train_time:165283ms step_avg:99.45ms
step:1673/1770 train_time:165388ms step_avg:99.45ms
step:1674/1770 train_time:165492ms step_avg:99.45ms
step:1675/1770 train_time:165594ms step_avg:99.46ms
step:1676/1770 train_time:165699ms step_avg:99.46ms
step:1677/1770 train_time:165806ms step_avg:99.46ms
step:1678/1770 train_time:165909ms step_avg:99.47ms
step:1679/1770 train_time:166012ms step_avg:99.47ms
step:1680/1770 train_time:166115ms step_avg:99.47ms
step:1681/1770 train_time:166221ms step_avg:99.47ms
step:1682/1770 train_time:166326ms step_avg:99.48ms
step:1683/1770 train_time:166429ms step_avg:99.48ms
step:1684/1770 train_time:166533ms step_avg:99.48ms
step:1685/1770 train_time:166636ms step_avg:99.48ms
step:1686/1770 train_time:166741ms step_avg:99.49ms
step:1687/1770 train_time:166845ms step_avg:99.49ms
step:1688/1770 train_time:166948ms step_avg:99.49ms
step:1689/1770 train_time:167052ms step_avg:99.49ms
step:1690/1770 train_time:167155ms step_avg:99.50ms
step:1691/1770 train_time:167258ms step_avg:99.50ms
step:1692/1770 train_time:167362ms step_avg:99.50ms
step:1693/1770 train_time:167467ms step_avg:99.50ms
step:1694/1770 train_time:167570ms step_avg:99.51ms
step:1695/1770 train_time:167674ms step_avg:99.51ms
step:1696/1770 train_time:167779ms step_avg:99.51ms
step:1697/1770 train_time:167884ms step_avg:99.52ms
step:1698/1770 train_time:167988ms step_avg:99.52ms
step:1699/1770 train_time:168092ms step_avg:99.52ms
step:1700/1770 train_time:168195ms step_avg:99.52ms
step:1701/1770 train_time:168298ms step_avg:99.53ms
step:1702/1770 train_time:168401ms step_avg:99.53ms
step:1703/1770 train_time:168504ms step_avg:99.53ms
step:1704/1770 train_time:168607ms step_avg:99.53ms
step:1705/1770 train_time:168710ms step_avg:99.53ms
step:1706/1770 train_time:168813ms step_avg:99.54ms
step:1707/1770 train_time:168917ms step_avg:99.54ms
step:1708/1770 train_time:169022ms step_avg:99.54ms
step:1709/1770 train_time:169127ms step_avg:99.55ms
step:1710/1770 train_time:169234ms step_avg:99.55ms
step:1711/1770 train_time:169339ms step_avg:99.55ms
step:1712/1770 train_time:169444ms step_avg:99.56ms
step:1713/1770 train_time:169547ms step_avg:99.56ms
step:1714/1770 train_time:169651ms step_avg:99.56ms
step:1715/1770 train_time:169755ms step_avg:99.56ms
step:1716/1770 train_time:169860ms step_avg:99.57ms
step:1717/1770 train_time:169964ms step_avg:99.57ms
step:1718/1770 train_time:170069ms step_avg:99.57ms
step:1719/1770 train_time:170174ms step_avg:99.58ms
step:1720/1770 train_time:170279ms step_avg:99.58ms
step:1721/1770 train_time:170383ms step_avg:99.58ms
step:1722/1770 train_time:170490ms step_avg:99.59ms
step:1723/1770 train_time:170595ms step_avg:99.59ms
step:1724/1770 train_time:170701ms step_avg:99.59ms
step:1725/1770 train_time:170807ms step_avg:99.60ms
step:1726/1770 train_time:170913ms step_avg:99.60ms
step:1727/1770 train_time:171017ms step_avg:99.60ms
step:1728/1770 train_time:171123ms step_avg:99.61ms
step:1729/1770 train_time:171227ms step_avg:99.61ms
step:1730/1770 train_time:171332ms step_avg:99.61ms
step:1731/1770 train_time:171438ms step_avg:99.62ms
step:1732/1770 train_time:171542ms step_avg:99.62ms
step:1733/1770 train_time:171647ms step_avg:99.62ms
step:1734/1770 train_time:171751ms step_avg:99.62ms
step:1735/1770 train_time:171857ms step_avg:99.63ms
step:1736/1770 train_time:171960ms step_avg:99.63ms
step:1737/1770 train_time:172064ms step_avg:99.63ms
step:1738/1770 train_time:172168ms step_avg:99.63ms
step:1739/1770 train_time:172273ms step_avg:99.64ms
step:1740/1770 train_time:172377ms step_avg:99.64ms
step:1741/1770 train_time:172484ms step_avg:99.64ms
step:1742/1770 train_time:172591ms step_avg:99.65ms
step:1743/1770 train_time:172696ms step_avg:99.65ms
step:1744/1770 train_time:172802ms step_avg:99.65ms
step:1745/1770 train_time:172905ms step_avg:99.66ms
step:1746/1770 train_time:173012ms step_avg:99.66ms
step:1747/1770 train_time:173115ms step_avg:99.66ms
step:1748/1770 train_time:173221ms step_avg:99.67ms
step:1749/1770 train_time:173326ms step_avg:99.67ms
step:1750/1770 train_time:173430ms step_avg:99.67ms
step:1750/1770 val_loss:3.2819 train_time:173533ms step_avg:99.73ms
step:1751/1770 train_time:173554ms step_avg:99.69ms
step:1752/1770 train_time:173646ms step_avg:99.68ms
step:1753/1770 train_time:173750ms step_avg:99.68ms
step:1754/1770 train_time:173855ms step_avg:99.69ms
step:1755/1770 train_time:173959ms step_avg:99.69ms
step:1756/1770 train_time:174064ms step_avg:99.69ms
step:1757/1770 train_time:174168ms step_avg:99.70ms
step:1758/1770 train_time:174272ms step_avg:99.70ms
step:1759/1770 train_time:174377ms step_avg:99.70ms
step:1760/1770 train_time:174482ms step_avg:99.70ms
step:1761/1770 train_time:174589ms step_avg:99.71ms
step:1762/1770 train_time:174697ms step_avg:99.71ms
step:1763/1770 train_time:174800ms step_avg:99.71ms
step:1764/1770 train_time:174905ms step_avg:99.72ms
step:1765/1770 train_time:175009ms step_avg:99.72ms
step:1766/1770 train_time:175117ms step_avg:99.72ms
step:1767/1770 train_time:175220ms step_avg:99.73ms
step:1768/1770 train_time:175325ms step_avg:99.73ms
step:1769/1770 train_time:175428ms step_avg:99.73ms
step:1770/1770 train_time:175532ms step_avg:99.73ms
step:1770/1770 val_loss:3.2789 train_time:175636ms step_avg:99.79ms
peak memory allocated: 28840 MiB reserved: 32252 MiB
