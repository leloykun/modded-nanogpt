import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 16:20:32 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23679ms step_avg:nanms
step:2/1770 train_time:24200ms step_avg:nanms
step:3/1770 train_time:24312ms step_avg:nanms
step:4/1770 train_time:24405ms step_avg:nanms
step:5/1770 train_time:24499ms step_avg:nanms
step:6/1770 train_time:24592ms step_avg:nanms
step:7/1770 train_time:24686ms step_avg:nanms
step:8/1770 train_time:24780ms step_avg:nanms
step:9/1770 train_time:24874ms step_avg:nanms
step:10/1770 train_time:24968ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:93.89ms
step:14/1770 train_time:376ms step_avg:93.99ms
step:15/1770 train_time:470ms step_avg:94.08ms
step:16/1770 train_time:564ms step_avg:94.01ms
step:17/1770 train_time:658ms step_avg:94.03ms
step:18/1770 train_time:752ms step_avg:94.04ms
step:19/1770 train_time:846ms step_avg:94.01ms
step:20/1770 train_time:940ms step_avg:94.03ms
step:21/1770 train_time:1034ms step_avg:94.03ms
step:22/1770 train_time:1129ms step_avg:94.05ms
step:23/1770 train_time:1223ms step_avg:94.06ms
step:24/1770 train_time:1317ms step_avg:94.06ms
step:25/1770 train_time:1412ms step_avg:94.10ms
step:26/1770 train_time:1506ms step_avg:94.10ms
step:27/1770 train_time:1599ms step_avg:94.08ms
step:28/1770 train_time:1694ms step_avg:94.11ms
step:29/1770 train_time:1788ms step_avg:94.09ms
step:30/1770 train_time:1881ms step_avg:94.07ms
step:31/1770 train_time:1976ms step_avg:94.09ms
step:32/1770 train_time:2070ms step_avg:94.07ms
step:33/1770 train_time:2163ms step_avg:94.06ms
step:34/1770 train_time:2258ms step_avg:94.07ms
step:35/1770 train_time:2352ms step_avg:94.08ms
step:36/1770 train_time:2446ms step_avg:94.08ms
step:37/1770 train_time:2540ms step_avg:94.08ms
step:38/1770 train_time:2634ms step_avg:94.08ms
step:39/1770 train_time:2728ms step_avg:94.08ms
step:40/1770 train_time:2822ms step_avg:94.07ms
step:41/1770 train_time:2916ms step_avg:94.07ms
step:42/1770 train_time:3010ms step_avg:94.07ms
step:43/1770 train_time:3105ms step_avg:94.08ms
step:44/1770 train_time:3199ms step_avg:94.08ms
step:45/1770 train_time:3293ms step_avg:94.09ms
step:46/1770 train_time:3387ms step_avg:94.09ms
step:47/1770 train_time:3482ms step_avg:94.10ms
step:48/1770 train_time:3576ms step_avg:94.10ms
step:49/1770 train_time:3670ms step_avg:94.10ms
step:50/1770 train_time:3764ms step_avg:94.11ms
step:51/1770 train_time:3858ms step_avg:94.11ms
step:52/1770 train_time:3953ms step_avg:94.13ms
step:53/1770 train_time:4047ms step_avg:94.12ms
step:54/1770 train_time:4142ms step_avg:94.13ms
step:55/1770 train_time:4236ms step_avg:94.12ms
step:56/1770 train_time:4330ms step_avg:94.13ms
step:57/1770 train_time:4425ms step_avg:94.14ms
step:58/1770 train_time:4520ms step_avg:94.16ms
step:59/1770 train_time:4614ms step_avg:94.17ms
step:60/1770 train_time:4709ms step_avg:94.17ms
step:61/1770 train_time:4802ms step_avg:94.16ms
step:62/1770 train_time:4897ms step_avg:94.17ms
step:63/1770 train_time:4991ms step_avg:94.17ms
step:64/1770 train_time:5085ms step_avg:94.16ms
step:65/1770 train_time:5179ms step_avg:94.16ms
step:66/1770 train_time:5273ms step_avg:94.17ms
step:67/1770 train_time:5367ms step_avg:94.16ms
step:68/1770 train_time:5462ms step_avg:94.17ms
step:69/1770 train_time:5556ms step_avg:94.17ms
step:70/1770 train_time:5651ms step_avg:94.18ms
step:71/1770 train_time:5745ms step_avg:94.18ms
step:72/1770 train_time:5839ms step_avg:94.18ms
step:73/1770 train_time:5933ms step_avg:94.18ms
step:74/1770 train_time:6027ms step_avg:94.17ms
step:75/1770 train_time:6121ms step_avg:94.17ms
step:76/1770 train_time:6216ms step_avg:94.18ms
step:77/1770 train_time:6310ms step_avg:94.17ms
step:78/1770 train_time:6403ms step_avg:94.16ms
step:79/1770 train_time:6497ms step_avg:94.16ms
step:80/1770 train_time:6591ms step_avg:94.16ms
step:81/1770 train_time:6685ms step_avg:94.16ms
step:82/1770 train_time:6779ms step_avg:94.16ms
step:83/1770 train_time:6874ms step_avg:94.16ms
step:84/1770 train_time:6967ms step_avg:94.15ms
step:85/1770 train_time:7061ms step_avg:94.14ms
step:86/1770 train_time:7155ms step_avg:94.14ms
step:87/1770 train_time:7249ms step_avg:94.14ms
step:88/1770 train_time:7343ms step_avg:94.14ms
step:89/1770 train_time:7437ms step_avg:94.14ms
step:90/1770 train_time:7532ms step_avg:94.15ms
step:91/1770 train_time:7625ms step_avg:94.14ms
step:92/1770 train_time:7719ms step_avg:94.13ms
step:93/1770 train_time:7814ms step_avg:94.14ms
step:94/1770 train_time:7908ms step_avg:94.14ms
step:95/1770 train_time:8001ms step_avg:94.14ms
step:96/1770 train_time:8096ms step_avg:94.14ms
step:97/1770 train_time:8190ms step_avg:94.14ms
step:98/1770 train_time:8284ms step_avg:94.14ms
step:99/1770 train_time:8378ms step_avg:94.14ms
step:100/1770 train_time:8473ms step_avg:94.14ms
step:101/1770 train_time:8567ms step_avg:94.14ms
step:102/1770 train_time:8661ms step_avg:94.14ms
step:103/1770 train_time:8755ms step_avg:94.14ms
step:104/1770 train_time:8850ms step_avg:94.15ms
step:105/1770 train_time:8944ms step_avg:94.15ms
step:106/1770 train_time:9038ms step_avg:94.15ms
step:107/1770 train_time:9132ms step_avg:94.14ms
step:108/1770 train_time:9226ms step_avg:94.14ms
step:109/1770 train_time:9320ms step_avg:94.14ms
step:110/1770 train_time:9413ms step_avg:94.13ms
step:111/1770 train_time:9507ms step_avg:94.13ms
step:112/1770 train_time:9601ms step_avg:94.13ms
step:113/1770 train_time:9695ms step_avg:94.13ms
step:114/1770 train_time:9789ms step_avg:94.13ms
step:115/1770 train_time:9883ms step_avg:94.13ms
step:116/1770 train_time:9978ms step_avg:94.13ms
step:117/1770 train_time:10073ms step_avg:94.14ms
step:118/1770 train_time:10166ms step_avg:94.13ms
step:119/1770 train_time:10260ms step_avg:94.13ms
step:120/1770 train_time:10354ms step_avg:94.13ms
step:121/1770 train_time:10448ms step_avg:94.13ms
step:122/1770 train_time:10543ms step_avg:94.13ms
step:123/1770 train_time:10637ms step_avg:94.13ms
step:124/1770 train_time:10731ms step_avg:94.13ms
step:125/1770 train_time:10824ms step_avg:94.13ms
step:125/1770 val_loss:4.6498 train_time:10917ms step_avg:94.93ms
step:126/1770 train_time:10940ms step_avg:94.31ms
step:127/1770 train_time:11026ms step_avg:94.24ms
step:128/1770 train_time:11126ms step_avg:94.29ms
step:129/1770 train_time:11220ms step_avg:94.29ms
step:130/1770 train_time:11314ms step_avg:94.29ms
step:131/1770 train_time:11408ms step_avg:94.28ms
step:132/1770 train_time:11501ms step_avg:94.27ms
step:133/1770 train_time:11595ms step_avg:94.27ms
step:134/1770 train_time:11689ms step_avg:94.27ms
step:135/1770 train_time:11783ms step_avg:94.27ms
step:136/1770 train_time:11877ms step_avg:94.27ms
step:137/1770 train_time:11972ms step_avg:94.27ms
step:138/1770 train_time:12067ms step_avg:94.27ms
step:139/1770 train_time:12162ms step_avg:94.28ms
step:140/1770 train_time:12256ms step_avg:94.28ms
step:141/1770 train_time:12351ms step_avg:94.29ms
step:142/1770 train_time:12446ms step_avg:94.29ms
step:143/1770 train_time:12540ms step_avg:94.28ms
step:144/1770 train_time:12635ms step_avg:94.29ms
step:145/1770 train_time:12730ms step_avg:94.29ms
step:146/1770 train_time:12824ms step_avg:94.29ms
step:147/1770 train_time:12918ms step_avg:94.29ms
step:148/1770 train_time:13013ms step_avg:94.30ms
step:149/1770 train_time:13108ms step_avg:94.30ms
step:150/1770 train_time:13202ms step_avg:94.30ms
step:151/1770 train_time:13298ms step_avg:94.31ms
step:152/1770 train_time:13393ms step_avg:94.31ms
step:153/1770 train_time:13488ms step_avg:94.32ms
step:154/1770 train_time:13583ms step_avg:94.33ms
step:155/1770 train_time:13678ms step_avg:94.33ms
step:156/1770 train_time:13773ms step_avg:94.33ms
step:157/1770 train_time:13867ms step_avg:94.34ms
step:158/1770 train_time:13961ms step_avg:94.33ms
step:159/1770 train_time:14056ms step_avg:94.34ms
step:160/1770 train_time:14151ms step_avg:94.34ms
step:161/1770 train_time:14246ms step_avg:94.34ms
step:162/1770 train_time:14341ms step_avg:94.35ms
step:163/1770 train_time:14435ms step_avg:94.35ms
step:164/1770 train_time:14530ms step_avg:94.35ms
step:165/1770 train_time:14625ms step_avg:94.35ms
step:166/1770 train_time:14720ms step_avg:94.36ms
step:167/1770 train_time:14814ms step_avg:94.36ms
step:168/1770 train_time:14909ms step_avg:94.36ms
step:169/1770 train_time:15004ms step_avg:94.36ms
step:170/1770 train_time:15098ms step_avg:94.36ms
step:171/1770 train_time:15193ms step_avg:94.37ms
step:172/1770 train_time:15288ms step_avg:94.37ms
step:173/1770 train_time:15382ms step_avg:94.37ms
step:174/1770 train_time:15477ms step_avg:94.37ms
step:175/1770 train_time:15572ms step_avg:94.37ms
step:176/1770 train_time:15666ms step_avg:94.38ms
step:177/1770 train_time:15762ms step_avg:94.38ms
step:178/1770 train_time:15857ms step_avg:94.39ms
step:179/1770 train_time:15952ms step_avg:94.39ms
step:180/1770 train_time:16047ms step_avg:94.39ms
step:181/1770 train_time:16142ms step_avg:94.39ms
step:182/1770 train_time:16236ms step_avg:94.40ms
step:183/1770 train_time:16332ms step_avg:94.40ms
step:184/1770 train_time:16426ms step_avg:94.40ms
step:185/1770 train_time:16520ms step_avg:94.40ms
step:186/1770 train_time:16615ms step_avg:94.40ms
step:187/1770 train_time:16710ms step_avg:94.41ms
step:188/1770 train_time:16805ms step_avg:94.41ms
step:189/1770 train_time:16899ms step_avg:94.41ms
step:190/1770 train_time:16994ms step_avg:94.41ms
step:191/1770 train_time:17090ms step_avg:94.42ms
step:192/1770 train_time:17185ms step_avg:94.42ms
step:193/1770 train_time:17280ms step_avg:94.42ms
step:194/1770 train_time:17374ms step_avg:94.42ms
step:195/1770 train_time:17469ms step_avg:94.43ms
step:196/1770 train_time:17564ms step_avg:94.43ms
step:197/1770 train_time:17658ms step_avg:94.43ms
step:198/1770 train_time:17753ms step_avg:94.43ms
step:199/1770 train_time:17848ms step_avg:94.43ms
step:200/1770 train_time:17942ms step_avg:94.43ms
step:201/1770 train_time:18037ms step_avg:94.44ms
step:202/1770 train_time:18132ms step_avg:94.44ms
step:203/1770 train_time:18226ms step_avg:94.43ms
step:204/1770 train_time:18320ms step_avg:94.43ms
step:205/1770 train_time:18415ms step_avg:94.44ms
step:206/1770 train_time:18510ms step_avg:94.44ms
step:207/1770 train_time:18604ms step_avg:94.44ms
step:208/1770 train_time:18699ms step_avg:94.44ms
step:209/1770 train_time:18794ms step_avg:94.44ms
step:210/1770 train_time:18889ms step_avg:94.44ms
step:211/1770 train_time:18984ms step_avg:94.45ms
step:212/1770 train_time:19078ms step_avg:94.45ms
step:213/1770 train_time:19174ms step_avg:94.45ms
step:214/1770 train_time:19268ms step_avg:94.45ms
step:215/1770 train_time:19363ms step_avg:94.45ms
step:216/1770 train_time:19458ms step_avg:94.46ms
step:217/1770 train_time:19553ms step_avg:94.46ms
step:218/1770 train_time:19647ms step_avg:94.46ms
step:219/1770 train_time:19742ms step_avg:94.46ms
step:220/1770 train_time:19837ms step_avg:94.46ms
step:221/1770 train_time:19931ms step_avg:94.46ms
step:222/1770 train_time:20026ms step_avg:94.46ms
step:223/1770 train_time:20120ms step_avg:94.46ms
step:224/1770 train_time:20215ms step_avg:94.46ms
step:225/1770 train_time:20310ms step_avg:94.47ms
step:226/1770 train_time:20404ms step_avg:94.46ms
step:227/1770 train_time:20499ms step_avg:94.46ms
step:228/1770 train_time:20594ms step_avg:94.47ms
step:229/1770 train_time:20689ms step_avg:94.47ms
step:230/1770 train_time:20783ms step_avg:94.47ms
step:231/1770 train_time:20877ms step_avg:94.47ms
step:232/1770 train_time:20972ms step_avg:94.47ms
step:233/1770 train_time:21066ms step_avg:94.47ms
step:234/1770 train_time:21161ms step_avg:94.47ms
step:235/1770 train_time:21255ms step_avg:94.47ms
step:236/1770 train_time:21350ms step_avg:94.47ms
step:237/1770 train_time:21444ms step_avg:94.47ms
step:238/1770 train_time:21539ms step_avg:94.47ms
step:239/1770 train_time:21633ms step_avg:94.47ms
step:240/1770 train_time:21728ms step_avg:94.47ms
step:241/1770 train_time:21823ms step_avg:94.47ms
step:242/1770 train_time:21918ms step_avg:94.47ms
step:243/1770 train_time:22012ms step_avg:94.47ms
step:244/1770 train_time:22106ms step_avg:94.47ms
step:245/1770 train_time:22201ms step_avg:94.47ms
step:246/1770 train_time:22295ms step_avg:94.47ms
step:247/1770 train_time:22390ms step_avg:94.47ms
step:248/1770 train_time:22484ms step_avg:94.47ms
step:249/1770 train_time:22579ms step_avg:94.47ms
step:250/1770 train_time:22674ms step_avg:94.48ms
step:250/1770 val_loss:4.1168 train_time:22768ms step_avg:94.87ms
step:251/1770 train_time:22789ms step_avg:94.56ms
step:252/1770 train_time:22874ms step_avg:94.52ms
step:253/1770 train_time:22972ms step_avg:94.53ms
step:254/1770 train_time:23066ms step_avg:94.53ms
step:255/1770 train_time:23161ms step_avg:94.53ms
step:256/1770 train_time:23254ms step_avg:94.53ms
step:257/1770 train_time:23348ms step_avg:94.53ms
step:258/1770 train_time:23443ms step_avg:94.53ms
step:259/1770 train_time:23537ms step_avg:94.53ms
step:260/1770 train_time:23631ms step_avg:94.52ms
step:261/1770 train_time:23726ms step_avg:94.52ms
step:262/1770 train_time:23820ms step_avg:94.52ms
step:263/1770 train_time:23916ms step_avg:94.53ms
step:264/1770 train_time:24011ms step_avg:94.53ms
step:265/1770 train_time:24107ms step_avg:94.54ms
step:266/1770 train_time:24202ms step_avg:94.54ms
step:267/1770 train_time:24297ms step_avg:94.54ms
step:268/1770 train_time:24393ms step_avg:94.55ms
step:269/1770 train_time:24488ms step_avg:94.55ms
step:270/1770 train_time:24583ms step_avg:94.55ms
step:271/1770 train_time:24677ms step_avg:94.55ms
step:272/1770 train_time:24772ms step_avg:94.55ms
step:273/1770 train_time:24867ms step_avg:94.55ms
step:274/1770 train_time:24963ms step_avg:94.56ms
step:275/1770 train_time:25058ms step_avg:94.56ms
step:276/1770 train_time:25153ms step_avg:94.56ms
step:277/1770 train_time:25248ms step_avg:94.56ms
step:278/1770 train_time:25343ms step_avg:94.56ms
step:279/1770 train_time:25439ms step_avg:94.57ms
step:280/1770 train_time:25534ms step_avg:94.57ms
step:281/1770 train_time:25629ms step_avg:94.57ms
step:282/1770 train_time:25724ms step_avg:94.57ms
step:283/1770 train_time:25819ms step_avg:94.57ms
step:284/1770 train_time:25914ms step_avg:94.58ms
step:285/1770 train_time:26009ms step_avg:94.58ms
step:286/1770 train_time:26105ms step_avg:94.58ms
step:287/1770 train_time:26200ms step_avg:94.58ms
step:288/1770 train_time:26295ms step_avg:94.59ms
step:289/1770 train_time:26390ms step_avg:94.59ms
step:290/1770 train_time:26485ms step_avg:94.59ms
step:291/1770 train_time:26581ms step_avg:94.59ms
step:292/1770 train_time:26675ms step_avg:94.59ms
step:293/1770 train_time:26770ms step_avg:94.59ms
step:294/1770 train_time:26866ms step_avg:94.60ms
step:295/1770 train_time:26961ms step_avg:94.60ms
step:296/1770 train_time:27056ms step_avg:94.60ms
step:297/1770 train_time:27151ms step_avg:94.60ms
step:298/1770 train_time:27246ms step_avg:94.61ms
step:299/1770 train_time:27342ms step_avg:94.61ms
step:300/1770 train_time:27437ms step_avg:94.61ms
step:301/1770 train_time:27532ms step_avg:94.61ms
step:302/1770 train_time:27628ms step_avg:94.62ms
step:303/1770 train_time:27723ms step_avg:94.62ms
step:304/1770 train_time:27819ms step_avg:94.62ms
step:305/1770 train_time:27914ms step_avg:94.62ms
step:306/1770 train_time:28009ms step_avg:94.63ms
step:307/1770 train_time:28105ms step_avg:94.63ms
step:308/1770 train_time:28200ms step_avg:94.63ms
step:309/1770 train_time:28295ms step_avg:94.63ms
step:310/1770 train_time:28390ms step_avg:94.63ms
step:311/1770 train_time:28485ms step_avg:94.64ms
step:312/1770 train_time:28580ms step_avg:94.64ms
step:313/1770 train_time:28675ms step_avg:94.64ms
step:314/1770 train_time:28770ms step_avg:94.64ms
step:315/1770 train_time:28866ms step_avg:94.64ms
step:316/1770 train_time:28961ms step_avg:94.64ms
step:317/1770 train_time:29056ms step_avg:94.65ms
step:318/1770 train_time:29151ms step_avg:94.65ms
step:319/1770 train_time:29247ms step_avg:94.65ms
step:320/1770 train_time:29343ms step_avg:94.65ms
step:321/1770 train_time:29438ms step_avg:94.65ms
step:322/1770 train_time:29533ms step_avg:94.66ms
step:323/1770 train_time:29628ms step_avg:94.66ms
step:324/1770 train_time:29724ms step_avg:94.66ms
step:325/1770 train_time:29820ms step_avg:94.67ms
step:326/1770 train_time:29914ms step_avg:94.67ms
step:327/1770 train_time:30009ms step_avg:94.67ms
step:328/1770 train_time:30105ms step_avg:94.67ms
step:329/1770 train_time:30200ms step_avg:94.67ms
step:330/1770 train_time:30295ms step_avg:94.67ms
step:331/1770 train_time:30390ms step_avg:94.67ms
step:332/1770 train_time:30485ms step_avg:94.67ms
step:333/1770 train_time:30580ms step_avg:94.68ms
step:334/1770 train_time:30675ms step_avg:94.68ms
step:335/1770 train_time:30770ms step_avg:94.68ms
step:336/1770 train_time:30865ms step_avg:94.68ms
step:337/1770 train_time:30960ms step_avg:94.68ms
step:338/1770 train_time:31055ms step_avg:94.68ms
step:339/1770 train_time:31150ms step_avg:94.68ms
step:340/1770 train_time:31246ms step_avg:94.68ms
step:341/1770 train_time:31341ms step_avg:94.69ms
step:342/1770 train_time:31436ms step_avg:94.69ms
step:343/1770 train_time:31531ms step_avg:94.69ms
step:344/1770 train_time:31626ms step_avg:94.69ms
step:345/1770 train_time:31722ms step_avg:94.69ms
step:346/1770 train_time:31817ms step_avg:94.69ms
step:347/1770 train_time:31912ms step_avg:94.69ms
step:348/1770 train_time:32007ms step_avg:94.70ms
step:349/1770 train_time:32102ms step_avg:94.70ms
step:350/1770 train_time:32197ms step_avg:94.70ms
step:351/1770 train_time:32293ms step_avg:94.70ms
step:352/1770 train_time:32388ms step_avg:94.70ms
step:353/1770 train_time:32483ms step_avg:94.70ms
step:354/1770 train_time:32578ms step_avg:94.70ms
step:355/1770 train_time:32673ms step_avg:94.70ms
step:356/1770 train_time:32768ms step_avg:94.70ms
step:357/1770 train_time:32863ms step_avg:94.71ms
step:358/1770 train_time:32958ms step_avg:94.71ms
step:359/1770 train_time:33054ms step_avg:94.71ms
step:360/1770 train_time:33149ms step_avg:94.71ms
step:361/1770 train_time:33244ms step_avg:94.71ms
step:362/1770 train_time:33339ms step_avg:94.71ms
step:363/1770 train_time:33435ms step_avg:94.72ms
step:364/1770 train_time:33530ms step_avg:94.72ms
step:365/1770 train_time:33626ms step_avg:94.72ms
step:366/1770 train_time:33721ms step_avg:94.72ms
step:367/1770 train_time:33815ms step_avg:94.72ms
step:368/1770 train_time:33910ms step_avg:94.72ms
step:369/1770 train_time:34006ms step_avg:94.73ms
step:370/1770 train_time:34102ms step_avg:94.73ms
step:371/1770 train_time:34197ms step_avg:94.73ms
step:372/1770 train_time:34292ms step_avg:94.73ms
step:373/1770 train_time:34387ms step_avg:94.73ms
step:374/1770 train_time:34482ms step_avg:94.73ms
step:375/1770 train_time:34577ms step_avg:94.73ms
step:375/1770 val_loss:3.9076 train_time:34671ms step_avg:94.99ms
step:376/1770 train_time:34693ms step_avg:94.79ms
step:377/1770 train_time:34779ms step_avg:94.77ms
step:378/1770 train_time:34877ms step_avg:94.78ms
step:379/1770 train_time:34973ms step_avg:94.78ms
step:380/1770 train_time:35067ms step_avg:94.78ms
step:381/1770 train_time:35165ms step_avg:94.78ms
step:382/1770 train_time:35257ms step_avg:94.78ms
step:383/1770 train_time:35352ms step_avg:94.78ms
step:384/1770 train_time:35447ms step_avg:94.78ms
step:385/1770 train_time:35541ms step_avg:94.78ms
step:386/1770 train_time:35636ms step_avg:94.78ms
step:387/1770 train_time:35732ms step_avg:94.78ms
step:388/1770 train_time:35828ms step_avg:94.78ms
step:389/1770 train_time:35925ms step_avg:94.79ms
step:390/1770 train_time:36021ms step_avg:94.79ms
step:391/1770 train_time:36116ms step_avg:94.79ms
step:392/1770 train_time:36211ms step_avg:94.79ms
step:393/1770 train_time:36306ms step_avg:94.79ms
step:394/1770 train_time:36401ms step_avg:94.79ms
step:395/1770 train_time:36495ms step_avg:94.79ms
step:396/1770 train_time:36593ms step_avg:94.80ms
step:397/1770 train_time:36689ms step_avg:94.80ms
step:398/1770 train_time:36786ms step_avg:94.81ms
step:399/1770 train_time:36883ms step_avg:94.82ms
step:400/1770 train_time:36980ms step_avg:94.82ms
step:401/1770 train_time:37078ms step_avg:94.83ms
step:402/1770 train_time:37175ms step_avg:94.83ms
step:403/1770 train_time:37271ms step_avg:94.84ms
step:404/1770 train_time:37369ms step_avg:94.84ms
step:405/1770 train_time:37465ms step_avg:94.85ms
step:406/1770 train_time:37562ms step_avg:94.85ms
step:407/1770 train_time:37659ms step_avg:94.86ms
step:408/1770 train_time:37757ms step_avg:94.87ms
step:409/1770 train_time:37854ms step_avg:94.87ms
step:410/1770 train_time:37951ms step_avg:94.88ms
step:411/1770 train_time:38048ms step_avg:94.88ms
step:412/1770 train_time:38146ms step_avg:94.89ms
step:413/1770 train_time:38243ms step_avg:94.89ms
step:414/1770 train_time:38339ms step_avg:94.90ms
step:415/1770 train_time:38437ms step_avg:94.91ms
step:416/1770 train_time:38533ms step_avg:94.91ms
step:417/1770 train_time:38630ms step_avg:94.91ms
step:418/1770 train_time:38727ms step_avg:94.92ms
step:419/1770 train_time:38825ms step_avg:94.93ms
step:420/1770 train_time:38922ms step_avg:94.93ms
step:421/1770 train_time:39019ms step_avg:94.94ms
step:422/1770 train_time:39117ms step_avg:94.94ms
step:423/1770 train_time:39215ms step_avg:94.95ms
step:424/1770 train_time:39312ms step_avg:94.96ms
step:425/1770 train_time:39409ms step_avg:94.96ms
step:426/1770 train_time:39506ms step_avg:94.97ms
step:427/1770 train_time:39604ms step_avg:94.97ms
step:428/1770 train_time:39701ms step_avg:94.98ms
step:429/1770 train_time:39798ms step_avg:94.98ms
step:430/1770 train_time:39896ms step_avg:94.99ms
step:431/1770 train_time:39992ms step_avg:94.99ms
step:432/1770 train_time:40090ms step_avg:95.00ms
step:433/1770 train_time:40188ms step_avg:95.01ms
step:434/1770 train_time:40286ms step_avg:95.01ms
step:435/1770 train_time:40383ms step_avg:95.02ms
step:436/1770 train_time:40480ms step_avg:95.02ms
step:437/1770 train_time:40577ms step_avg:95.03ms
step:438/1770 train_time:40674ms step_avg:95.03ms
step:439/1770 train_time:40772ms step_avg:95.04ms
step:440/1770 train_time:40868ms step_avg:95.04ms
step:441/1770 train_time:40966ms step_avg:95.05ms
step:442/1770 train_time:41063ms step_avg:95.05ms
step:443/1770 train_time:41160ms step_avg:95.06ms
step:444/1770 train_time:41259ms step_avg:95.07ms
step:445/1770 train_time:41357ms step_avg:95.07ms
step:446/1770 train_time:41454ms step_avg:95.08ms
step:447/1770 train_time:41551ms step_avg:95.08ms
step:448/1770 train_time:41648ms step_avg:95.09ms
step:449/1770 train_time:41745ms step_avg:95.09ms
step:450/1770 train_time:41842ms step_avg:95.10ms
step:451/1770 train_time:41940ms step_avg:95.10ms
step:452/1770 train_time:42037ms step_avg:95.11ms
step:453/1770 train_time:42134ms step_avg:95.11ms
step:454/1770 train_time:42231ms step_avg:95.11ms
step:455/1770 train_time:42327ms step_avg:95.12ms
step:456/1770 train_time:42424ms step_avg:95.12ms
step:457/1770 train_time:42521ms step_avg:95.13ms
step:458/1770 train_time:42619ms step_avg:95.13ms
step:459/1770 train_time:42716ms step_avg:95.14ms
step:460/1770 train_time:42813ms step_avg:95.14ms
step:461/1770 train_time:42910ms step_avg:95.15ms
step:462/1770 train_time:43007ms step_avg:95.15ms
step:463/1770 train_time:43104ms step_avg:95.15ms
step:464/1770 train_time:43201ms step_avg:95.16ms
step:465/1770 train_time:43298ms step_avg:95.16ms
step:466/1770 train_time:43396ms step_avg:95.17ms
step:467/1770 train_time:43493ms step_avg:95.17ms
step:468/1770 train_time:43590ms step_avg:95.18ms
step:469/1770 train_time:43687ms step_avg:95.18ms
step:470/1770 train_time:43784ms step_avg:95.18ms
step:471/1770 train_time:43880ms step_avg:95.18ms
step:472/1770 train_time:43977ms step_avg:95.19ms
step:473/1770 train_time:44074ms step_avg:95.19ms
step:474/1770 train_time:44171ms step_avg:95.20ms
step:475/1770 train_time:44268ms step_avg:95.20ms
step:476/1770 train_time:44366ms step_avg:95.20ms
step:477/1770 train_time:44463ms step_avg:95.21ms
step:478/1770 train_time:44560ms step_avg:95.21ms
step:479/1770 train_time:44657ms step_avg:95.22ms
step:480/1770 train_time:44754ms step_avg:95.22ms
step:481/1770 train_time:44854ms step_avg:95.23ms
step:482/1770 train_time:44947ms step_avg:95.23ms
step:483/1770 train_time:45044ms step_avg:95.23ms
step:484/1770 train_time:45141ms step_avg:95.23ms
step:485/1770 train_time:45239ms step_avg:95.24ms
step:486/1770 train_time:45336ms step_avg:95.24ms
step:487/1770 train_time:45434ms step_avg:95.25ms
step:488/1770 train_time:45531ms step_avg:95.25ms
step:489/1770 train_time:45628ms step_avg:95.26ms
step:490/1770 train_time:45725ms step_avg:95.26ms
step:491/1770 train_time:45822ms step_avg:95.26ms
step:492/1770 train_time:45919ms step_avg:95.27ms
step:493/1770 train_time:46017ms step_avg:95.27ms
step:494/1770 train_time:46114ms step_avg:95.28ms
step:495/1770 train_time:46211ms step_avg:95.28ms
step:496/1770 train_time:46308ms step_avg:95.28ms
step:497/1770 train_time:46405ms step_avg:95.29ms
step:498/1770 train_time:46502ms step_avg:95.29ms
step:499/1770 train_time:46599ms step_avg:95.30ms
step:500/1770 train_time:46697ms step_avg:95.30ms
step:500/1770 val_loss:3.7531 train_time:46792ms step_avg:95.49ms
step:501/1770 train_time:46813ms step_avg:95.34ms
step:502/1770 train_time:46901ms step_avg:95.33ms
step:503/1770 train_time:47001ms step_avg:95.34ms
step:504/1770 train_time:47098ms step_avg:95.34ms
step:505/1770 train_time:47194ms step_avg:95.34ms
step:506/1770 train_time:47291ms step_avg:95.35ms
step:507/1770 train_time:47388ms step_avg:95.35ms
step:508/1770 train_time:47484ms step_avg:95.35ms
step:509/1770 train_time:47581ms step_avg:95.35ms
step:510/1770 train_time:47677ms step_avg:95.35ms
step:511/1770 train_time:47775ms step_avg:95.36ms
step:512/1770 train_time:47873ms step_avg:95.36ms
step:513/1770 train_time:47970ms step_avg:95.37ms
step:514/1770 train_time:48068ms step_avg:95.37ms
step:515/1770 train_time:48164ms step_avg:95.38ms
step:516/1770 train_time:48262ms step_avg:95.38ms
step:517/1770 train_time:48359ms step_avg:95.38ms
step:518/1770 train_time:48456ms step_avg:95.39ms
step:519/1770 train_time:48553ms step_avg:95.39ms
step:520/1770 train_time:48650ms step_avg:95.39ms
step:521/1770 train_time:48747ms step_avg:95.40ms
step:522/1770 train_time:48844ms step_avg:95.40ms
step:523/1770 train_time:48942ms step_avg:95.40ms
step:524/1770 train_time:49039ms step_avg:95.41ms
step:525/1770 train_time:49136ms step_avg:95.41ms
step:526/1770 train_time:49233ms step_avg:95.41ms
step:527/1770 train_time:49331ms step_avg:95.42ms
step:528/1770 train_time:49428ms step_avg:95.42ms
step:529/1770 train_time:49525ms step_avg:95.42ms
step:530/1770 train_time:49622ms step_avg:95.43ms
step:531/1770 train_time:49720ms step_avg:95.43ms
step:532/1770 train_time:49817ms step_avg:95.44ms
step:533/1770 train_time:49915ms step_avg:95.44ms
step:534/1770 train_time:50013ms step_avg:95.44ms
step:535/1770 train_time:50110ms step_avg:95.45ms
step:536/1770 train_time:50207ms step_avg:95.45ms
step:537/1770 train_time:50305ms step_avg:95.45ms
step:538/1770 train_time:50402ms step_avg:95.46ms
step:539/1770 train_time:50499ms step_avg:95.46ms
step:540/1770 train_time:50597ms step_avg:95.47ms
step:541/1770 train_time:50695ms step_avg:95.47ms
step:542/1770 train_time:50793ms step_avg:95.48ms
step:543/1770 train_time:50890ms step_avg:95.48ms
step:544/1770 train_time:50988ms step_avg:95.48ms
step:545/1770 train_time:51085ms step_avg:95.49ms
step:546/1770 train_time:51181ms step_avg:95.49ms
step:547/1770 train_time:51279ms step_avg:95.49ms
step:548/1770 train_time:51376ms step_avg:95.49ms
step:549/1770 train_time:51474ms step_avg:95.50ms
step:550/1770 train_time:51571ms step_avg:95.50ms
step:551/1770 train_time:51668ms step_avg:95.51ms
step:552/1770 train_time:51765ms step_avg:95.51ms
step:553/1770 train_time:51863ms step_avg:95.51ms
step:554/1770 train_time:51961ms step_avg:95.52ms
step:555/1770 train_time:52058ms step_avg:95.52ms
step:556/1770 train_time:52157ms step_avg:95.53ms
step:557/1770 train_time:52254ms step_avg:95.53ms
step:558/1770 train_time:52352ms step_avg:95.53ms
step:559/1770 train_time:52450ms step_avg:95.54ms
step:560/1770 train_time:52547ms step_avg:95.54ms
step:561/1770 train_time:52644ms step_avg:95.54ms
step:562/1770 train_time:52741ms step_avg:95.55ms
step:563/1770 train_time:52839ms step_avg:95.55ms
step:564/1770 train_time:52936ms step_avg:95.55ms
step:565/1770 train_time:53034ms step_avg:95.56ms
step:566/1770 train_time:53133ms step_avg:95.56ms
step:567/1770 train_time:53230ms step_avg:95.57ms
step:568/1770 train_time:53327ms step_avg:95.57ms
step:569/1770 train_time:53424ms step_avg:95.57ms
step:570/1770 train_time:53521ms step_avg:95.57ms
step:571/1770 train_time:53618ms step_avg:95.58ms
step:572/1770 train_time:53716ms step_avg:95.58ms
step:573/1770 train_time:53814ms step_avg:95.58ms
step:574/1770 train_time:53912ms step_avg:95.59ms
step:575/1770 train_time:54009ms step_avg:95.59ms
step:576/1770 train_time:54107ms step_avg:95.60ms
step:577/1770 train_time:54204ms step_avg:95.60ms
step:578/1770 train_time:54302ms step_avg:95.60ms
step:579/1770 train_time:54400ms step_avg:95.61ms
step:580/1770 train_time:54497ms step_avg:95.61ms
step:581/1770 train_time:54594ms step_avg:95.61ms
step:582/1770 train_time:54692ms step_avg:95.62ms
step:583/1770 train_time:54789ms step_avg:95.62ms
step:584/1770 train_time:54887ms step_avg:95.62ms
step:585/1770 train_time:54984ms step_avg:95.62ms
step:586/1770 train_time:55081ms step_avg:95.63ms
step:587/1770 train_time:55179ms step_avg:95.63ms
step:588/1770 train_time:55277ms step_avg:95.64ms
step:589/1770 train_time:55376ms step_avg:95.64ms
step:590/1770 train_time:55474ms step_avg:95.64ms
step:591/1770 train_time:55571ms step_avg:95.65ms
step:592/1770 train_time:55669ms step_avg:95.65ms
step:593/1770 train_time:55766ms step_avg:95.65ms
step:594/1770 train_time:55863ms step_avg:95.66ms
step:595/1770 train_time:55960ms step_avg:95.66ms
step:596/1770 train_time:56058ms step_avg:95.66ms
step:597/1770 train_time:56155ms step_avg:95.66ms
step:598/1770 train_time:56253ms step_avg:95.67ms
step:599/1770 train_time:56351ms step_avg:95.67ms
step:600/1770 train_time:56448ms step_avg:95.68ms
step:601/1770 train_time:56545ms step_avg:95.68ms
step:602/1770 train_time:56643ms step_avg:95.68ms
step:603/1770 train_time:56740ms step_avg:95.68ms
step:604/1770 train_time:56837ms step_avg:95.68ms
step:605/1770 train_time:56935ms step_avg:95.69ms
step:606/1770 train_time:57032ms step_avg:95.69ms
step:607/1770 train_time:57130ms step_avg:95.69ms
step:608/1770 train_time:57227ms step_avg:95.70ms
step:609/1770 train_time:57324ms step_avg:95.70ms
step:610/1770 train_time:57423ms step_avg:95.70ms
step:611/1770 train_time:57521ms step_avg:95.71ms
step:612/1770 train_time:57619ms step_avg:95.71ms
step:613/1770 train_time:57716ms step_avg:95.71ms
step:614/1770 train_time:57813ms step_avg:95.72ms
step:615/1770 train_time:57911ms step_avg:95.72ms
step:616/1770 train_time:58008ms step_avg:95.72ms
step:617/1770 train_time:58106ms step_avg:95.73ms
step:618/1770 train_time:58203ms step_avg:95.73ms
step:619/1770 train_time:58300ms step_avg:95.73ms
step:620/1770 train_time:58398ms step_avg:95.73ms
step:621/1770 train_time:58497ms step_avg:95.74ms
step:622/1770 train_time:58595ms step_avg:95.74ms
step:623/1770 train_time:58693ms step_avg:95.75ms
step:624/1770 train_time:58790ms step_avg:95.75ms
step:625/1770 train_time:58888ms step_avg:95.75ms
step:625/1770 val_loss:3.6686 train_time:58983ms step_avg:95.91ms
step:626/1770 train_time:59004ms step_avg:95.79ms
step:627/1770 train_time:59094ms step_avg:95.78ms
step:628/1770 train_time:59194ms step_avg:95.78ms
step:629/1770 train_time:59293ms step_avg:95.79ms
step:630/1770 train_time:59390ms step_avg:95.79ms
step:631/1770 train_time:59488ms step_avg:95.79ms
step:632/1770 train_time:59585ms step_avg:95.80ms
step:633/1770 train_time:59682ms step_avg:95.80ms
step:634/1770 train_time:59779ms step_avg:95.80ms
step:635/1770 train_time:59876ms step_avg:95.80ms
step:636/1770 train_time:59973ms step_avg:95.80ms
step:637/1770 train_time:60071ms step_avg:95.81ms
step:638/1770 train_time:60169ms step_avg:95.81ms
step:639/1770 train_time:60268ms step_avg:95.82ms
step:640/1770 train_time:60366ms step_avg:95.82ms
step:641/1770 train_time:60463ms step_avg:95.82ms
step:642/1770 train_time:60560ms step_avg:95.82ms
step:643/1770 train_time:60657ms step_avg:95.82ms
step:644/1770 train_time:60754ms step_avg:95.83ms
step:645/1770 train_time:60851ms step_avg:95.83ms
step:646/1770 train_time:60948ms step_avg:95.83ms
step:647/1770 train_time:61052ms step_avg:95.84ms
step:648/1770 train_time:61144ms step_avg:95.84ms
step:649/1770 train_time:61241ms step_avg:95.84ms
step:650/1770 train_time:61338ms step_avg:95.84ms
step:651/1770 train_time:61436ms step_avg:95.84ms
step:652/1770 train_time:61534ms step_avg:95.85ms
step:653/1770 train_time:61631ms step_avg:95.85ms
step:654/1770 train_time:61729ms step_avg:95.85ms
step:655/1770 train_time:61827ms step_avg:95.86ms
step:656/1770 train_time:61924ms step_avg:95.86ms
step:657/1770 train_time:62022ms step_avg:95.86ms
step:658/1770 train_time:62120ms step_avg:95.86ms
step:659/1770 train_time:62219ms step_avg:95.87ms
step:660/1770 train_time:62317ms step_avg:95.87ms
step:661/1770 train_time:62416ms step_avg:95.88ms
step:662/1770 train_time:62516ms step_avg:95.88ms
step:663/1770 train_time:62615ms step_avg:95.89ms
step:664/1770 train_time:62714ms step_avg:95.89ms
step:665/1770 train_time:62813ms step_avg:95.90ms
step:666/1770 train_time:62913ms step_avg:95.90ms
step:667/1770 train_time:63013ms step_avg:95.91ms
step:668/1770 train_time:63111ms step_avg:95.91ms
step:669/1770 train_time:63211ms step_avg:95.92ms
step:670/1770 train_time:63310ms step_avg:95.92ms
step:671/1770 train_time:63409ms step_avg:95.93ms
step:672/1770 train_time:63509ms step_avg:95.94ms
step:673/1770 train_time:63610ms step_avg:95.94ms
step:674/1770 train_time:63710ms step_avg:95.95ms
step:675/1770 train_time:63809ms step_avg:95.95ms
step:676/1770 train_time:63909ms step_avg:95.96ms
step:677/1770 train_time:64008ms step_avg:95.96ms
step:678/1770 train_time:64108ms step_avg:95.97ms
step:679/1770 train_time:64208ms step_avg:95.98ms
step:680/1770 train_time:64307ms step_avg:95.98ms
step:681/1770 train_time:64406ms step_avg:95.99ms
step:682/1770 train_time:64506ms step_avg:95.99ms
step:683/1770 train_time:64606ms step_avg:96.00ms
step:684/1770 train_time:64707ms step_avg:96.00ms
step:685/1770 train_time:64806ms step_avg:96.01ms
step:686/1770 train_time:64906ms step_avg:96.01ms
step:687/1770 train_time:65005ms step_avg:96.02ms
step:688/1770 train_time:65105ms step_avg:96.02ms
step:689/1770 train_time:65204ms step_avg:96.03ms
step:690/1770 train_time:65303ms step_avg:96.03ms
step:691/1770 train_time:65402ms step_avg:96.04ms
step:692/1770 train_time:65501ms step_avg:96.04ms
step:693/1770 train_time:65600ms step_avg:96.05ms
step:694/1770 train_time:65699ms step_avg:96.05ms
step:695/1770 train_time:65798ms step_avg:96.06ms
step:696/1770 train_time:65896ms step_avg:96.06ms
step:697/1770 train_time:65996ms step_avg:96.06ms
step:698/1770 train_time:66097ms step_avg:96.07ms
step:699/1770 train_time:66197ms step_avg:96.08ms
step:700/1770 train_time:66296ms step_avg:96.08ms
step:701/1770 train_time:66395ms step_avg:96.09ms
step:702/1770 train_time:66494ms step_avg:96.09ms
step:703/1770 train_time:66594ms step_avg:96.10ms
step:704/1770 train_time:66693ms step_avg:96.10ms
step:705/1770 train_time:66792ms step_avg:96.10ms
step:706/1770 train_time:66891ms step_avg:96.11ms
step:707/1770 train_time:66990ms step_avg:96.11ms
step:708/1770 train_time:67090ms step_avg:96.12ms
step:709/1770 train_time:67190ms step_avg:96.12ms
step:710/1770 train_time:67290ms step_avg:96.13ms
step:711/1770 train_time:67389ms step_avg:96.13ms
step:712/1770 train_time:67489ms step_avg:96.14ms
step:713/1770 train_time:67589ms step_avg:96.14ms
step:714/1770 train_time:67689ms step_avg:96.15ms
step:715/1770 train_time:67789ms step_avg:96.15ms
step:716/1770 train_time:67888ms step_avg:96.16ms
step:717/1770 train_time:67987ms step_avg:96.16ms
step:718/1770 train_time:68087ms step_avg:96.17ms
step:719/1770 train_time:68187ms step_avg:96.17ms
step:720/1770 train_time:68287ms step_avg:96.18ms
step:721/1770 train_time:68387ms step_avg:96.18ms
step:722/1770 train_time:68487ms step_avg:96.19ms
step:723/1770 train_time:68587ms step_avg:96.19ms
step:724/1770 train_time:68687ms step_avg:96.20ms
step:725/1770 train_time:68787ms step_avg:96.21ms
step:726/1770 train_time:68886ms step_avg:96.21ms
step:727/1770 train_time:68986ms step_avg:96.22ms
step:728/1770 train_time:69086ms step_avg:96.22ms
step:729/1770 train_time:69187ms step_avg:96.23ms
step:730/1770 train_time:69286ms step_avg:96.23ms
step:731/1770 train_time:69387ms step_avg:96.24ms
step:732/1770 train_time:69488ms step_avg:96.24ms
step:733/1770 train_time:69587ms step_avg:96.25ms
step:734/1770 train_time:69687ms step_avg:96.25ms
step:735/1770 train_time:69787ms step_avg:96.26ms
step:736/1770 train_time:69886ms step_avg:96.26ms
step:737/1770 train_time:69985ms step_avg:96.27ms
step:738/1770 train_time:70084ms step_avg:96.27ms
step:739/1770 train_time:70183ms step_avg:96.27ms
step:740/1770 train_time:70281ms step_avg:96.28ms
step:741/1770 train_time:70380ms step_avg:96.28ms
step:742/1770 train_time:70479ms step_avg:96.28ms
step:743/1770 train_time:70578ms step_avg:96.29ms
step:744/1770 train_time:70678ms step_avg:96.29ms
step:745/1770 train_time:70777ms step_avg:96.29ms
step:746/1770 train_time:70876ms step_avg:96.30ms
step:747/1770 train_time:70975ms step_avg:96.30ms
step:748/1770 train_time:71075ms step_avg:96.31ms
step:749/1770 train_time:71175ms step_avg:96.31ms
step:750/1770 train_time:71275ms step_avg:96.32ms
step:750/1770 val_loss:3.6018 train_time:71372ms step_avg:96.45ms
step:751/1770 train_time:71394ms step_avg:96.35ms
step:752/1770 train_time:71484ms step_avg:96.34ms
step:753/1770 train_time:71584ms step_avg:96.34ms
step:754/1770 train_time:71683ms step_avg:96.35ms
step:755/1770 train_time:71781ms step_avg:96.35ms
step:756/1770 train_time:71880ms step_avg:96.35ms
step:757/1770 train_time:71978ms step_avg:96.36ms
step:758/1770 train_time:72077ms step_avg:96.36ms
step:759/1770 train_time:72176ms step_avg:96.36ms
step:760/1770 train_time:72274ms step_avg:96.37ms
step:761/1770 train_time:72374ms step_avg:96.37ms
step:762/1770 train_time:72474ms step_avg:96.38ms
step:763/1770 train_time:72575ms step_avg:96.38ms
step:764/1770 train_time:72674ms step_avg:96.38ms
step:765/1770 train_time:72775ms step_avg:96.39ms
step:766/1770 train_time:72875ms step_avg:96.40ms
step:767/1770 train_time:72976ms step_avg:96.40ms
step:768/1770 train_time:73075ms step_avg:96.40ms
step:769/1770 train_time:73174ms step_avg:96.41ms
step:770/1770 train_time:73273ms step_avg:96.41ms
step:771/1770 train_time:73373ms step_avg:96.42ms
step:772/1770 train_time:73473ms step_avg:96.42ms
step:773/1770 train_time:73573ms step_avg:96.43ms
step:774/1770 train_time:73673ms step_avg:96.43ms
step:775/1770 train_time:73774ms step_avg:96.44ms
step:776/1770 train_time:73874ms step_avg:96.44ms
step:777/1770 train_time:73974ms step_avg:96.45ms
step:778/1770 train_time:74073ms step_avg:96.45ms
step:779/1770 train_time:74173ms step_avg:96.45ms
step:780/1770 train_time:74272ms step_avg:96.46ms
step:781/1770 train_time:74371ms step_avg:96.46ms
step:782/1770 train_time:74470ms step_avg:96.46ms
step:783/1770 train_time:74570ms step_avg:96.47ms
step:784/1770 train_time:74669ms step_avg:96.47ms
step:785/1770 train_time:74769ms step_avg:96.48ms
step:786/1770 train_time:74868ms step_avg:96.48ms
step:787/1770 train_time:74967ms step_avg:96.48ms
step:788/1770 train_time:75066ms step_avg:96.49ms
step:789/1770 train_time:75166ms step_avg:96.49ms
step:790/1770 train_time:75265ms step_avg:96.49ms
step:791/1770 train_time:75365ms step_avg:96.50ms
step:792/1770 train_time:75465ms step_avg:96.50ms
step:793/1770 train_time:75564ms step_avg:96.51ms
step:794/1770 train_time:75664ms step_avg:96.51ms
step:795/1770 train_time:75764ms step_avg:96.51ms
step:796/1770 train_time:75865ms step_avg:96.52ms
step:797/1770 train_time:75965ms step_avg:96.52ms
step:798/1770 train_time:76064ms step_avg:96.53ms
step:799/1770 train_time:76163ms step_avg:96.53ms
step:800/1770 train_time:76263ms step_avg:96.53ms
step:801/1770 train_time:76362ms step_avg:96.54ms
step:802/1770 train_time:76461ms step_avg:96.54ms
step:803/1770 train_time:76561ms step_avg:96.55ms
step:804/1770 train_time:76660ms step_avg:96.55ms
step:805/1770 train_time:76759ms step_avg:96.55ms
step:806/1770 train_time:76858ms step_avg:96.56ms
step:807/1770 train_time:76957ms step_avg:96.56ms
step:808/1770 train_time:77056ms step_avg:96.56ms
step:809/1770 train_time:77155ms step_avg:96.56ms
step:810/1770 train_time:77256ms step_avg:96.57ms
step:811/1770 train_time:77356ms step_avg:96.57ms
step:812/1770 train_time:77457ms step_avg:96.58ms
step:813/1770 train_time:77557ms step_avg:96.58ms
step:814/1770 train_time:77657ms step_avg:96.59ms
step:815/1770 train_time:77757ms step_avg:96.59ms
step:816/1770 train_time:77856ms step_avg:96.60ms
step:817/1770 train_time:77956ms step_avg:96.60ms
step:818/1770 train_time:78055ms step_avg:96.60ms
step:819/1770 train_time:78154ms step_avg:96.61ms
step:820/1770 train_time:78254ms step_avg:96.61ms
step:821/1770 train_time:78354ms step_avg:96.61ms
step:822/1770 train_time:78455ms step_avg:96.62ms
step:823/1770 train_time:78555ms step_avg:96.62ms
step:824/1770 train_time:78655ms step_avg:96.63ms
step:825/1770 train_time:78755ms step_avg:96.63ms
step:826/1770 train_time:78855ms step_avg:96.64ms
step:827/1770 train_time:78955ms step_avg:96.64ms
step:828/1770 train_time:79054ms step_avg:96.64ms
step:829/1770 train_time:79154ms step_avg:96.65ms
step:830/1770 train_time:79253ms step_avg:96.65ms
step:831/1770 train_time:79353ms step_avg:96.65ms
step:832/1770 train_time:79453ms step_avg:96.66ms
step:833/1770 train_time:79554ms step_avg:96.66ms
step:834/1770 train_time:79656ms step_avg:96.67ms
step:835/1770 train_time:79754ms step_avg:96.67ms
step:836/1770 train_time:79854ms step_avg:96.68ms
step:837/1770 train_time:79954ms step_avg:96.68ms
step:838/1770 train_time:80054ms step_avg:96.68ms
step:839/1770 train_time:80154ms step_avg:96.69ms
step:840/1770 train_time:80254ms step_avg:96.69ms
step:841/1770 train_time:80354ms step_avg:96.70ms
step:842/1770 train_time:80453ms step_avg:96.70ms
step:843/1770 train_time:80553ms step_avg:96.70ms
step:844/1770 train_time:80653ms step_avg:96.71ms
step:845/1770 train_time:80754ms step_avg:96.71ms
step:846/1770 train_time:80854ms step_avg:96.72ms
step:847/1770 train_time:80953ms step_avg:96.72ms
step:848/1770 train_time:81054ms step_avg:96.72ms
step:849/1770 train_time:81152ms step_avg:96.72ms
step:850/1770 train_time:81252ms step_avg:96.73ms
step:851/1770 train_time:81352ms step_avg:96.73ms
step:852/1770 train_time:81452ms step_avg:96.74ms
step:853/1770 train_time:81552ms step_avg:96.74ms
step:854/1770 train_time:81651ms step_avg:96.74ms
step:855/1770 train_time:81751ms step_avg:96.75ms
step:856/1770 train_time:81851ms step_avg:96.75ms
step:857/1770 train_time:81951ms step_avg:96.75ms
step:858/1770 train_time:82051ms step_avg:96.76ms
step:859/1770 train_time:82151ms step_avg:96.76ms
step:860/1770 train_time:82250ms step_avg:96.76ms
step:861/1770 train_time:82349ms step_avg:96.77ms
step:862/1770 train_time:82449ms step_avg:96.77ms
step:863/1770 train_time:82549ms step_avg:96.77ms
step:864/1770 train_time:82650ms step_avg:96.78ms
step:865/1770 train_time:82747ms step_avg:96.78ms
step:866/1770 train_time:82847ms step_avg:96.78ms
step:867/1770 train_time:82946ms step_avg:96.79ms
step:868/1770 train_time:83045ms step_avg:96.79ms
step:869/1770 train_time:83144ms step_avg:96.79ms
step:870/1770 train_time:83243ms step_avg:96.79ms
step:871/1770 train_time:83343ms step_avg:96.80ms
step:872/1770 train_time:83442ms step_avg:96.80ms
step:873/1770 train_time:83542ms step_avg:96.80ms
step:874/1770 train_time:83642ms step_avg:96.81ms
step:875/1770 train_time:83742ms step_avg:96.81ms
step:875/1770 val_loss:3.5531 train_time:83839ms step_avg:96.92ms
step:876/1770 train_time:83860ms step_avg:96.84ms
step:877/1770 train_time:83950ms step_avg:96.83ms
step:878/1770 train_time:84050ms step_avg:96.83ms
step:879/1770 train_time:84150ms step_avg:96.84ms
step:880/1770 train_time:84250ms step_avg:96.84ms
step:881/1770 train_time:84349ms step_avg:96.84ms
step:882/1770 train_time:84448ms step_avg:96.84ms
step:883/1770 train_time:84547ms step_avg:96.85ms
step:884/1770 train_time:84646ms step_avg:96.85ms
step:885/1770 train_time:84745ms step_avg:96.85ms
step:886/1770 train_time:84845ms step_avg:96.85ms
step:887/1770 train_time:84947ms step_avg:96.86ms
step:888/1770 train_time:85048ms step_avg:96.87ms
step:889/1770 train_time:85149ms step_avg:96.87ms
step:890/1770 train_time:85250ms step_avg:96.87ms
step:891/1770 train_time:85350ms step_avg:96.88ms
step:892/1770 train_time:85450ms step_avg:96.88ms
step:893/1770 train_time:85549ms step_avg:96.88ms
step:894/1770 train_time:85649ms step_avg:96.89ms
step:895/1770 train_time:85748ms step_avg:96.89ms
step:896/1770 train_time:85848ms step_avg:96.89ms
step:897/1770 train_time:85947ms step_avg:96.90ms
step:898/1770 train_time:86047ms step_avg:96.90ms
step:899/1770 train_time:86148ms step_avg:96.90ms
step:900/1770 train_time:86247ms step_avg:96.91ms
step:901/1770 train_time:86347ms step_avg:96.91ms
step:902/1770 train_time:86447ms step_avg:96.91ms
step:903/1770 train_time:86547ms step_avg:96.92ms
step:904/1770 train_time:86647ms step_avg:96.92ms
step:905/1770 train_time:86747ms step_avg:96.92ms
step:906/1770 train_time:86847ms step_avg:96.93ms
step:907/1770 train_time:86947ms step_avg:96.93ms
step:908/1770 train_time:87047ms step_avg:96.93ms
step:909/1770 train_time:87147ms step_avg:96.94ms
step:910/1770 train_time:87248ms step_avg:96.94ms
step:911/1770 train_time:87349ms step_avg:96.95ms
step:912/1770 train_time:87448ms step_avg:96.95ms
step:913/1770 train_time:87547ms step_avg:96.95ms
step:914/1770 train_time:87647ms step_avg:96.95ms
step:915/1770 train_time:87747ms step_avg:96.96ms
step:916/1770 train_time:87847ms step_avg:96.96ms
step:917/1770 train_time:87946ms step_avg:96.96ms
step:918/1770 train_time:88046ms step_avg:96.97ms
step:919/1770 train_time:88147ms step_avg:96.97ms
step:920/1770 train_time:88249ms step_avg:96.98ms
step:921/1770 train_time:88351ms step_avg:96.98ms
step:922/1770 train_time:88452ms step_avg:96.99ms
step:923/1770 train_time:88552ms step_avg:96.99ms
step:924/1770 train_time:88653ms step_avg:96.99ms
step:925/1770 train_time:88753ms step_avg:97.00ms
step:926/1770 train_time:88854ms step_avg:97.00ms
step:927/1770 train_time:88955ms step_avg:97.01ms
step:928/1770 train_time:89057ms step_avg:97.01ms
step:929/1770 train_time:89158ms step_avg:97.02ms
step:930/1770 train_time:89258ms step_avg:97.02ms
step:931/1770 train_time:89358ms step_avg:97.02ms
step:932/1770 train_time:89459ms step_avg:97.03ms
step:933/1770 train_time:89559ms step_avg:97.03ms
step:934/1770 train_time:89659ms step_avg:97.03ms
step:935/1770 train_time:89760ms step_avg:97.04ms
step:936/1770 train_time:89860ms step_avg:97.04ms
step:937/1770 train_time:89961ms step_avg:97.05ms
step:938/1770 train_time:90063ms step_avg:97.05ms
step:939/1770 train_time:90163ms step_avg:97.05ms
step:940/1770 train_time:90264ms step_avg:97.06ms
step:941/1770 train_time:90366ms step_avg:97.06ms
step:942/1770 train_time:90468ms step_avg:97.07ms
step:943/1770 train_time:90569ms step_avg:97.07ms
step:944/1770 train_time:90670ms step_avg:97.08ms
step:945/1770 train_time:90771ms step_avg:97.08ms
step:946/1770 train_time:90873ms step_avg:97.09ms
step:947/1770 train_time:90974ms step_avg:97.09ms
step:948/1770 train_time:91075ms step_avg:97.10ms
step:949/1770 train_time:91177ms step_avg:97.10ms
step:950/1770 train_time:91279ms step_avg:97.11ms
step:951/1770 train_time:91381ms step_avg:97.11ms
step:952/1770 train_time:91482ms step_avg:97.11ms
step:953/1770 train_time:91582ms step_avg:97.12ms
step:954/1770 train_time:91683ms step_avg:97.12ms
step:955/1770 train_time:91784ms step_avg:97.13ms
step:956/1770 train_time:91885ms step_avg:97.13ms
step:957/1770 train_time:91986ms step_avg:97.13ms
step:958/1770 train_time:92088ms step_avg:97.14ms
step:959/1770 train_time:92190ms step_avg:97.14ms
step:960/1770 train_time:92292ms step_avg:97.15ms
step:961/1770 train_time:92393ms step_avg:97.15ms
step:962/1770 train_time:92494ms step_avg:97.16ms
step:963/1770 train_time:92595ms step_avg:97.16ms
step:964/1770 train_time:92697ms step_avg:97.17ms
step:965/1770 train_time:92799ms step_avg:97.17ms
step:966/1770 train_time:92899ms step_avg:97.17ms
step:967/1770 train_time:93000ms step_avg:97.18ms
step:968/1770 train_time:93101ms step_avg:97.18ms
step:969/1770 train_time:93202ms step_avg:97.19ms
step:970/1770 train_time:93302ms step_avg:97.19ms
step:971/1770 train_time:93402ms step_avg:97.19ms
step:972/1770 train_time:93502ms step_avg:97.20ms
step:973/1770 train_time:93602ms step_avg:97.20ms
step:974/1770 train_time:93703ms step_avg:97.20ms
step:975/1770 train_time:93806ms step_avg:97.21ms
step:976/1770 train_time:93906ms step_avg:97.21ms
step:977/1770 train_time:94008ms step_avg:97.22ms
step:978/1770 train_time:94111ms step_avg:97.22ms
step:979/1770 train_time:94212ms step_avg:97.23ms
step:980/1770 train_time:94313ms step_avg:97.23ms
step:981/1770 train_time:94413ms step_avg:97.23ms
step:982/1770 train_time:94515ms step_avg:97.24ms
step:983/1770 train_time:94618ms step_avg:97.24ms
step:984/1770 train_time:94716ms step_avg:97.24ms
step:985/1770 train_time:94818ms step_avg:97.25ms
step:986/1770 train_time:94918ms step_avg:97.25ms
step:987/1770 train_time:95019ms step_avg:97.26ms
step:988/1770 train_time:95120ms step_avg:97.26ms
step:989/1770 train_time:95222ms step_avg:97.26ms
step:990/1770 train_time:95323ms step_avg:97.27ms
step:991/1770 train_time:95423ms step_avg:97.27ms
step:992/1770 train_time:95523ms step_avg:97.27ms
step:993/1770 train_time:95626ms step_avg:97.28ms
step:994/1770 train_time:95727ms step_avg:97.28ms
step:995/1770 train_time:95829ms step_avg:97.29ms
step:996/1770 train_time:95931ms step_avg:97.29ms
step:997/1770 train_time:96032ms step_avg:97.30ms
step:998/1770 train_time:96134ms step_avg:97.30ms
step:999/1770 train_time:96234ms step_avg:97.30ms
step:1000/1770 train_time:96336ms step_avg:97.31ms
step:1000/1770 val_loss:3.5159 train_time:96435ms step_avg:97.41ms
step:1001/1770 train_time:96456ms step_avg:97.33ms
step:1002/1770 train_time:96549ms step_avg:97.33ms
step:1003/1770 train_time:96653ms step_avg:97.33ms
step:1004/1770 train_time:96754ms step_avg:97.34ms
step:1005/1770 train_time:96854ms step_avg:97.34ms
step:1006/1770 train_time:96955ms step_avg:97.34ms
step:1007/1770 train_time:97055ms step_avg:97.35ms
step:1008/1770 train_time:97156ms step_avg:97.35ms
step:1009/1770 train_time:97256ms step_avg:97.35ms
step:1010/1770 train_time:97355ms step_avg:97.36ms
step:1011/1770 train_time:97457ms step_avg:97.36ms
step:1012/1770 train_time:97559ms step_avg:97.36ms
step:1013/1770 train_time:97663ms step_avg:97.37ms
step:1014/1770 train_time:97764ms step_avg:97.37ms
step:1015/1770 train_time:97864ms step_avg:97.38ms
step:1016/1770 train_time:97964ms step_avg:97.38ms
step:1017/1770 train_time:98064ms step_avg:97.38ms
step:1018/1770 train_time:98164ms step_avg:97.39ms
step:1019/1770 train_time:98265ms step_avg:97.39ms
step:1020/1770 train_time:98366ms step_avg:97.39ms
step:1021/1770 train_time:98467ms step_avg:97.40ms
step:1022/1770 train_time:98568ms step_avg:97.40ms
step:1023/1770 train_time:98669ms step_avg:97.40ms
step:1024/1770 train_time:98771ms step_avg:97.41ms
step:1025/1770 train_time:98872ms step_avg:97.41ms
step:1026/1770 train_time:98975ms step_avg:97.42ms
step:1027/1770 train_time:99075ms step_avg:97.42ms
step:1028/1770 train_time:99179ms step_avg:97.43ms
step:1029/1770 train_time:99276ms step_avg:97.42ms
step:1030/1770 train_time:99376ms step_avg:97.43ms
step:1031/1770 train_time:99478ms step_avg:97.43ms
step:1032/1770 train_time:99579ms step_avg:97.44ms
step:1033/1770 train_time:99681ms step_avg:97.44ms
step:1034/1770 train_time:99782ms step_avg:97.44ms
step:1035/1770 train_time:99882ms step_avg:97.45ms
step:1036/1770 train_time:99983ms step_avg:97.45ms
step:1037/1770 train_time:100084ms step_avg:97.45ms
step:1038/1770 train_time:100184ms step_avg:97.46ms
step:1039/1770 train_time:100284ms step_avg:97.46ms
step:1040/1770 train_time:100385ms step_avg:97.46ms
step:1041/1770 train_time:100485ms step_avg:97.46ms
step:1042/1770 train_time:100586ms step_avg:97.47ms
step:1043/1770 train_time:100688ms step_avg:97.47ms
step:1044/1770 train_time:100789ms step_avg:97.47ms
step:1045/1770 train_time:100890ms step_avg:97.48ms
step:1046/1770 train_time:100992ms step_avg:97.48ms
step:1047/1770 train_time:101093ms step_avg:97.49ms
step:1048/1770 train_time:101194ms step_avg:97.49ms
step:1049/1770 train_time:101296ms step_avg:97.49ms
step:1050/1770 train_time:101396ms step_avg:97.50ms
step:1051/1770 train_time:101497ms step_avg:97.50ms
step:1052/1770 train_time:101598ms step_avg:97.50ms
step:1053/1770 train_time:101699ms step_avg:97.51ms
step:1054/1770 train_time:101800ms step_avg:97.51ms
step:1055/1770 train_time:101900ms step_avg:97.51ms
step:1056/1770 train_time:102001ms step_avg:97.52ms
step:1057/1770 train_time:102102ms step_avg:97.52ms
step:1058/1770 train_time:102203ms step_avg:97.52ms
step:1059/1770 train_time:102304ms step_avg:97.53ms
step:1060/1770 train_time:102405ms step_avg:97.53ms
step:1061/1770 train_time:102506ms step_avg:97.53ms
step:1062/1770 train_time:102608ms step_avg:97.54ms
step:1063/1770 train_time:102710ms step_avg:97.54ms
step:1064/1770 train_time:102812ms step_avg:97.54ms
step:1065/1770 train_time:102914ms step_avg:97.55ms
step:1066/1770 train_time:103015ms step_avg:97.55ms
step:1067/1770 train_time:103116ms step_avg:97.56ms
step:1068/1770 train_time:103218ms step_avg:97.56ms
step:1069/1770 train_time:103320ms step_avg:97.56ms
step:1070/1770 train_time:103422ms step_avg:97.57ms
step:1071/1770 train_time:103523ms step_avg:97.57ms
step:1072/1770 train_time:103623ms step_avg:97.57ms
step:1073/1770 train_time:103724ms step_avg:97.58ms
step:1074/1770 train_time:103825ms step_avg:97.58ms
step:1075/1770 train_time:103926ms step_avg:97.58ms
step:1076/1770 train_time:104027ms step_avg:97.59ms
step:1077/1770 train_time:104128ms step_avg:97.59ms
step:1078/1770 train_time:104230ms step_avg:97.59ms
step:1079/1770 train_time:104332ms step_avg:97.60ms
step:1080/1770 train_time:104434ms step_avg:97.60ms
step:1081/1770 train_time:104535ms step_avg:97.60ms
step:1082/1770 train_time:104637ms step_avg:97.61ms
step:1083/1770 train_time:104738ms step_avg:97.61ms
step:1084/1770 train_time:104839ms step_avg:97.62ms
step:1085/1770 train_time:104940ms step_avg:97.62ms
step:1086/1770 train_time:105041ms step_avg:97.62ms
step:1087/1770 train_time:105143ms step_avg:97.63ms
step:1088/1770 train_time:105244ms step_avg:97.63ms
step:1089/1770 train_time:105346ms step_avg:97.63ms
step:1090/1770 train_time:105447ms step_avg:97.64ms
step:1091/1770 train_time:105547ms step_avg:97.64ms
step:1092/1770 train_time:105648ms step_avg:97.64ms
step:1093/1770 train_time:105749ms step_avg:97.64ms
step:1094/1770 train_time:105852ms step_avg:97.65ms
step:1095/1770 train_time:105955ms step_avg:97.65ms
step:1096/1770 train_time:106056ms step_avg:97.66ms
step:1097/1770 train_time:106157ms step_avg:97.66ms
step:1098/1770 train_time:106257ms step_avg:97.66ms
step:1099/1770 train_time:106358ms step_avg:97.67ms
step:1100/1770 train_time:106460ms step_avg:97.67ms
step:1101/1770 train_time:106560ms step_avg:97.67ms
step:1102/1770 train_time:106662ms step_avg:97.68ms
step:1103/1770 train_time:106763ms step_avg:97.68ms
step:1104/1770 train_time:106864ms step_avg:97.68ms
step:1105/1770 train_time:106965ms step_avg:97.68ms
step:1106/1770 train_time:107066ms step_avg:97.69ms
step:1107/1770 train_time:107166ms step_avg:97.69ms
step:1108/1770 train_time:107269ms step_avg:97.69ms
step:1109/1770 train_time:107370ms step_avg:97.70ms
step:1110/1770 train_time:107472ms step_avg:97.70ms
step:1111/1770 train_time:107573ms step_avg:97.71ms
step:1112/1770 train_time:107675ms step_avg:97.71ms
step:1113/1770 train_time:107776ms step_avg:97.71ms
step:1114/1770 train_time:107881ms step_avg:97.72ms
step:1115/1770 train_time:107978ms step_avg:97.72ms
step:1116/1770 train_time:108080ms step_avg:97.72ms
step:1117/1770 train_time:108181ms step_avg:97.72ms
step:1118/1770 train_time:108282ms step_avg:97.73ms
step:1119/1770 train_time:108383ms step_avg:97.73ms
step:1120/1770 train_time:108484ms step_avg:97.73ms
step:1121/1770 train_time:108584ms step_avg:97.74ms
step:1122/1770 train_time:108684ms step_avg:97.74ms
step:1123/1770 train_time:108785ms step_avg:97.74ms
step:1124/1770 train_time:108886ms step_avg:97.74ms
step:1125/1770 train_time:108987ms step_avg:97.75ms
step:1125/1770 val_loss:3.4728 train_time:109085ms step_avg:97.83ms
step:1126/1770 train_time:109106ms step_avg:97.77ms
step:1127/1770 train_time:109197ms step_avg:97.76ms
step:1128/1770 train_time:109300ms step_avg:97.76ms
step:1129/1770 train_time:109400ms step_avg:97.77ms
step:1130/1770 train_time:109500ms step_avg:97.77ms
step:1131/1770 train_time:109601ms step_avg:97.77ms
step:1132/1770 train_time:109702ms step_avg:97.77ms
step:1133/1770 train_time:109802ms step_avg:97.78ms
step:1134/1770 train_time:109903ms step_avg:97.78ms
step:1135/1770 train_time:110004ms step_avg:97.78ms
step:1136/1770 train_time:110108ms step_avg:97.79ms
step:1137/1770 train_time:110211ms step_avg:97.79ms
step:1138/1770 train_time:110311ms step_avg:97.79ms
step:1139/1770 train_time:110412ms step_avg:97.80ms
step:1140/1770 train_time:110513ms step_avg:97.80ms
step:1141/1770 train_time:110614ms step_avg:97.80ms
step:1142/1770 train_time:110715ms step_avg:97.81ms
step:1143/1770 train_time:110817ms step_avg:97.81ms
step:1144/1770 train_time:110918ms step_avg:97.81ms
step:1145/1770 train_time:111020ms step_avg:97.81ms
step:1146/1770 train_time:111122ms step_avg:97.82ms
step:1147/1770 train_time:111223ms step_avg:97.82ms
step:1148/1770 train_time:111324ms step_avg:97.82ms
step:1149/1770 train_time:111425ms step_avg:97.83ms
step:1150/1770 train_time:111526ms step_avg:97.83ms
step:1151/1770 train_time:111627ms step_avg:97.83ms
step:1152/1770 train_time:111729ms step_avg:97.84ms
step:1153/1770 train_time:111829ms step_avg:97.84ms
step:1154/1770 train_time:111929ms step_avg:97.84ms
step:1155/1770 train_time:112030ms step_avg:97.84ms
step:1156/1770 train_time:112132ms step_avg:97.85ms
step:1157/1770 train_time:112235ms step_avg:97.85ms
step:1158/1770 train_time:112337ms step_avg:97.85ms
step:1159/1770 train_time:112437ms step_avg:97.86ms
step:1160/1770 train_time:112539ms step_avg:97.86ms
step:1161/1770 train_time:112641ms step_avg:97.86ms
step:1162/1770 train_time:112742ms step_avg:97.87ms
step:1163/1770 train_time:112843ms step_avg:97.87ms
step:1164/1770 train_time:112944ms step_avg:97.87ms
step:1165/1770 train_time:113046ms step_avg:97.87ms
step:1166/1770 train_time:113147ms step_avg:97.88ms
step:1167/1770 train_time:113248ms step_avg:97.88ms
step:1168/1770 train_time:113352ms step_avg:97.89ms
step:1169/1770 train_time:113450ms step_avg:97.89ms
step:1170/1770 train_time:113551ms step_avg:97.89ms
step:1171/1770 train_time:113652ms step_avg:97.89ms
step:1172/1770 train_time:113753ms step_avg:97.89ms
step:1173/1770 train_time:113854ms step_avg:97.90ms
step:1174/1770 train_time:113957ms step_avg:97.90ms
step:1175/1770 train_time:114058ms step_avg:97.90ms
step:1176/1770 train_time:114159ms step_avg:97.91ms
step:1177/1770 train_time:114260ms step_avg:97.91ms
step:1178/1770 train_time:114362ms step_avg:97.91ms
step:1179/1770 train_time:114463ms step_avg:97.92ms
step:1180/1770 train_time:114565ms step_avg:97.92ms
step:1181/1770 train_time:114667ms step_avg:97.92ms
step:1182/1770 train_time:114768ms step_avg:97.92ms
step:1183/1770 train_time:114869ms step_avg:97.93ms
step:1184/1770 train_time:114972ms step_avg:97.93ms
step:1185/1770 train_time:115074ms step_avg:97.93ms
step:1186/1770 train_time:115176ms step_avg:97.94ms
step:1187/1770 train_time:115282ms step_avg:97.95ms
step:1188/1770 train_time:115384ms step_avg:97.95ms
step:1189/1770 train_time:115485ms step_avg:97.95ms
step:1190/1770 train_time:115587ms step_avg:97.95ms
step:1191/1770 train_time:115689ms step_avg:97.96ms
step:1192/1770 train_time:115791ms step_avg:97.96ms
step:1193/1770 train_time:115894ms step_avg:97.97ms
step:1194/1770 train_time:115997ms step_avg:97.97ms
step:1195/1770 train_time:116099ms step_avg:97.97ms
step:1196/1770 train_time:116203ms step_avg:97.98ms
step:1197/1770 train_time:116304ms step_avg:97.98ms
step:1198/1770 train_time:116406ms step_avg:97.98ms
step:1199/1770 train_time:116509ms step_avg:97.99ms
step:1200/1770 train_time:116612ms step_avg:97.99ms
step:1201/1770 train_time:116714ms step_avg:98.00ms
step:1202/1770 train_time:116816ms step_avg:98.00ms
step:1203/1770 train_time:116918ms step_avg:98.00ms
step:1204/1770 train_time:117021ms step_avg:98.01ms
step:1205/1770 train_time:117122ms step_avg:98.01ms
step:1206/1770 train_time:117225ms step_avg:98.01ms
step:1207/1770 train_time:117326ms step_avg:98.02ms
step:1208/1770 train_time:117429ms step_avg:98.02ms
step:1209/1770 train_time:117531ms step_avg:98.02ms
step:1210/1770 train_time:117632ms step_avg:98.03ms
step:1211/1770 train_time:117735ms step_avg:98.03ms
step:1212/1770 train_time:117839ms step_avg:98.04ms
step:1213/1770 train_time:117942ms step_avg:98.04ms
step:1214/1770 train_time:118044ms step_avg:98.04ms
step:1215/1770 train_time:118146ms step_avg:98.05ms
step:1216/1770 train_time:118250ms step_avg:98.05ms
step:1217/1770 train_time:118354ms step_avg:98.06ms
step:1218/1770 train_time:118453ms step_avg:98.06ms
step:1219/1770 train_time:118556ms step_avg:98.06ms
step:1220/1770 train_time:118658ms step_avg:98.06ms
step:1221/1770 train_time:118761ms step_avg:98.07ms
step:1222/1770 train_time:118864ms step_avg:98.07ms
step:1223/1770 train_time:118966ms step_avg:98.08ms
step:1224/1770 train_time:119069ms step_avg:98.08ms
step:1225/1770 train_time:119171ms step_avg:98.08ms
step:1226/1770 train_time:119274ms step_avg:98.09ms
step:1227/1770 train_time:119380ms step_avg:98.09ms
step:1228/1770 train_time:119484ms step_avg:98.10ms
step:1229/1770 train_time:119586ms step_avg:98.10ms
step:1230/1770 train_time:119688ms step_avg:98.11ms
step:1231/1770 train_time:119790ms step_avg:98.11ms
step:1232/1770 train_time:119892ms step_avg:98.11ms
step:1233/1770 train_time:119994ms step_avg:98.11ms
step:1234/1770 train_time:120096ms step_avg:98.12ms
step:1235/1770 train_time:120198ms step_avg:98.12ms
step:1236/1770 train_time:120301ms step_avg:98.12ms
step:1237/1770 train_time:120403ms step_avg:98.13ms
step:1238/1770 train_time:120505ms step_avg:98.13ms
step:1239/1770 train_time:120608ms step_avg:98.14ms
step:1240/1770 train_time:120710ms step_avg:98.14ms
step:1241/1770 train_time:120812ms step_avg:98.14ms
step:1242/1770 train_time:120913ms step_avg:98.14ms
step:1243/1770 train_time:121016ms step_avg:98.15ms
step:1244/1770 train_time:121118ms step_avg:98.15ms
step:1245/1770 train_time:121220ms step_avg:98.15ms
step:1246/1770 train_time:121323ms step_avg:98.16ms
step:1247/1770 train_time:121425ms step_avg:98.16ms
step:1248/1770 train_time:121528ms step_avg:98.16ms
step:1249/1770 train_time:121630ms step_avg:98.17ms
step:1250/1770 train_time:121731ms step_avg:98.17ms
step:1250/1770 val_loss:3.4262 train_time:121833ms step_avg:98.25ms
step:1251/1770 train_time:121855ms step_avg:98.19ms
step:1252/1770 train_time:121946ms step_avg:98.19ms
step:1253/1770 train_time:122050ms step_avg:98.19ms
step:1254/1770 train_time:122152ms step_avg:98.19ms
step:1255/1770 train_time:122255ms step_avg:98.20ms
step:1256/1770 train_time:122357ms step_avg:98.20ms
step:1257/1770 train_time:122459ms step_avg:98.20ms
step:1258/1770 train_time:122561ms step_avg:98.21ms
step:1259/1770 train_time:122663ms step_avg:98.21ms
step:1260/1770 train_time:122764ms step_avg:98.21ms
step:1261/1770 train_time:122867ms step_avg:98.22ms
step:1262/1770 train_time:122971ms step_avg:98.22ms
step:1263/1770 train_time:123073ms step_avg:98.22ms
step:1264/1770 train_time:123176ms step_avg:98.23ms
step:1265/1770 train_time:123278ms step_avg:98.23ms
step:1266/1770 train_time:123381ms step_avg:98.23ms
step:1267/1770 train_time:123483ms step_avg:98.24ms
step:1268/1770 train_time:123586ms step_avg:98.24ms
step:1269/1770 train_time:123688ms step_avg:98.24ms
step:1270/1770 train_time:123791ms step_avg:98.25ms
step:1271/1770 train_time:123893ms step_avg:98.25ms
step:1272/1770 train_time:123994ms step_avg:98.25ms
step:1273/1770 train_time:124098ms step_avg:98.26ms
step:1274/1770 train_time:124201ms step_avg:98.26ms
step:1275/1770 train_time:124303ms step_avg:98.26ms
step:1276/1770 train_time:124405ms step_avg:98.27ms
step:1277/1770 train_time:124507ms step_avg:98.27ms
step:1278/1770 train_time:124609ms step_avg:98.27ms
step:1279/1770 train_time:124712ms step_avg:98.28ms
step:1280/1770 train_time:124816ms step_avg:98.28ms
step:1281/1770 train_time:124917ms step_avg:98.28ms
step:1282/1770 train_time:125020ms step_avg:98.29ms
step:1283/1770 train_time:125123ms step_avg:98.29ms
step:1284/1770 train_time:125226ms step_avg:98.29ms
step:1285/1770 train_time:125328ms step_avg:98.30ms
step:1286/1770 train_time:125432ms step_avg:98.30ms
step:1287/1770 train_time:125536ms step_avg:98.31ms
step:1288/1770 train_time:125638ms step_avg:98.31ms
step:1289/1770 train_time:125742ms step_avg:98.31ms
step:1290/1770 train_time:125844ms step_avg:98.32ms
step:1291/1770 train_time:125945ms step_avg:98.32ms
step:1292/1770 train_time:126048ms step_avg:98.32ms
step:1293/1770 train_time:126150ms step_avg:98.32ms
step:1294/1770 train_time:126251ms step_avg:98.33ms
step:1295/1770 train_time:126353ms step_avg:98.33ms
step:1296/1770 train_time:126456ms step_avg:98.33ms
step:1297/1770 train_time:126558ms step_avg:98.34ms
step:1298/1770 train_time:126661ms step_avg:98.34ms
step:1299/1770 train_time:126763ms step_avg:98.34ms
step:1300/1770 train_time:126865ms step_avg:98.35ms
step:1301/1770 train_time:126967ms step_avg:98.35ms
step:1302/1770 train_time:127069ms step_avg:98.35ms
step:1303/1770 train_time:127172ms step_avg:98.35ms
step:1304/1770 train_time:127274ms step_avg:98.36ms
step:1305/1770 train_time:127375ms step_avg:98.36ms
step:1306/1770 train_time:127478ms step_avg:98.36ms
step:1307/1770 train_time:127580ms step_avg:98.37ms
step:1308/1770 train_time:127682ms step_avg:98.37ms
step:1309/1770 train_time:127784ms step_avg:98.37ms
step:1310/1770 train_time:127886ms step_avg:98.37ms
step:1311/1770 train_time:127988ms step_avg:98.38ms
step:1312/1770 train_time:128090ms step_avg:98.38ms
step:1313/1770 train_time:128192ms step_avg:98.38ms
step:1314/1770 train_time:128294ms step_avg:98.38ms
step:1315/1770 train_time:128396ms step_avg:98.39ms
step:1316/1770 train_time:128499ms step_avg:98.39ms
step:1317/1770 train_time:128602ms step_avg:98.39ms
step:1318/1770 train_time:128707ms step_avg:98.40ms
step:1319/1770 train_time:128815ms step_avg:98.41ms
step:1320/1770 train_time:128912ms step_avg:98.41ms
step:1321/1770 train_time:129014ms step_avg:98.41ms
step:1322/1770 train_time:129116ms step_avg:98.41ms
step:1323/1770 train_time:129220ms step_avg:98.42ms
step:1324/1770 train_time:129323ms step_avg:98.42ms
step:1325/1770 train_time:129426ms step_avg:98.42ms
step:1326/1770 train_time:129529ms step_avg:98.43ms
step:1327/1770 train_time:129633ms step_avg:98.43ms
step:1328/1770 train_time:129735ms step_avg:98.43ms
step:1329/1770 train_time:129837ms step_avg:98.44ms
step:1330/1770 train_time:129940ms step_avg:98.44ms
step:1331/1770 train_time:130042ms step_avg:98.44ms
step:1332/1770 train_time:130143ms step_avg:98.44ms
step:1333/1770 train_time:130245ms step_avg:98.45ms
step:1334/1770 train_time:130347ms step_avg:98.45ms
step:1335/1770 train_time:130449ms step_avg:98.45ms
step:1336/1770 train_time:130551ms step_avg:98.45ms
step:1337/1770 train_time:130653ms step_avg:98.46ms
step:1338/1770 train_time:130755ms step_avg:98.46ms
step:1339/1770 train_time:130858ms step_avg:98.46ms
step:1340/1770 train_time:130962ms step_avg:98.47ms
step:1341/1770 train_time:131064ms step_avg:98.47ms
step:1342/1770 train_time:131167ms step_avg:98.47ms
step:1343/1770 train_time:131270ms step_avg:98.48ms
step:1344/1770 train_time:131372ms step_avg:98.48ms
step:1345/1770 train_time:131475ms step_avg:98.48ms
step:1346/1770 train_time:131577ms step_avg:98.49ms
step:1347/1770 train_time:131679ms step_avg:98.49ms
step:1348/1770 train_time:131783ms step_avg:98.49ms
step:1349/1770 train_time:131886ms step_avg:98.50ms
step:1350/1770 train_time:131988ms step_avg:98.50ms
step:1351/1770 train_time:132090ms step_avg:98.50ms
step:1352/1770 train_time:132192ms step_avg:98.50ms
step:1353/1770 train_time:132295ms step_avg:98.51ms
step:1354/1770 train_time:132398ms step_avg:98.51ms
step:1355/1770 train_time:132500ms step_avg:98.51ms
step:1356/1770 train_time:132601ms step_avg:98.51ms
step:1357/1770 train_time:132703ms step_avg:98.52ms
step:1358/1770 train_time:132806ms step_avg:98.52ms
step:1359/1770 train_time:132909ms step_avg:98.52ms
step:1360/1770 train_time:133012ms step_avg:98.53ms
step:1361/1770 train_time:133115ms step_avg:98.53ms
step:1362/1770 train_time:133217ms step_avg:98.53ms
step:1363/1770 train_time:133320ms step_avg:98.54ms
step:1364/1770 train_time:133423ms step_avg:98.54ms
step:1365/1770 train_time:133525ms step_avg:98.54ms
step:1366/1770 train_time:133627ms step_avg:98.55ms
step:1367/1770 train_time:133730ms step_avg:98.55ms
step:1368/1770 train_time:133832ms step_avg:98.55ms
step:1369/1770 train_time:133935ms step_avg:98.55ms
step:1370/1770 train_time:134038ms step_avg:98.56ms
step:1371/1770 train_time:134140ms step_avg:98.56ms
step:1372/1770 train_time:134242ms step_avg:98.56ms
step:1373/1770 train_time:134345ms step_avg:98.57ms
step:1374/1770 train_time:134448ms step_avg:98.57ms
step:1375/1770 train_time:134552ms step_avg:98.57ms
step:1375/1770 val_loss:3.3822 train_time:134653ms step_avg:98.65ms
step:1376/1770 train_time:134674ms step_avg:98.59ms
step:1377/1770 train_time:134766ms step_avg:98.59ms
step:1378/1770 train_time:134869ms step_avg:98.59ms
step:1379/1770 train_time:134971ms step_avg:98.59ms
step:1380/1770 train_time:135073ms step_avg:98.59ms
step:1381/1770 train_time:135176ms step_avg:98.60ms
step:1382/1770 train_time:135277ms step_avg:98.60ms
step:1383/1770 train_time:135381ms step_avg:98.60ms
step:1384/1770 train_time:135481ms step_avg:98.60ms
step:1385/1770 train_time:135583ms step_avg:98.61ms
step:1386/1770 train_time:135686ms step_avg:98.61ms
step:1387/1770 train_time:135789ms step_avg:98.61ms
step:1388/1770 train_time:135891ms step_avg:98.62ms
step:1389/1770 train_time:135995ms step_avg:98.62ms
step:1390/1770 train_time:136097ms step_avg:98.62ms
step:1391/1770 train_time:136198ms step_avg:98.62ms
step:1392/1770 train_time:136299ms step_avg:98.62ms
step:1393/1770 train_time:136401ms step_avg:98.63ms
step:1394/1770 train_time:136503ms step_avg:98.63ms
step:1395/1770 train_time:136607ms step_avg:98.63ms
step:1396/1770 train_time:136711ms step_avg:98.64ms
step:1397/1770 train_time:136813ms step_avg:98.64ms
step:1398/1770 train_time:136916ms step_avg:98.64ms
step:1399/1770 train_time:137019ms step_avg:98.65ms
step:1400/1770 train_time:137120ms step_avg:98.65ms
step:1401/1770 train_time:137223ms step_avg:98.65ms
step:1402/1770 train_time:137326ms step_avg:98.65ms
step:1403/1770 train_time:137427ms step_avg:98.66ms
step:1404/1770 train_time:137530ms step_avg:98.66ms
step:1405/1770 train_time:137632ms step_avg:98.66ms
step:1406/1770 train_time:137734ms step_avg:98.66ms
step:1407/1770 train_time:137836ms step_avg:98.67ms
step:1408/1770 train_time:137938ms step_avg:98.67ms
step:1409/1770 train_time:138041ms step_avg:98.67ms
step:1410/1770 train_time:138144ms step_avg:98.67ms
step:1411/1770 train_time:138246ms step_avg:98.68ms
step:1412/1770 train_time:138348ms step_avg:98.68ms
step:1413/1770 train_time:138450ms step_avg:98.68ms
step:1414/1770 train_time:138553ms step_avg:98.68ms
step:1415/1770 train_time:138656ms step_avg:98.69ms
step:1416/1770 train_time:138759ms step_avg:98.69ms
step:1417/1770 train_time:138861ms step_avg:98.69ms
step:1418/1770 train_time:138964ms step_avg:98.70ms
step:1419/1770 train_time:139067ms step_avg:98.70ms
step:1420/1770 train_time:139170ms step_avg:98.70ms
step:1421/1770 train_time:139272ms step_avg:98.70ms
step:1422/1770 train_time:139376ms step_avg:98.71ms
step:1423/1770 train_time:139477ms step_avg:98.71ms
step:1424/1770 train_time:139580ms step_avg:98.71ms
step:1425/1770 train_time:139682ms step_avg:98.71ms
step:1426/1770 train_time:139784ms step_avg:98.72ms
step:1427/1770 train_time:139886ms step_avg:98.72ms
step:1428/1770 train_time:139990ms step_avg:98.72ms
step:1429/1770 train_time:140092ms step_avg:98.73ms
step:1430/1770 train_time:140194ms step_avg:98.73ms
step:1431/1770 train_time:140298ms step_avg:98.73ms
step:1432/1770 train_time:140400ms step_avg:98.73ms
step:1433/1770 train_time:140502ms step_avg:98.74ms
step:1434/1770 train_time:140603ms step_avg:98.74ms
step:1435/1770 train_time:140705ms step_avg:98.74ms
step:1436/1770 train_time:140808ms step_avg:98.74ms
step:1437/1770 train_time:140911ms step_avg:98.75ms
step:1438/1770 train_time:141014ms step_avg:98.75ms
step:1439/1770 train_time:141116ms step_avg:98.75ms
step:1440/1770 train_time:141218ms step_avg:98.75ms
step:1441/1770 train_time:141323ms step_avg:98.76ms
step:1442/1770 train_time:141425ms step_avg:98.76ms
step:1443/1770 train_time:141527ms step_avg:98.76ms
step:1444/1770 train_time:141630ms step_avg:98.77ms
step:1445/1770 train_time:141733ms step_avg:98.77ms
step:1446/1770 train_time:141836ms step_avg:98.77ms
step:1447/1770 train_time:141941ms step_avg:98.78ms
step:1448/1770 train_time:142045ms step_avg:98.78ms
step:1449/1770 train_time:142148ms step_avg:98.78ms
step:1450/1770 train_time:142251ms step_avg:98.79ms
step:1451/1770 train_time:142355ms step_avg:98.79ms
step:1452/1770 train_time:142463ms step_avg:98.80ms
step:1453/1770 train_time:142562ms step_avg:98.80ms
step:1454/1770 train_time:142664ms step_avg:98.80ms
step:1455/1770 train_time:142768ms step_avg:98.80ms
step:1456/1770 train_time:142873ms step_avg:98.81ms
step:1457/1770 train_time:142978ms step_avg:98.81ms
step:1458/1770 train_time:143082ms step_avg:98.81ms
step:1459/1770 train_time:143186ms step_avg:98.82ms
step:1460/1770 train_time:143289ms step_avg:98.82ms
step:1461/1770 train_time:143393ms step_avg:98.82ms
step:1462/1770 train_time:143496ms step_avg:98.83ms
step:1463/1770 train_time:143600ms step_avg:98.83ms
step:1464/1770 train_time:143705ms step_avg:98.83ms
step:1465/1770 train_time:143808ms step_avg:98.84ms
step:1466/1770 train_time:143912ms step_avg:98.84ms
step:1467/1770 train_time:144017ms step_avg:98.84ms
step:1468/1770 train_time:144120ms step_avg:98.85ms
step:1469/1770 train_time:144223ms step_avg:98.85ms
step:1470/1770 train_time:144326ms step_avg:98.85ms
step:1471/1770 train_time:144429ms step_avg:98.86ms
step:1472/1770 train_time:144533ms step_avg:98.86ms
step:1473/1770 train_time:144637ms step_avg:98.86ms
step:1474/1770 train_time:144741ms step_avg:98.87ms
step:1475/1770 train_time:144844ms step_avg:98.87ms
step:1476/1770 train_time:144947ms step_avg:98.87ms
step:1477/1770 train_time:145052ms step_avg:98.88ms
step:1478/1770 train_time:145156ms step_avg:98.88ms
step:1479/1770 train_time:145259ms step_avg:98.88ms
step:1480/1770 train_time:145364ms step_avg:98.89ms
step:1481/1770 train_time:145471ms step_avg:98.89ms
step:1482/1770 train_time:145574ms step_avg:98.90ms
step:1483/1770 train_time:145677ms step_avg:98.90ms
step:1484/1770 train_time:145780ms step_avg:98.90ms
step:1485/1770 train_time:145884ms step_avg:98.90ms
step:1486/1770 train_time:145986ms step_avg:98.91ms
step:1487/1770 train_time:146089ms step_avg:98.91ms
step:1488/1770 train_time:146194ms step_avg:98.91ms
step:1489/1770 train_time:146299ms step_avg:98.92ms
step:1490/1770 train_time:146403ms step_avg:98.92ms
step:1491/1770 train_time:146506ms step_avg:98.92ms
step:1492/1770 train_time:146609ms step_avg:98.93ms
step:1493/1770 train_time:146715ms step_avg:98.93ms
step:1494/1770 train_time:146821ms step_avg:98.94ms
step:1495/1770 train_time:146924ms step_avg:98.94ms
step:1496/1770 train_time:147027ms step_avg:98.94ms
step:1497/1770 train_time:147131ms step_avg:98.94ms
step:1498/1770 train_time:147235ms step_avg:98.95ms
step:1499/1770 train_time:147338ms step_avg:98.95ms
step:1500/1770 train_time:147440ms step_avg:98.95ms
step:1500/1770 val_loss:3.3444 train_time:147543ms step_avg:99.02ms
step:1501/1770 train_time:147564ms step_avg:98.97ms
step:1502/1770 train_time:147655ms step_avg:98.96ms
step:1503/1770 train_time:147760ms step_avg:98.97ms
step:1504/1770 train_time:147863ms step_avg:98.97ms
step:1505/1770 train_time:147969ms step_avg:98.98ms
step:1506/1770 train_time:148073ms step_avg:98.98ms
step:1507/1770 train_time:148176ms step_avg:98.98ms
step:1508/1770 train_time:148281ms step_avg:98.99ms
step:1509/1770 train_time:148384ms step_avg:98.99ms
step:1510/1770 train_time:148486ms step_avg:98.99ms
step:1511/1770 train_time:148591ms step_avg:98.99ms
step:1512/1770 train_time:148695ms step_avg:99.00ms
step:1513/1770 train_time:148799ms step_avg:99.00ms
step:1514/1770 train_time:148902ms step_avg:99.00ms
step:1515/1770 train_time:149006ms step_avg:99.01ms
step:1516/1770 train_time:149109ms step_avg:99.01ms
step:1517/1770 train_time:149213ms step_avg:99.01ms
step:1518/1770 train_time:149318ms step_avg:99.02ms
step:1519/1770 train_time:149420ms step_avg:99.02ms
step:1520/1770 train_time:149525ms step_avg:99.02ms
step:1521/1770 train_time:149628ms step_avg:99.03ms
step:1522/1770 train_time:149732ms step_avg:99.03ms
step:1523/1770 train_time:149837ms step_avg:99.03ms
step:1524/1770 train_time:149939ms step_avg:99.04ms
step:1525/1770 train_time:150042ms step_avg:99.04ms
step:1526/1770 train_time:150147ms step_avg:99.04ms
step:1527/1770 train_time:150251ms step_avg:99.05ms
step:1528/1770 train_time:150357ms step_avg:99.05ms
step:1529/1770 train_time:150460ms step_avg:99.05ms
step:1530/1770 train_time:150563ms step_avg:99.05ms
step:1531/1770 train_time:150667ms step_avg:99.06ms
step:1532/1770 train_time:150772ms step_avg:99.06ms
step:1533/1770 train_time:150876ms step_avg:99.07ms
step:1534/1770 train_time:150981ms step_avg:99.07ms
step:1535/1770 train_time:151083ms step_avg:99.07ms
step:1536/1770 train_time:151186ms step_avg:99.07ms
step:1537/1770 train_time:151289ms step_avg:99.08ms
step:1538/1770 train_time:151394ms step_avg:99.08ms
step:1539/1770 train_time:151497ms step_avg:99.08ms
step:1540/1770 train_time:151604ms step_avg:99.09ms
step:1541/1770 train_time:151709ms step_avg:99.09ms
step:1542/1770 train_time:151813ms step_avg:99.09ms
step:1543/1770 train_time:151915ms step_avg:99.10ms
step:1544/1770 train_time:152022ms step_avg:99.10ms
step:1545/1770 train_time:152125ms step_avg:99.10ms
step:1546/1770 train_time:152228ms step_avg:99.11ms
step:1547/1770 train_time:152332ms step_avg:99.11ms
step:1548/1770 train_time:152435ms step_avg:99.11ms
step:1549/1770 train_time:152538ms step_avg:99.12ms
step:1550/1770 train_time:152643ms step_avg:99.12ms
step:1551/1770 train_time:152746ms step_avg:99.12ms
step:1552/1770 train_time:152851ms step_avg:99.13ms
step:1553/1770 train_time:152954ms step_avg:99.13ms
step:1554/1770 train_time:153057ms step_avg:99.13ms
step:1555/1770 train_time:153161ms step_avg:99.13ms
step:1556/1770 train_time:153263ms step_avg:99.14ms
step:1557/1770 train_time:153366ms step_avg:99.14ms
step:1558/1770 train_time:153470ms step_avg:99.14ms
step:1559/1770 train_time:153573ms step_avg:99.14ms
step:1560/1770 train_time:153675ms step_avg:99.15ms
step:1561/1770 train_time:153780ms step_avg:99.15ms
step:1562/1770 train_time:153884ms step_avg:99.15ms
step:1563/1770 train_time:153987ms step_avg:99.15ms
step:1564/1770 train_time:154090ms step_avg:99.16ms
step:1565/1770 train_time:154194ms step_avg:99.16ms
step:1566/1770 train_time:154298ms step_avg:99.16ms
step:1567/1770 train_time:154401ms step_avg:99.17ms
step:1568/1770 train_time:154504ms step_avg:99.17ms
step:1569/1770 train_time:154610ms step_avg:99.17ms
step:1570/1770 train_time:154713ms step_avg:99.18ms
step:1571/1770 train_time:154817ms step_avg:99.18ms
step:1572/1770 train_time:154922ms step_avg:99.18ms
step:1573/1770 train_time:155027ms step_avg:99.19ms
step:1574/1770 train_time:155131ms step_avg:99.19ms
step:1575/1770 train_time:155233ms step_avg:99.19ms
step:1576/1770 train_time:155336ms step_avg:99.19ms
step:1577/1770 train_time:155441ms step_avg:99.20ms
step:1578/1770 train_time:155546ms step_avg:99.20ms
step:1579/1770 train_time:155649ms step_avg:99.20ms
step:1580/1770 train_time:155752ms step_avg:99.21ms
step:1581/1770 train_time:155859ms step_avg:99.21ms
step:1582/1770 train_time:155963ms step_avg:99.21ms
step:1583/1770 train_time:156066ms step_avg:99.22ms
step:1584/1770 train_time:156171ms step_avg:99.22ms
step:1585/1770 train_time:156275ms step_avg:99.22ms
step:1586/1770 train_time:156382ms step_avg:99.23ms
step:1587/1770 train_time:156485ms step_avg:99.23ms
step:1588/1770 train_time:156589ms step_avg:99.23ms
step:1589/1770 train_time:156694ms step_avg:99.24ms
step:1590/1770 train_time:156797ms step_avg:99.24ms
step:1591/1770 train_time:156900ms step_avg:99.24ms
step:1592/1770 train_time:157004ms step_avg:99.24ms
step:1593/1770 train_time:157107ms step_avg:99.25ms
step:1594/1770 train_time:157210ms step_avg:99.25ms
step:1595/1770 train_time:157313ms step_avg:99.25ms
step:1596/1770 train_time:157419ms step_avg:99.26ms
step:1597/1770 train_time:157522ms step_avg:99.26ms
step:1598/1770 train_time:157625ms step_avg:99.26ms
step:1599/1770 train_time:157729ms step_avg:99.26ms
step:1600/1770 train_time:157834ms step_avg:99.27ms
step:1601/1770 train_time:157939ms step_avg:99.27ms
step:1602/1770 train_time:158043ms step_avg:99.27ms
step:1603/1770 train_time:158147ms step_avg:99.28ms
step:1604/1770 train_time:158249ms step_avg:99.28ms
step:1605/1770 train_time:158352ms step_avg:99.28ms
step:1606/1770 train_time:158456ms step_avg:99.28ms
step:1607/1770 train_time:158563ms step_avg:99.29ms
step:1608/1770 train_time:158666ms step_avg:99.29ms
step:1609/1770 train_time:158770ms step_avg:99.29ms
step:1610/1770 train_time:158874ms step_avg:99.30ms
step:1611/1770 train_time:158980ms step_avg:99.30ms
step:1612/1770 train_time:159085ms step_avg:99.30ms
step:1613/1770 train_time:159189ms step_avg:99.31ms
step:1614/1770 train_time:159291ms step_avg:99.31ms
step:1615/1770 train_time:159395ms step_avg:99.31ms
step:1616/1770 train_time:159498ms step_avg:99.31ms
step:1617/1770 train_time:159604ms step_avg:99.32ms
step:1618/1770 train_time:159708ms step_avg:99.32ms
step:1619/1770 train_time:159812ms step_avg:99.32ms
step:1620/1770 train_time:159916ms step_avg:99.33ms
step:1621/1770 train_time:160020ms step_avg:99.33ms
step:1622/1770 train_time:160125ms step_avg:99.33ms
step:1623/1770 train_time:160231ms step_avg:99.34ms
step:1624/1770 train_time:160334ms step_avg:99.34ms
step:1625/1770 train_time:160436ms step_avg:99.34ms
step:1625/1770 val_loss:3.3099 train_time:160539ms step_avg:99.40ms
step:1626/1770 train_time:160559ms step_avg:99.36ms
step:1627/1770 train_time:160652ms step_avg:99.35ms
step:1628/1770 train_time:160755ms step_avg:99.35ms
step:1629/1770 train_time:160856ms step_avg:99.36ms
step:1630/1770 train_time:160960ms step_avg:99.36ms
step:1631/1770 train_time:161063ms step_avg:99.36ms
step:1632/1770 train_time:161166ms step_avg:99.36ms
step:1633/1770 train_time:161270ms step_avg:99.37ms
step:1634/1770 train_time:161372ms step_avg:99.37ms
step:1635/1770 train_time:161475ms step_avg:99.37ms
step:1636/1770 train_time:161582ms step_avg:99.37ms
step:1637/1770 train_time:161687ms step_avg:99.38ms
step:1638/1770 train_time:161790ms step_avg:99.38ms
step:1639/1770 train_time:161894ms step_avg:99.38ms
step:1640/1770 train_time:161998ms step_avg:99.39ms
step:1641/1770 train_time:162101ms step_avg:99.39ms
step:1642/1770 train_time:162204ms step_avg:99.39ms
step:1643/1770 train_time:162308ms step_avg:99.39ms
step:1644/1770 train_time:162413ms step_avg:99.40ms
step:1645/1770 train_time:162516ms step_avg:99.40ms
step:1646/1770 train_time:162622ms step_avg:99.40ms
step:1647/1770 train_time:162727ms step_avg:99.41ms
step:1648/1770 train_time:162830ms step_avg:99.41ms
step:1649/1770 train_time:162934ms step_avg:99.41ms
step:1650/1770 train_time:163038ms step_avg:99.41ms
step:1651/1770 train_time:163142ms step_avg:99.42ms
step:1652/1770 train_time:163246ms step_avg:99.42ms
step:1653/1770 train_time:163350ms step_avg:99.42ms
step:1654/1770 train_time:163461ms step_avg:99.43ms
step:1655/1770 train_time:163562ms step_avg:99.43ms
step:1656/1770 train_time:163665ms step_avg:99.43ms
step:1657/1770 train_time:163771ms step_avg:99.44ms
step:1658/1770 train_time:163875ms step_avg:99.44ms
step:1659/1770 train_time:163980ms step_avg:99.44ms
step:1660/1770 train_time:164084ms step_avg:99.44ms
step:1661/1770 train_time:164189ms step_avg:99.45ms
step:1662/1770 train_time:164294ms step_avg:99.45ms
step:1663/1770 train_time:164396ms step_avg:99.45ms
step:1664/1770 train_time:164500ms step_avg:99.46ms
step:1665/1770 train_time:164603ms step_avg:99.46ms
step:1666/1770 train_time:164708ms step_avg:99.46ms
step:1667/1770 train_time:164811ms step_avg:99.46ms
step:1668/1770 train_time:164914ms step_avg:99.47ms
step:1669/1770 train_time:165016ms step_avg:99.47ms
step:1670/1770 train_time:165119ms step_avg:99.47ms
step:1671/1770 train_time:165223ms step_avg:99.47ms
step:1672/1770 train_time:165328ms step_avg:99.48ms
step:1673/1770 train_time:165434ms step_avg:99.48ms
step:1674/1770 train_time:165537ms step_avg:99.48ms
step:1675/1770 train_time:165640ms step_avg:99.48ms
step:1676/1770 train_time:165745ms step_avg:99.49ms
step:1677/1770 train_time:165853ms step_avg:99.49ms
step:1678/1770 train_time:165955ms step_avg:99.49ms
step:1679/1770 train_time:166058ms step_avg:99.50ms
step:1680/1770 train_time:166161ms step_avg:99.50ms
step:1681/1770 train_time:166266ms step_avg:99.50ms
step:1682/1770 train_time:166372ms step_avg:99.50ms
step:1683/1770 train_time:166475ms step_avg:99.51ms
step:1684/1770 train_time:166577ms step_avg:99.51ms
step:1685/1770 train_time:166681ms step_avg:99.51ms
step:1686/1770 train_time:166786ms step_avg:99.51ms
step:1687/1770 train_time:166891ms step_avg:99.52ms
step:1688/1770 train_time:166994ms step_avg:99.52ms
step:1689/1770 train_time:167098ms step_avg:99.52ms
step:1690/1770 train_time:167202ms step_avg:99.53ms
step:1691/1770 train_time:167306ms step_avg:99.53ms
step:1692/1770 train_time:167411ms step_avg:99.53ms
step:1693/1770 train_time:167516ms step_avg:99.53ms
step:1694/1770 train_time:167619ms step_avg:99.54ms
step:1695/1770 train_time:167723ms step_avg:99.54ms
step:1696/1770 train_time:167828ms step_avg:99.54ms
step:1697/1770 train_time:167933ms step_avg:99.55ms
step:1698/1770 train_time:168037ms step_avg:99.55ms
step:1699/1770 train_time:168140ms step_avg:99.55ms
step:1700/1770 train_time:168245ms step_avg:99.55ms
step:1701/1770 train_time:168348ms step_avg:99.56ms
step:1702/1770 train_time:168451ms step_avg:99.56ms
step:1703/1770 train_time:168554ms step_avg:99.56ms
step:1704/1770 train_time:168657ms step_avg:99.56ms
step:1705/1770 train_time:168760ms step_avg:99.56ms
step:1706/1770 train_time:168866ms step_avg:99.57ms
step:1707/1770 train_time:168968ms step_avg:99.57ms
step:1708/1770 train_time:169073ms step_avg:99.57ms
step:1709/1770 train_time:169178ms step_avg:99.58ms
step:1710/1770 train_time:169285ms step_avg:99.58ms
step:1711/1770 train_time:169392ms step_avg:99.58ms
step:1712/1770 train_time:169496ms step_avg:99.59ms
step:1713/1770 train_time:169600ms step_avg:99.59ms
step:1714/1770 train_time:169704ms step_avg:99.59ms
step:1715/1770 train_time:169808ms step_avg:99.59ms
step:1716/1770 train_time:169913ms step_avg:99.60ms
step:1717/1770 train_time:170016ms step_avg:99.60ms
step:1718/1770 train_time:170122ms step_avg:99.60ms
step:1719/1770 train_time:170228ms step_avg:99.61ms
step:1720/1770 train_time:170333ms step_avg:99.61ms
step:1721/1770 train_time:170436ms step_avg:99.61ms
step:1722/1770 train_time:170543ms step_avg:99.62ms
step:1723/1770 train_time:170649ms step_avg:99.62ms
step:1724/1770 train_time:170755ms step_avg:99.62ms
step:1725/1770 train_time:170861ms step_avg:99.63ms
step:1726/1770 train_time:170968ms step_avg:99.63ms
step:1727/1770 train_time:171072ms step_avg:99.63ms
step:1728/1770 train_time:171177ms step_avg:99.64ms
step:1729/1770 train_time:171282ms step_avg:99.64ms
step:1730/1770 train_time:171388ms step_avg:99.64ms
step:1731/1770 train_time:171494ms step_avg:99.65ms
step:1732/1770 train_time:171598ms step_avg:99.65ms
step:1733/1770 train_time:171704ms step_avg:99.65ms
step:1734/1770 train_time:171807ms step_avg:99.66ms
step:1735/1770 train_time:171912ms step_avg:99.66ms
step:1736/1770 train_time:172016ms step_avg:99.66ms
step:1737/1770 train_time:172120ms step_avg:99.66ms
step:1738/1770 train_time:172225ms step_avg:99.67ms
step:1739/1770 train_time:172329ms step_avg:99.67ms
step:1740/1770 train_time:172434ms step_avg:99.67ms
step:1741/1770 train_time:172541ms step_avg:99.68ms
step:1742/1770 train_time:172648ms step_avg:99.68ms
step:1743/1770 train_time:172754ms step_avg:99.68ms
step:1744/1770 train_time:172861ms step_avg:99.69ms
step:1745/1770 train_time:172962ms step_avg:99.69ms
step:1746/1770 train_time:173069ms step_avg:99.69ms
step:1747/1770 train_time:173172ms step_avg:99.70ms
step:1748/1770 train_time:173278ms step_avg:99.70ms
step:1749/1770 train_time:173384ms step_avg:99.70ms
step:1750/1770 train_time:173488ms step_avg:99.71ms
step:1750/1770 val_loss:3.2832 train_time:173591ms step_avg:99.77ms
step:1751/1770 train_time:173620ms step_avg:99.72ms
step:1752/1770 train_time:173706ms step_avg:99.72ms
step:1753/1770 train_time:173811ms step_avg:99.72ms
step:1754/1770 train_time:173916ms step_avg:99.72ms
step:1755/1770 train_time:174020ms step_avg:99.73ms
step:1756/1770 train_time:174125ms step_avg:99.73ms
step:1757/1770 train_time:174230ms step_avg:99.73ms
step:1758/1770 train_time:174334ms step_avg:99.73ms
step:1759/1770 train_time:174438ms step_avg:99.74ms
step:1760/1770 train_time:174543ms step_avg:99.74ms
step:1761/1770 train_time:174649ms step_avg:99.74ms
step:1762/1770 train_time:174758ms step_avg:99.75ms
step:1763/1770 train_time:174860ms step_avg:99.75ms
step:1764/1770 train_time:174965ms step_avg:99.75ms
step:1765/1770 train_time:175070ms step_avg:99.75ms
step:1766/1770 train_time:175178ms step_avg:99.76ms
step:1767/1770 train_time:175281ms step_avg:99.76ms
step:1768/1770 train_time:175386ms step_avg:99.76ms
step:1769/1770 train_time:175489ms step_avg:99.77ms
step:1770/1770 train_time:175593ms step_avg:99.77ms
step:1770/1770 val_loss:3.2803 train_time:175697ms step_avg:99.83ms
peak memory allocated: 28840 MiB reserved: 32212 MiB
