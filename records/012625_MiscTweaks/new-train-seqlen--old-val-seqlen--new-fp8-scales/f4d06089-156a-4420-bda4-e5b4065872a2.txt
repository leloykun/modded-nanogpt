import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 16:16:09 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24340ms step_avg:nanms
step:2/1770 train_time:24883ms step_avg:nanms
step:3/1770 train_time:24979ms step_avg:nanms
step:4/1770 train_time:25072ms step_avg:nanms
step:5/1770 train_time:25165ms step_avg:nanms
step:6/1770 train_time:25259ms step_avg:nanms
step:7/1770 train_time:25353ms step_avg:nanms
step:8/1770 train_time:25447ms step_avg:nanms
step:9/1770 train_time:25540ms step_avg:nanms
step:10/1770 train_time:25634ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:94.03ms
step:14/1770 train_time:377ms step_avg:94.27ms
step:15/1770 train_time:471ms step_avg:94.18ms
step:16/1770 train_time:564ms step_avg:94.07ms
step:17/1770 train_time:658ms step_avg:94.03ms
step:18/1770 train_time:752ms step_avg:94.06ms
step:19/1770 train_time:846ms step_avg:93.96ms
step:20/1770 train_time:939ms step_avg:93.92ms
step:21/1770 train_time:1033ms step_avg:93.90ms
step:22/1770 train_time:1127ms step_avg:93.88ms
step:23/1770 train_time:1220ms step_avg:93.88ms
step:24/1770 train_time:1315ms step_avg:93.89ms
step:25/1770 train_time:1408ms step_avg:93.89ms
step:26/1770 train_time:1503ms step_avg:93.91ms
step:27/1770 train_time:1597ms step_avg:93.91ms
step:28/1770 train_time:1690ms step_avg:93.90ms
step:29/1770 train_time:1784ms step_avg:93.90ms
step:30/1770 train_time:1878ms step_avg:93.92ms
step:31/1770 train_time:1973ms step_avg:93.93ms
step:32/1770 train_time:2067ms step_avg:93.93ms
step:33/1770 train_time:2161ms step_avg:93.96ms
step:34/1770 train_time:2255ms step_avg:93.94ms
step:35/1770 train_time:2349ms step_avg:93.94ms
step:36/1770 train_time:2443ms step_avg:93.96ms
step:37/1770 train_time:2537ms step_avg:93.96ms
step:38/1770 train_time:2631ms step_avg:93.97ms
step:39/1770 train_time:2725ms step_avg:93.96ms
step:40/1770 train_time:2819ms step_avg:93.98ms
step:41/1770 train_time:2914ms step_avg:94.00ms
step:42/1770 train_time:3008ms step_avg:93.99ms
step:43/1770 train_time:3101ms step_avg:93.98ms
step:44/1770 train_time:3196ms step_avg:93.99ms
step:45/1770 train_time:3289ms step_avg:93.98ms
step:46/1770 train_time:3383ms step_avg:93.98ms
step:47/1770 train_time:3477ms step_avg:93.98ms
step:48/1770 train_time:3572ms step_avg:93.99ms
step:49/1770 train_time:3666ms step_avg:94.00ms
step:50/1770 train_time:3760ms step_avg:93.99ms
step:51/1770 train_time:3854ms step_avg:94.00ms
step:52/1770 train_time:3948ms step_avg:93.99ms
step:53/1770 train_time:4041ms step_avg:93.98ms
step:54/1770 train_time:4136ms step_avg:93.99ms
step:55/1770 train_time:4229ms step_avg:93.98ms
step:56/1770 train_time:4323ms step_avg:93.98ms
step:57/1770 train_time:4417ms step_avg:93.97ms
step:58/1770 train_time:4510ms step_avg:93.97ms
step:59/1770 train_time:4604ms step_avg:93.96ms
step:60/1770 train_time:4698ms step_avg:93.97ms
step:61/1770 train_time:4792ms step_avg:93.95ms
step:62/1770 train_time:4886ms step_avg:93.95ms
step:63/1770 train_time:4980ms step_avg:93.96ms
step:64/1770 train_time:5075ms step_avg:93.98ms
step:65/1770 train_time:5169ms step_avg:93.98ms
step:66/1770 train_time:5263ms step_avg:93.98ms
step:67/1770 train_time:5357ms step_avg:93.98ms
step:68/1770 train_time:5451ms step_avg:93.98ms
step:69/1770 train_time:5544ms step_avg:93.97ms
step:70/1770 train_time:5638ms step_avg:93.97ms
step:71/1770 train_time:5732ms step_avg:93.97ms
step:72/1770 train_time:5826ms step_avg:93.97ms
step:73/1770 train_time:5920ms step_avg:93.97ms
step:74/1770 train_time:6014ms step_avg:93.97ms
step:75/1770 train_time:6108ms step_avg:93.97ms
step:76/1770 train_time:6202ms step_avg:93.97ms
step:77/1770 train_time:6296ms step_avg:93.97ms
step:78/1770 train_time:6390ms step_avg:93.97ms
step:79/1770 train_time:6483ms step_avg:93.96ms
step:80/1770 train_time:6578ms step_avg:93.97ms
step:81/1770 train_time:6672ms step_avg:93.97ms
step:82/1770 train_time:6766ms step_avg:93.97ms
step:83/1770 train_time:6860ms step_avg:93.97ms
step:84/1770 train_time:6954ms step_avg:93.97ms
step:85/1770 train_time:7048ms step_avg:93.97ms
step:86/1770 train_time:7142ms step_avg:93.98ms
step:87/1770 train_time:7236ms step_avg:93.98ms
step:88/1770 train_time:7330ms step_avg:93.97ms
step:89/1770 train_time:7424ms step_avg:93.97ms
step:90/1770 train_time:7518ms step_avg:93.98ms
step:91/1770 train_time:7612ms step_avg:93.98ms
step:92/1770 train_time:7705ms step_avg:93.97ms
step:93/1770 train_time:7799ms step_avg:93.97ms
step:94/1770 train_time:7894ms step_avg:93.97ms
step:95/1770 train_time:7988ms step_avg:93.98ms
step:96/1770 train_time:8083ms step_avg:93.99ms
step:97/1770 train_time:8177ms step_avg:93.99ms
step:98/1770 train_time:8271ms step_avg:93.99ms
step:99/1770 train_time:8365ms step_avg:93.99ms
step:100/1770 train_time:8459ms step_avg:93.99ms
step:101/1770 train_time:8553ms step_avg:93.99ms
step:102/1770 train_time:8647ms step_avg:93.99ms
step:103/1770 train_time:8741ms step_avg:93.99ms
step:104/1770 train_time:8836ms step_avg:94.00ms
step:105/1770 train_time:8929ms step_avg:93.99ms
step:106/1770 train_time:9023ms step_avg:93.99ms
step:107/1770 train_time:9117ms step_avg:93.99ms
step:108/1770 train_time:9211ms step_avg:93.99ms
step:109/1770 train_time:9305ms step_avg:93.99ms
step:110/1770 train_time:9399ms step_avg:93.99ms
step:111/1770 train_time:9493ms step_avg:93.99ms
step:112/1770 train_time:9587ms step_avg:93.99ms
step:113/1770 train_time:9680ms step_avg:93.98ms
step:114/1770 train_time:9774ms step_avg:93.98ms
step:115/1770 train_time:9868ms step_avg:93.98ms
step:116/1770 train_time:9961ms step_avg:93.98ms
step:117/1770 train_time:10055ms step_avg:93.98ms
step:118/1770 train_time:10149ms step_avg:93.97ms
step:119/1770 train_time:10243ms step_avg:93.97ms
step:120/1770 train_time:10336ms step_avg:93.97ms
step:121/1770 train_time:10430ms step_avg:93.97ms
step:122/1770 train_time:10525ms step_avg:93.97ms
step:123/1770 train_time:10618ms step_avg:93.97ms
step:124/1770 train_time:10713ms step_avg:93.97ms
step:125/1770 train_time:10806ms step_avg:93.97ms
step:125/1770 val_loss:4.6529 train_time:10899ms step_avg:94.77ms
step:126/1770 train_time:10920ms step_avg:94.14ms
step:127/1770 train_time:11004ms step_avg:94.05ms
step:128/1770 train_time:11104ms step_avg:94.10ms
step:129/1770 train_time:11199ms step_avg:94.11ms
step:130/1770 train_time:11293ms step_avg:94.11ms
step:131/1770 train_time:11387ms step_avg:94.11ms
step:132/1770 train_time:11480ms step_avg:94.10ms
step:133/1770 train_time:11574ms step_avg:94.10ms
step:134/1770 train_time:11668ms step_avg:94.10ms
step:135/1770 train_time:11762ms step_avg:94.10ms
step:136/1770 train_time:11856ms step_avg:94.10ms
step:137/1770 train_time:11951ms step_avg:94.10ms
step:138/1770 train_time:12046ms step_avg:94.11ms
step:139/1770 train_time:12142ms step_avg:94.12ms
step:140/1770 train_time:12237ms step_avg:94.13ms
step:141/1770 train_time:12331ms step_avg:94.13ms
step:142/1770 train_time:12425ms step_avg:94.13ms
step:143/1770 train_time:12519ms step_avg:94.13ms
step:144/1770 train_time:12613ms step_avg:94.13ms
step:145/1770 train_time:12708ms step_avg:94.13ms
step:146/1770 train_time:12802ms step_avg:94.13ms
step:147/1770 train_time:12896ms step_avg:94.13ms
step:148/1770 train_time:12991ms step_avg:94.14ms
step:149/1770 train_time:13086ms step_avg:94.14ms
step:150/1770 train_time:13181ms step_avg:94.15ms
step:151/1770 train_time:13276ms step_avg:94.15ms
step:152/1770 train_time:13371ms step_avg:94.16ms
step:153/1770 train_time:13465ms step_avg:94.16ms
step:154/1770 train_time:13560ms step_avg:94.16ms
step:155/1770 train_time:13655ms step_avg:94.17ms
step:156/1770 train_time:13750ms step_avg:94.18ms
step:157/1770 train_time:13844ms step_avg:94.18ms
step:158/1770 train_time:13939ms step_avg:94.18ms
step:159/1770 train_time:14034ms step_avg:94.19ms
step:160/1770 train_time:14129ms step_avg:94.19ms
step:161/1770 train_time:14224ms step_avg:94.20ms
step:162/1770 train_time:14319ms step_avg:94.20ms
step:163/1770 train_time:14413ms step_avg:94.20ms
step:164/1770 train_time:14507ms step_avg:94.20ms
step:165/1770 train_time:14602ms step_avg:94.21ms
step:166/1770 train_time:14696ms step_avg:94.21ms
step:167/1770 train_time:14792ms step_avg:94.21ms
step:168/1770 train_time:14886ms step_avg:94.21ms
step:169/1770 train_time:14980ms step_avg:94.21ms
step:170/1770 train_time:15075ms step_avg:94.22ms
step:171/1770 train_time:15170ms step_avg:94.23ms
step:172/1770 train_time:15264ms step_avg:94.22ms
step:173/1770 train_time:15360ms step_avg:94.23ms
step:174/1770 train_time:15454ms step_avg:94.23ms
step:175/1770 train_time:15549ms step_avg:94.23ms
step:176/1770 train_time:15643ms step_avg:94.23ms
step:177/1770 train_time:15737ms step_avg:94.24ms
step:178/1770 train_time:15833ms step_avg:94.24ms
step:179/1770 train_time:15927ms step_avg:94.24ms
step:180/1770 train_time:16022ms step_avg:94.24ms
step:181/1770 train_time:16116ms step_avg:94.25ms
step:182/1770 train_time:16211ms step_avg:94.25ms
step:183/1770 train_time:16305ms step_avg:94.25ms
step:184/1770 train_time:16400ms step_avg:94.25ms
step:185/1770 train_time:16494ms step_avg:94.25ms
step:186/1770 train_time:16589ms step_avg:94.25ms
step:187/1770 train_time:16683ms step_avg:94.25ms
step:188/1770 train_time:16778ms step_avg:94.26ms
step:189/1770 train_time:16873ms step_avg:94.26ms
step:190/1770 train_time:16968ms step_avg:94.27ms
step:191/1770 train_time:17062ms step_avg:94.27ms
step:192/1770 train_time:17157ms step_avg:94.27ms
step:193/1770 train_time:17252ms step_avg:94.27ms
step:194/1770 train_time:17347ms step_avg:94.28ms
step:195/1770 train_time:17442ms step_avg:94.28ms
step:196/1770 train_time:17537ms step_avg:94.28ms
step:197/1770 train_time:17631ms step_avg:94.29ms
step:198/1770 train_time:17726ms step_avg:94.29ms
step:199/1770 train_time:17820ms step_avg:94.29ms
step:200/1770 train_time:17915ms step_avg:94.29ms
step:201/1770 train_time:18010ms step_avg:94.29ms
step:202/1770 train_time:18104ms step_avg:94.29ms
step:203/1770 train_time:18198ms step_avg:94.29ms
step:204/1770 train_time:18293ms step_avg:94.29ms
step:205/1770 train_time:18387ms step_avg:94.29ms
step:206/1770 train_time:18481ms step_avg:94.29ms
step:207/1770 train_time:18576ms step_avg:94.29ms
step:208/1770 train_time:18670ms step_avg:94.29ms
step:209/1770 train_time:18765ms step_avg:94.30ms
step:210/1770 train_time:18860ms step_avg:94.30ms
step:211/1770 train_time:18955ms step_avg:94.30ms
step:212/1770 train_time:19049ms step_avg:94.30ms
step:213/1770 train_time:19144ms step_avg:94.30ms
step:214/1770 train_time:19238ms step_avg:94.30ms
step:215/1770 train_time:19333ms step_avg:94.31ms
step:216/1770 train_time:19427ms step_avg:94.31ms
step:217/1770 train_time:19522ms step_avg:94.31ms
step:218/1770 train_time:19617ms step_avg:94.31ms
step:219/1770 train_time:19711ms step_avg:94.31ms
step:220/1770 train_time:19805ms step_avg:94.31ms
step:221/1770 train_time:19900ms step_avg:94.31ms
step:222/1770 train_time:19995ms step_avg:94.31ms
step:223/1770 train_time:20090ms step_avg:94.32ms
step:224/1770 train_time:20184ms step_avg:94.32ms
step:225/1770 train_time:20278ms step_avg:94.32ms
step:226/1770 train_time:20374ms step_avg:94.32ms
step:227/1770 train_time:20468ms step_avg:94.32ms
step:228/1770 train_time:20562ms step_avg:94.32ms
step:229/1770 train_time:20657ms step_avg:94.32ms
step:230/1770 train_time:20752ms step_avg:94.33ms
step:231/1770 train_time:20846ms step_avg:94.32ms
step:232/1770 train_time:20940ms step_avg:94.33ms
step:233/1770 train_time:21035ms step_avg:94.33ms
step:234/1770 train_time:21130ms step_avg:94.33ms
step:235/1770 train_time:21224ms step_avg:94.33ms
step:236/1770 train_time:21318ms step_avg:94.33ms
step:237/1770 train_time:21413ms step_avg:94.33ms
step:238/1770 train_time:21508ms step_avg:94.33ms
step:239/1770 train_time:21602ms step_avg:94.33ms
step:240/1770 train_time:21696ms step_avg:94.33ms
step:241/1770 train_time:21792ms step_avg:94.34ms
step:242/1770 train_time:21886ms step_avg:94.33ms
step:243/1770 train_time:21980ms step_avg:94.33ms
step:244/1770 train_time:22075ms step_avg:94.34ms
step:245/1770 train_time:22169ms step_avg:94.34ms
step:246/1770 train_time:22264ms step_avg:94.34ms
step:247/1770 train_time:22359ms step_avg:94.34ms
step:248/1770 train_time:22453ms step_avg:94.34ms
step:249/1770 train_time:22548ms step_avg:94.34ms
step:250/1770 train_time:22642ms step_avg:94.34ms
step:250/1770 val_loss:4.1109 train_time:22735ms step_avg:94.73ms
step:251/1770 train_time:22756ms step_avg:94.42ms
step:252/1770 train_time:22840ms step_avg:94.38ms
step:253/1770 train_time:22938ms step_avg:94.40ms
step:254/1770 train_time:23033ms step_avg:94.40ms
step:255/1770 train_time:23128ms step_avg:94.40ms
step:256/1770 train_time:23222ms step_avg:94.40ms
step:257/1770 train_time:23316ms step_avg:94.40ms
step:258/1770 train_time:23410ms step_avg:94.40ms
step:259/1770 train_time:23505ms step_avg:94.40ms
step:260/1770 train_time:23598ms step_avg:94.39ms
step:261/1770 train_time:23692ms step_avg:94.39ms
step:262/1770 train_time:23788ms step_avg:94.40ms
step:263/1770 train_time:23883ms step_avg:94.40ms
step:264/1770 train_time:23979ms step_avg:94.40ms
step:265/1770 train_time:24075ms step_avg:94.41ms
step:266/1770 train_time:24170ms step_avg:94.41ms
step:267/1770 train_time:24265ms step_avg:94.41ms
step:268/1770 train_time:24359ms step_avg:94.42ms
step:269/1770 train_time:24454ms step_avg:94.42ms
step:270/1770 train_time:24550ms step_avg:94.42ms
step:271/1770 train_time:24644ms step_avg:94.42ms
step:272/1770 train_time:24740ms step_avg:94.43ms
step:273/1770 train_time:24835ms step_avg:94.43ms
step:274/1770 train_time:24930ms step_avg:94.43ms
step:275/1770 train_time:25025ms step_avg:94.43ms
step:276/1770 train_time:25120ms step_avg:94.44ms
step:277/1770 train_time:25215ms step_avg:94.44ms
step:278/1770 train_time:25311ms step_avg:94.44ms
step:279/1770 train_time:25405ms step_avg:94.44ms
step:280/1770 train_time:25501ms step_avg:94.45ms
step:281/1770 train_time:25595ms step_avg:94.45ms
step:282/1770 train_time:25690ms step_avg:94.45ms
step:283/1770 train_time:25785ms step_avg:94.45ms
step:284/1770 train_time:25880ms step_avg:94.45ms
step:285/1770 train_time:25975ms step_avg:94.46ms
step:286/1770 train_time:26071ms step_avg:94.46ms
step:287/1770 train_time:26166ms step_avg:94.46ms
step:288/1770 train_time:26261ms step_avg:94.46ms
step:289/1770 train_time:26357ms step_avg:94.47ms
step:290/1770 train_time:26452ms step_avg:94.47ms
step:291/1770 train_time:26547ms step_avg:94.47ms
step:292/1770 train_time:26642ms step_avg:94.48ms
step:293/1770 train_time:26737ms step_avg:94.48ms
step:294/1770 train_time:26832ms step_avg:94.48ms
step:295/1770 train_time:26928ms step_avg:94.48ms
step:296/1770 train_time:27023ms step_avg:94.49ms
step:297/1770 train_time:27119ms step_avg:94.49ms
step:298/1770 train_time:27214ms step_avg:94.49ms
step:299/1770 train_time:27310ms step_avg:94.50ms
step:300/1770 train_time:27405ms step_avg:94.50ms
step:301/1770 train_time:27500ms step_avg:94.50ms
step:302/1770 train_time:27595ms step_avg:94.50ms
step:303/1770 train_time:27691ms step_avg:94.51ms
step:304/1770 train_time:27785ms step_avg:94.51ms
step:305/1770 train_time:27880ms step_avg:94.51ms
step:306/1770 train_time:27975ms step_avg:94.51ms
step:307/1770 train_time:28075ms step_avg:94.53ms
step:308/1770 train_time:28166ms step_avg:94.52ms
step:309/1770 train_time:28261ms step_avg:94.52ms
step:310/1770 train_time:28357ms step_avg:94.52ms
step:311/1770 train_time:28453ms step_avg:94.53ms
step:312/1770 train_time:28548ms step_avg:94.53ms
step:313/1770 train_time:28642ms step_avg:94.53ms
step:314/1770 train_time:28737ms step_avg:94.53ms
step:315/1770 train_time:28832ms step_avg:94.53ms
step:316/1770 train_time:28927ms step_avg:94.53ms
step:317/1770 train_time:29023ms step_avg:94.54ms
step:318/1770 train_time:29118ms step_avg:94.54ms
step:319/1770 train_time:29213ms step_avg:94.54ms
step:320/1770 train_time:29308ms step_avg:94.54ms
step:321/1770 train_time:29403ms step_avg:94.54ms
step:322/1770 train_time:29498ms step_avg:94.55ms
step:323/1770 train_time:29593ms step_avg:94.55ms
step:324/1770 train_time:29691ms step_avg:94.56ms
step:325/1770 train_time:29784ms step_avg:94.55ms
step:326/1770 train_time:29880ms step_avg:94.56ms
step:327/1770 train_time:29975ms step_avg:94.56ms
step:328/1770 train_time:30070ms step_avg:94.56ms
step:329/1770 train_time:30165ms step_avg:94.56ms
step:330/1770 train_time:30260ms step_avg:94.56ms
step:331/1770 train_time:30355ms step_avg:94.56ms
step:332/1770 train_time:30450ms step_avg:94.57ms
step:333/1770 train_time:30545ms step_avg:94.57ms
step:334/1770 train_time:30639ms step_avg:94.57ms
step:335/1770 train_time:30735ms step_avg:94.57ms
step:336/1770 train_time:30830ms step_avg:94.57ms
step:337/1770 train_time:30924ms step_avg:94.57ms
step:338/1770 train_time:31019ms step_avg:94.57ms
step:339/1770 train_time:31115ms step_avg:94.57ms
step:340/1770 train_time:31210ms step_avg:94.57ms
step:341/1770 train_time:31305ms step_avg:94.58ms
step:342/1770 train_time:31399ms step_avg:94.58ms
step:343/1770 train_time:31494ms step_avg:94.58ms
step:344/1770 train_time:31590ms step_avg:94.58ms
step:345/1770 train_time:31685ms step_avg:94.58ms
step:346/1770 train_time:31780ms step_avg:94.58ms
step:347/1770 train_time:31876ms step_avg:94.59ms
step:348/1770 train_time:31970ms step_avg:94.59ms
step:349/1770 train_time:32065ms step_avg:94.59ms
step:350/1770 train_time:32160ms step_avg:94.59ms
step:351/1770 train_time:32256ms step_avg:94.59ms
step:352/1770 train_time:32352ms step_avg:94.60ms
step:353/1770 train_time:32446ms step_avg:94.60ms
step:354/1770 train_time:32541ms step_avg:94.60ms
step:355/1770 train_time:32636ms step_avg:94.60ms
step:356/1770 train_time:32731ms step_avg:94.60ms
step:357/1770 train_time:32826ms step_avg:94.60ms
step:358/1770 train_time:32921ms step_avg:94.60ms
step:359/1770 train_time:33017ms step_avg:94.60ms
step:360/1770 train_time:33113ms step_avg:94.61ms
step:361/1770 train_time:33207ms step_avg:94.61ms
step:362/1770 train_time:33302ms step_avg:94.61ms
step:363/1770 train_time:33397ms step_avg:94.61ms
step:364/1770 train_time:33493ms step_avg:94.61ms
step:365/1770 train_time:33588ms step_avg:94.61ms
step:366/1770 train_time:33683ms step_avg:94.62ms
step:367/1770 train_time:33778ms step_avg:94.62ms
step:368/1770 train_time:33874ms step_avg:94.62ms
step:369/1770 train_time:33969ms step_avg:94.62ms
step:370/1770 train_time:34065ms step_avg:94.62ms
step:371/1770 train_time:34160ms step_avg:94.62ms
step:372/1770 train_time:34255ms step_avg:94.63ms
step:373/1770 train_time:34350ms step_avg:94.63ms
step:374/1770 train_time:34445ms step_avg:94.63ms
step:375/1770 train_time:34540ms step_avg:94.63ms
step:375/1770 val_loss:3.9126 train_time:34634ms step_avg:94.89ms
step:376/1770 train_time:34655ms step_avg:94.69ms
step:377/1770 train_time:34741ms step_avg:94.66ms
step:378/1770 train_time:34840ms step_avg:94.68ms
step:379/1770 train_time:34936ms step_avg:94.68ms
step:380/1770 train_time:35031ms step_avg:94.68ms
step:381/1770 train_time:35126ms step_avg:94.68ms
step:382/1770 train_time:35220ms step_avg:94.68ms
step:383/1770 train_time:35315ms step_avg:94.68ms
step:384/1770 train_time:35409ms step_avg:94.68ms
step:385/1770 train_time:35504ms step_avg:94.68ms
step:386/1770 train_time:35598ms step_avg:94.68ms
step:387/1770 train_time:35694ms step_avg:94.68ms
step:388/1770 train_time:35791ms step_avg:94.68ms
step:389/1770 train_time:35887ms step_avg:94.69ms
step:390/1770 train_time:35982ms step_avg:94.69ms
step:391/1770 train_time:36077ms step_avg:94.69ms
step:392/1770 train_time:36171ms step_avg:94.69ms
step:393/1770 train_time:36267ms step_avg:94.69ms
step:394/1770 train_time:36361ms step_avg:94.69ms
step:395/1770 train_time:36456ms step_avg:94.69ms
step:396/1770 train_time:36553ms step_avg:94.70ms
step:397/1770 train_time:36650ms step_avg:94.70ms
step:398/1770 train_time:36748ms step_avg:94.71ms
step:399/1770 train_time:36845ms step_avg:94.72ms
step:400/1770 train_time:36942ms step_avg:94.72ms
step:401/1770 train_time:37039ms step_avg:94.73ms
step:402/1770 train_time:37135ms step_avg:94.73ms
step:403/1770 train_time:37232ms step_avg:94.74ms
step:404/1770 train_time:37329ms step_avg:94.74ms
step:405/1770 train_time:37426ms step_avg:94.75ms
step:406/1770 train_time:37523ms step_avg:94.75ms
step:407/1770 train_time:37620ms step_avg:94.76ms
step:408/1770 train_time:37716ms step_avg:94.76ms
step:409/1770 train_time:37813ms step_avg:94.77ms
step:410/1770 train_time:37910ms step_avg:94.78ms
step:411/1770 train_time:38008ms step_avg:94.78ms
step:412/1770 train_time:38105ms step_avg:94.79ms
step:413/1770 train_time:38204ms step_avg:94.80ms
step:414/1770 train_time:38299ms step_avg:94.80ms
step:415/1770 train_time:38396ms step_avg:94.81ms
step:416/1770 train_time:38493ms step_avg:94.81ms
step:417/1770 train_time:38590ms step_avg:94.82ms
step:418/1770 train_time:38687ms step_avg:94.82ms
step:419/1770 train_time:38784ms step_avg:94.83ms
step:420/1770 train_time:38881ms step_avg:94.83ms
step:421/1770 train_time:38977ms step_avg:94.84ms
step:422/1770 train_time:39074ms step_avg:94.84ms
step:423/1770 train_time:39171ms step_avg:94.85ms
step:424/1770 train_time:39268ms step_avg:94.85ms
step:425/1770 train_time:39366ms step_avg:94.86ms
step:426/1770 train_time:39463ms step_avg:94.86ms
step:427/1770 train_time:39560ms step_avg:94.87ms
step:428/1770 train_time:39657ms step_avg:94.87ms
step:429/1770 train_time:39754ms step_avg:94.88ms
step:430/1770 train_time:39851ms step_avg:94.88ms
step:431/1770 train_time:39948ms step_avg:94.89ms
step:432/1770 train_time:40045ms step_avg:94.89ms
step:433/1770 train_time:40142ms step_avg:94.90ms
step:434/1770 train_time:40239ms step_avg:94.90ms
step:435/1770 train_time:40335ms step_avg:94.91ms
step:436/1770 train_time:40432ms step_avg:94.91ms
step:437/1770 train_time:40529ms step_avg:94.92ms
step:438/1770 train_time:40626ms step_avg:94.92ms
step:439/1770 train_time:40723ms step_avg:94.93ms
step:440/1770 train_time:40819ms step_avg:94.93ms
step:441/1770 train_time:40916ms step_avg:94.93ms
step:442/1770 train_time:41013ms step_avg:94.94ms
step:443/1770 train_time:41110ms step_avg:94.94ms
step:444/1770 train_time:41208ms step_avg:94.95ms
step:445/1770 train_time:41305ms step_avg:94.95ms
step:446/1770 train_time:41402ms step_avg:94.96ms
step:447/1770 train_time:41499ms step_avg:94.96ms
step:448/1770 train_time:41596ms step_avg:94.97ms
step:449/1770 train_time:41698ms step_avg:94.98ms
step:450/1770 train_time:41790ms step_avg:94.98ms
step:451/1770 train_time:41888ms step_avg:94.98ms
step:452/1770 train_time:41985ms step_avg:94.99ms
step:453/1770 train_time:42081ms step_avg:94.99ms
step:454/1770 train_time:42178ms step_avg:95.00ms
step:455/1770 train_time:42275ms step_avg:95.00ms
step:456/1770 train_time:42372ms step_avg:95.00ms
step:457/1770 train_time:42469ms step_avg:95.01ms
step:458/1770 train_time:42566ms step_avg:95.01ms
step:459/1770 train_time:42664ms step_avg:95.02ms
step:460/1770 train_time:42760ms step_avg:95.02ms
step:461/1770 train_time:42857ms step_avg:95.03ms
step:462/1770 train_time:42953ms step_avg:95.03ms
step:463/1770 train_time:43050ms step_avg:95.03ms
step:464/1770 train_time:43148ms step_avg:95.04ms
step:465/1770 train_time:43245ms step_avg:95.04ms
step:466/1770 train_time:43342ms step_avg:95.05ms
step:467/1770 train_time:43438ms step_avg:95.05ms
step:468/1770 train_time:43535ms step_avg:95.05ms
step:469/1770 train_time:43631ms step_avg:95.06ms
step:470/1770 train_time:43728ms step_avg:95.06ms
step:471/1770 train_time:43826ms step_avg:95.07ms
step:472/1770 train_time:43922ms step_avg:95.07ms
step:473/1770 train_time:44019ms step_avg:95.07ms
step:474/1770 train_time:44116ms step_avg:95.08ms
step:475/1770 train_time:44213ms step_avg:95.08ms
step:476/1770 train_time:44310ms step_avg:95.09ms
step:477/1770 train_time:44408ms step_avg:95.09ms
step:478/1770 train_time:44505ms step_avg:95.10ms
step:479/1770 train_time:44602ms step_avg:95.10ms
step:480/1770 train_time:44698ms step_avg:95.10ms
step:481/1770 train_time:44795ms step_avg:95.11ms
step:482/1770 train_time:44892ms step_avg:95.11ms
step:483/1770 train_time:44989ms step_avg:95.11ms
step:484/1770 train_time:45086ms step_avg:95.12ms
step:485/1770 train_time:45184ms step_avg:95.12ms
step:486/1770 train_time:45280ms step_avg:95.13ms
step:487/1770 train_time:45377ms step_avg:95.13ms
step:488/1770 train_time:45474ms step_avg:95.13ms
step:489/1770 train_time:45571ms step_avg:95.14ms
step:490/1770 train_time:45668ms step_avg:95.14ms
step:491/1770 train_time:45766ms step_avg:95.15ms
step:492/1770 train_time:45863ms step_avg:95.15ms
step:493/1770 train_time:45960ms step_avg:95.15ms
step:494/1770 train_time:46057ms step_avg:95.16ms
step:495/1770 train_time:46153ms step_avg:95.16ms
step:496/1770 train_time:46250ms step_avg:95.16ms
step:497/1770 train_time:46347ms step_avg:95.17ms
step:498/1770 train_time:46445ms step_avg:95.17ms
step:499/1770 train_time:46542ms step_avg:95.18ms
step:500/1770 train_time:46638ms step_avg:95.18ms
step:500/1770 val_loss:3.7626 train_time:46734ms step_avg:95.38ms
step:501/1770 train_time:46757ms step_avg:95.23ms
step:502/1770 train_time:46845ms step_avg:95.21ms
step:503/1770 train_time:46943ms step_avg:95.22ms
step:504/1770 train_time:47040ms step_avg:95.22ms
step:505/1770 train_time:47136ms step_avg:95.23ms
step:506/1770 train_time:47233ms step_avg:95.23ms
step:507/1770 train_time:47330ms step_avg:95.23ms
step:508/1770 train_time:47426ms step_avg:95.23ms
step:509/1770 train_time:47522ms step_avg:95.24ms
step:510/1770 train_time:47618ms step_avg:95.24ms
step:511/1770 train_time:47715ms step_avg:95.24ms
step:512/1770 train_time:47814ms step_avg:95.25ms
step:513/1770 train_time:47913ms step_avg:95.25ms
step:514/1770 train_time:48011ms step_avg:95.26ms
step:515/1770 train_time:48109ms step_avg:95.27ms
step:516/1770 train_time:48206ms step_avg:95.27ms
step:517/1770 train_time:48306ms step_avg:95.28ms
step:518/1770 train_time:48400ms step_avg:95.28ms
step:519/1770 train_time:48496ms step_avg:95.28ms
step:520/1770 train_time:48593ms step_avg:95.28ms
step:521/1770 train_time:48690ms step_avg:95.28ms
step:522/1770 train_time:48787ms step_avg:95.29ms
step:523/1770 train_time:48884ms step_avg:95.29ms
step:524/1770 train_time:48982ms step_avg:95.29ms
step:525/1770 train_time:49079ms step_avg:95.30ms
step:526/1770 train_time:49176ms step_avg:95.30ms
step:527/1770 train_time:49274ms step_avg:95.31ms
step:528/1770 train_time:49372ms step_avg:95.31ms
step:529/1770 train_time:49469ms step_avg:95.32ms
step:530/1770 train_time:49566ms step_avg:95.32ms
step:531/1770 train_time:49663ms step_avg:95.32ms
step:532/1770 train_time:49760ms step_avg:95.32ms
step:533/1770 train_time:49857ms step_avg:95.33ms
step:534/1770 train_time:49954ms step_avg:95.33ms
step:535/1770 train_time:50053ms step_avg:95.34ms
step:536/1770 train_time:50151ms step_avg:95.34ms
step:537/1770 train_time:50249ms step_avg:95.35ms
step:538/1770 train_time:50346ms step_avg:95.35ms
step:539/1770 train_time:50443ms step_avg:95.36ms
step:540/1770 train_time:50541ms step_avg:95.36ms
step:541/1770 train_time:50637ms step_avg:95.36ms
step:542/1770 train_time:50734ms step_avg:95.37ms
step:543/1770 train_time:50832ms step_avg:95.37ms
step:544/1770 train_time:50930ms step_avg:95.37ms
step:545/1770 train_time:51027ms step_avg:95.38ms
step:546/1770 train_time:51124ms step_avg:95.38ms
step:547/1770 train_time:51221ms step_avg:95.38ms
step:548/1770 train_time:51319ms step_avg:95.39ms
step:549/1770 train_time:51416ms step_avg:95.39ms
step:550/1770 train_time:51514ms step_avg:95.40ms
step:551/1770 train_time:51611ms step_avg:95.40ms
step:552/1770 train_time:51709ms step_avg:95.40ms
step:553/1770 train_time:51806ms step_avg:95.41ms
step:554/1770 train_time:51903ms step_avg:95.41ms
step:555/1770 train_time:52003ms step_avg:95.42ms
step:556/1770 train_time:52097ms step_avg:95.42ms
step:557/1770 train_time:52194ms step_avg:95.42ms
step:558/1770 train_time:52293ms step_avg:95.42ms
step:559/1770 train_time:52390ms step_avg:95.43ms
step:560/1770 train_time:52488ms step_avg:95.43ms
step:561/1770 train_time:52585ms step_avg:95.44ms
step:562/1770 train_time:52682ms step_avg:95.44ms
step:563/1770 train_time:52779ms step_avg:95.44ms
step:564/1770 train_time:52876ms step_avg:95.44ms
step:565/1770 train_time:52974ms step_avg:95.45ms
step:566/1770 train_time:53071ms step_avg:95.45ms
step:567/1770 train_time:53169ms step_avg:95.46ms
step:568/1770 train_time:53267ms step_avg:95.46ms
step:569/1770 train_time:53363ms step_avg:95.46ms
step:570/1770 train_time:53460ms step_avg:95.46ms
step:571/1770 train_time:53558ms step_avg:95.47ms
step:572/1770 train_time:53655ms step_avg:95.47ms
step:573/1770 train_time:53753ms step_avg:95.48ms
step:574/1770 train_time:53851ms step_avg:95.48ms
step:575/1770 train_time:53948ms step_avg:95.48ms
step:576/1770 train_time:54045ms step_avg:95.49ms
step:577/1770 train_time:54142ms step_avg:95.49ms
step:578/1770 train_time:54239ms step_avg:95.49ms
step:579/1770 train_time:54336ms step_avg:95.49ms
step:580/1770 train_time:54434ms step_avg:95.50ms
step:581/1770 train_time:54532ms step_avg:95.50ms
step:582/1770 train_time:54630ms step_avg:95.51ms
step:583/1770 train_time:54727ms step_avg:95.51ms
step:584/1770 train_time:54824ms step_avg:95.51ms
step:585/1770 train_time:54921ms step_avg:95.51ms
step:586/1770 train_time:55018ms step_avg:95.52ms
step:587/1770 train_time:55115ms step_avg:95.52ms
step:588/1770 train_time:55213ms step_avg:95.52ms
step:589/1770 train_time:55311ms step_avg:95.53ms
step:590/1770 train_time:55410ms step_avg:95.54ms
step:591/1770 train_time:55506ms step_avg:95.54ms
step:592/1770 train_time:55604ms step_avg:95.54ms
step:593/1770 train_time:55701ms step_avg:95.54ms
step:594/1770 train_time:55800ms step_avg:95.55ms
step:595/1770 train_time:55897ms step_avg:95.55ms
step:596/1770 train_time:55994ms step_avg:95.55ms
step:597/1770 train_time:56091ms step_avg:95.56ms
step:598/1770 train_time:56189ms step_avg:95.56ms
step:599/1770 train_time:56285ms step_avg:95.56ms
step:600/1770 train_time:56382ms step_avg:95.56ms
step:601/1770 train_time:56480ms step_avg:95.57ms
step:602/1770 train_time:56577ms step_avg:95.57ms
step:603/1770 train_time:56675ms step_avg:95.57ms
step:604/1770 train_time:56773ms step_avg:95.58ms
step:605/1770 train_time:56871ms step_avg:95.58ms
step:606/1770 train_time:56969ms step_avg:95.59ms
step:607/1770 train_time:57066ms step_avg:95.59ms
step:608/1770 train_time:57163ms step_avg:95.59ms
step:609/1770 train_time:57260ms step_avg:95.59ms
step:610/1770 train_time:57357ms step_avg:95.60ms
step:611/1770 train_time:57454ms step_avg:95.60ms
step:612/1770 train_time:57552ms step_avg:95.60ms
step:613/1770 train_time:57650ms step_avg:95.60ms
step:614/1770 train_time:57748ms step_avg:95.61ms
step:615/1770 train_time:57846ms step_avg:95.61ms
step:616/1770 train_time:57943ms step_avg:95.62ms
step:617/1770 train_time:58040ms step_avg:95.62ms
step:618/1770 train_time:58138ms step_avg:95.62ms
step:619/1770 train_time:58235ms step_avg:95.62ms
step:620/1770 train_time:58332ms step_avg:95.63ms
step:621/1770 train_time:58430ms step_avg:95.63ms
step:622/1770 train_time:58527ms step_avg:95.63ms
step:623/1770 train_time:58625ms step_avg:95.64ms
step:624/1770 train_time:58723ms step_avg:95.64ms
step:625/1770 train_time:58821ms step_avg:95.64ms
step:625/1770 val_loss:3.6748 train_time:58916ms step_avg:95.80ms
step:626/1770 train_time:58937ms step_avg:95.68ms
step:627/1770 train_time:59023ms step_avg:95.66ms
step:628/1770 train_time:59123ms step_avg:95.67ms
step:629/1770 train_time:59221ms step_avg:95.67ms
step:630/1770 train_time:59318ms step_avg:95.67ms
step:631/1770 train_time:59415ms step_avg:95.68ms
step:632/1770 train_time:59512ms step_avg:95.68ms
step:633/1770 train_time:59609ms step_avg:95.68ms
step:634/1770 train_time:59706ms step_avg:95.68ms
step:635/1770 train_time:59803ms step_avg:95.68ms
step:636/1770 train_time:59900ms step_avg:95.69ms
step:637/1770 train_time:60000ms step_avg:95.69ms
step:638/1770 train_time:60099ms step_avg:95.70ms
step:639/1770 train_time:60197ms step_avg:95.70ms
step:640/1770 train_time:60294ms step_avg:95.70ms
step:641/1770 train_time:60391ms step_avg:95.71ms
step:642/1770 train_time:60490ms step_avg:95.71ms
step:643/1770 train_time:60587ms step_avg:95.71ms
step:644/1770 train_time:60684ms step_avg:95.72ms
step:645/1770 train_time:60781ms step_avg:95.72ms
step:646/1770 train_time:60878ms step_avg:95.72ms
step:647/1770 train_time:60975ms step_avg:95.72ms
step:648/1770 train_time:61074ms step_avg:95.73ms
step:649/1770 train_time:61172ms step_avg:95.73ms
step:650/1770 train_time:61270ms step_avg:95.73ms
step:651/1770 train_time:61368ms step_avg:95.74ms
step:652/1770 train_time:61465ms step_avg:95.74ms
step:653/1770 train_time:61562ms step_avg:95.74ms
step:654/1770 train_time:61659ms step_avg:95.74ms
step:655/1770 train_time:61757ms step_avg:95.75ms
step:656/1770 train_time:61854ms step_avg:95.75ms
step:657/1770 train_time:61951ms step_avg:95.75ms
step:658/1770 train_time:62050ms step_avg:95.76ms
step:659/1770 train_time:62149ms step_avg:95.76ms
step:660/1770 train_time:62248ms step_avg:95.77ms
step:661/1770 train_time:62347ms step_avg:95.77ms
step:662/1770 train_time:62446ms step_avg:95.78ms
step:663/1770 train_time:62546ms step_avg:95.78ms
step:664/1770 train_time:62645ms step_avg:95.79ms
step:665/1770 train_time:62744ms step_avg:95.79ms
step:666/1770 train_time:62843ms step_avg:95.80ms
step:667/1770 train_time:62942ms step_avg:95.80ms
step:668/1770 train_time:63041ms step_avg:95.81ms
step:669/1770 train_time:63140ms step_avg:95.81ms
step:670/1770 train_time:63239ms step_avg:95.82ms
step:671/1770 train_time:63337ms step_avg:95.82ms
step:672/1770 train_time:63437ms step_avg:95.83ms
step:673/1770 train_time:63536ms step_avg:95.83ms
step:674/1770 train_time:63635ms step_avg:95.84ms
step:675/1770 train_time:63734ms step_avg:95.84ms
step:676/1770 train_time:63833ms step_avg:95.84ms
step:677/1770 train_time:63932ms step_avg:95.85ms
step:678/1770 train_time:64031ms step_avg:95.85ms
step:679/1770 train_time:64130ms step_avg:95.86ms
step:680/1770 train_time:64229ms step_avg:95.86ms
step:681/1770 train_time:64329ms step_avg:95.87ms
step:682/1770 train_time:64429ms step_avg:95.88ms
step:683/1770 train_time:64528ms step_avg:95.88ms
step:684/1770 train_time:64628ms step_avg:95.89ms
step:685/1770 train_time:64729ms step_avg:95.89ms
step:686/1770 train_time:64829ms step_avg:95.90ms
step:687/1770 train_time:64928ms step_avg:95.91ms
step:688/1770 train_time:65028ms step_avg:95.91ms
step:689/1770 train_time:65128ms step_avg:95.92ms
step:690/1770 train_time:65227ms step_avg:95.92ms
step:691/1770 train_time:65326ms step_avg:95.93ms
step:692/1770 train_time:65425ms step_avg:95.93ms
step:693/1770 train_time:65524ms step_avg:95.94ms
step:694/1770 train_time:65623ms step_avg:95.94ms
step:695/1770 train_time:65722ms step_avg:95.95ms
step:696/1770 train_time:65821ms step_avg:95.95ms
step:697/1770 train_time:65921ms step_avg:95.95ms
step:698/1770 train_time:66019ms step_avg:95.96ms
step:699/1770 train_time:66119ms step_avg:95.96ms
step:700/1770 train_time:66218ms step_avg:95.97ms
step:701/1770 train_time:66318ms step_avg:95.97ms
step:702/1770 train_time:66417ms step_avg:95.98ms
step:703/1770 train_time:66516ms step_avg:95.98ms
step:704/1770 train_time:66615ms step_avg:95.99ms
step:705/1770 train_time:66714ms step_avg:95.99ms
step:706/1770 train_time:66813ms step_avg:96.00ms
step:707/1770 train_time:66912ms step_avg:96.00ms
step:708/1770 train_time:67011ms step_avg:96.01ms
step:709/1770 train_time:67112ms step_avg:96.01ms
step:710/1770 train_time:67211ms step_avg:96.02ms
step:711/1770 train_time:67311ms step_avg:96.02ms
step:712/1770 train_time:67411ms step_avg:96.03ms
step:713/1770 train_time:67511ms step_avg:96.03ms
step:714/1770 train_time:67611ms step_avg:96.04ms
step:715/1770 train_time:67711ms step_avg:96.04ms
step:716/1770 train_time:67810ms step_avg:96.05ms
step:717/1770 train_time:67909ms step_avg:96.05ms
step:718/1770 train_time:68008ms step_avg:96.06ms
step:719/1770 train_time:68108ms step_avg:96.06ms
step:720/1770 train_time:68207ms step_avg:96.07ms
step:721/1770 train_time:68307ms step_avg:96.07ms
step:722/1770 train_time:68406ms step_avg:96.08ms
step:723/1770 train_time:68505ms step_avg:96.08ms
step:724/1770 train_time:68604ms step_avg:96.08ms
step:725/1770 train_time:68704ms step_avg:96.09ms
step:726/1770 train_time:68803ms step_avg:96.09ms
step:727/1770 train_time:68904ms step_avg:96.10ms
step:728/1770 train_time:69001ms step_avg:96.10ms
step:729/1770 train_time:69100ms step_avg:96.11ms
step:730/1770 train_time:69200ms step_avg:96.11ms
step:731/1770 train_time:69298ms step_avg:96.11ms
step:732/1770 train_time:69397ms step_avg:96.12ms
step:733/1770 train_time:69496ms step_avg:96.12ms
step:734/1770 train_time:69595ms step_avg:96.13ms
step:735/1770 train_time:69695ms step_avg:96.13ms
step:736/1770 train_time:69794ms step_avg:96.14ms
step:737/1770 train_time:69893ms step_avg:96.14ms
step:738/1770 train_time:69993ms step_avg:96.14ms
step:739/1770 train_time:70092ms step_avg:96.15ms
step:740/1770 train_time:70191ms step_avg:96.15ms
step:741/1770 train_time:70292ms step_avg:96.16ms
step:742/1770 train_time:70391ms step_avg:96.16ms
step:743/1770 train_time:70491ms step_avg:96.17ms
step:744/1770 train_time:70591ms step_avg:96.17ms
step:745/1770 train_time:70691ms step_avg:96.18ms
step:746/1770 train_time:70791ms step_avg:96.18ms
step:747/1770 train_time:70897ms step_avg:96.20ms
step:748/1770 train_time:70991ms step_avg:96.19ms
step:749/1770 train_time:71092ms step_avg:96.20ms
step:750/1770 train_time:71191ms step_avg:96.20ms
step:750/1770 val_loss:3.6109 train_time:71289ms step_avg:96.34ms
step:751/1770 train_time:71309ms step_avg:96.23ms
step:752/1770 train_time:71399ms step_avg:96.23ms
step:753/1770 train_time:71500ms step_avg:96.23ms
step:754/1770 train_time:71599ms step_avg:96.24ms
step:755/1770 train_time:71698ms step_avg:96.24ms
step:756/1770 train_time:71796ms step_avg:96.24ms
step:757/1770 train_time:71895ms step_avg:96.25ms
step:758/1770 train_time:71995ms step_avg:96.25ms
step:759/1770 train_time:72093ms step_avg:96.25ms
step:760/1770 train_time:72192ms step_avg:96.26ms
step:761/1770 train_time:72291ms step_avg:96.26ms
step:762/1770 train_time:72391ms step_avg:96.26ms
step:763/1770 train_time:72492ms step_avg:96.27ms
step:764/1770 train_time:72591ms step_avg:96.27ms
step:765/1770 train_time:72692ms step_avg:96.28ms
step:766/1770 train_time:72793ms step_avg:96.29ms
step:767/1770 train_time:72894ms step_avg:96.29ms
step:768/1770 train_time:72992ms step_avg:96.30ms
step:769/1770 train_time:73092ms step_avg:96.30ms
step:770/1770 train_time:73191ms step_avg:96.30ms
step:771/1770 train_time:73290ms step_avg:96.31ms
step:772/1770 train_time:73389ms step_avg:96.31ms
step:773/1770 train_time:73488ms step_avg:96.31ms
step:774/1770 train_time:73587ms step_avg:96.32ms
step:775/1770 train_time:73687ms step_avg:96.32ms
step:776/1770 train_time:73788ms step_avg:96.33ms
step:777/1770 train_time:73885ms step_avg:96.33ms
step:778/1770 train_time:73984ms step_avg:96.33ms
step:779/1770 train_time:74082ms step_avg:96.34ms
step:780/1770 train_time:74181ms step_avg:96.34ms
step:781/1770 train_time:74280ms step_avg:96.34ms
step:782/1770 train_time:74379ms step_avg:96.35ms
step:783/1770 train_time:74479ms step_avg:96.35ms
step:784/1770 train_time:74579ms step_avg:96.36ms
step:785/1770 train_time:74679ms step_avg:96.36ms
step:786/1770 train_time:74779ms step_avg:96.36ms
step:787/1770 train_time:74879ms step_avg:96.37ms
step:788/1770 train_time:74979ms step_avg:96.37ms
step:789/1770 train_time:75078ms step_avg:96.38ms
step:790/1770 train_time:75178ms step_avg:96.38ms
step:791/1770 train_time:75278ms step_avg:96.39ms
step:792/1770 train_time:75376ms step_avg:96.39ms
step:793/1770 train_time:75476ms step_avg:96.39ms
step:794/1770 train_time:75580ms step_avg:96.40ms
step:795/1770 train_time:75675ms step_avg:96.40ms
step:796/1770 train_time:75774ms step_avg:96.41ms
step:797/1770 train_time:75874ms step_avg:96.41ms
step:798/1770 train_time:75973ms step_avg:96.41ms
step:799/1770 train_time:76073ms step_avg:96.42ms
step:800/1770 train_time:76173ms step_avg:96.42ms
step:801/1770 train_time:76272ms step_avg:96.43ms
step:802/1770 train_time:76372ms step_avg:96.43ms
step:803/1770 train_time:76473ms step_avg:96.43ms
step:804/1770 train_time:76572ms step_avg:96.44ms
step:805/1770 train_time:76672ms step_avg:96.44ms
step:806/1770 train_time:76772ms step_avg:96.45ms
step:807/1770 train_time:76872ms step_avg:96.45ms
step:808/1770 train_time:76972ms step_avg:96.46ms
step:809/1770 train_time:77075ms step_avg:96.46ms
step:810/1770 train_time:77172ms step_avg:96.47ms
step:811/1770 train_time:77272ms step_avg:96.47ms
step:812/1770 train_time:77372ms step_avg:96.47ms
step:813/1770 train_time:77472ms step_avg:96.48ms
step:814/1770 train_time:77572ms step_avg:96.48ms
step:815/1770 train_time:77671ms step_avg:96.49ms
step:816/1770 train_time:77771ms step_avg:96.49ms
step:817/1770 train_time:77871ms step_avg:96.49ms
step:818/1770 train_time:77970ms step_avg:96.50ms
step:819/1770 train_time:78071ms step_avg:96.50ms
step:820/1770 train_time:78171ms step_avg:96.51ms
step:821/1770 train_time:78271ms step_avg:96.51ms
step:822/1770 train_time:78370ms step_avg:96.52ms
step:823/1770 train_time:78470ms step_avg:96.52ms
step:824/1770 train_time:78570ms step_avg:96.52ms
step:825/1770 train_time:78672ms step_avg:96.53ms
step:826/1770 train_time:78769ms step_avg:96.53ms
step:827/1770 train_time:78869ms step_avg:96.54ms
step:828/1770 train_time:78970ms step_avg:96.54ms
step:829/1770 train_time:79070ms step_avg:96.54ms
step:830/1770 train_time:79169ms step_avg:96.55ms
step:831/1770 train_time:79269ms step_avg:96.55ms
step:832/1770 train_time:79369ms step_avg:96.56ms
step:833/1770 train_time:79470ms step_avg:96.56ms
step:834/1770 train_time:79570ms step_avg:96.57ms
step:835/1770 train_time:79670ms step_avg:96.57ms
step:836/1770 train_time:79770ms step_avg:96.57ms
step:837/1770 train_time:79869ms step_avg:96.58ms
step:838/1770 train_time:79969ms step_avg:96.58ms
step:839/1770 train_time:80069ms step_avg:96.59ms
step:840/1770 train_time:80169ms step_avg:96.59ms
step:841/1770 train_time:80268ms step_avg:96.59ms
step:842/1770 train_time:80367ms step_avg:96.60ms
step:843/1770 train_time:80467ms step_avg:96.60ms
step:844/1770 train_time:80566ms step_avg:96.60ms
step:845/1770 train_time:80665ms step_avg:96.60ms
step:846/1770 train_time:80764ms step_avg:96.61ms
step:847/1770 train_time:80862ms step_avg:96.61ms
step:848/1770 train_time:80961ms step_avg:96.61ms
step:849/1770 train_time:81061ms step_avg:96.62ms
step:850/1770 train_time:81160ms step_avg:96.62ms
step:851/1770 train_time:81260ms step_avg:96.62ms
step:852/1770 train_time:81360ms step_avg:96.63ms
step:853/1770 train_time:81460ms step_avg:96.63ms
step:854/1770 train_time:81560ms step_avg:96.63ms
step:855/1770 train_time:81660ms step_avg:96.64ms
step:856/1770 train_time:81759ms step_avg:96.64ms
step:857/1770 train_time:81859ms step_avg:96.65ms
step:858/1770 train_time:81959ms step_avg:96.65ms
step:859/1770 train_time:82058ms step_avg:96.65ms
step:860/1770 train_time:82158ms step_avg:96.66ms
step:861/1770 train_time:82258ms step_avg:96.66ms
step:862/1770 train_time:82358ms step_avg:96.66ms
step:863/1770 train_time:82457ms step_avg:96.67ms
step:864/1770 train_time:82556ms step_avg:96.67ms
step:865/1770 train_time:82655ms step_avg:96.67ms
step:866/1770 train_time:82755ms step_avg:96.68ms
step:867/1770 train_time:82854ms step_avg:96.68ms
step:868/1770 train_time:82954ms step_avg:96.68ms
step:869/1770 train_time:83053ms step_avg:96.69ms
step:870/1770 train_time:83153ms step_avg:96.69ms
step:871/1770 train_time:83252ms step_avg:96.69ms
step:872/1770 train_time:83352ms step_avg:96.70ms
step:873/1770 train_time:83452ms step_avg:96.70ms
step:874/1770 train_time:83551ms step_avg:96.70ms
step:875/1770 train_time:83651ms step_avg:96.71ms
step:875/1770 val_loss:3.5590 train_time:83749ms step_avg:96.82ms
step:876/1770 train_time:83770ms step_avg:96.73ms
step:877/1770 train_time:83861ms step_avg:96.73ms
step:878/1770 train_time:83962ms step_avg:96.73ms
step:879/1770 train_time:84061ms step_avg:96.73ms
step:880/1770 train_time:84160ms step_avg:96.74ms
step:881/1770 train_time:84259ms step_avg:96.74ms
step:882/1770 train_time:84362ms step_avg:96.75ms
step:883/1770 train_time:84457ms step_avg:96.74ms
step:884/1770 train_time:84555ms step_avg:96.75ms
step:885/1770 train_time:84655ms step_avg:96.75ms
step:886/1770 train_time:84754ms step_avg:96.75ms
step:887/1770 train_time:84856ms step_avg:96.76ms
step:888/1770 train_time:84957ms step_avg:96.76ms
step:889/1770 train_time:85057ms step_avg:96.77ms
step:890/1770 train_time:85156ms step_avg:96.77ms
step:891/1770 train_time:85256ms step_avg:96.77ms
step:892/1770 train_time:85357ms step_avg:96.78ms
step:893/1770 train_time:85457ms step_avg:96.78ms
step:894/1770 train_time:85556ms step_avg:96.78ms
step:895/1770 train_time:85655ms step_avg:96.79ms
step:896/1770 train_time:85754ms step_avg:96.79ms
step:897/1770 train_time:85854ms step_avg:96.79ms
step:898/1770 train_time:85954ms step_avg:96.80ms
step:899/1770 train_time:86053ms step_avg:96.80ms
step:900/1770 train_time:86153ms step_avg:96.80ms
step:901/1770 train_time:86254ms step_avg:96.81ms
step:902/1770 train_time:86354ms step_avg:96.81ms
step:903/1770 train_time:86455ms step_avg:96.81ms
step:904/1770 train_time:86556ms step_avg:96.82ms
step:905/1770 train_time:86656ms step_avg:96.82ms
step:906/1770 train_time:86755ms step_avg:96.83ms
step:907/1770 train_time:86856ms step_avg:96.83ms
step:908/1770 train_time:86956ms step_avg:96.83ms
step:909/1770 train_time:87055ms step_avg:96.84ms
step:910/1770 train_time:87154ms step_avg:96.84ms
step:911/1770 train_time:87255ms step_avg:96.84ms
step:912/1770 train_time:87355ms step_avg:96.85ms
step:913/1770 train_time:87456ms step_avg:96.85ms
step:914/1770 train_time:87556ms step_avg:96.85ms
step:915/1770 train_time:87656ms step_avg:96.86ms
step:916/1770 train_time:87759ms step_avg:96.86ms
step:917/1770 train_time:87856ms step_avg:96.86ms
step:918/1770 train_time:87956ms step_avg:96.87ms
step:919/1770 train_time:88056ms step_avg:96.87ms
step:920/1770 train_time:88157ms step_avg:96.88ms
step:921/1770 train_time:88258ms step_avg:96.88ms
step:922/1770 train_time:88359ms step_avg:96.88ms
step:923/1770 train_time:88460ms step_avg:96.89ms
step:924/1770 train_time:88561ms step_avg:96.89ms
step:925/1770 train_time:88662ms step_avg:96.90ms
step:926/1770 train_time:88764ms step_avg:96.90ms
step:927/1770 train_time:88865ms step_avg:96.91ms
step:928/1770 train_time:88966ms step_avg:96.91ms
step:929/1770 train_time:89066ms step_avg:96.92ms
step:930/1770 train_time:89169ms step_avg:96.92ms
step:931/1770 train_time:89267ms step_avg:96.92ms
step:932/1770 train_time:89368ms step_avg:96.93ms
step:933/1770 train_time:89468ms step_avg:96.93ms
step:934/1770 train_time:89568ms step_avg:96.94ms
step:935/1770 train_time:89669ms step_avg:96.94ms
step:936/1770 train_time:89769ms step_avg:96.94ms
step:937/1770 train_time:89870ms step_avg:96.95ms
step:938/1770 train_time:89971ms step_avg:96.95ms
step:939/1770 train_time:90072ms step_avg:96.96ms
step:940/1770 train_time:90174ms step_avg:96.96ms
step:941/1770 train_time:90275ms step_avg:96.97ms
step:942/1770 train_time:90377ms step_avg:96.97ms
step:943/1770 train_time:90477ms step_avg:96.97ms
step:944/1770 train_time:90578ms step_avg:96.98ms
step:945/1770 train_time:90678ms step_avg:96.98ms
step:946/1770 train_time:90781ms step_avg:96.99ms
step:947/1770 train_time:90881ms step_avg:96.99ms
step:948/1770 train_time:90982ms step_avg:97.00ms
step:949/1770 train_time:91085ms step_avg:97.00ms
step:950/1770 train_time:91187ms step_avg:97.01ms
step:951/1770 train_time:91289ms step_avg:97.01ms
step:952/1770 train_time:91389ms step_avg:97.02ms
step:953/1770 train_time:91489ms step_avg:97.02ms
step:954/1770 train_time:91589ms step_avg:97.02ms
step:955/1770 train_time:91690ms step_avg:97.03ms
step:956/1770 train_time:91790ms step_avg:97.03ms
step:957/1770 train_time:91891ms step_avg:97.03ms
step:958/1770 train_time:91992ms step_avg:97.04ms
step:959/1770 train_time:92094ms step_avg:97.04ms
step:960/1770 train_time:92195ms step_avg:97.05ms
step:961/1770 train_time:92296ms step_avg:97.05ms
step:962/1770 train_time:92397ms step_avg:97.06ms
step:963/1770 train_time:92498ms step_avg:97.06ms
step:964/1770 train_time:92598ms step_avg:97.06ms
step:965/1770 train_time:92699ms step_avg:97.07ms
step:966/1770 train_time:92800ms step_avg:97.07ms
step:967/1770 train_time:92901ms step_avg:97.08ms
step:968/1770 train_time:93002ms step_avg:97.08ms
step:969/1770 train_time:93104ms step_avg:97.08ms
step:970/1770 train_time:93205ms step_avg:97.09ms
step:971/1770 train_time:93305ms step_avg:97.09ms
step:972/1770 train_time:93405ms step_avg:97.10ms
step:973/1770 train_time:93506ms step_avg:97.10ms
step:974/1770 train_time:93606ms step_avg:97.10ms
step:975/1770 train_time:93707ms step_avg:97.11ms
step:976/1770 train_time:93807ms step_avg:97.11ms
step:977/1770 train_time:93908ms step_avg:97.11ms
step:978/1770 train_time:94009ms step_avg:97.12ms
step:979/1770 train_time:94110ms step_avg:97.12ms
step:980/1770 train_time:94211ms step_avg:97.12ms
step:981/1770 train_time:94311ms step_avg:97.13ms
step:982/1770 train_time:94412ms step_avg:97.13ms
step:983/1770 train_time:94514ms step_avg:97.14ms
step:984/1770 train_time:94615ms step_avg:97.14ms
step:985/1770 train_time:94716ms step_avg:97.14ms
step:986/1770 train_time:94817ms step_avg:97.15ms
step:987/1770 train_time:94918ms step_avg:97.15ms
step:988/1770 train_time:95018ms step_avg:97.16ms
step:989/1770 train_time:95120ms step_avg:97.16ms
step:990/1770 train_time:95221ms step_avg:97.16ms
step:991/1770 train_time:95323ms step_avg:97.17ms
step:992/1770 train_time:95425ms step_avg:97.17ms
step:993/1770 train_time:95526ms step_avg:97.18ms
step:994/1770 train_time:95627ms step_avg:97.18ms
step:995/1770 train_time:95728ms step_avg:97.19ms
step:996/1770 train_time:95827ms step_avg:97.19ms
step:997/1770 train_time:95928ms step_avg:97.19ms
step:998/1770 train_time:96027ms step_avg:97.19ms
step:999/1770 train_time:96127ms step_avg:97.20ms
step:1000/1770 train_time:96228ms step_avg:97.20ms
step:1000/1770 val_loss:3.5213 train_time:96327ms step_avg:97.30ms
step:1001/1770 train_time:96356ms step_avg:97.23ms
step:1002/1770 train_time:96440ms step_avg:97.22ms
step:1003/1770 train_time:96542ms step_avg:97.22ms
step:1004/1770 train_time:96642ms step_avg:97.23ms
step:1005/1770 train_time:96742ms step_avg:97.23ms
step:1006/1770 train_time:96843ms step_avg:97.23ms
step:1007/1770 train_time:96943ms step_avg:97.23ms
step:1008/1770 train_time:97044ms step_avg:97.24ms
step:1009/1770 train_time:97144ms step_avg:97.24ms
step:1010/1770 train_time:97244ms step_avg:97.24ms
step:1011/1770 train_time:97348ms step_avg:97.25ms
step:1012/1770 train_time:97450ms step_avg:97.26ms
step:1013/1770 train_time:97551ms step_avg:97.26ms
step:1014/1770 train_time:97651ms step_avg:97.26ms
step:1015/1770 train_time:97753ms step_avg:97.27ms
step:1016/1770 train_time:97853ms step_avg:97.27ms
step:1017/1770 train_time:97953ms step_avg:97.27ms
step:1018/1770 train_time:98054ms step_avg:97.28ms
step:1019/1770 train_time:98155ms step_avg:97.28ms
step:1020/1770 train_time:98256ms step_avg:97.28ms
step:1021/1770 train_time:98357ms step_avg:97.29ms
step:1022/1770 train_time:98458ms step_avg:97.29ms
step:1023/1770 train_time:98558ms step_avg:97.29ms
step:1024/1770 train_time:98658ms step_avg:97.30ms
step:1025/1770 train_time:98759ms step_avg:97.30ms
step:1026/1770 train_time:98859ms step_avg:97.30ms
step:1027/1770 train_time:98960ms step_avg:97.31ms
step:1028/1770 train_time:99061ms step_avg:97.31ms
step:1029/1770 train_time:99162ms step_avg:97.31ms
step:1030/1770 train_time:99265ms step_avg:97.32ms
step:1031/1770 train_time:99366ms step_avg:97.32ms
step:1032/1770 train_time:99467ms step_avg:97.33ms
step:1033/1770 train_time:99568ms step_avg:97.33ms
step:1034/1770 train_time:99669ms step_avg:97.33ms
step:1035/1770 train_time:99769ms step_avg:97.34ms
step:1036/1770 train_time:99869ms step_avg:97.34ms
step:1037/1770 train_time:99971ms step_avg:97.34ms
step:1038/1770 train_time:100072ms step_avg:97.35ms
step:1039/1770 train_time:100174ms step_avg:97.35ms
step:1040/1770 train_time:100275ms step_avg:97.35ms
step:1041/1770 train_time:100376ms step_avg:97.36ms
step:1042/1770 train_time:100477ms step_avg:97.36ms
step:1043/1770 train_time:100577ms step_avg:97.36ms
step:1044/1770 train_time:100677ms step_avg:97.37ms
step:1045/1770 train_time:100777ms step_avg:97.37ms
step:1046/1770 train_time:100878ms step_avg:97.37ms
step:1047/1770 train_time:100978ms step_avg:97.37ms
step:1048/1770 train_time:101079ms step_avg:97.38ms
step:1049/1770 train_time:101179ms step_avg:97.38ms
step:1050/1770 train_time:101281ms step_avg:97.39ms
step:1051/1770 train_time:101384ms step_avg:97.39ms
step:1052/1770 train_time:101485ms step_avg:97.39ms
step:1053/1770 train_time:101586ms step_avg:97.40ms
step:1054/1770 train_time:101686ms step_avg:97.40ms
step:1055/1770 train_time:101787ms step_avg:97.40ms
step:1056/1770 train_time:101887ms step_avg:97.41ms
step:1057/1770 train_time:101987ms step_avg:97.41ms
step:1058/1770 train_time:102089ms step_avg:97.41ms
step:1059/1770 train_time:102190ms step_avg:97.42ms
step:1060/1770 train_time:102291ms step_avg:97.42ms
step:1061/1770 train_time:102393ms step_avg:97.42ms
step:1062/1770 train_time:102494ms step_avg:97.43ms
step:1063/1770 train_time:102596ms step_avg:97.43ms
step:1064/1770 train_time:102697ms step_avg:97.44ms
step:1065/1770 train_time:102798ms step_avg:97.44ms
step:1066/1770 train_time:102898ms step_avg:97.44ms
step:1067/1770 train_time:102999ms step_avg:97.44ms
step:1068/1770 train_time:103100ms step_avg:97.45ms
step:1069/1770 train_time:103201ms step_avg:97.45ms
step:1070/1770 train_time:103303ms step_avg:97.46ms
step:1071/1770 train_time:103404ms step_avg:97.46ms
step:1072/1770 train_time:103506ms step_avg:97.46ms
step:1073/1770 train_time:103607ms step_avg:97.47ms
step:1074/1770 train_time:103709ms step_avg:97.47ms
step:1075/1770 train_time:103810ms step_avg:97.47ms
step:1076/1770 train_time:103911ms step_avg:97.48ms
step:1077/1770 train_time:104012ms step_avg:97.48ms
step:1078/1770 train_time:104113ms step_avg:97.48ms
step:1079/1770 train_time:104214ms step_avg:97.49ms
step:1080/1770 train_time:104315ms step_avg:97.49ms
step:1081/1770 train_time:104416ms step_avg:97.49ms
step:1082/1770 train_time:104517ms step_avg:97.50ms
step:1083/1770 train_time:104618ms step_avg:97.50ms
step:1084/1770 train_time:104719ms step_avg:97.50ms
step:1085/1770 train_time:104820ms step_avg:97.51ms
step:1086/1770 train_time:104920ms step_avg:97.51ms
step:1087/1770 train_time:105021ms step_avg:97.51ms
step:1088/1770 train_time:105122ms step_avg:97.52ms
step:1089/1770 train_time:105222ms step_avg:97.52ms
step:1090/1770 train_time:105324ms step_avg:97.52ms
step:1091/1770 train_time:105427ms step_avg:97.53ms
step:1092/1770 train_time:105528ms step_avg:97.53ms
step:1093/1770 train_time:105629ms step_avg:97.53ms
step:1094/1770 train_time:105730ms step_avg:97.54ms
step:1095/1770 train_time:105832ms step_avg:97.54ms
step:1096/1770 train_time:105933ms step_avg:97.54ms
step:1097/1770 train_time:106035ms step_avg:97.55ms
step:1098/1770 train_time:106135ms step_avg:97.55ms
step:1099/1770 train_time:106236ms step_avg:97.55ms
step:1100/1770 train_time:106337ms step_avg:97.56ms
step:1101/1770 train_time:106438ms step_avg:97.56ms
step:1102/1770 train_time:106539ms step_avg:97.56ms
step:1103/1770 train_time:106639ms step_avg:97.57ms
step:1104/1770 train_time:106743ms step_avg:97.57ms
step:1105/1770 train_time:106842ms step_avg:97.57ms
step:1106/1770 train_time:106944ms step_avg:97.58ms
step:1107/1770 train_time:107046ms step_avg:97.58ms
step:1108/1770 train_time:107148ms step_avg:97.58ms
step:1109/1770 train_time:107249ms step_avg:97.59ms
step:1110/1770 train_time:107350ms step_avg:97.59ms
step:1111/1770 train_time:107451ms step_avg:97.59ms
step:1112/1770 train_time:107553ms step_avg:97.60ms
step:1113/1770 train_time:107654ms step_avg:97.60ms
step:1114/1770 train_time:107755ms step_avg:97.60ms
step:1115/1770 train_time:107858ms step_avg:97.61ms
step:1116/1770 train_time:107959ms step_avg:97.61ms
step:1117/1770 train_time:108059ms step_avg:97.61ms
step:1118/1770 train_time:108159ms step_avg:97.62ms
step:1119/1770 train_time:108260ms step_avg:97.62ms
step:1120/1770 train_time:108360ms step_avg:97.62ms
step:1121/1770 train_time:108461ms step_avg:97.63ms
step:1122/1770 train_time:108562ms step_avg:97.63ms
step:1123/1770 train_time:108664ms step_avg:97.63ms
step:1124/1770 train_time:108766ms step_avg:97.64ms
step:1125/1770 train_time:108868ms step_avg:97.64ms
step:1125/1770 val_loss:3.4786 train_time:108967ms step_avg:97.73ms
step:1126/1770 train_time:108987ms step_avg:97.66ms
step:1127/1770 train_time:109077ms step_avg:97.65ms
step:1128/1770 train_time:109179ms step_avg:97.66ms
step:1129/1770 train_time:109279ms step_avg:97.66ms
step:1130/1770 train_time:109379ms step_avg:97.66ms
step:1131/1770 train_time:109479ms step_avg:97.66ms
step:1132/1770 train_time:109580ms step_avg:97.66ms
step:1133/1770 train_time:109680ms step_avg:97.67ms
step:1134/1770 train_time:109780ms step_avg:97.67ms
step:1135/1770 train_time:109881ms step_avg:97.67ms
step:1136/1770 train_time:109984ms step_avg:97.68ms
step:1137/1770 train_time:110086ms step_avg:97.68ms
step:1138/1770 train_time:110186ms step_avg:97.68ms
step:1139/1770 train_time:110288ms step_avg:97.69ms
step:1140/1770 train_time:110388ms step_avg:97.69ms
step:1141/1770 train_time:110490ms step_avg:97.69ms
step:1142/1770 train_time:110590ms step_avg:97.69ms
step:1143/1770 train_time:110692ms step_avg:97.70ms
step:1144/1770 train_time:110794ms step_avg:97.70ms
step:1145/1770 train_time:110896ms step_avg:97.71ms
step:1146/1770 train_time:110998ms step_avg:97.71ms
step:1147/1770 train_time:111098ms step_avg:97.71ms
step:1148/1770 train_time:111199ms step_avg:97.71ms
step:1149/1770 train_time:111300ms step_avg:97.72ms
step:1150/1770 train_time:111401ms step_avg:97.72ms
step:1151/1770 train_time:111502ms step_avg:97.72ms
step:1152/1770 train_time:111603ms step_avg:97.73ms
step:1153/1770 train_time:111704ms step_avg:97.73ms
step:1154/1770 train_time:111805ms step_avg:97.73ms
step:1155/1770 train_time:111906ms step_avg:97.73ms
step:1156/1770 train_time:112007ms step_avg:97.74ms
step:1157/1770 train_time:112110ms step_avg:97.74ms
step:1158/1770 train_time:112211ms step_avg:97.74ms
step:1159/1770 train_time:112313ms step_avg:97.75ms
step:1160/1770 train_time:112415ms step_avg:97.75ms
step:1161/1770 train_time:112515ms step_avg:97.75ms
step:1162/1770 train_time:112616ms step_avg:97.76ms
step:1163/1770 train_time:112717ms step_avg:97.76ms
step:1164/1770 train_time:112818ms step_avg:97.76ms
step:1165/1770 train_time:112919ms step_avg:97.77ms
step:1166/1770 train_time:113021ms step_avg:97.77ms
step:1167/1770 train_time:113123ms step_avg:97.77ms
step:1168/1770 train_time:113225ms step_avg:97.78ms
step:1169/1770 train_time:113326ms step_avg:97.78ms
step:1170/1770 train_time:113426ms step_avg:97.78ms
step:1171/1770 train_time:113527ms step_avg:97.78ms
step:1172/1770 train_time:113628ms step_avg:97.79ms
step:1173/1770 train_time:113730ms step_avg:97.79ms
step:1174/1770 train_time:113831ms step_avg:97.79ms
step:1175/1770 train_time:113933ms step_avg:97.80ms
step:1176/1770 train_time:114034ms step_avg:97.80ms
step:1177/1770 train_time:114135ms step_avg:97.80ms
step:1178/1770 train_time:114237ms step_avg:97.81ms
step:1179/1770 train_time:114338ms step_avg:97.81ms
step:1180/1770 train_time:114439ms step_avg:97.81ms
step:1181/1770 train_time:114541ms step_avg:97.81ms
step:1182/1770 train_time:114642ms step_avg:97.82ms
step:1183/1770 train_time:114743ms step_avg:97.82ms
step:1184/1770 train_time:114847ms step_avg:97.83ms
step:1185/1770 train_time:114948ms step_avg:97.83ms
step:1186/1770 train_time:115050ms step_avg:97.83ms
step:1187/1770 train_time:115155ms step_avg:97.84ms
step:1188/1770 train_time:115256ms step_avg:97.84ms
step:1189/1770 train_time:115358ms step_avg:97.84ms
step:1190/1770 train_time:115459ms step_avg:97.85ms
step:1191/1770 train_time:115561ms step_avg:97.85ms
step:1192/1770 train_time:115664ms step_avg:97.85ms
step:1193/1770 train_time:115767ms step_avg:97.86ms
step:1194/1770 train_time:115869ms step_avg:97.86ms
step:1195/1770 train_time:115972ms step_avg:97.87ms
step:1196/1770 train_time:116075ms step_avg:97.87ms
step:1197/1770 train_time:116177ms step_avg:97.87ms
step:1198/1770 train_time:116279ms step_avg:97.88ms
step:1199/1770 train_time:116381ms step_avg:97.88ms
step:1200/1770 train_time:116483ms step_avg:97.89ms
step:1201/1770 train_time:116586ms step_avg:97.89ms
step:1202/1770 train_time:116686ms step_avg:97.89ms
step:1203/1770 train_time:116788ms step_avg:97.89ms
step:1204/1770 train_time:116891ms step_avg:97.90ms
step:1205/1770 train_time:116993ms step_avg:97.90ms
step:1206/1770 train_time:117095ms step_avg:97.91ms
step:1207/1770 train_time:117198ms step_avg:97.91ms
step:1208/1770 train_time:117299ms step_avg:97.91ms
step:1209/1770 train_time:117401ms step_avg:97.92ms
step:1210/1770 train_time:117502ms step_avg:97.92ms
step:1211/1770 train_time:117604ms step_avg:97.92ms
step:1212/1770 train_time:117708ms step_avg:97.93ms
step:1213/1770 train_time:117813ms step_avg:97.93ms
step:1214/1770 train_time:117911ms step_avg:97.93ms
step:1215/1770 train_time:118014ms step_avg:97.94ms
step:1216/1770 train_time:118118ms step_avg:97.94ms
step:1217/1770 train_time:118221ms step_avg:97.95ms
step:1218/1770 train_time:118323ms step_avg:97.95ms
step:1219/1770 train_time:118426ms step_avg:97.95ms
step:1220/1770 train_time:118527ms step_avg:97.96ms
step:1221/1770 train_time:118629ms step_avg:97.96ms
step:1222/1770 train_time:118733ms step_avg:97.96ms
step:1223/1770 train_time:118835ms step_avg:97.97ms
step:1224/1770 train_time:118938ms step_avg:97.97ms
step:1225/1770 train_time:119041ms step_avg:97.98ms
step:1226/1770 train_time:119142ms step_avg:97.98ms
step:1227/1770 train_time:119247ms step_avg:97.98ms
step:1228/1770 train_time:119351ms step_avg:97.99ms
step:1229/1770 train_time:119453ms step_avg:97.99ms
step:1230/1770 train_time:119555ms step_avg:98.00ms
step:1231/1770 train_time:119657ms step_avg:98.00ms
step:1232/1770 train_time:119759ms step_avg:98.00ms
step:1233/1770 train_time:119861ms step_avg:98.01ms
step:1234/1770 train_time:119963ms step_avg:98.01ms
step:1235/1770 train_time:120065ms step_avg:98.01ms
step:1236/1770 train_time:120167ms step_avg:98.02ms
step:1237/1770 train_time:120269ms step_avg:98.02ms
step:1238/1770 train_time:120372ms step_avg:98.02ms
step:1239/1770 train_time:120475ms step_avg:98.03ms
step:1240/1770 train_time:120577ms step_avg:98.03ms
step:1241/1770 train_time:120680ms step_avg:98.03ms
step:1242/1770 train_time:120781ms step_avg:98.04ms
step:1243/1770 train_time:120883ms step_avg:98.04ms
step:1244/1770 train_time:120984ms step_avg:98.04ms
step:1245/1770 train_time:121087ms step_avg:98.05ms
step:1246/1770 train_time:121189ms step_avg:98.05ms
step:1247/1770 train_time:121291ms step_avg:98.05ms
step:1248/1770 train_time:121394ms step_avg:98.06ms
step:1249/1770 train_time:121496ms step_avg:98.06ms
step:1250/1770 train_time:121598ms step_avg:98.06ms
step:1250/1770 val_loss:3.4315 train_time:121699ms step_avg:98.14ms
step:1251/1770 train_time:121722ms step_avg:98.08ms
step:1252/1770 train_time:121813ms step_avg:98.08ms
step:1253/1770 train_time:121915ms step_avg:98.08ms
step:1254/1770 train_time:122017ms step_avg:98.08ms
step:1255/1770 train_time:122121ms step_avg:98.09ms
step:1256/1770 train_time:122223ms step_avg:98.09ms
step:1257/1770 train_time:122324ms step_avg:98.09ms
step:1258/1770 train_time:122426ms step_avg:98.10ms
step:1259/1770 train_time:122528ms step_avg:98.10ms
step:1260/1770 train_time:122629ms step_avg:98.10ms
step:1261/1770 train_time:122733ms step_avg:98.11ms
step:1262/1770 train_time:122836ms step_avg:98.11ms
step:1263/1770 train_time:122938ms step_avg:98.12ms
step:1264/1770 train_time:123041ms step_avg:98.12ms
step:1265/1770 train_time:123143ms step_avg:98.12ms
step:1266/1770 train_time:123245ms step_avg:98.13ms
step:1267/1770 train_time:123348ms step_avg:98.13ms
step:1268/1770 train_time:123451ms step_avg:98.13ms
step:1269/1770 train_time:123553ms step_avg:98.14ms
step:1270/1770 train_time:123655ms step_avg:98.14ms
step:1271/1770 train_time:123757ms step_avg:98.14ms
step:1272/1770 train_time:123858ms step_avg:98.14ms
step:1273/1770 train_time:123961ms step_avg:98.15ms
step:1274/1770 train_time:124064ms step_avg:98.15ms
step:1275/1770 train_time:124166ms step_avg:98.15ms
step:1276/1770 train_time:124268ms step_avg:98.16ms
step:1277/1770 train_time:124370ms step_avg:98.16ms
step:1278/1770 train_time:124473ms step_avg:98.16ms
step:1279/1770 train_time:124575ms step_avg:98.17ms
step:1280/1770 train_time:124677ms step_avg:98.17ms
step:1281/1770 train_time:124779ms step_avg:98.17ms
step:1282/1770 train_time:124881ms step_avg:98.18ms
step:1283/1770 train_time:124984ms step_avg:98.18ms
step:1284/1770 train_time:125086ms step_avg:98.18ms
step:1285/1770 train_time:125189ms step_avg:98.19ms
step:1286/1770 train_time:125292ms step_avg:98.19ms
step:1287/1770 train_time:125395ms step_avg:98.20ms
step:1288/1770 train_time:125498ms step_avg:98.20ms
step:1289/1770 train_time:125600ms step_avg:98.20ms
step:1290/1770 train_time:125701ms step_avg:98.20ms
step:1291/1770 train_time:125803ms step_avg:98.21ms
step:1292/1770 train_time:125904ms step_avg:98.21ms
step:1293/1770 train_time:126007ms step_avg:98.21ms
step:1294/1770 train_time:126109ms step_avg:98.22ms
step:1295/1770 train_time:126211ms step_avg:98.22ms
step:1296/1770 train_time:126313ms step_avg:98.22ms
step:1297/1770 train_time:126415ms step_avg:98.22ms
step:1298/1770 train_time:126517ms step_avg:98.23ms
step:1299/1770 train_time:126619ms step_avg:98.23ms
step:1300/1770 train_time:126721ms step_avg:98.23ms
step:1301/1770 train_time:126823ms step_avg:98.24ms
step:1302/1770 train_time:126925ms step_avg:98.24ms
step:1303/1770 train_time:127026ms step_avg:98.24ms
step:1304/1770 train_time:127128ms step_avg:98.24ms
step:1305/1770 train_time:127230ms step_avg:98.25ms
step:1306/1770 train_time:127332ms step_avg:98.25ms
step:1307/1770 train_time:127435ms step_avg:98.25ms
step:1308/1770 train_time:127537ms step_avg:98.26ms
step:1309/1770 train_time:127639ms step_avg:98.26ms
step:1310/1770 train_time:127741ms step_avg:98.26ms
step:1311/1770 train_time:127843ms step_avg:98.27ms
step:1312/1770 train_time:127945ms step_avg:98.27ms
step:1313/1770 train_time:128046ms step_avg:98.27ms
step:1314/1770 train_time:128149ms step_avg:98.27ms
step:1315/1770 train_time:128251ms step_avg:98.28ms
step:1316/1770 train_time:128354ms step_avg:98.28ms
step:1317/1770 train_time:128457ms step_avg:98.28ms
step:1318/1770 train_time:128562ms step_avg:98.29ms
step:1319/1770 train_time:128665ms step_avg:98.29ms
step:1320/1770 train_time:128767ms step_avg:98.30ms
step:1321/1770 train_time:128870ms step_avg:98.30ms
step:1322/1770 train_time:128973ms step_avg:98.30ms
step:1323/1770 train_time:129077ms step_avg:98.31ms
step:1324/1770 train_time:129180ms step_avg:98.31ms
step:1325/1770 train_time:129283ms step_avg:98.31ms
step:1326/1770 train_time:129384ms step_avg:98.32ms
step:1327/1770 train_time:129489ms step_avg:98.32ms
step:1328/1770 train_time:129592ms step_avg:98.32ms
step:1329/1770 train_time:129694ms step_avg:98.33ms
step:1330/1770 train_time:129795ms step_avg:98.33ms
step:1331/1770 train_time:129898ms step_avg:98.33ms
step:1332/1770 train_time:129999ms step_avg:98.34ms
step:1333/1770 train_time:130101ms step_avg:98.34ms
step:1334/1770 train_time:130204ms step_avg:98.34ms
step:1335/1770 train_time:130305ms step_avg:98.34ms
step:1336/1770 train_time:130406ms step_avg:98.35ms
step:1337/1770 train_time:130509ms step_avg:98.35ms
step:1338/1770 train_time:130612ms step_avg:98.35ms
step:1339/1770 train_time:130716ms step_avg:98.36ms
step:1340/1770 train_time:130819ms step_avg:98.36ms
step:1341/1770 train_time:130920ms step_avg:98.36ms
step:1342/1770 train_time:131023ms step_avg:98.37ms
step:1343/1770 train_time:131126ms step_avg:98.37ms
step:1344/1770 train_time:131228ms step_avg:98.37ms
step:1345/1770 train_time:131330ms step_avg:98.37ms
step:1346/1770 train_time:131432ms step_avg:98.38ms
step:1347/1770 train_time:131535ms step_avg:98.38ms
step:1348/1770 train_time:131640ms step_avg:98.39ms
step:1349/1770 train_time:131742ms step_avg:98.39ms
step:1350/1770 train_time:131843ms step_avg:98.39ms
step:1351/1770 train_time:131945ms step_avg:98.39ms
step:1352/1770 train_time:132047ms step_avg:98.40ms
step:1353/1770 train_time:132150ms step_avg:98.40ms
step:1354/1770 train_time:132252ms step_avg:98.40ms
step:1355/1770 train_time:132355ms step_avg:98.41ms
step:1356/1770 train_time:132457ms step_avg:98.41ms
step:1357/1770 train_time:132558ms step_avg:98.41ms
step:1358/1770 train_time:132662ms step_avg:98.41ms
step:1359/1770 train_time:132764ms step_avg:98.42ms
step:1360/1770 train_time:132866ms step_avg:98.42ms
step:1361/1770 train_time:132969ms step_avg:98.42ms
step:1362/1770 train_time:133072ms step_avg:98.43ms
step:1363/1770 train_time:133175ms step_avg:98.43ms
step:1364/1770 train_time:133278ms step_avg:98.43ms
step:1365/1770 train_time:133380ms step_avg:98.44ms
step:1366/1770 train_time:133481ms step_avg:98.44ms
step:1367/1770 train_time:133584ms step_avg:98.44ms
step:1368/1770 train_time:133687ms step_avg:98.44ms
step:1369/1770 train_time:133789ms step_avg:98.45ms
step:1370/1770 train_time:133891ms step_avg:98.45ms
step:1371/1770 train_time:133994ms step_avg:98.45ms
step:1372/1770 train_time:134094ms step_avg:98.45ms
step:1373/1770 train_time:134197ms step_avg:98.46ms
step:1374/1770 train_time:134300ms step_avg:98.46ms
step:1375/1770 train_time:134402ms step_avg:98.46ms
step:1375/1770 val_loss:3.3872 train_time:134503ms step_avg:98.54ms
step:1376/1770 train_time:134524ms step_avg:98.48ms
step:1377/1770 train_time:134616ms step_avg:98.48ms
step:1378/1770 train_time:134721ms step_avg:98.48ms
step:1379/1770 train_time:134819ms step_avg:98.48ms
step:1380/1770 train_time:134920ms step_avg:98.48ms
step:1381/1770 train_time:135023ms step_avg:98.48ms
step:1382/1770 train_time:135124ms step_avg:98.49ms
step:1383/1770 train_time:135228ms step_avg:98.49ms
step:1384/1770 train_time:135329ms step_avg:98.49ms
step:1385/1770 train_time:135431ms step_avg:98.50ms
step:1386/1770 train_time:135535ms step_avg:98.50ms
step:1387/1770 train_time:135639ms step_avg:98.50ms
step:1388/1770 train_time:135740ms step_avg:98.51ms
step:1389/1770 train_time:135843ms step_avg:98.51ms
step:1390/1770 train_time:135945ms step_avg:98.51ms
step:1391/1770 train_time:136047ms step_avg:98.51ms
step:1392/1770 train_time:136150ms step_avg:98.52ms
step:1393/1770 train_time:136252ms step_avg:98.52ms
step:1394/1770 train_time:136354ms step_avg:98.52ms
step:1395/1770 train_time:136457ms step_avg:98.52ms
step:1396/1770 train_time:136561ms step_avg:98.53ms
step:1397/1770 train_time:136663ms step_avg:98.53ms
step:1398/1770 train_time:136767ms step_avg:98.54ms
step:1399/1770 train_time:136869ms step_avg:98.54ms
step:1400/1770 train_time:136971ms step_avg:98.54ms
step:1401/1770 train_time:137073ms step_avg:98.54ms
step:1402/1770 train_time:137176ms step_avg:98.55ms
step:1403/1770 train_time:137277ms step_avg:98.55ms
step:1404/1770 train_time:137380ms step_avg:98.55ms
step:1405/1770 train_time:137482ms step_avg:98.55ms
step:1406/1770 train_time:137584ms step_avg:98.56ms
step:1407/1770 train_time:137687ms step_avg:98.56ms
step:1408/1770 train_time:137790ms step_avg:98.56ms
step:1409/1770 train_time:137893ms step_avg:98.57ms
step:1410/1770 train_time:137995ms step_avg:98.57ms
step:1411/1770 train_time:138098ms step_avg:98.57ms
step:1412/1770 train_time:138199ms step_avg:98.57ms
step:1413/1770 train_time:138301ms step_avg:98.58ms
step:1414/1770 train_time:138403ms step_avg:98.58ms
step:1415/1770 train_time:138506ms step_avg:98.58ms
step:1416/1770 train_time:138609ms step_avg:98.58ms
step:1417/1770 train_time:138712ms step_avg:98.59ms
step:1418/1770 train_time:138814ms step_avg:98.59ms
step:1419/1770 train_time:138919ms step_avg:98.59ms
step:1420/1770 train_time:139020ms step_avg:98.60ms
step:1421/1770 train_time:139122ms step_avg:98.60ms
step:1422/1770 train_time:139225ms step_avg:98.60ms
step:1423/1770 train_time:139327ms step_avg:98.60ms
step:1424/1770 train_time:139430ms step_avg:98.61ms
step:1425/1770 train_time:139532ms step_avg:98.61ms
step:1426/1770 train_time:139634ms step_avg:98.61ms
step:1427/1770 train_time:139736ms step_avg:98.61ms
step:1428/1770 train_time:139841ms step_avg:98.62ms
step:1429/1770 train_time:139943ms step_avg:98.62ms
step:1430/1770 train_time:140045ms step_avg:98.62ms
step:1431/1770 train_time:140148ms step_avg:98.63ms
step:1432/1770 train_time:140251ms step_avg:98.63ms
step:1433/1770 train_time:140352ms step_avg:98.63ms
step:1434/1770 train_time:140453ms step_avg:98.63ms
step:1435/1770 train_time:140556ms step_avg:98.64ms
step:1436/1770 train_time:140659ms step_avg:98.64ms
step:1437/1770 train_time:140760ms step_avg:98.64ms
step:1438/1770 train_time:140862ms step_avg:98.64ms
step:1439/1770 train_time:140964ms step_avg:98.64ms
step:1440/1770 train_time:141066ms step_avg:98.65ms
step:1441/1770 train_time:141172ms step_avg:98.65ms
step:1442/1770 train_time:141273ms step_avg:98.65ms
step:1443/1770 train_time:141375ms step_avg:98.66ms
step:1444/1770 train_time:141478ms step_avg:98.66ms
step:1445/1770 train_time:141581ms step_avg:98.66ms
step:1446/1770 train_time:141683ms step_avg:98.67ms
step:1447/1770 train_time:141787ms step_avg:98.67ms
step:1448/1770 train_time:141890ms step_avg:98.67ms
step:1449/1770 train_time:141994ms step_avg:98.68ms
step:1450/1770 train_time:142097ms step_avg:98.68ms
step:1451/1770 train_time:142201ms step_avg:98.68ms
step:1452/1770 train_time:142303ms step_avg:98.68ms
step:1453/1770 train_time:142407ms step_avg:98.69ms
step:1454/1770 train_time:142511ms step_avg:98.69ms
step:1455/1770 train_time:142615ms step_avg:98.70ms
step:1456/1770 train_time:142719ms step_avg:98.70ms
step:1457/1770 train_time:142822ms step_avg:98.70ms
step:1458/1770 train_time:142925ms step_avg:98.71ms
step:1459/1770 train_time:143030ms step_avg:98.71ms
step:1460/1770 train_time:143133ms step_avg:98.71ms
step:1461/1770 train_time:143237ms step_avg:98.72ms
step:1462/1770 train_time:143340ms step_avg:98.72ms
step:1463/1770 train_time:143442ms step_avg:98.72ms
step:1464/1770 train_time:143547ms step_avg:98.73ms
step:1465/1770 train_time:143651ms step_avg:98.73ms
step:1466/1770 train_time:143754ms step_avg:98.73ms
step:1467/1770 train_time:143859ms step_avg:98.74ms
step:1468/1770 train_time:143962ms step_avg:98.74ms
step:1469/1770 train_time:144066ms step_avg:98.74ms
step:1470/1770 train_time:144168ms step_avg:98.75ms
step:1471/1770 train_time:144271ms step_avg:98.75ms
step:1472/1770 train_time:144374ms step_avg:98.75ms
step:1473/1770 train_time:144478ms step_avg:98.75ms
step:1474/1770 train_time:144583ms step_avg:98.76ms
step:1475/1770 train_time:144686ms step_avg:98.76ms
step:1476/1770 train_time:144789ms step_avg:98.76ms
step:1477/1770 train_time:144895ms step_avg:98.77ms
step:1478/1770 train_time:144999ms step_avg:98.77ms
step:1479/1770 train_time:145101ms step_avg:98.78ms
step:1480/1770 train_time:145204ms step_avg:98.78ms
step:1481/1770 train_time:145311ms step_avg:98.78ms
step:1482/1770 train_time:145413ms step_avg:98.79ms
step:1483/1770 train_time:145517ms step_avg:98.79ms
step:1484/1770 train_time:145620ms step_avg:98.79ms
step:1485/1770 train_time:145723ms step_avg:98.80ms
step:1486/1770 train_time:145826ms step_avg:98.80ms
step:1487/1770 train_time:145930ms step_avg:98.80ms
step:1488/1770 train_time:146034ms step_avg:98.80ms
step:1489/1770 train_time:146137ms step_avg:98.81ms
step:1490/1770 train_time:146241ms step_avg:98.81ms
step:1491/1770 train_time:146344ms step_avg:98.81ms
step:1492/1770 train_time:146448ms step_avg:98.82ms
step:1493/1770 train_time:146555ms step_avg:98.82ms
step:1494/1770 train_time:146661ms step_avg:98.83ms
step:1495/1770 train_time:146763ms step_avg:98.83ms
step:1496/1770 train_time:146866ms step_avg:98.83ms
step:1497/1770 train_time:146970ms step_avg:98.84ms
step:1498/1770 train_time:147072ms step_avg:98.84ms
step:1499/1770 train_time:147174ms step_avg:98.84ms
step:1500/1770 train_time:147277ms step_avg:98.84ms
step:1500/1770 val_loss:3.3495 train_time:147378ms step_avg:98.91ms
step:1501/1770 train_time:147399ms step_avg:98.86ms
step:1502/1770 train_time:147490ms step_avg:98.85ms
step:1503/1770 train_time:147593ms step_avg:98.86ms
step:1504/1770 train_time:147696ms step_avg:98.86ms
step:1505/1770 train_time:147802ms step_avg:98.86ms
step:1506/1770 train_time:147905ms step_avg:98.87ms
step:1507/1770 train_time:148008ms step_avg:98.87ms
step:1508/1770 train_time:148114ms step_avg:98.87ms
step:1509/1770 train_time:148217ms step_avg:98.88ms
step:1510/1770 train_time:148319ms step_avg:98.88ms
step:1511/1770 train_time:148426ms step_avg:98.88ms
step:1512/1770 train_time:148530ms step_avg:98.89ms
step:1513/1770 train_time:148634ms step_avg:98.89ms
step:1514/1770 train_time:148739ms step_avg:98.90ms
step:1515/1770 train_time:148841ms step_avg:98.90ms
step:1516/1770 train_time:148945ms step_avg:98.90ms
step:1517/1770 train_time:149047ms step_avg:98.90ms
step:1518/1770 train_time:149152ms step_avg:98.91ms
step:1519/1770 train_time:149254ms step_avg:98.91ms
step:1520/1770 train_time:149359ms step_avg:98.91ms
step:1521/1770 train_time:149462ms step_avg:98.92ms
step:1522/1770 train_time:149567ms step_avg:98.92ms
step:1523/1770 train_time:149671ms step_avg:98.92ms
step:1524/1770 train_time:149774ms step_avg:98.93ms
step:1525/1770 train_time:149877ms step_avg:98.93ms
step:1526/1770 train_time:149980ms step_avg:98.93ms
step:1527/1770 train_time:150085ms step_avg:98.94ms
step:1528/1770 train_time:150190ms step_avg:98.94ms
step:1529/1770 train_time:150293ms step_avg:98.94ms
step:1530/1770 train_time:150396ms step_avg:98.94ms
step:1531/1770 train_time:150499ms step_avg:98.95ms
step:1532/1770 train_time:150604ms step_avg:98.95ms
step:1533/1770 train_time:150708ms step_avg:98.95ms
step:1534/1770 train_time:150812ms step_avg:98.96ms
step:1535/1770 train_time:150914ms step_avg:98.96ms
step:1536/1770 train_time:151017ms step_avg:98.96ms
step:1537/1770 train_time:151120ms step_avg:98.97ms
step:1538/1770 train_time:151225ms step_avg:98.97ms
step:1539/1770 train_time:151327ms step_avg:98.97ms
step:1540/1770 train_time:151434ms step_avg:98.98ms
step:1541/1770 train_time:151540ms step_avg:98.98ms
step:1542/1770 train_time:151644ms step_avg:98.98ms
step:1543/1770 train_time:151746ms step_avg:98.99ms
step:1544/1770 train_time:151852ms step_avg:98.99ms
step:1545/1770 train_time:151955ms step_avg:98.99ms
step:1546/1770 train_time:152060ms step_avg:99.00ms
step:1547/1770 train_time:152162ms step_avg:99.00ms
step:1548/1770 train_time:152267ms step_avg:99.00ms
step:1549/1770 train_time:152371ms step_avg:99.01ms
step:1550/1770 train_time:152473ms step_avg:99.01ms
step:1551/1770 train_time:152580ms step_avg:99.01ms
step:1552/1770 train_time:152682ms step_avg:99.02ms
step:1553/1770 train_time:152785ms step_avg:99.02ms
step:1554/1770 train_time:152887ms step_avg:99.02ms
step:1555/1770 train_time:152992ms step_avg:99.02ms
step:1556/1770 train_time:153095ms step_avg:99.03ms
step:1557/1770 train_time:153199ms step_avg:99.03ms
step:1558/1770 train_time:153302ms step_avg:99.03ms
step:1559/1770 train_time:153406ms step_avg:99.04ms
step:1560/1770 train_time:153508ms step_avg:99.04ms
step:1561/1770 train_time:153613ms step_avg:99.04ms
step:1562/1770 train_time:153717ms step_avg:99.04ms
step:1563/1770 train_time:153820ms step_avg:99.05ms
step:1564/1770 train_time:153923ms step_avg:99.05ms
step:1565/1770 train_time:154027ms step_avg:99.05ms
step:1566/1770 train_time:154130ms step_avg:99.06ms
step:1567/1770 train_time:154233ms step_avg:99.06ms
step:1568/1770 train_time:154336ms step_avg:99.06ms
step:1569/1770 train_time:154443ms step_avg:99.07ms
step:1570/1770 train_time:154545ms step_avg:99.07ms
step:1571/1770 train_time:154648ms step_avg:99.07ms
step:1572/1770 train_time:154751ms step_avg:99.07ms
step:1573/1770 train_time:154857ms step_avg:99.08ms
step:1574/1770 train_time:154960ms step_avg:99.08ms
step:1575/1770 train_time:155061ms step_avg:99.08ms
step:1576/1770 train_time:155164ms step_avg:99.08ms
step:1577/1770 train_time:155269ms step_avg:99.09ms
step:1578/1770 train_time:155374ms step_avg:99.09ms
step:1579/1770 train_time:155478ms step_avg:99.09ms
step:1580/1770 train_time:155581ms step_avg:99.10ms
step:1581/1770 train_time:155686ms step_avg:99.10ms
step:1582/1770 train_time:155790ms step_avg:99.10ms
step:1583/1770 train_time:155893ms step_avg:99.11ms
step:1584/1770 train_time:155998ms step_avg:99.11ms
step:1585/1770 train_time:156102ms step_avg:99.11ms
step:1586/1770 train_time:156209ms step_avg:99.12ms
step:1587/1770 train_time:156313ms step_avg:99.12ms
step:1588/1770 train_time:156416ms step_avg:99.12ms
step:1589/1770 train_time:156522ms step_avg:99.13ms
step:1590/1770 train_time:156625ms step_avg:99.13ms
step:1591/1770 train_time:156728ms step_avg:99.13ms
step:1592/1770 train_time:156832ms step_avg:99.14ms
step:1593/1770 train_time:156935ms step_avg:99.14ms
step:1594/1770 train_time:157038ms step_avg:99.14ms
step:1595/1770 train_time:157142ms step_avg:99.14ms
step:1596/1770 train_time:157247ms step_avg:99.15ms
step:1597/1770 train_time:157349ms step_avg:99.15ms
step:1598/1770 train_time:157452ms step_avg:99.15ms
step:1599/1770 train_time:157558ms step_avg:99.16ms
step:1600/1770 train_time:157663ms step_avg:99.16ms
step:1601/1770 train_time:157767ms step_avg:99.16ms
step:1602/1770 train_time:157871ms step_avg:99.17ms
step:1603/1770 train_time:157975ms step_avg:99.17ms
step:1604/1770 train_time:158077ms step_avg:99.17ms
step:1605/1770 train_time:158180ms step_avg:99.17ms
step:1606/1770 train_time:158288ms step_avg:99.18ms
step:1607/1770 train_time:158392ms step_avg:99.18ms
step:1608/1770 train_time:158495ms step_avg:99.18ms
step:1609/1770 train_time:158600ms step_avg:99.19ms
step:1610/1770 train_time:158704ms step_avg:99.19ms
step:1611/1770 train_time:158809ms step_avg:99.19ms
step:1612/1770 train_time:158915ms step_avg:99.20ms
step:1613/1770 train_time:159017ms step_avg:99.20ms
step:1614/1770 train_time:159120ms step_avg:99.20ms
step:1615/1770 train_time:159224ms step_avg:99.20ms
step:1616/1770 train_time:159327ms step_avg:99.21ms
step:1617/1770 train_time:159433ms step_avg:99.21ms
step:1618/1770 train_time:159538ms step_avg:99.21ms
step:1619/1770 train_time:159642ms step_avg:99.22ms
step:1620/1770 train_time:159745ms step_avg:99.22ms
step:1621/1770 train_time:159849ms step_avg:99.22ms
step:1622/1770 train_time:159952ms step_avg:99.23ms
step:1623/1770 train_time:160059ms step_avg:99.23ms
step:1624/1770 train_time:160162ms step_avg:99.23ms
step:1625/1770 train_time:160265ms step_avg:99.24ms
step:1625/1770 val_loss:3.3144 train_time:160367ms step_avg:99.30ms
step:1626/1770 train_time:160388ms step_avg:99.25ms
step:1627/1770 train_time:160478ms step_avg:99.24ms
step:1628/1770 train_time:160581ms step_avg:99.25ms
step:1629/1770 train_time:160683ms step_avg:99.25ms
step:1630/1770 train_time:160785ms step_avg:99.25ms
step:1631/1770 train_time:160888ms step_avg:99.25ms
step:1632/1770 train_time:160990ms step_avg:99.25ms
step:1633/1770 train_time:161093ms step_avg:99.26ms
step:1634/1770 train_time:161196ms step_avg:99.26ms
step:1635/1770 train_time:161300ms step_avg:99.26ms
step:1636/1770 train_time:161405ms step_avg:99.26ms
step:1637/1770 train_time:161511ms step_avg:99.27ms
step:1638/1770 train_time:161614ms step_avg:99.27ms
step:1639/1770 train_time:161719ms step_avg:99.27ms
step:1640/1770 train_time:161822ms step_avg:99.28ms
step:1641/1770 train_time:161926ms step_avg:99.28ms
step:1642/1770 train_time:162028ms step_avg:99.28ms
step:1643/1770 train_time:162131ms step_avg:99.28ms
step:1644/1770 train_time:162236ms step_avg:99.29ms
step:1645/1770 train_time:162339ms step_avg:99.29ms
step:1646/1770 train_time:162446ms step_avg:99.29ms
step:1647/1770 train_time:162549ms step_avg:99.30ms
step:1648/1770 train_time:162653ms step_avg:99.30ms
step:1649/1770 train_time:162755ms step_avg:99.30ms
step:1650/1770 train_time:162860ms step_avg:99.30ms
step:1651/1770 train_time:162963ms step_avg:99.31ms
step:1652/1770 train_time:163065ms step_avg:99.31ms
step:1653/1770 train_time:163169ms step_avg:99.31ms
step:1654/1770 train_time:163276ms step_avg:99.32ms
step:1655/1770 train_time:163382ms step_avg:99.32ms
step:1656/1770 train_time:163486ms step_avg:99.32ms
step:1657/1770 train_time:163591ms step_avg:99.33ms
step:1658/1770 train_time:163694ms step_avg:99.33ms
step:1659/1770 train_time:163798ms step_avg:99.33ms
step:1660/1770 train_time:163901ms step_avg:99.33ms
step:1661/1770 train_time:164005ms step_avg:99.34ms
step:1662/1770 train_time:164109ms step_avg:99.34ms
step:1663/1770 train_time:164212ms step_avg:99.34ms
step:1664/1770 train_time:164314ms step_avg:99.34ms
step:1665/1770 train_time:164418ms step_avg:99.35ms
step:1666/1770 train_time:164523ms step_avg:99.35ms
step:1667/1770 train_time:164626ms step_avg:99.35ms
step:1668/1770 train_time:164730ms step_avg:99.35ms
step:1669/1770 train_time:164832ms step_avg:99.36ms
step:1670/1770 train_time:164936ms step_avg:99.36ms
step:1671/1770 train_time:165040ms step_avg:99.36ms
step:1672/1770 train_time:165144ms step_avg:99.36ms
step:1673/1770 train_time:165249ms step_avg:99.37ms
step:1674/1770 train_time:165352ms step_avg:99.37ms
step:1675/1770 train_time:165454ms step_avg:99.37ms
step:1676/1770 train_time:165559ms step_avg:99.37ms
step:1677/1770 train_time:165666ms step_avg:99.38ms
step:1678/1770 train_time:165768ms step_avg:99.38ms
step:1679/1770 train_time:165872ms step_avg:99.38ms
step:1680/1770 train_time:165975ms step_avg:99.39ms
step:1681/1770 train_time:166080ms step_avg:99.39ms
step:1682/1770 train_time:166184ms step_avg:99.39ms
step:1683/1770 train_time:166287ms step_avg:99.39ms
step:1684/1770 train_time:166390ms step_avg:99.40ms
step:1685/1770 train_time:166494ms step_avg:99.40ms
step:1686/1770 train_time:166599ms step_avg:99.40ms
step:1687/1770 train_time:166704ms step_avg:99.41ms
step:1688/1770 train_time:166807ms step_avg:99.41ms
step:1689/1770 train_time:166910ms step_avg:99.41ms
step:1690/1770 train_time:167014ms step_avg:99.41ms
step:1691/1770 train_time:167119ms step_avg:99.42ms
step:1692/1770 train_time:167222ms step_avg:99.42ms
step:1693/1770 train_time:167326ms step_avg:99.42ms
step:1694/1770 train_time:167430ms step_avg:99.42ms
step:1695/1770 train_time:167535ms step_avg:99.43ms
step:1696/1770 train_time:167641ms step_avg:99.43ms
step:1697/1770 train_time:167746ms step_avg:99.43ms
step:1698/1770 train_time:167851ms step_avg:99.44ms
step:1699/1770 train_time:167953ms step_avg:99.44ms
step:1700/1770 train_time:168057ms step_avg:99.44ms
step:1701/1770 train_time:168160ms step_avg:99.44ms
step:1702/1770 train_time:168265ms step_avg:99.45ms
step:1703/1770 train_time:168368ms step_avg:99.45ms
step:1704/1770 train_time:168472ms step_avg:99.45ms
step:1705/1770 train_time:168575ms step_avg:99.45ms
step:1706/1770 train_time:168677ms step_avg:99.46ms
step:1707/1770 train_time:168782ms step_avg:99.46ms
step:1708/1770 train_time:168885ms step_avg:99.46ms
step:1709/1770 train_time:168990ms step_avg:99.46ms
step:1710/1770 train_time:169096ms step_avg:99.47ms
step:1711/1770 train_time:169202ms step_avg:99.47ms
step:1712/1770 train_time:169306ms step_avg:99.47ms
step:1713/1770 train_time:169409ms step_avg:99.48ms
step:1714/1770 train_time:169513ms step_avg:99.48ms
step:1715/1770 train_time:169616ms step_avg:99.48ms
step:1716/1770 train_time:169720ms step_avg:99.48ms
step:1717/1770 train_time:169824ms step_avg:99.49ms
step:1718/1770 train_time:169930ms step_avg:99.49ms
step:1719/1770 train_time:170036ms step_avg:99.49ms
step:1720/1770 train_time:170141ms step_avg:99.50ms
step:1721/1770 train_time:170245ms step_avg:99.50ms
step:1722/1770 train_time:170351ms step_avg:99.50ms
step:1723/1770 train_time:170457ms step_avg:99.51ms
step:1724/1770 train_time:170563ms step_avg:99.51ms
step:1725/1770 train_time:170669ms step_avg:99.52ms
step:1726/1770 train_time:170775ms step_avg:99.52ms
step:1727/1770 train_time:170878ms step_avg:99.52ms
step:1728/1770 train_time:170985ms step_avg:99.53ms
step:1729/1770 train_time:171088ms step_avg:99.53ms
step:1730/1770 train_time:171193ms step_avg:99.53ms
step:1731/1770 train_time:171300ms step_avg:99.54ms
step:1732/1770 train_time:171403ms step_avg:99.54ms
step:1733/1770 train_time:171509ms step_avg:99.54ms
step:1734/1770 train_time:171613ms step_avg:99.54ms
step:1735/1770 train_time:171719ms step_avg:99.55ms
step:1736/1770 train_time:171822ms step_avg:99.55ms
step:1737/1770 train_time:171927ms step_avg:99.55ms
step:1738/1770 train_time:172031ms step_avg:99.55ms
step:1739/1770 train_time:172135ms step_avg:99.56ms
step:1740/1770 train_time:172238ms step_avg:99.56ms
step:1741/1770 train_time:172345ms step_avg:99.56ms
step:1742/1770 train_time:172453ms step_avg:99.57ms
step:1743/1770 train_time:172558ms step_avg:99.57ms
step:1744/1770 train_time:172662ms step_avg:99.57ms
step:1745/1770 train_time:172766ms step_avg:99.58ms
step:1746/1770 train_time:172873ms step_avg:99.58ms
step:1747/1770 train_time:172976ms step_avg:99.58ms
step:1748/1770 train_time:173082ms step_avg:99.59ms
step:1749/1770 train_time:173187ms step_avg:99.59ms
step:1750/1770 train_time:173292ms step_avg:99.59ms
step:1750/1770 val_loss:3.2876 train_time:173394ms step_avg:99.65ms
step:1751/1770 train_time:173415ms step_avg:99.61ms
step:1752/1770 train_time:173506ms step_avg:99.60ms
step:1753/1770 train_time:173610ms step_avg:99.60ms
step:1754/1770 train_time:173714ms step_avg:99.61ms
step:1755/1770 train_time:173818ms step_avg:99.61ms
step:1756/1770 train_time:173923ms step_avg:99.61ms
step:1757/1770 train_time:174027ms step_avg:99.61ms
step:1758/1770 train_time:174132ms step_avg:99.62ms
step:1759/1770 train_time:174236ms step_avg:99.62ms
step:1760/1770 train_time:174341ms step_avg:99.62ms
step:1761/1770 train_time:174447ms step_avg:99.63ms
step:1762/1770 train_time:174557ms step_avg:99.63ms
step:1763/1770 train_time:174659ms step_avg:99.63ms
step:1764/1770 train_time:174765ms step_avg:99.64ms
step:1765/1770 train_time:174868ms step_avg:99.64ms
step:1766/1770 train_time:174977ms step_avg:99.65ms
step:1767/1770 train_time:175080ms step_avg:99.65ms
step:1768/1770 train_time:175184ms step_avg:99.65ms
step:1769/1770 train_time:175288ms step_avg:99.65ms
step:1770/1770 train_time:175392ms step_avg:99.65ms
step:1770/1770 val_loss:3.2844 train_time:175496ms step_avg:99.71ms
peak memory allocated: 28840 MiB reserved: 32212 MiB
