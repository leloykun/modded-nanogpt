import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul for lm_head by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_scale: float = 1.0, w_scale: float = 1.0, grad_scale: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_scale = x_scale
        self.w_scale = w_scale
        self.grad_scale = grad_scale

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_scale, w_s=self.w_scale, grad_s=self.grad_scale)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_scale=2.0, w_scale=2.0**9, grad_scale=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Jan 27 15:49:57 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   28C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24838ms step_avg:nanms
step:2/1770 train_time:25356ms step_avg:nanms
step:3/1770 train_time:25452ms step_avg:nanms
step:4/1770 train_time:25545ms step_avg:nanms
step:5/1770 train_time:25639ms step_avg:nanms
step:6/1770 train_time:25733ms step_avg:nanms
step:7/1770 train_time:25827ms step_avg:nanms
step:8/1770 train_time:25921ms step_avg:nanms
step:9/1770 train_time:26014ms step_avg:nanms
step:10/1770 train_time:26108ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.46ms
step:14/1770 train_time:378ms step_avg:94.44ms
step:15/1770 train_time:472ms step_avg:94.44ms
step:16/1770 train_time:566ms step_avg:94.38ms
step:17/1770 train_time:660ms step_avg:94.32ms
step:18/1770 train_time:755ms step_avg:94.35ms
step:19/1770 train_time:849ms step_avg:94.34ms
step:20/1770 train_time:943ms step_avg:94.28ms
step:21/1770 train_time:1037ms step_avg:94.27ms
step:22/1770 train_time:1131ms step_avg:94.23ms
step:23/1770 train_time:1225ms step_avg:94.20ms
step:24/1770 train_time:1319ms step_avg:94.23ms
step:25/1770 train_time:1413ms step_avg:94.20ms
step:26/1770 train_time:1507ms step_avg:94.19ms
step:27/1770 train_time:1601ms step_avg:94.17ms
step:28/1770 train_time:1695ms step_avg:94.17ms
step:29/1770 train_time:1789ms step_avg:94.15ms
step:30/1770 train_time:1883ms step_avg:94.15ms
step:31/1770 train_time:1976ms step_avg:94.11ms
step:32/1770 train_time:2071ms step_avg:94.12ms
step:33/1770 train_time:2164ms step_avg:94.10ms
step:34/1770 train_time:2258ms step_avg:94.09ms
step:35/1770 train_time:2352ms step_avg:94.08ms
step:36/1770 train_time:2446ms step_avg:94.07ms
step:37/1770 train_time:2540ms step_avg:94.06ms
step:38/1770 train_time:2633ms step_avg:94.05ms
step:39/1770 train_time:2727ms step_avg:94.03ms
step:40/1770 train_time:2821ms step_avg:94.03ms
step:41/1770 train_time:2915ms step_avg:94.03ms
step:42/1770 train_time:3009ms step_avg:94.03ms
step:43/1770 train_time:3103ms step_avg:94.02ms
step:44/1770 train_time:3196ms step_avg:94.01ms
step:45/1770 train_time:3290ms step_avg:94.00ms
step:46/1770 train_time:3384ms step_avg:94.00ms
step:47/1770 train_time:3478ms step_avg:93.99ms
step:48/1770 train_time:3572ms step_avg:94.00ms
step:49/1770 train_time:3666ms step_avg:94.00ms
step:50/1770 train_time:3760ms step_avg:94.01ms
step:51/1770 train_time:3855ms step_avg:94.02ms
step:52/1770 train_time:3949ms step_avg:94.02ms
step:53/1770 train_time:4043ms step_avg:94.02ms
step:54/1770 train_time:4137ms step_avg:94.03ms
step:55/1770 train_time:4231ms step_avg:94.02ms
step:56/1770 train_time:4325ms step_avg:94.02ms
step:57/1770 train_time:4419ms step_avg:94.02ms
step:58/1770 train_time:4513ms step_avg:94.02ms
step:59/1770 train_time:4607ms step_avg:94.01ms
step:60/1770 train_time:4701ms step_avg:94.01ms
step:61/1770 train_time:4795ms step_avg:94.01ms
step:62/1770 train_time:4888ms step_avg:94.00ms
step:63/1770 train_time:4982ms step_avg:94.00ms
step:64/1770 train_time:5076ms step_avg:94.00ms
step:65/1770 train_time:5170ms step_avg:94.00ms
step:66/1770 train_time:5263ms step_avg:93.99ms
step:67/1770 train_time:5357ms step_avg:93.98ms
step:68/1770 train_time:5451ms step_avg:93.98ms
step:69/1770 train_time:5544ms step_avg:93.97ms
step:70/1770 train_time:5638ms step_avg:93.97ms
step:71/1770 train_time:5732ms step_avg:93.97ms
step:72/1770 train_time:5826ms step_avg:93.97ms
step:73/1770 train_time:5920ms step_avg:93.97ms
step:74/1770 train_time:6014ms step_avg:93.97ms
step:75/1770 train_time:6108ms step_avg:93.97ms
step:76/1770 train_time:6202ms step_avg:93.97ms
step:77/1770 train_time:6296ms step_avg:93.97ms
step:78/1770 train_time:6390ms step_avg:93.97ms
step:79/1770 train_time:6484ms step_avg:93.96ms
step:80/1770 train_time:6578ms step_avg:93.96ms
step:81/1770 train_time:6672ms step_avg:93.97ms
step:82/1770 train_time:6765ms step_avg:93.96ms
step:83/1770 train_time:6859ms step_avg:93.96ms
step:84/1770 train_time:6953ms step_avg:93.96ms
step:85/1770 train_time:7047ms step_avg:93.96ms
step:86/1770 train_time:7141ms step_avg:93.96ms
step:87/1770 train_time:7236ms step_avg:93.97ms
step:88/1770 train_time:7329ms step_avg:93.97ms
step:89/1770 train_time:7423ms step_avg:93.96ms
step:90/1770 train_time:7517ms step_avg:93.96ms
step:91/1770 train_time:7610ms step_avg:93.95ms
step:92/1770 train_time:7704ms step_avg:93.95ms
step:93/1770 train_time:7798ms step_avg:93.95ms
step:94/1770 train_time:7891ms step_avg:93.94ms
step:95/1770 train_time:7985ms step_avg:93.94ms
step:96/1770 train_time:8079ms step_avg:93.94ms
step:97/1770 train_time:8173ms step_avg:93.94ms
step:98/1770 train_time:8267ms step_avg:93.94ms
step:99/1770 train_time:8361ms step_avg:93.94ms
step:100/1770 train_time:8456ms step_avg:93.95ms
step:101/1770 train_time:8550ms step_avg:93.95ms
step:102/1770 train_time:8643ms step_avg:93.95ms
step:103/1770 train_time:8737ms step_avg:93.95ms
step:104/1770 train_time:8830ms step_avg:93.94ms
step:105/1770 train_time:8924ms step_avg:93.94ms
step:106/1770 train_time:9018ms step_avg:93.94ms
step:107/1770 train_time:9112ms step_avg:93.94ms
step:108/1770 train_time:9206ms step_avg:93.94ms
step:109/1770 train_time:9300ms step_avg:93.94ms
step:110/1770 train_time:9394ms step_avg:93.94ms
step:111/1770 train_time:9488ms step_avg:93.94ms
step:112/1770 train_time:9583ms step_avg:93.95ms
step:113/1770 train_time:9677ms step_avg:93.95ms
step:114/1770 train_time:9771ms step_avg:93.95ms
step:115/1770 train_time:9864ms step_avg:93.95ms
step:116/1770 train_time:9958ms step_avg:93.94ms
step:117/1770 train_time:10052ms step_avg:93.95ms
step:118/1770 train_time:10146ms step_avg:93.94ms
step:119/1770 train_time:10240ms step_avg:93.95ms
step:120/1770 train_time:10334ms step_avg:93.95ms
step:121/1770 train_time:10428ms step_avg:93.94ms
step:122/1770 train_time:10522ms step_avg:93.95ms
step:123/1770 train_time:10616ms step_avg:93.95ms
step:124/1770 train_time:10711ms step_avg:93.95ms
step:125/1770 train_time:10805ms step_avg:93.95ms
step:125/1770 val_loss:4.6522 train_time:10897ms step_avg:94.76ms
step:126/1770 train_time:10919ms step_avg:94.13ms
step:127/1770 train_time:11085ms step_avg:94.74ms
step:128/1770 train_time:11131ms step_avg:94.33ms
step:129/1770 train_time:11224ms step_avg:94.32ms
step:130/1770 train_time:11318ms step_avg:94.32ms
step:131/1770 train_time:11411ms step_avg:94.31ms
step:132/1770 train_time:11505ms step_avg:94.30ms
step:133/1770 train_time:11599ms step_avg:94.30ms
step:134/1770 train_time:11693ms step_avg:94.30ms
step:135/1770 train_time:11787ms step_avg:94.30ms
step:136/1770 train_time:11881ms step_avg:94.29ms
step:137/1770 train_time:11975ms step_avg:94.29ms
step:138/1770 train_time:12071ms step_avg:94.30ms
step:139/1770 train_time:12166ms step_avg:94.31ms
step:140/1770 train_time:12260ms step_avg:94.31ms
step:141/1770 train_time:12355ms step_avg:94.31ms
step:142/1770 train_time:12450ms step_avg:94.32ms
step:143/1770 train_time:12544ms step_avg:94.32ms
step:144/1770 train_time:12639ms step_avg:94.32ms
step:145/1770 train_time:12733ms step_avg:94.32ms
step:146/1770 train_time:12828ms step_avg:94.32ms
step:147/1770 train_time:12922ms step_avg:94.32ms
step:148/1770 train_time:13016ms step_avg:94.32ms
step:149/1770 train_time:13111ms step_avg:94.32ms
step:150/1770 train_time:13205ms step_avg:94.32ms
step:151/1770 train_time:13300ms step_avg:94.32ms
step:152/1770 train_time:13394ms step_avg:94.33ms
step:153/1770 train_time:13490ms step_avg:94.33ms
step:154/1770 train_time:13584ms step_avg:94.33ms
step:155/1770 train_time:13679ms step_avg:94.34ms
step:156/1770 train_time:13774ms step_avg:94.34ms
step:157/1770 train_time:13868ms step_avg:94.34ms
step:158/1770 train_time:13963ms step_avg:94.34ms
step:159/1770 train_time:14058ms step_avg:94.35ms
step:160/1770 train_time:14152ms step_avg:94.35ms
step:161/1770 train_time:14247ms step_avg:94.35ms
step:162/1770 train_time:14341ms step_avg:94.35ms
step:163/1770 train_time:14436ms step_avg:94.35ms
step:164/1770 train_time:14530ms step_avg:94.35ms
step:165/1770 train_time:14625ms step_avg:94.35ms
step:166/1770 train_time:14719ms step_avg:94.35ms
step:167/1770 train_time:14814ms step_avg:94.35ms
step:168/1770 train_time:14909ms step_avg:94.36ms
step:169/1770 train_time:15003ms step_avg:94.36ms
step:170/1770 train_time:15097ms step_avg:94.36ms
step:171/1770 train_time:15192ms step_avg:94.36ms
step:172/1770 train_time:15287ms step_avg:94.36ms
step:173/1770 train_time:15381ms step_avg:94.36ms
step:174/1770 train_time:15475ms step_avg:94.36ms
step:175/1770 train_time:15570ms step_avg:94.36ms
step:176/1770 train_time:15665ms step_avg:94.36ms
step:177/1770 train_time:15759ms step_avg:94.37ms
step:178/1770 train_time:15854ms step_avg:94.37ms
step:179/1770 train_time:15948ms step_avg:94.37ms
step:180/1770 train_time:16043ms step_avg:94.37ms
step:181/1770 train_time:16137ms step_avg:94.37ms
step:182/1770 train_time:16232ms step_avg:94.37ms
step:183/1770 train_time:16327ms step_avg:94.37ms
step:184/1770 train_time:16421ms step_avg:94.37ms
step:185/1770 train_time:16515ms step_avg:94.37ms
step:186/1770 train_time:16610ms step_avg:94.38ms
step:187/1770 train_time:16705ms step_avg:94.38ms
step:188/1770 train_time:16799ms step_avg:94.38ms
step:189/1770 train_time:16894ms step_avg:94.38ms
step:190/1770 train_time:16989ms step_avg:94.38ms
step:191/1770 train_time:17083ms step_avg:94.38ms
step:192/1770 train_time:17178ms step_avg:94.39ms
step:193/1770 train_time:17274ms step_avg:94.39ms
step:194/1770 train_time:17369ms step_avg:94.39ms
step:195/1770 train_time:17463ms step_avg:94.39ms
step:196/1770 train_time:17557ms step_avg:94.39ms
step:197/1770 train_time:17652ms step_avg:94.40ms
step:198/1770 train_time:17748ms step_avg:94.40ms
step:199/1770 train_time:17841ms step_avg:94.40ms
step:200/1770 train_time:17936ms step_avg:94.40ms
step:201/1770 train_time:18031ms step_avg:94.40ms
step:202/1770 train_time:18125ms step_avg:94.40ms
step:203/1770 train_time:18220ms step_avg:94.40ms
step:204/1770 train_time:18314ms step_avg:94.40ms
step:205/1770 train_time:18409ms step_avg:94.40ms
step:206/1770 train_time:18504ms step_avg:94.41ms
step:207/1770 train_time:18598ms step_avg:94.41ms
step:208/1770 train_time:18693ms step_avg:94.41ms
step:209/1770 train_time:18787ms step_avg:94.41ms
step:210/1770 train_time:18881ms step_avg:94.41ms
step:211/1770 train_time:18976ms step_avg:94.41ms
step:212/1770 train_time:19071ms step_avg:94.41ms
step:213/1770 train_time:19166ms step_avg:94.41ms
step:214/1770 train_time:19261ms step_avg:94.42ms
step:215/1770 train_time:19355ms step_avg:94.42ms
step:216/1770 train_time:19451ms step_avg:94.42ms
step:217/1770 train_time:19545ms step_avg:94.42ms
step:218/1770 train_time:19639ms step_avg:94.42ms
step:219/1770 train_time:19734ms step_avg:94.42ms
step:220/1770 train_time:19829ms step_avg:94.42ms
step:221/1770 train_time:19924ms step_avg:94.42ms
step:222/1770 train_time:20018ms step_avg:94.42ms
step:223/1770 train_time:20113ms step_avg:94.43ms
step:224/1770 train_time:20208ms step_avg:94.43ms
step:225/1770 train_time:20302ms step_avg:94.43ms
step:226/1770 train_time:20397ms step_avg:94.43ms
step:227/1770 train_time:20492ms step_avg:94.43ms
step:228/1770 train_time:20586ms step_avg:94.43ms
step:229/1770 train_time:20680ms step_avg:94.43ms
step:230/1770 train_time:20775ms step_avg:94.43ms
step:231/1770 train_time:20870ms step_avg:94.43ms
step:232/1770 train_time:20964ms step_avg:94.43ms
step:233/1770 train_time:21058ms step_avg:94.43ms
step:234/1770 train_time:21153ms step_avg:94.43ms
step:235/1770 train_time:21249ms step_avg:94.44ms
step:236/1770 train_time:21343ms step_avg:94.44ms
step:237/1770 train_time:21438ms step_avg:94.44ms
step:238/1770 train_time:21533ms step_avg:94.44ms
step:239/1770 train_time:21628ms step_avg:94.45ms
step:240/1770 train_time:21723ms step_avg:94.45ms
step:241/1770 train_time:21818ms step_avg:94.45ms
step:242/1770 train_time:21913ms step_avg:94.45ms
step:243/1770 train_time:22007ms step_avg:94.45ms
step:244/1770 train_time:22102ms step_avg:94.45ms
step:245/1770 train_time:22197ms step_avg:94.45ms
step:246/1770 train_time:22292ms step_avg:94.46ms
step:247/1770 train_time:22386ms step_avg:94.46ms
step:248/1770 train_time:22481ms step_avg:94.46ms
step:249/1770 train_time:22575ms step_avg:94.46ms
step:250/1770 train_time:22670ms step_avg:94.46ms
step:250/1770 val_loss:4.1101 train_time:22763ms step_avg:94.85ms
step:251/1770 train_time:22785ms step_avg:94.54ms
step:252/1770 train_time:22869ms step_avg:94.50ms
step:253/1770 train_time:22965ms step_avg:94.51ms
step:254/1770 train_time:23060ms step_avg:94.51ms
step:255/1770 train_time:23155ms step_avg:94.51ms
step:256/1770 train_time:23249ms step_avg:94.51ms
step:257/1770 train_time:23343ms step_avg:94.51ms
step:258/1770 train_time:23437ms step_avg:94.51ms
step:259/1770 train_time:23531ms step_avg:94.50ms
step:260/1770 train_time:23626ms step_avg:94.50ms
step:261/1770 train_time:23720ms step_avg:94.50ms
step:262/1770 train_time:23814ms step_avg:94.50ms
step:263/1770 train_time:23910ms step_avg:94.51ms
step:264/1770 train_time:24006ms step_avg:94.51ms
step:265/1770 train_time:24101ms step_avg:94.51ms
step:266/1770 train_time:24196ms step_avg:94.52ms
step:267/1770 train_time:24291ms step_avg:94.52ms
step:268/1770 train_time:24387ms step_avg:94.52ms
step:269/1770 train_time:24482ms step_avg:94.53ms
step:270/1770 train_time:24576ms step_avg:94.52ms
step:271/1770 train_time:24671ms step_avg:94.53ms
step:272/1770 train_time:24767ms step_avg:94.53ms
step:273/1770 train_time:24862ms step_avg:94.53ms
step:274/1770 train_time:24956ms step_avg:94.53ms
step:275/1770 train_time:25052ms step_avg:94.53ms
step:276/1770 train_time:25147ms step_avg:94.54ms
step:277/1770 train_time:25242ms step_avg:94.54ms
step:278/1770 train_time:25338ms step_avg:94.54ms
step:279/1770 train_time:25432ms step_avg:94.54ms
step:280/1770 train_time:25527ms step_avg:94.55ms
step:281/1770 train_time:25622ms step_avg:94.55ms
step:282/1770 train_time:25717ms step_avg:94.55ms
step:283/1770 train_time:25812ms step_avg:94.55ms
step:284/1770 train_time:25908ms step_avg:94.56ms
step:285/1770 train_time:26004ms step_avg:94.56ms
step:286/1770 train_time:26099ms step_avg:94.56ms
step:287/1770 train_time:26195ms step_avg:94.57ms
step:288/1770 train_time:26290ms step_avg:94.57ms
step:289/1770 train_time:26386ms step_avg:94.57ms
step:290/1770 train_time:26481ms step_avg:94.57ms
step:291/1770 train_time:26576ms step_avg:94.58ms
step:292/1770 train_time:26671ms step_avg:94.58ms
step:293/1770 train_time:26766ms step_avg:94.58ms
step:294/1770 train_time:26861ms step_avg:94.58ms
step:295/1770 train_time:26956ms step_avg:94.58ms
step:296/1770 train_time:27052ms step_avg:94.59ms
step:297/1770 train_time:27148ms step_avg:94.59ms
step:298/1770 train_time:27243ms step_avg:94.59ms
step:299/1770 train_time:27338ms step_avg:94.60ms
step:300/1770 train_time:27433ms step_avg:94.60ms
step:301/1770 train_time:27529ms step_avg:94.60ms
step:302/1770 train_time:27624ms step_avg:94.60ms
step:303/1770 train_time:27719ms step_avg:94.60ms
step:304/1770 train_time:27814ms step_avg:94.61ms
step:305/1770 train_time:27909ms step_avg:94.61ms
step:306/1770 train_time:28005ms step_avg:94.61ms
step:307/1770 train_time:28100ms step_avg:94.61ms
step:308/1770 train_time:28195ms step_avg:94.62ms
step:309/1770 train_time:28291ms step_avg:94.62ms
step:310/1770 train_time:28386ms step_avg:94.62ms
step:311/1770 train_time:28481ms step_avg:94.62ms
step:312/1770 train_time:28577ms step_avg:94.63ms
step:313/1770 train_time:28672ms step_avg:94.63ms
step:314/1770 train_time:28768ms step_avg:94.63ms
step:315/1770 train_time:28863ms step_avg:94.63ms
step:316/1770 train_time:28958ms step_avg:94.63ms
step:317/1770 train_time:29052ms step_avg:94.63ms
step:318/1770 train_time:29148ms step_avg:94.64ms
step:319/1770 train_time:29243ms step_avg:94.64ms
step:320/1770 train_time:29338ms step_avg:94.64ms
step:321/1770 train_time:29433ms step_avg:94.64ms
step:322/1770 train_time:29528ms step_avg:94.64ms
step:323/1770 train_time:29624ms step_avg:94.64ms
step:324/1770 train_time:29718ms step_avg:94.64ms
step:325/1770 train_time:29814ms step_avg:94.65ms
step:326/1770 train_time:29909ms step_avg:94.65ms
step:327/1770 train_time:30004ms step_avg:94.65ms
step:328/1770 train_time:30099ms step_avg:94.65ms
step:329/1770 train_time:30194ms step_avg:94.65ms
step:330/1770 train_time:30290ms step_avg:94.65ms
step:331/1770 train_time:30385ms step_avg:94.66ms
step:332/1770 train_time:30480ms step_avg:94.66ms
step:333/1770 train_time:30575ms step_avg:94.66ms
step:334/1770 train_time:30670ms step_avg:94.66ms
step:335/1770 train_time:30765ms step_avg:94.66ms
step:336/1770 train_time:30861ms step_avg:94.67ms
step:337/1770 train_time:30956ms step_avg:94.67ms
step:338/1770 train_time:31051ms step_avg:94.67ms
step:339/1770 train_time:31146ms step_avg:94.67ms
step:340/1770 train_time:31241ms step_avg:94.67ms
step:341/1770 train_time:31336ms step_avg:94.67ms
step:342/1770 train_time:31433ms step_avg:94.68ms
step:343/1770 train_time:31527ms step_avg:94.68ms
step:344/1770 train_time:31622ms step_avg:94.68ms
step:345/1770 train_time:31717ms step_avg:94.68ms
step:346/1770 train_time:31812ms step_avg:94.68ms
step:347/1770 train_time:31908ms step_avg:94.68ms
step:348/1770 train_time:32003ms step_avg:94.68ms
step:349/1770 train_time:32098ms step_avg:94.68ms
step:350/1770 train_time:32193ms step_avg:94.68ms
step:351/1770 train_time:32288ms step_avg:94.69ms
step:352/1770 train_time:32383ms step_avg:94.69ms
step:353/1770 train_time:32479ms step_avg:94.69ms
step:354/1770 train_time:32573ms step_avg:94.69ms
step:355/1770 train_time:32668ms step_avg:94.69ms
step:356/1770 train_time:32763ms step_avg:94.69ms
step:357/1770 train_time:32858ms step_avg:94.69ms
step:358/1770 train_time:32953ms step_avg:94.69ms
step:359/1770 train_time:33048ms step_avg:94.69ms
step:360/1770 train_time:33144ms step_avg:94.70ms
step:361/1770 train_time:33239ms step_avg:94.70ms
step:362/1770 train_time:33333ms step_avg:94.70ms
step:363/1770 train_time:33429ms step_avg:94.70ms
step:364/1770 train_time:33524ms step_avg:94.70ms
step:365/1770 train_time:33619ms step_avg:94.70ms
step:366/1770 train_time:33713ms step_avg:94.70ms
step:367/1770 train_time:33808ms step_avg:94.70ms
step:368/1770 train_time:33903ms step_avg:94.70ms
step:369/1770 train_time:33998ms step_avg:94.70ms
step:370/1770 train_time:34093ms step_avg:94.70ms
step:371/1770 train_time:34188ms step_avg:94.70ms
step:372/1770 train_time:34283ms step_avg:94.70ms
step:373/1770 train_time:34378ms step_avg:94.70ms
step:374/1770 train_time:34473ms step_avg:94.71ms
step:375/1770 train_time:34568ms step_avg:94.71ms
step:375/1770 val_loss:3.9103 train_time:34662ms step_avg:94.96ms
step:376/1770 train_time:34684ms step_avg:94.77ms
step:377/1770 train_time:34770ms step_avg:94.74ms
step:378/1770 train_time:34867ms step_avg:94.75ms
step:379/1770 train_time:34962ms step_avg:94.75ms
step:380/1770 train_time:35057ms step_avg:94.75ms
step:381/1770 train_time:35152ms step_avg:94.75ms
step:382/1770 train_time:35246ms step_avg:94.75ms
step:383/1770 train_time:35341ms step_avg:94.75ms
step:384/1770 train_time:35436ms step_avg:94.75ms
step:385/1770 train_time:35531ms step_avg:94.75ms
step:386/1770 train_time:35625ms step_avg:94.75ms
step:387/1770 train_time:35721ms step_avg:94.75ms
step:388/1770 train_time:35817ms step_avg:94.75ms
step:389/1770 train_time:35912ms step_avg:94.75ms
step:390/1770 train_time:36007ms step_avg:94.75ms
step:391/1770 train_time:36101ms step_avg:94.75ms
step:392/1770 train_time:36196ms step_avg:94.75ms
step:393/1770 train_time:36291ms step_avg:94.75ms
step:394/1770 train_time:36386ms step_avg:94.75ms
step:395/1770 train_time:36480ms step_avg:94.75ms
step:396/1770 train_time:36577ms step_avg:94.76ms
step:397/1770 train_time:36675ms step_avg:94.77ms
step:398/1770 train_time:36772ms step_avg:94.77ms
step:399/1770 train_time:36870ms step_avg:94.78ms
step:400/1770 train_time:36967ms step_avg:94.79ms
step:401/1770 train_time:37064ms step_avg:94.79ms
step:402/1770 train_time:37161ms step_avg:94.80ms
step:403/1770 train_time:37258ms step_avg:94.80ms
step:404/1770 train_time:37355ms step_avg:94.81ms
step:405/1770 train_time:37451ms step_avg:94.81ms
step:406/1770 train_time:37547ms step_avg:94.82ms
step:407/1770 train_time:37645ms step_avg:94.82ms
step:408/1770 train_time:37741ms step_avg:94.83ms
step:409/1770 train_time:37839ms step_avg:94.83ms
step:410/1770 train_time:37936ms step_avg:94.84ms
step:411/1770 train_time:38033ms step_avg:94.85ms
step:412/1770 train_time:38130ms step_avg:94.85ms
step:413/1770 train_time:38227ms step_avg:94.86ms
step:414/1770 train_time:38324ms step_avg:94.86ms
step:415/1770 train_time:38421ms step_avg:94.87ms
step:416/1770 train_time:38518ms step_avg:94.87ms
step:417/1770 train_time:38615ms step_avg:94.88ms
step:418/1770 train_time:38711ms step_avg:94.88ms
step:419/1770 train_time:38809ms step_avg:94.89ms
step:420/1770 train_time:38906ms step_avg:94.89ms
step:421/1770 train_time:39004ms step_avg:94.90ms
step:422/1770 train_time:39101ms step_avg:94.91ms
step:423/1770 train_time:39199ms step_avg:94.91ms
step:424/1770 train_time:39296ms step_avg:94.92ms
step:425/1770 train_time:39393ms step_avg:94.92ms
step:426/1770 train_time:39490ms step_avg:94.93ms
step:427/1770 train_time:39587ms step_avg:94.93ms
step:428/1770 train_time:39684ms step_avg:94.94ms
step:429/1770 train_time:39781ms step_avg:94.94ms
step:430/1770 train_time:39878ms step_avg:94.95ms
step:431/1770 train_time:39975ms step_avg:94.95ms
step:432/1770 train_time:40071ms step_avg:94.96ms
step:433/1770 train_time:40168ms step_avg:94.96ms
step:434/1770 train_time:40265ms step_avg:94.96ms
step:435/1770 train_time:40362ms step_avg:94.97ms
step:436/1770 train_time:40459ms step_avg:94.97ms
step:437/1770 train_time:40557ms step_avg:94.98ms
step:438/1770 train_time:40653ms step_avg:94.98ms
step:439/1770 train_time:40750ms step_avg:94.99ms
step:440/1770 train_time:40847ms step_avg:94.99ms
step:441/1770 train_time:40944ms step_avg:95.00ms
step:442/1770 train_time:41041ms step_avg:95.00ms
step:443/1770 train_time:41138ms step_avg:95.01ms
step:444/1770 train_time:41235ms step_avg:95.01ms
step:445/1770 train_time:41332ms step_avg:95.02ms
step:446/1770 train_time:41429ms step_avg:95.02ms
step:447/1770 train_time:41526ms step_avg:95.03ms
step:448/1770 train_time:41624ms step_avg:95.03ms
step:449/1770 train_time:41721ms step_avg:95.04ms
step:450/1770 train_time:41817ms step_avg:95.04ms
step:451/1770 train_time:41914ms step_avg:95.04ms
step:452/1770 train_time:42011ms step_avg:95.05ms
step:453/1770 train_time:42108ms step_avg:95.05ms
step:454/1770 train_time:42205ms step_avg:95.06ms
step:455/1770 train_time:42302ms step_avg:95.06ms
step:456/1770 train_time:42399ms step_avg:95.06ms
step:457/1770 train_time:42496ms step_avg:95.07ms
step:458/1770 train_time:42594ms step_avg:95.08ms
step:459/1770 train_time:42691ms step_avg:95.08ms
step:460/1770 train_time:42788ms step_avg:95.08ms
step:461/1770 train_time:42885ms step_avg:95.09ms
step:462/1770 train_time:42982ms step_avg:95.09ms
step:463/1770 train_time:43080ms step_avg:95.10ms
step:464/1770 train_time:43177ms step_avg:95.10ms
step:465/1770 train_time:43274ms step_avg:95.11ms
step:466/1770 train_time:43371ms step_avg:95.11ms
step:467/1770 train_time:43467ms step_avg:95.11ms
step:468/1770 train_time:43564ms step_avg:95.12ms
step:469/1770 train_time:43661ms step_avg:95.12ms
step:470/1770 train_time:43758ms step_avg:95.13ms
step:471/1770 train_time:43855ms step_avg:95.13ms
step:472/1770 train_time:43952ms step_avg:95.13ms
step:473/1770 train_time:44049ms step_avg:95.14ms
step:474/1770 train_time:44146ms step_avg:95.14ms
step:475/1770 train_time:44242ms step_avg:95.14ms
step:476/1770 train_time:44340ms step_avg:95.15ms
step:477/1770 train_time:44438ms step_avg:95.16ms
step:478/1770 train_time:44535ms step_avg:95.16ms
step:479/1770 train_time:44632ms step_avg:95.16ms
step:480/1770 train_time:44728ms step_avg:95.17ms
step:481/1770 train_time:44825ms step_avg:95.17ms
step:482/1770 train_time:44922ms step_avg:95.17ms
step:483/1770 train_time:45018ms step_avg:95.18ms
step:484/1770 train_time:45115ms step_avg:95.18ms
step:485/1770 train_time:45212ms step_avg:95.18ms
step:486/1770 train_time:45309ms step_avg:95.19ms
step:487/1770 train_time:45406ms step_avg:95.19ms
step:488/1770 train_time:45503ms step_avg:95.20ms
step:489/1770 train_time:45601ms step_avg:95.20ms
step:490/1770 train_time:45698ms step_avg:95.20ms
step:491/1770 train_time:45796ms step_avg:95.21ms
step:492/1770 train_time:45892ms step_avg:95.21ms
step:493/1770 train_time:45989ms step_avg:95.21ms
step:494/1770 train_time:46086ms step_avg:95.22ms
step:495/1770 train_time:46183ms step_avg:95.22ms
step:496/1770 train_time:46280ms step_avg:95.23ms
step:497/1770 train_time:46378ms step_avg:95.23ms
step:498/1770 train_time:46475ms step_avg:95.24ms
step:499/1770 train_time:46572ms step_avg:95.24ms
step:500/1770 train_time:46669ms step_avg:95.24ms
step:500/1770 val_loss:3.7579 train_time:46764ms step_avg:95.44ms
step:501/1770 train_time:46787ms step_avg:95.29ms
step:502/1770 train_time:46874ms step_avg:95.27ms
step:503/1770 train_time:46973ms step_avg:95.28ms
step:504/1770 train_time:47070ms step_avg:95.28ms
step:505/1770 train_time:47166ms step_avg:95.29ms
step:506/1770 train_time:47263ms step_avg:95.29ms
step:507/1770 train_time:47360ms step_avg:95.29ms
step:508/1770 train_time:47456ms step_avg:95.29ms
step:509/1770 train_time:47553ms step_avg:95.30ms
step:510/1770 train_time:47649ms step_avg:95.30ms
step:511/1770 train_time:47746ms step_avg:95.30ms
step:512/1770 train_time:47843ms step_avg:95.30ms
step:513/1770 train_time:47940ms step_avg:95.31ms
step:514/1770 train_time:48038ms step_avg:95.31ms
step:515/1770 train_time:48136ms step_avg:95.32ms
step:516/1770 train_time:48233ms step_avg:95.32ms
step:517/1770 train_time:48329ms step_avg:95.32ms
step:518/1770 train_time:48426ms step_avg:95.33ms
step:519/1770 train_time:48523ms step_avg:95.33ms
step:520/1770 train_time:48620ms step_avg:95.33ms
step:521/1770 train_time:48717ms step_avg:95.34ms
step:522/1770 train_time:48815ms step_avg:95.34ms
step:523/1770 train_time:48912ms step_avg:95.34ms
step:524/1770 train_time:49009ms step_avg:95.35ms
step:525/1770 train_time:49106ms step_avg:95.35ms
step:526/1770 train_time:49203ms step_avg:95.36ms
step:527/1770 train_time:49301ms step_avg:95.36ms
step:528/1770 train_time:49399ms step_avg:95.37ms
step:529/1770 train_time:49497ms step_avg:95.37ms
step:530/1770 train_time:49593ms step_avg:95.37ms
step:531/1770 train_time:49691ms step_avg:95.38ms
step:532/1770 train_time:49787ms step_avg:95.38ms
step:533/1770 train_time:49885ms step_avg:95.38ms
step:534/1770 train_time:49982ms step_avg:95.38ms
step:535/1770 train_time:50080ms step_avg:95.39ms
step:536/1770 train_time:50178ms step_avg:95.39ms
step:537/1770 train_time:50275ms step_avg:95.40ms
step:538/1770 train_time:50372ms step_avg:95.40ms
step:539/1770 train_time:50470ms step_avg:95.41ms
step:540/1770 train_time:50567ms step_avg:95.41ms
step:541/1770 train_time:50664ms step_avg:95.41ms
step:542/1770 train_time:50762ms step_avg:95.42ms
step:543/1770 train_time:50859ms step_avg:95.42ms
step:544/1770 train_time:50956ms step_avg:95.42ms
step:545/1770 train_time:51054ms step_avg:95.43ms
step:546/1770 train_time:51151ms step_avg:95.43ms
step:547/1770 train_time:51248ms step_avg:95.43ms
step:548/1770 train_time:51345ms step_avg:95.44ms
step:549/1770 train_time:51442ms step_avg:95.44ms
step:550/1770 train_time:51540ms step_avg:95.44ms
step:551/1770 train_time:51638ms step_avg:95.45ms
step:552/1770 train_time:51734ms step_avg:95.45ms
step:553/1770 train_time:51832ms step_avg:95.45ms
step:554/1770 train_time:51929ms step_avg:95.46ms
step:555/1770 train_time:52027ms step_avg:95.46ms
step:556/1770 train_time:52124ms step_avg:95.47ms
step:557/1770 train_time:52222ms step_avg:95.47ms
step:558/1770 train_time:52319ms step_avg:95.47ms
step:559/1770 train_time:52417ms step_avg:95.48ms
step:560/1770 train_time:52514ms step_avg:95.48ms
step:561/1770 train_time:52611ms step_avg:95.48ms
step:562/1770 train_time:52708ms step_avg:95.49ms
step:563/1770 train_time:52805ms step_avg:95.49ms
step:564/1770 train_time:52902ms step_avg:95.49ms
step:565/1770 train_time:53000ms step_avg:95.50ms
step:566/1770 train_time:53098ms step_avg:95.50ms
step:567/1770 train_time:53196ms step_avg:95.50ms
step:568/1770 train_time:53293ms step_avg:95.51ms
step:569/1770 train_time:53390ms step_avg:95.51ms
step:570/1770 train_time:53488ms step_avg:95.51ms
step:571/1770 train_time:53585ms step_avg:95.52ms
step:572/1770 train_time:53682ms step_avg:95.52ms
step:573/1770 train_time:53780ms step_avg:95.52ms
step:574/1770 train_time:53878ms step_avg:95.53ms
step:575/1770 train_time:53975ms step_avg:95.53ms
step:576/1770 train_time:54072ms step_avg:95.53ms
step:577/1770 train_time:54169ms step_avg:95.54ms
step:578/1770 train_time:54266ms step_avg:95.54ms
step:579/1770 train_time:54363ms step_avg:95.54ms
step:580/1770 train_time:54461ms step_avg:95.55ms
step:581/1770 train_time:54560ms step_avg:95.55ms
step:582/1770 train_time:54657ms step_avg:95.55ms
step:583/1770 train_time:54754ms step_avg:95.56ms
step:584/1770 train_time:54851ms step_avg:95.56ms
step:585/1770 train_time:54948ms step_avg:95.56ms
step:586/1770 train_time:55045ms step_avg:95.56ms
step:587/1770 train_time:55142ms step_avg:95.57ms
step:588/1770 train_time:55239ms step_avg:95.57ms
step:589/1770 train_time:55337ms step_avg:95.57ms
step:590/1770 train_time:55435ms step_avg:95.58ms
step:591/1770 train_time:55532ms step_avg:95.58ms
step:592/1770 train_time:55629ms step_avg:95.58ms
step:593/1770 train_time:55726ms step_avg:95.59ms
step:594/1770 train_time:55824ms step_avg:95.59ms
step:595/1770 train_time:55922ms step_avg:95.59ms
step:596/1770 train_time:56019ms step_avg:95.60ms
step:597/1770 train_time:56117ms step_avg:95.60ms
step:598/1770 train_time:56214ms step_avg:95.60ms
step:599/1770 train_time:56311ms step_avg:95.60ms
step:600/1770 train_time:56409ms step_avg:95.61ms
step:601/1770 train_time:56506ms step_avg:95.61ms
step:602/1770 train_time:56604ms step_avg:95.61ms
step:603/1770 train_time:56702ms step_avg:95.62ms
step:604/1770 train_time:56800ms step_avg:95.62ms
step:605/1770 train_time:56897ms step_avg:95.63ms
step:606/1770 train_time:56994ms step_avg:95.63ms
step:607/1770 train_time:57092ms step_avg:95.63ms
step:608/1770 train_time:57189ms step_avg:95.63ms
step:609/1770 train_time:57286ms step_avg:95.64ms
step:610/1770 train_time:57383ms step_avg:95.64ms
step:611/1770 train_time:57481ms step_avg:95.64ms
step:612/1770 train_time:57579ms step_avg:95.65ms
step:613/1770 train_time:57676ms step_avg:95.65ms
step:614/1770 train_time:57773ms step_avg:95.65ms
step:615/1770 train_time:57871ms step_avg:95.65ms
step:616/1770 train_time:57968ms step_avg:95.66ms
step:617/1770 train_time:58066ms step_avg:95.66ms
step:618/1770 train_time:58163ms step_avg:95.66ms
step:619/1770 train_time:58261ms step_avg:95.67ms
step:620/1770 train_time:58359ms step_avg:95.67ms
step:621/1770 train_time:58457ms step_avg:95.67ms
step:622/1770 train_time:58554ms step_avg:95.68ms
step:623/1770 train_time:58651ms step_avg:95.68ms
step:624/1770 train_time:58749ms step_avg:95.68ms
step:625/1770 train_time:58846ms step_avg:95.68ms
step:625/1770 val_loss:3.6702 train_time:58942ms step_avg:95.84ms
step:626/1770 train_time:58964ms step_avg:95.72ms
step:627/1770 train_time:59048ms step_avg:95.70ms
step:628/1770 train_time:59147ms step_avg:95.71ms
step:629/1770 train_time:59244ms step_avg:95.71ms
step:630/1770 train_time:59341ms step_avg:95.71ms
step:631/1770 train_time:59438ms step_avg:95.71ms
step:632/1770 train_time:59535ms step_avg:95.71ms
step:633/1770 train_time:59632ms step_avg:95.72ms
step:634/1770 train_time:59729ms step_avg:95.72ms
step:635/1770 train_time:59826ms step_avg:95.72ms
step:636/1770 train_time:59923ms step_avg:95.72ms
step:637/1770 train_time:60021ms step_avg:95.73ms
step:638/1770 train_time:60119ms step_avg:95.73ms
step:639/1770 train_time:60216ms step_avg:95.73ms
step:640/1770 train_time:60314ms step_avg:95.74ms
step:641/1770 train_time:60412ms step_avg:95.74ms
step:642/1770 train_time:60509ms step_avg:95.74ms
step:643/1770 train_time:60606ms step_avg:95.74ms
step:644/1770 train_time:60703ms step_avg:95.75ms
step:645/1770 train_time:60799ms step_avg:95.75ms
step:646/1770 train_time:60897ms step_avg:95.75ms
step:647/1770 train_time:60995ms step_avg:95.75ms
step:648/1770 train_time:61093ms step_avg:95.76ms
step:649/1770 train_time:61190ms step_avg:95.76ms
step:650/1770 train_time:61288ms step_avg:95.76ms
step:651/1770 train_time:61385ms step_avg:95.77ms
step:652/1770 train_time:61483ms step_avg:95.77ms
step:653/1770 train_time:61580ms step_avg:95.77ms
step:654/1770 train_time:61677ms step_avg:95.77ms
step:655/1770 train_time:61775ms step_avg:95.78ms
step:656/1770 train_time:61873ms step_avg:95.78ms
step:657/1770 train_time:61971ms step_avg:95.78ms
step:658/1770 train_time:62070ms step_avg:95.79ms
step:659/1770 train_time:62169ms step_avg:95.79ms
step:660/1770 train_time:62269ms step_avg:95.80ms
step:661/1770 train_time:62368ms step_avg:95.80ms
step:662/1770 train_time:62467ms step_avg:95.81ms
step:663/1770 train_time:62566ms step_avg:95.81ms
step:664/1770 train_time:62665ms step_avg:95.82ms
step:665/1770 train_time:62764ms step_avg:95.82ms
step:666/1770 train_time:62863ms step_avg:95.83ms
step:667/1770 train_time:62963ms step_avg:95.83ms
step:668/1770 train_time:63062ms step_avg:95.84ms
step:669/1770 train_time:63162ms step_avg:95.85ms
step:670/1770 train_time:63261ms step_avg:95.85ms
step:671/1770 train_time:63362ms step_avg:95.86ms
step:672/1770 train_time:63461ms step_avg:95.86ms
step:673/1770 train_time:63560ms step_avg:95.87ms
step:674/1770 train_time:63659ms step_avg:95.87ms
step:675/1770 train_time:63758ms step_avg:95.88ms
step:676/1770 train_time:63857ms step_avg:95.88ms
step:677/1770 train_time:63956ms step_avg:95.89ms
step:678/1770 train_time:64055ms step_avg:95.89ms
step:679/1770 train_time:64154ms step_avg:95.90ms
step:680/1770 train_time:64253ms step_avg:95.90ms
step:681/1770 train_time:64353ms step_avg:95.91ms
step:682/1770 train_time:64452ms step_avg:95.91ms
step:683/1770 train_time:64553ms step_avg:95.92ms
step:684/1770 train_time:64652ms step_avg:95.92ms
step:685/1770 train_time:64751ms step_avg:95.93ms
step:686/1770 train_time:64850ms step_avg:95.93ms
step:687/1770 train_time:64950ms step_avg:95.94ms
step:688/1770 train_time:65049ms step_avg:95.94ms
step:689/1770 train_time:65148ms step_avg:95.95ms
step:690/1770 train_time:65248ms step_avg:95.95ms
step:691/1770 train_time:65347ms step_avg:95.96ms
step:692/1770 train_time:65446ms step_avg:95.96ms
step:693/1770 train_time:65547ms step_avg:95.97ms
step:694/1770 train_time:65646ms step_avg:95.97ms
step:695/1770 train_time:65745ms step_avg:95.98ms
step:696/1770 train_time:65844ms step_avg:95.98ms
step:697/1770 train_time:65943ms step_avg:95.99ms
step:698/1770 train_time:66043ms step_avg:95.99ms
step:699/1770 train_time:66142ms step_avg:96.00ms
step:700/1770 train_time:66241ms step_avg:96.00ms
step:701/1770 train_time:66340ms step_avg:96.01ms
step:702/1770 train_time:66439ms step_avg:96.01ms
step:703/1770 train_time:66538ms step_avg:96.01ms
step:704/1770 train_time:66637ms step_avg:96.02ms
step:705/1770 train_time:66737ms step_avg:96.02ms
step:706/1770 train_time:66836ms step_avg:96.03ms
step:707/1770 train_time:66935ms step_avg:96.03ms
step:708/1770 train_time:67034ms step_avg:96.04ms
step:709/1770 train_time:67133ms step_avg:96.04ms
step:710/1770 train_time:67232ms step_avg:96.05ms
step:711/1770 train_time:67331ms step_avg:96.05ms
step:712/1770 train_time:67432ms step_avg:96.06ms
step:713/1770 train_time:67532ms step_avg:96.06ms
step:714/1770 train_time:67633ms step_avg:96.07ms
step:715/1770 train_time:67732ms step_avg:96.07ms
step:716/1770 train_time:67831ms step_avg:96.08ms
step:717/1770 train_time:67931ms step_avg:96.08ms
step:718/1770 train_time:68031ms step_avg:96.09ms
step:719/1770 train_time:68131ms step_avg:96.09ms
step:720/1770 train_time:68231ms step_avg:96.10ms
step:721/1770 train_time:68330ms step_avg:96.10ms
step:722/1770 train_time:68429ms step_avg:96.11ms
step:723/1770 train_time:68528ms step_avg:96.11ms
step:724/1770 train_time:68627ms step_avg:96.12ms
step:725/1770 train_time:68726ms step_avg:96.12ms
step:726/1770 train_time:68825ms step_avg:96.12ms
step:727/1770 train_time:68924ms step_avg:96.13ms
step:728/1770 train_time:69023ms step_avg:96.13ms
step:729/1770 train_time:69123ms step_avg:96.14ms
step:730/1770 train_time:69223ms step_avg:96.14ms
step:731/1770 train_time:69322ms step_avg:96.15ms
step:732/1770 train_time:69421ms step_avg:96.15ms
step:733/1770 train_time:69520ms step_avg:96.16ms
step:734/1770 train_time:69619ms step_avg:96.16ms
step:735/1770 train_time:69719ms step_avg:96.16ms
step:736/1770 train_time:69818ms step_avg:96.17ms
step:737/1770 train_time:69918ms step_avg:96.17ms
step:738/1770 train_time:70017ms step_avg:96.18ms
step:739/1770 train_time:70116ms step_avg:96.18ms
step:740/1770 train_time:70215ms step_avg:96.18ms
step:741/1770 train_time:70315ms step_avg:96.19ms
step:742/1770 train_time:70415ms step_avg:96.19ms
step:743/1770 train_time:70515ms step_avg:96.20ms
step:744/1770 train_time:70615ms step_avg:96.21ms
step:745/1770 train_time:70715ms step_avg:96.21ms
step:746/1770 train_time:70814ms step_avg:96.21ms
step:747/1770 train_time:70913ms step_avg:96.22ms
step:748/1770 train_time:71013ms step_avg:96.22ms
step:749/1770 train_time:71112ms step_avg:96.23ms
step:750/1770 train_time:71211ms step_avg:96.23ms
step:750/1770 val_loss:3.6050 train_time:71308ms step_avg:96.36ms
step:751/1770 train_time:71329ms step_avg:96.26ms
step:752/1770 train_time:71418ms step_avg:96.25ms
step:753/1770 train_time:71518ms step_avg:96.26ms
step:754/1770 train_time:71617ms step_avg:96.26ms
step:755/1770 train_time:71716ms step_avg:96.26ms
step:756/1770 train_time:71815ms step_avg:96.27ms
step:757/1770 train_time:71914ms step_avg:96.27ms
step:758/1770 train_time:72013ms step_avg:96.27ms
step:759/1770 train_time:72111ms step_avg:96.28ms
step:760/1770 train_time:72209ms step_avg:96.28ms
step:761/1770 train_time:72308ms step_avg:96.28ms
step:762/1770 train_time:72408ms step_avg:96.29ms
step:763/1770 train_time:72509ms step_avg:96.29ms
step:764/1770 train_time:72608ms step_avg:96.30ms
step:765/1770 train_time:72707ms step_avg:96.30ms
step:766/1770 train_time:72807ms step_avg:96.31ms
step:767/1770 train_time:72906ms step_avg:96.31ms
step:768/1770 train_time:73005ms step_avg:96.31ms
step:769/1770 train_time:73105ms step_avg:96.32ms
step:770/1770 train_time:73204ms step_avg:96.32ms
step:771/1770 train_time:73303ms step_avg:96.32ms
step:772/1770 train_time:73402ms step_avg:96.33ms
step:773/1770 train_time:73501ms step_avg:96.33ms
step:774/1770 train_time:73600ms step_avg:96.34ms
step:775/1770 train_time:73699ms step_avg:96.34ms
step:776/1770 train_time:73798ms step_avg:96.34ms
step:777/1770 train_time:73898ms step_avg:96.35ms
step:778/1770 train_time:73997ms step_avg:96.35ms
step:779/1770 train_time:74097ms step_avg:96.35ms
step:780/1770 train_time:74197ms step_avg:96.36ms
step:781/1770 train_time:74297ms step_avg:96.36ms
step:782/1770 train_time:74397ms step_avg:96.37ms
step:783/1770 train_time:74496ms step_avg:96.37ms
step:784/1770 train_time:74594ms step_avg:96.37ms
step:785/1770 train_time:74693ms step_avg:96.38ms
step:786/1770 train_time:74792ms step_avg:96.38ms
step:787/1770 train_time:74891ms step_avg:96.38ms
step:788/1770 train_time:74991ms step_avg:96.39ms
step:789/1770 train_time:75090ms step_avg:96.39ms
step:790/1770 train_time:75189ms step_avg:96.40ms
step:791/1770 train_time:75288ms step_avg:96.40ms
step:792/1770 train_time:75388ms step_avg:96.40ms
step:793/1770 train_time:75487ms step_avg:96.41ms
step:794/1770 train_time:75586ms step_avg:96.41ms
step:795/1770 train_time:75685ms step_avg:96.41ms
step:796/1770 train_time:75785ms step_avg:96.42ms
step:797/1770 train_time:75884ms step_avg:96.42ms
step:798/1770 train_time:75984ms step_avg:96.43ms
step:799/1770 train_time:76083ms step_avg:96.43ms
step:800/1770 train_time:76182ms step_avg:96.43ms
step:801/1770 train_time:76282ms step_avg:96.44ms
step:802/1770 train_time:76381ms step_avg:96.44ms
step:803/1770 train_time:76480ms step_avg:96.44ms
step:804/1770 train_time:76580ms step_avg:96.45ms
step:805/1770 train_time:76681ms step_avg:96.45ms
step:806/1770 train_time:76780ms step_avg:96.46ms
step:807/1770 train_time:76879ms step_avg:96.46ms
step:808/1770 train_time:76978ms step_avg:96.46ms
step:809/1770 train_time:77078ms step_avg:96.47ms
step:810/1770 train_time:77178ms step_avg:96.47ms
step:811/1770 train_time:77277ms step_avg:96.48ms
step:812/1770 train_time:77376ms step_avg:96.48ms
step:813/1770 train_time:77476ms step_avg:96.48ms
step:814/1770 train_time:77575ms step_avg:96.49ms
step:815/1770 train_time:77676ms step_avg:96.49ms
step:816/1770 train_time:77777ms step_avg:96.50ms
step:817/1770 train_time:77877ms step_avg:96.50ms
step:818/1770 train_time:77977ms step_avg:96.51ms
step:819/1770 train_time:78077ms step_avg:96.51ms
step:820/1770 train_time:78176ms step_avg:96.51ms
step:821/1770 train_time:78277ms step_avg:96.52ms
step:822/1770 train_time:78377ms step_avg:96.52ms
step:823/1770 train_time:78477ms step_avg:96.53ms
step:824/1770 train_time:78576ms step_avg:96.53ms
step:825/1770 train_time:78677ms step_avg:96.54ms
step:826/1770 train_time:78777ms step_avg:96.54ms
step:827/1770 train_time:78878ms step_avg:96.55ms
step:828/1770 train_time:78977ms step_avg:96.55ms
step:829/1770 train_time:79076ms step_avg:96.55ms
step:830/1770 train_time:79176ms step_avg:96.56ms
step:831/1770 train_time:79276ms step_avg:96.56ms
step:832/1770 train_time:79376ms step_avg:96.56ms
step:833/1770 train_time:79476ms step_avg:96.57ms
step:834/1770 train_time:79576ms step_avg:96.57ms
step:835/1770 train_time:79676ms step_avg:96.58ms
step:836/1770 train_time:79776ms step_avg:96.58ms
step:837/1770 train_time:79877ms step_avg:96.59ms
step:838/1770 train_time:79977ms step_avg:96.59ms
step:839/1770 train_time:80077ms step_avg:96.59ms
step:840/1770 train_time:80177ms step_avg:96.60ms
step:841/1770 train_time:80276ms step_avg:96.60ms
step:842/1770 train_time:80376ms step_avg:96.61ms
step:843/1770 train_time:80476ms step_avg:96.61ms
step:844/1770 train_time:80576ms step_avg:96.61ms
step:845/1770 train_time:80675ms step_avg:96.62ms
step:846/1770 train_time:80775ms step_avg:96.62ms
step:847/1770 train_time:80874ms step_avg:96.62ms
step:848/1770 train_time:80974ms step_avg:96.63ms
step:849/1770 train_time:81074ms step_avg:96.63ms
step:850/1770 train_time:81173ms step_avg:96.63ms
step:851/1770 train_time:81272ms step_avg:96.64ms
step:852/1770 train_time:81371ms step_avg:96.64ms
step:853/1770 train_time:81471ms step_avg:96.64ms
step:854/1770 train_time:81570ms step_avg:96.65ms
step:855/1770 train_time:81670ms step_avg:96.65ms
step:856/1770 train_time:81769ms step_avg:96.65ms
step:857/1770 train_time:81869ms step_avg:96.66ms
step:858/1770 train_time:81969ms step_avg:96.66ms
step:859/1770 train_time:82070ms step_avg:96.67ms
step:860/1770 train_time:82170ms step_avg:96.67ms
step:861/1770 train_time:82269ms step_avg:96.67ms
step:862/1770 train_time:82369ms step_avg:96.68ms
step:863/1770 train_time:82468ms step_avg:96.68ms
step:864/1770 train_time:82567ms step_avg:96.68ms
step:865/1770 train_time:82666ms step_avg:96.69ms
step:866/1770 train_time:82765ms step_avg:96.69ms
step:867/1770 train_time:82865ms step_avg:96.69ms
step:868/1770 train_time:82964ms step_avg:96.70ms
step:869/1770 train_time:83064ms step_avg:96.70ms
step:870/1770 train_time:83163ms step_avg:96.70ms
step:871/1770 train_time:83263ms step_avg:96.70ms
step:872/1770 train_time:83362ms step_avg:96.71ms
step:873/1770 train_time:83461ms step_avg:96.71ms
step:874/1770 train_time:83560ms step_avg:96.71ms
step:875/1770 train_time:83660ms step_avg:96.72ms
step:875/1770 val_loss:3.5552 train_time:83758ms step_avg:96.83ms
step:876/1770 train_time:83779ms step_avg:96.74ms
step:877/1770 train_time:83869ms step_avg:96.73ms
step:878/1770 train_time:83970ms step_avg:96.74ms
step:879/1770 train_time:84069ms step_avg:96.74ms
step:880/1770 train_time:84168ms step_avg:96.74ms
step:881/1770 train_time:84266ms step_avg:96.75ms
step:882/1770 train_time:84365ms step_avg:96.75ms
step:883/1770 train_time:84463ms step_avg:96.75ms
step:884/1770 train_time:84562ms step_avg:96.75ms
step:885/1770 train_time:84661ms step_avg:96.75ms
step:886/1770 train_time:84760ms step_avg:96.76ms
step:887/1770 train_time:84860ms step_avg:96.76ms
step:888/1770 train_time:84960ms step_avg:96.77ms
step:889/1770 train_time:85061ms step_avg:96.77ms
step:890/1770 train_time:85161ms step_avg:96.77ms
step:891/1770 train_time:85261ms step_avg:96.78ms
step:892/1770 train_time:85361ms step_avg:96.78ms
step:893/1770 train_time:85460ms step_avg:96.78ms
step:894/1770 train_time:85561ms step_avg:96.79ms
step:895/1770 train_time:85660ms step_avg:96.79ms
step:896/1770 train_time:85759ms step_avg:96.79ms
step:897/1770 train_time:85859ms step_avg:96.80ms
step:898/1770 train_time:85958ms step_avg:96.80ms
step:899/1770 train_time:86058ms step_avg:96.80ms
step:900/1770 train_time:86158ms step_avg:96.81ms
step:901/1770 train_time:86258ms step_avg:96.81ms
step:902/1770 train_time:86359ms step_avg:96.81ms
step:903/1770 train_time:86459ms step_avg:96.82ms
step:904/1770 train_time:86559ms step_avg:96.82ms
step:905/1770 train_time:86658ms step_avg:96.82ms
step:906/1770 train_time:86757ms step_avg:96.83ms
step:907/1770 train_time:86856ms step_avg:96.83ms
step:908/1770 train_time:86956ms step_avg:96.83ms
step:909/1770 train_time:87055ms step_avg:96.84ms
step:910/1770 train_time:87155ms step_avg:96.84ms
step:911/1770 train_time:87254ms step_avg:96.84ms
step:912/1770 train_time:87353ms step_avg:96.84ms
step:913/1770 train_time:87453ms step_avg:96.85ms
step:914/1770 train_time:87553ms step_avg:96.85ms
step:915/1770 train_time:87652ms step_avg:96.85ms
step:916/1770 train_time:87752ms step_avg:96.86ms
step:917/1770 train_time:87851ms step_avg:96.86ms
step:918/1770 train_time:87950ms step_avg:96.86ms
step:919/1770 train_time:88049ms step_avg:96.86ms
step:920/1770 train_time:88151ms step_avg:96.87ms
step:921/1770 train_time:88252ms step_avg:96.87ms
step:922/1770 train_time:88354ms step_avg:96.88ms
step:923/1770 train_time:88454ms step_avg:96.88ms
step:924/1770 train_time:88555ms step_avg:96.89ms
step:925/1770 train_time:88655ms step_avg:96.89ms
step:926/1770 train_time:88756ms step_avg:96.90ms
step:927/1770 train_time:88857ms step_avg:96.90ms
step:928/1770 train_time:88958ms step_avg:96.90ms
step:929/1770 train_time:89061ms step_avg:96.91ms
step:930/1770 train_time:89162ms step_avg:96.92ms
step:931/1770 train_time:89262ms step_avg:96.92ms
step:932/1770 train_time:89362ms step_avg:96.92ms
step:933/1770 train_time:89462ms step_avg:96.93ms
step:934/1770 train_time:89565ms step_avg:96.93ms
step:935/1770 train_time:89665ms step_avg:96.94ms
step:936/1770 train_time:89767ms step_avg:96.94ms
step:937/1770 train_time:89868ms step_avg:96.95ms
step:938/1770 train_time:89969ms step_avg:96.95ms
step:939/1770 train_time:90070ms step_avg:96.95ms
step:940/1770 train_time:90171ms step_avg:96.96ms
step:941/1770 train_time:90271ms step_avg:96.96ms
step:942/1770 train_time:90373ms step_avg:96.97ms
step:943/1770 train_time:90474ms step_avg:96.97ms
step:944/1770 train_time:90574ms step_avg:96.97ms
step:945/1770 train_time:90675ms step_avg:96.98ms
step:946/1770 train_time:90777ms step_avg:96.98ms
step:947/1770 train_time:90878ms step_avg:96.99ms
step:948/1770 train_time:90980ms step_avg:96.99ms
step:949/1770 train_time:91081ms step_avg:97.00ms
step:950/1770 train_time:91182ms step_avg:97.00ms
step:951/1770 train_time:91284ms step_avg:97.01ms
step:952/1770 train_time:91384ms step_avg:97.01ms
step:953/1770 train_time:91485ms step_avg:97.01ms
step:954/1770 train_time:91586ms step_avg:97.02ms
step:955/1770 train_time:91688ms step_avg:97.02ms
step:956/1770 train_time:91789ms step_avg:97.03ms
step:957/1770 train_time:91891ms step_avg:97.03ms
step:958/1770 train_time:91992ms step_avg:97.04ms
step:959/1770 train_time:92093ms step_avg:97.04ms
step:960/1770 train_time:92193ms step_avg:97.05ms
step:961/1770 train_time:92293ms step_avg:97.05ms
step:962/1770 train_time:92394ms step_avg:97.05ms
step:963/1770 train_time:92494ms step_avg:97.06ms
step:964/1770 train_time:92595ms step_avg:97.06ms
step:965/1770 train_time:92697ms step_avg:97.07ms
step:966/1770 train_time:92799ms step_avg:97.07ms
step:967/1770 train_time:92900ms step_avg:97.07ms
step:968/1770 train_time:93001ms step_avg:97.08ms
step:969/1770 train_time:93102ms step_avg:97.08ms
step:970/1770 train_time:93203ms step_avg:97.09ms
step:971/1770 train_time:93304ms step_avg:97.09ms
step:972/1770 train_time:93404ms step_avg:97.09ms
step:973/1770 train_time:93506ms step_avg:97.10ms
step:974/1770 train_time:93607ms step_avg:97.10ms
step:975/1770 train_time:93708ms step_avg:97.11ms
step:976/1770 train_time:93808ms step_avg:97.11ms
step:977/1770 train_time:93910ms step_avg:97.11ms
step:978/1770 train_time:94011ms step_avg:97.12ms
step:979/1770 train_time:94113ms step_avg:97.12ms
step:980/1770 train_time:94213ms step_avg:97.13ms
step:981/1770 train_time:94313ms step_avg:97.13ms
step:982/1770 train_time:94414ms step_avg:97.13ms
step:983/1770 train_time:94515ms step_avg:97.14ms
step:984/1770 train_time:94616ms step_avg:97.14ms
step:985/1770 train_time:94717ms step_avg:97.15ms
step:986/1770 train_time:94818ms step_avg:97.15ms
step:987/1770 train_time:94920ms step_avg:97.15ms
step:988/1770 train_time:95021ms step_avg:97.16ms
step:989/1770 train_time:95125ms step_avg:97.17ms
step:990/1770 train_time:95226ms step_avg:97.17ms
step:991/1770 train_time:95326ms step_avg:97.17ms
step:992/1770 train_time:95426ms step_avg:97.18ms
step:993/1770 train_time:95526ms step_avg:97.18ms
step:994/1770 train_time:95627ms step_avg:97.18ms
step:995/1770 train_time:95727ms step_avg:97.18ms
step:996/1770 train_time:95828ms step_avg:97.19ms
step:997/1770 train_time:95928ms step_avg:97.19ms
step:998/1770 train_time:96030ms step_avg:97.20ms
step:999/1770 train_time:96131ms step_avg:97.20ms
step:1000/1770 train_time:96233ms step_avg:97.20ms
step:1000/1770 val_loss:3.5160 train_time:96332ms step_avg:97.31ms
step:1001/1770 train_time:96353ms step_avg:97.23ms
step:1002/1770 train_time:96444ms step_avg:97.22ms
step:1003/1770 train_time:96545ms step_avg:97.23ms
step:1004/1770 train_time:96646ms step_avg:97.23ms
step:1005/1770 train_time:96746ms step_avg:97.23ms
step:1006/1770 train_time:96846ms step_avg:97.23ms
step:1007/1770 train_time:96946ms step_avg:97.24ms
step:1008/1770 train_time:97046ms step_avg:97.24ms
step:1009/1770 train_time:97145ms step_avg:97.24ms
step:1010/1770 train_time:97245ms step_avg:97.25ms
step:1011/1770 train_time:97348ms step_avg:97.25ms
step:1012/1770 train_time:97450ms step_avg:97.26ms
step:1013/1770 train_time:97551ms step_avg:97.26ms
step:1014/1770 train_time:97652ms step_avg:97.26ms
step:1015/1770 train_time:97752ms step_avg:97.27ms
step:1016/1770 train_time:97853ms step_avg:97.27ms
step:1017/1770 train_time:97954ms step_avg:97.27ms
step:1018/1770 train_time:98055ms step_avg:97.28ms
step:1019/1770 train_time:98155ms step_avg:97.28ms
step:1020/1770 train_time:98257ms step_avg:97.28ms
step:1021/1770 train_time:98358ms step_avg:97.29ms
step:1022/1770 train_time:98458ms step_avg:97.29ms
step:1023/1770 train_time:98559ms step_avg:97.29ms
step:1024/1770 train_time:98661ms step_avg:97.30ms
step:1025/1770 train_time:98763ms step_avg:97.30ms
step:1026/1770 train_time:98864ms step_avg:97.31ms
step:1027/1770 train_time:98964ms step_avg:97.31ms
step:1028/1770 train_time:99065ms step_avg:97.31ms
step:1029/1770 train_time:99166ms step_avg:97.32ms
step:1030/1770 train_time:99267ms step_avg:97.32ms
step:1031/1770 train_time:99368ms step_avg:97.32ms
step:1032/1770 train_time:99469ms step_avg:97.33ms
step:1033/1770 train_time:99571ms step_avg:97.33ms
step:1034/1770 train_time:99672ms step_avg:97.34ms
step:1035/1770 train_time:99774ms step_avg:97.34ms
step:1036/1770 train_time:99874ms step_avg:97.34ms
step:1037/1770 train_time:99975ms step_avg:97.35ms
step:1038/1770 train_time:100076ms step_avg:97.35ms
step:1039/1770 train_time:100176ms step_avg:97.35ms
step:1040/1770 train_time:100277ms step_avg:97.36ms
step:1041/1770 train_time:100378ms step_avg:97.36ms
step:1042/1770 train_time:100479ms step_avg:97.36ms
step:1043/1770 train_time:100581ms step_avg:97.37ms
step:1044/1770 train_time:100683ms step_avg:97.37ms
step:1045/1770 train_time:100784ms step_avg:97.38ms
step:1046/1770 train_time:100884ms step_avg:97.38ms
step:1047/1770 train_time:100984ms step_avg:97.38ms
step:1048/1770 train_time:101086ms step_avg:97.39ms
step:1049/1770 train_time:101186ms step_avg:97.39ms
step:1050/1770 train_time:101288ms step_avg:97.39ms
step:1051/1770 train_time:101390ms step_avg:97.40ms
step:1052/1770 train_time:101493ms step_avg:97.40ms
step:1053/1770 train_time:101594ms step_avg:97.41ms
step:1054/1770 train_time:101695ms step_avg:97.41ms
step:1055/1770 train_time:101795ms step_avg:97.41ms
step:1056/1770 train_time:101895ms step_avg:97.41ms
step:1057/1770 train_time:101995ms step_avg:97.42ms
step:1058/1770 train_time:102097ms step_avg:97.42ms
step:1059/1770 train_time:102197ms step_avg:97.42ms
step:1060/1770 train_time:102298ms step_avg:97.43ms
step:1061/1770 train_time:102400ms step_avg:97.43ms
step:1062/1770 train_time:102502ms step_avg:97.44ms
step:1063/1770 train_time:102605ms step_avg:97.44ms
step:1064/1770 train_time:102706ms step_avg:97.44ms
step:1065/1770 train_time:102806ms step_avg:97.45ms
step:1066/1770 train_time:102907ms step_avg:97.45ms
step:1067/1770 train_time:103008ms step_avg:97.45ms
step:1068/1770 train_time:103109ms step_avg:97.46ms
step:1069/1770 train_time:103211ms step_avg:97.46ms
step:1070/1770 train_time:103313ms step_avg:97.46ms
step:1071/1770 train_time:103414ms step_avg:97.47ms
step:1072/1770 train_time:103515ms step_avg:97.47ms
step:1073/1770 train_time:103615ms step_avg:97.47ms
step:1074/1770 train_time:103716ms step_avg:97.48ms
step:1075/1770 train_time:103817ms step_avg:97.48ms
step:1076/1770 train_time:103918ms step_avg:97.48ms
step:1077/1770 train_time:104020ms step_avg:97.49ms
step:1078/1770 train_time:104121ms step_avg:97.49ms
step:1079/1770 train_time:104221ms step_avg:97.49ms
step:1080/1770 train_time:104322ms step_avg:97.50ms
step:1081/1770 train_time:104424ms step_avg:97.50ms
step:1082/1770 train_time:104525ms step_avg:97.50ms
step:1083/1770 train_time:104626ms step_avg:97.51ms
step:1084/1770 train_time:104727ms step_avg:97.51ms
step:1085/1770 train_time:104829ms step_avg:97.51ms
step:1086/1770 train_time:104930ms step_avg:97.52ms
step:1087/1770 train_time:105031ms step_avg:97.52ms
step:1088/1770 train_time:105132ms step_avg:97.52ms
step:1089/1770 train_time:105233ms step_avg:97.53ms
step:1090/1770 train_time:105334ms step_avg:97.53ms
step:1091/1770 train_time:105435ms step_avg:97.53ms
step:1092/1770 train_time:105535ms step_avg:97.54ms
step:1093/1770 train_time:105635ms step_avg:97.54ms
step:1094/1770 train_time:105736ms step_avg:97.54ms
step:1095/1770 train_time:105837ms step_avg:97.55ms
step:1096/1770 train_time:105939ms step_avg:97.55ms
step:1097/1770 train_time:106041ms step_avg:97.55ms
step:1098/1770 train_time:106141ms step_avg:97.56ms
step:1099/1770 train_time:106242ms step_avg:97.56ms
step:1100/1770 train_time:106342ms step_avg:97.56ms
step:1101/1770 train_time:106443ms step_avg:97.56ms
step:1102/1770 train_time:106543ms step_avg:97.57ms
step:1103/1770 train_time:106644ms step_avg:97.57ms
step:1104/1770 train_time:106746ms step_avg:97.57ms
step:1105/1770 train_time:106847ms step_avg:97.58ms
step:1106/1770 train_time:106949ms step_avg:97.58ms
step:1107/1770 train_time:107050ms step_avg:97.58ms
step:1108/1770 train_time:107151ms step_avg:97.59ms
step:1109/1770 train_time:107251ms step_avg:97.59ms
step:1110/1770 train_time:107352ms step_avg:97.59ms
step:1111/1770 train_time:107452ms step_avg:97.60ms
step:1112/1770 train_time:107554ms step_avg:97.60ms
step:1113/1770 train_time:107655ms step_avg:97.60ms
step:1114/1770 train_time:107757ms step_avg:97.61ms
step:1115/1770 train_time:107859ms step_avg:97.61ms
step:1116/1770 train_time:107961ms step_avg:97.61ms
step:1117/1770 train_time:108063ms step_avg:97.62ms
step:1118/1770 train_time:108164ms step_avg:97.62ms
step:1119/1770 train_time:108265ms step_avg:97.62ms
step:1120/1770 train_time:108366ms step_avg:97.63ms
step:1121/1770 train_time:108466ms step_avg:97.63ms
step:1122/1770 train_time:108567ms step_avg:97.63ms
step:1123/1770 train_time:108669ms step_avg:97.64ms
step:1124/1770 train_time:108770ms step_avg:97.64ms
step:1125/1770 train_time:108872ms step_avg:97.64ms
step:1125/1770 val_loss:3.4749 train_time:108971ms step_avg:97.73ms
step:1126/1770 train_time:108993ms step_avg:97.66ms
step:1127/1770 train_time:109083ms step_avg:97.66ms
step:1128/1770 train_time:109184ms step_avg:97.66ms
step:1129/1770 train_time:109284ms step_avg:97.66ms
step:1130/1770 train_time:109385ms step_avg:97.67ms
step:1131/1770 train_time:109486ms step_avg:97.67ms
step:1132/1770 train_time:109586ms step_avg:97.67ms
step:1133/1770 train_time:109686ms step_avg:97.67ms
step:1134/1770 train_time:109787ms step_avg:97.68ms
step:1135/1770 train_time:109887ms step_avg:97.68ms
step:1136/1770 train_time:109991ms step_avg:97.68ms
step:1137/1770 train_time:110095ms step_avg:97.69ms
step:1138/1770 train_time:110196ms step_avg:97.69ms
step:1139/1770 train_time:110296ms step_avg:97.69ms
step:1140/1770 train_time:110396ms step_avg:97.70ms
step:1141/1770 train_time:110497ms step_avg:97.70ms
step:1142/1770 train_time:110598ms step_avg:97.70ms
step:1143/1770 train_time:110700ms step_avg:97.71ms
step:1144/1770 train_time:110801ms step_avg:97.71ms
step:1145/1770 train_time:110903ms step_avg:97.71ms
step:1146/1770 train_time:111005ms step_avg:97.72ms
step:1147/1770 train_time:111106ms step_avg:97.72ms
step:1148/1770 train_time:111207ms step_avg:97.72ms
step:1149/1770 train_time:111308ms step_avg:97.72ms
step:1150/1770 train_time:111408ms step_avg:97.73ms
step:1151/1770 train_time:111510ms step_avg:97.73ms
step:1152/1770 train_time:111611ms step_avg:97.73ms
step:1153/1770 train_time:111713ms step_avg:97.74ms
step:1154/1770 train_time:111815ms step_avg:97.74ms
step:1155/1770 train_time:111916ms step_avg:97.74ms
step:1156/1770 train_time:112017ms step_avg:97.75ms
step:1157/1770 train_time:112119ms step_avg:97.75ms
step:1158/1770 train_time:112220ms step_avg:97.75ms
step:1159/1770 train_time:112321ms step_avg:97.76ms
step:1160/1770 train_time:112422ms step_avg:97.76ms
step:1161/1770 train_time:112523ms step_avg:97.76ms
step:1162/1770 train_time:112625ms step_avg:97.76ms
step:1163/1770 train_time:112727ms step_avg:97.77ms
step:1164/1770 train_time:112828ms step_avg:97.77ms
step:1165/1770 train_time:112929ms step_avg:97.77ms
step:1166/1770 train_time:113030ms step_avg:97.78ms
step:1167/1770 train_time:113131ms step_avg:97.78ms
step:1168/1770 train_time:113232ms step_avg:97.78ms
step:1169/1770 train_time:113333ms step_avg:97.78ms
step:1170/1770 train_time:113433ms step_avg:97.79ms
step:1171/1770 train_time:113535ms step_avg:97.79ms
step:1172/1770 train_time:113636ms step_avg:97.79ms
step:1173/1770 train_time:113738ms step_avg:97.80ms
step:1174/1770 train_time:113839ms step_avg:97.80ms
step:1175/1770 train_time:113941ms step_avg:97.80ms
step:1176/1770 train_time:114042ms step_avg:97.81ms
step:1177/1770 train_time:114143ms step_avg:97.81ms
step:1178/1770 train_time:114243ms step_avg:97.81ms
step:1179/1770 train_time:114344ms step_avg:97.81ms
step:1180/1770 train_time:114444ms step_avg:97.82ms
step:1181/1770 train_time:114545ms step_avg:97.82ms
step:1182/1770 train_time:114647ms step_avg:97.82ms
step:1183/1770 train_time:114749ms step_avg:97.83ms
step:1184/1770 train_time:114854ms step_avg:97.83ms
step:1185/1770 train_time:114955ms step_avg:97.83ms
step:1186/1770 train_time:115058ms step_avg:97.84ms
step:1187/1770 train_time:115163ms step_avg:97.84ms
step:1188/1770 train_time:115264ms step_avg:97.85ms
step:1189/1770 train_time:115366ms step_avg:97.85ms
step:1190/1770 train_time:115467ms step_avg:97.85ms
step:1191/1770 train_time:115569ms step_avg:97.86ms
step:1192/1770 train_time:115671ms step_avg:97.86ms
step:1193/1770 train_time:115773ms step_avg:97.86ms
step:1194/1770 train_time:115875ms step_avg:97.87ms
step:1195/1770 train_time:115978ms step_avg:97.87ms
step:1196/1770 train_time:116081ms step_avg:97.88ms
step:1197/1770 train_time:116182ms step_avg:97.88ms
step:1198/1770 train_time:116284ms step_avg:97.88ms
step:1199/1770 train_time:116387ms step_avg:97.89ms
step:1200/1770 train_time:116489ms step_avg:97.89ms
step:1201/1770 train_time:116591ms step_avg:97.89ms
step:1202/1770 train_time:116692ms step_avg:97.90ms
step:1203/1770 train_time:116795ms step_avg:97.90ms
step:1204/1770 train_time:116898ms step_avg:97.90ms
step:1205/1770 train_time:117000ms step_avg:97.91ms
step:1206/1770 train_time:117101ms step_avg:97.91ms
step:1207/1770 train_time:117203ms step_avg:97.91ms
step:1208/1770 train_time:117305ms step_avg:97.92ms
step:1209/1770 train_time:117407ms step_avg:97.92ms
step:1210/1770 train_time:117509ms step_avg:97.92ms
step:1211/1770 train_time:117611ms step_avg:97.93ms
step:1212/1770 train_time:117715ms step_avg:97.93ms
step:1213/1770 train_time:117817ms step_avg:97.94ms
step:1214/1770 train_time:117918ms step_avg:97.94ms
step:1215/1770 train_time:118020ms step_avg:97.94ms
step:1216/1770 train_time:118124ms step_avg:97.95ms
step:1217/1770 train_time:118226ms step_avg:97.95ms
step:1218/1770 train_time:118328ms step_avg:97.95ms
step:1219/1770 train_time:118430ms step_avg:97.96ms
step:1220/1770 train_time:118532ms step_avg:97.96ms
step:1221/1770 train_time:118634ms step_avg:97.96ms
step:1222/1770 train_time:118737ms step_avg:97.97ms
step:1223/1770 train_time:118839ms step_avg:97.97ms
step:1224/1770 train_time:118943ms step_avg:97.98ms
step:1225/1770 train_time:119045ms step_avg:97.98ms
step:1226/1770 train_time:119147ms step_avg:97.98ms
step:1227/1770 train_time:119250ms step_avg:97.99ms
step:1228/1770 train_time:119354ms step_avg:97.99ms
step:1229/1770 train_time:119457ms step_avg:98.00ms
step:1230/1770 train_time:119559ms step_avg:98.00ms
step:1231/1770 train_time:119661ms step_avg:98.00ms
step:1232/1770 train_time:119763ms step_avg:98.01ms
step:1233/1770 train_time:119865ms step_avg:98.01ms
step:1234/1770 train_time:119967ms step_avg:98.01ms
step:1235/1770 train_time:120068ms step_avg:98.01ms
step:1236/1770 train_time:120171ms step_avg:98.02ms
step:1237/1770 train_time:120273ms step_avg:98.02ms
step:1238/1770 train_time:120376ms step_avg:98.03ms
step:1239/1770 train_time:120477ms step_avg:98.03ms
step:1240/1770 train_time:120580ms step_avg:98.03ms
step:1241/1770 train_time:120683ms step_avg:98.04ms
step:1242/1770 train_time:120785ms step_avg:98.04ms
step:1243/1770 train_time:120888ms step_avg:98.04ms
step:1244/1770 train_time:120989ms step_avg:98.05ms
step:1245/1770 train_time:121091ms step_avg:98.05ms
step:1246/1770 train_time:121194ms step_avg:98.05ms
step:1247/1770 train_time:121295ms step_avg:98.06ms
step:1248/1770 train_time:121397ms step_avg:98.06ms
step:1249/1770 train_time:121499ms step_avg:98.06ms
step:1250/1770 train_time:121602ms step_avg:98.07ms
step:1250/1770 val_loss:3.4273 train_time:121703ms step_avg:98.15ms
step:1251/1770 train_time:121725ms step_avg:98.09ms
step:1252/1770 train_time:121817ms step_avg:98.08ms
step:1253/1770 train_time:121919ms step_avg:98.08ms
step:1254/1770 train_time:122021ms step_avg:98.09ms
step:1255/1770 train_time:122126ms step_avg:98.09ms
step:1256/1770 train_time:122227ms step_avg:98.10ms
step:1257/1770 train_time:122328ms step_avg:98.10ms
step:1258/1770 train_time:122430ms step_avg:98.10ms
step:1259/1770 train_time:122532ms step_avg:98.10ms
step:1260/1770 train_time:122633ms step_avg:98.11ms
step:1261/1770 train_time:122737ms step_avg:98.11ms
step:1262/1770 train_time:122839ms step_avg:98.11ms
step:1263/1770 train_time:122941ms step_avg:98.12ms
step:1264/1770 train_time:123044ms step_avg:98.12ms
step:1265/1770 train_time:123146ms step_avg:98.12ms
step:1266/1770 train_time:123248ms step_avg:98.13ms
step:1267/1770 train_time:123350ms step_avg:98.13ms
step:1268/1770 train_time:123453ms step_avg:98.13ms
step:1269/1770 train_time:123554ms step_avg:98.14ms
step:1270/1770 train_time:123656ms step_avg:98.14ms
step:1271/1770 train_time:123760ms step_avg:98.14ms
step:1272/1770 train_time:123861ms step_avg:98.15ms
step:1273/1770 train_time:123964ms step_avg:98.15ms
step:1274/1770 train_time:124067ms step_avg:98.15ms
step:1275/1770 train_time:124168ms step_avg:98.16ms
step:1276/1770 train_time:124271ms step_avg:98.16ms
step:1277/1770 train_time:124372ms step_avg:98.16ms
step:1278/1770 train_time:124475ms step_avg:98.17ms
step:1279/1770 train_time:124577ms step_avg:98.17ms
step:1280/1770 train_time:124680ms step_avg:98.17ms
step:1281/1770 train_time:124782ms step_avg:98.18ms
step:1282/1770 train_time:124885ms step_avg:98.18ms
step:1283/1770 train_time:124987ms step_avg:98.18ms
step:1284/1770 train_time:125089ms step_avg:98.19ms
step:1285/1770 train_time:125191ms step_avg:98.19ms
step:1286/1770 train_time:125294ms step_avg:98.19ms
step:1287/1770 train_time:125398ms step_avg:98.20ms
step:1288/1770 train_time:125500ms step_avg:98.20ms
step:1289/1770 train_time:125603ms step_avg:98.20ms
step:1290/1770 train_time:125704ms step_avg:98.21ms
step:1291/1770 train_time:125807ms step_avg:98.21ms
step:1292/1770 train_time:125908ms step_avg:98.21ms
step:1293/1770 train_time:126011ms step_avg:98.22ms
step:1294/1770 train_time:126112ms step_avg:98.22ms
step:1295/1770 train_time:126215ms step_avg:98.22ms
step:1296/1770 train_time:126317ms step_avg:98.22ms
step:1297/1770 train_time:126418ms step_avg:98.23ms
step:1298/1770 train_time:126520ms step_avg:98.23ms
step:1299/1770 train_time:126621ms step_avg:98.23ms
step:1300/1770 train_time:126723ms step_avg:98.24ms
step:1301/1770 train_time:126826ms step_avg:98.24ms
step:1302/1770 train_time:126928ms step_avg:98.24ms
step:1303/1770 train_time:127030ms step_avg:98.24ms
step:1304/1770 train_time:127132ms step_avg:98.25ms
step:1305/1770 train_time:127235ms step_avg:98.25ms
step:1306/1770 train_time:127337ms step_avg:98.25ms
step:1307/1770 train_time:127439ms step_avg:98.26ms
step:1308/1770 train_time:127542ms step_avg:98.26ms
step:1309/1770 train_time:127644ms step_avg:98.26ms
step:1310/1770 train_time:127746ms step_avg:98.27ms
step:1311/1770 train_time:127848ms step_avg:98.27ms
step:1312/1770 train_time:127950ms step_avg:98.27ms
step:1313/1770 train_time:128051ms step_avg:98.27ms
step:1314/1770 train_time:128153ms step_avg:98.28ms
step:1315/1770 train_time:128256ms step_avg:98.28ms
step:1316/1770 train_time:128358ms step_avg:98.28ms
step:1317/1770 train_time:128460ms step_avg:98.29ms
step:1318/1770 train_time:128566ms step_avg:98.29ms
step:1319/1770 train_time:128668ms step_avg:98.30ms
step:1320/1770 train_time:128770ms step_avg:98.30ms
step:1321/1770 train_time:128872ms step_avg:98.30ms
step:1322/1770 train_time:128974ms step_avg:98.30ms
step:1323/1770 train_time:129076ms step_avg:98.31ms
step:1324/1770 train_time:129179ms step_avg:98.31ms
step:1325/1770 train_time:129283ms step_avg:98.31ms
step:1326/1770 train_time:129385ms step_avg:98.32ms
step:1327/1770 train_time:129490ms step_avg:98.32ms
step:1328/1770 train_time:129591ms step_avg:98.32ms
step:1329/1770 train_time:129694ms step_avg:98.33ms
step:1330/1770 train_time:129795ms step_avg:98.33ms
step:1331/1770 train_time:129897ms step_avg:98.33ms
step:1332/1770 train_time:129998ms step_avg:98.33ms
step:1333/1770 train_time:130101ms step_avg:98.34ms
step:1334/1770 train_time:130203ms step_avg:98.34ms
step:1335/1770 train_time:130305ms step_avg:98.34ms
step:1336/1770 train_time:130407ms step_avg:98.35ms
step:1337/1770 train_time:130510ms step_avg:98.35ms
step:1338/1770 train_time:130611ms step_avg:98.35ms
step:1339/1770 train_time:130714ms step_avg:98.36ms
step:1340/1770 train_time:130817ms step_avg:98.36ms
step:1341/1770 train_time:130919ms step_avg:98.36ms
step:1342/1770 train_time:131022ms step_avg:98.36ms
step:1343/1770 train_time:131124ms step_avg:98.37ms
step:1344/1770 train_time:131227ms step_avg:98.37ms
step:1345/1770 train_time:131328ms step_avg:98.37ms
step:1346/1770 train_time:131430ms step_avg:98.38ms
step:1347/1770 train_time:131533ms step_avg:98.38ms
step:1348/1770 train_time:131638ms step_avg:98.38ms
step:1349/1770 train_time:131740ms step_avg:98.39ms
step:1350/1770 train_time:131843ms step_avg:98.39ms
step:1351/1770 train_time:131944ms step_avg:98.39ms
step:1352/1770 train_time:132047ms step_avg:98.40ms
step:1353/1770 train_time:132150ms step_avg:98.40ms
step:1354/1770 train_time:132251ms step_avg:98.40ms
step:1355/1770 train_time:132353ms step_avg:98.40ms
step:1356/1770 train_time:132455ms step_avg:98.41ms
step:1357/1770 train_time:132558ms step_avg:98.41ms
step:1358/1770 train_time:132660ms step_avg:98.41ms
step:1359/1770 train_time:132763ms step_avg:98.42ms
step:1360/1770 train_time:132865ms step_avg:98.42ms
step:1361/1770 train_time:132968ms step_avg:98.42ms
step:1362/1770 train_time:133070ms step_avg:98.42ms
step:1363/1770 train_time:133174ms step_avg:98.43ms
step:1364/1770 train_time:133276ms step_avg:98.43ms
step:1365/1770 train_time:133377ms step_avg:98.43ms
step:1366/1770 train_time:133479ms step_avg:98.44ms
step:1367/1770 train_time:133582ms step_avg:98.44ms
step:1368/1770 train_time:133684ms step_avg:98.44ms
step:1369/1770 train_time:133787ms step_avg:98.45ms
step:1370/1770 train_time:133890ms step_avg:98.45ms
step:1371/1770 train_time:133992ms step_avg:98.45ms
step:1372/1770 train_time:134094ms step_avg:98.45ms
step:1373/1770 train_time:134196ms step_avg:98.46ms
step:1374/1770 train_time:134298ms step_avg:98.46ms
step:1375/1770 train_time:134401ms step_avg:98.46ms
step:1375/1770 val_loss:3.3832 train_time:134502ms step_avg:98.54ms
step:1376/1770 train_time:134523ms step_avg:98.48ms
step:1377/1770 train_time:134617ms step_avg:98.48ms
step:1378/1770 train_time:134719ms step_avg:98.48ms
step:1379/1770 train_time:134821ms step_avg:98.48ms
step:1380/1770 train_time:134922ms step_avg:98.48ms
step:1381/1770 train_time:135024ms step_avg:98.49ms
step:1382/1770 train_time:135125ms step_avg:98.49ms
step:1383/1770 train_time:135228ms step_avg:98.49ms
step:1384/1770 train_time:135330ms step_avg:98.49ms
step:1385/1770 train_time:135432ms step_avg:98.50ms
step:1386/1770 train_time:135535ms step_avg:98.50ms
step:1387/1770 train_time:135638ms step_avg:98.50ms
step:1388/1770 train_time:135740ms step_avg:98.50ms
step:1389/1770 train_time:135842ms step_avg:98.51ms
step:1390/1770 train_time:135943ms step_avg:98.51ms
step:1391/1770 train_time:136045ms step_avg:98.51ms
step:1392/1770 train_time:136147ms step_avg:98.51ms
step:1393/1770 train_time:136248ms step_avg:98.52ms
step:1394/1770 train_time:136349ms step_avg:98.52ms
step:1395/1770 train_time:136452ms step_avg:98.52ms
step:1396/1770 train_time:136555ms step_avg:98.52ms
step:1397/1770 train_time:136658ms step_avg:98.53ms
step:1398/1770 train_time:136760ms step_avg:98.53ms
step:1399/1770 train_time:136863ms step_avg:98.53ms
step:1400/1770 train_time:136966ms step_avg:98.54ms
step:1401/1770 train_time:137068ms step_avg:98.54ms
step:1402/1770 train_time:137170ms step_avg:98.54ms
step:1403/1770 train_time:137273ms step_avg:98.54ms
step:1404/1770 train_time:137375ms step_avg:98.55ms
step:1405/1770 train_time:137477ms step_avg:98.55ms
step:1406/1770 train_time:137580ms step_avg:98.55ms
step:1407/1770 train_time:137682ms step_avg:98.56ms
step:1408/1770 train_time:137783ms step_avg:98.56ms
step:1409/1770 train_time:137887ms step_avg:98.56ms
step:1410/1770 train_time:137989ms step_avg:98.56ms
step:1411/1770 train_time:138090ms step_avg:98.57ms
step:1412/1770 train_time:138191ms step_avg:98.57ms
step:1413/1770 train_time:138293ms step_avg:98.57ms
step:1414/1770 train_time:138396ms step_avg:98.57ms
step:1415/1770 train_time:138499ms step_avg:98.58ms
step:1416/1770 train_time:138601ms step_avg:98.58ms
step:1417/1770 train_time:138703ms step_avg:98.58ms
step:1418/1770 train_time:138805ms step_avg:98.58ms
step:1419/1770 train_time:138909ms step_avg:98.59ms
step:1420/1770 train_time:139011ms step_avg:98.59ms
step:1421/1770 train_time:139112ms step_avg:98.59ms
step:1422/1770 train_time:139214ms step_avg:98.59ms
step:1423/1770 train_time:139316ms step_avg:98.60ms
step:1424/1770 train_time:139418ms step_avg:98.60ms
step:1425/1770 train_time:139520ms step_avg:98.60ms
step:1426/1770 train_time:139624ms step_avg:98.60ms
step:1427/1770 train_time:139727ms step_avg:98.61ms
step:1428/1770 train_time:139831ms step_avg:98.61ms
step:1429/1770 train_time:139932ms step_avg:98.61ms
step:1430/1770 train_time:140034ms step_avg:98.62ms
step:1431/1770 train_time:140137ms step_avg:98.62ms
step:1432/1770 train_time:140238ms step_avg:98.62ms
step:1433/1770 train_time:140340ms step_avg:98.62ms
step:1434/1770 train_time:140441ms step_avg:98.62ms
step:1435/1770 train_time:140543ms step_avg:98.63ms
step:1436/1770 train_time:140647ms step_avg:98.63ms
step:1437/1770 train_time:140750ms step_avg:98.63ms
step:1438/1770 train_time:140852ms step_avg:98.64ms
step:1439/1770 train_time:140954ms step_avg:98.64ms
step:1440/1770 train_time:141056ms step_avg:98.64ms
step:1441/1770 train_time:141161ms step_avg:98.65ms
step:1442/1770 train_time:141263ms step_avg:98.65ms
step:1443/1770 train_time:141364ms step_avg:98.65ms
step:1444/1770 train_time:141468ms step_avg:98.65ms
step:1445/1770 train_time:141571ms step_avg:98.66ms
step:1446/1770 train_time:141673ms step_avg:98.66ms
step:1447/1770 train_time:141777ms step_avg:98.66ms
step:1448/1770 train_time:141880ms step_avg:98.66ms
step:1449/1770 train_time:141984ms step_avg:98.67ms
step:1450/1770 train_time:142086ms step_avg:98.67ms
step:1451/1770 train_time:142190ms step_avg:98.67ms
step:1452/1770 train_time:142293ms step_avg:98.68ms
step:1453/1770 train_time:142396ms step_avg:98.68ms
step:1454/1770 train_time:142500ms step_avg:98.68ms
step:1455/1770 train_time:142604ms step_avg:98.69ms
step:1456/1770 train_time:142709ms step_avg:98.69ms
step:1457/1770 train_time:142812ms step_avg:98.70ms
step:1458/1770 train_time:142916ms step_avg:98.70ms
step:1459/1770 train_time:143021ms step_avg:98.70ms
step:1460/1770 train_time:143123ms step_avg:98.71ms
step:1461/1770 train_time:143226ms step_avg:98.71ms
step:1462/1770 train_time:143329ms step_avg:98.71ms
step:1463/1770 train_time:143433ms step_avg:98.72ms
step:1464/1770 train_time:143537ms step_avg:98.72ms
step:1465/1770 train_time:143639ms step_avg:98.72ms
step:1466/1770 train_time:143743ms step_avg:98.72ms
step:1467/1770 train_time:143847ms step_avg:98.73ms
step:1468/1770 train_time:143951ms step_avg:98.73ms
step:1469/1770 train_time:144055ms step_avg:98.74ms
step:1470/1770 train_time:144158ms step_avg:98.74ms
step:1471/1770 train_time:144261ms step_avg:98.74ms
step:1472/1770 train_time:144364ms step_avg:98.74ms
step:1473/1770 train_time:144468ms step_avg:98.75ms
step:1474/1770 train_time:144573ms step_avg:98.75ms
step:1475/1770 train_time:144675ms step_avg:98.75ms
step:1476/1770 train_time:144779ms step_avg:98.76ms
step:1477/1770 train_time:144884ms step_avg:98.76ms
step:1478/1770 train_time:144987ms step_avg:98.77ms
step:1479/1770 train_time:145091ms step_avg:98.77ms
step:1480/1770 train_time:145194ms step_avg:98.77ms
step:1481/1770 train_time:145301ms step_avg:98.78ms
step:1482/1770 train_time:145403ms step_avg:98.78ms
step:1483/1770 train_time:145506ms step_avg:98.78ms
step:1484/1770 train_time:145610ms step_avg:98.79ms
step:1485/1770 train_time:145713ms step_avg:98.79ms
step:1486/1770 train_time:145816ms step_avg:98.79ms
step:1487/1770 train_time:145921ms step_avg:98.80ms
step:1488/1770 train_time:146025ms step_avg:98.80ms
step:1489/1770 train_time:146129ms step_avg:98.80ms
step:1490/1770 train_time:146232ms step_avg:98.81ms
step:1491/1770 train_time:146335ms step_avg:98.81ms
step:1492/1770 train_time:146438ms step_avg:98.81ms
step:1493/1770 train_time:146544ms step_avg:98.82ms
step:1494/1770 train_time:146651ms step_avg:98.82ms
step:1495/1770 train_time:146753ms step_avg:98.82ms
step:1496/1770 train_time:146856ms step_avg:98.83ms
step:1497/1770 train_time:146960ms step_avg:98.83ms
step:1498/1770 train_time:147063ms step_avg:98.83ms
step:1499/1770 train_time:147165ms step_avg:98.83ms
step:1500/1770 train_time:147267ms step_avg:98.84ms
step:1500/1770 val_loss:3.3450 train_time:147368ms step_avg:98.91ms
step:1501/1770 train_time:147391ms step_avg:98.85ms
step:1502/1770 train_time:147482ms step_avg:98.85ms
step:1503/1770 train_time:147585ms step_avg:98.85ms
step:1504/1770 train_time:147688ms step_avg:98.85ms
step:1505/1770 train_time:147793ms step_avg:98.86ms
step:1506/1770 train_time:147896ms step_avg:98.86ms
step:1507/1770 train_time:148000ms step_avg:98.86ms
step:1508/1770 train_time:148105ms step_avg:98.87ms
step:1509/1770 train_time:148208ms step_avg:98.87ms
step:1510/1770 train_time:148310ms step_avg:98.87ms
step:1511/1770 train_time:148416ms step_avg:98.88ms
step:1512/1770 train_time:148521ms step_avg:98.88ms
step:1513/1770 train_time:148625ms step_avg:98.89ms
step:1514/1770 train_time:148728ms step_avg:98.89ms
step:1515/1770 train_time:148831ms step_avg:98.89ms
step:1516/1770 train_time:148934ms step_avg:98.89ms
step:1517/1770 train_time:149037ms step_avg:98.90ms
step:1518/1770 train_time:149144ms step_avg:98.90ms
step:1519/1770 train_time:149246ms step_avg:98.90ms
step:1520/1770 train_time:149351ms step_avg:98.91ms
step:1521/1770 train_time:149454ms step_avg:98.91ms
step:1522/1770 train_time:149559ms step_avg:98.91ms
step:1523/1770 train_time:149662ms step_avg:98.92ms
step:1524/1770 train_time:149765ms step_avg:98.92ms
step:1525/1770 train_time:149869ms step_avg:98.92ms
step:1526/1770 train_time:149971ms step_avg:98.93ms
step:1527/1770 train_time:150075ms step_avg:98.93ms
step:1528/1770 train_time:150181ms step_avg:98.93ms
step:1529/1770 train_time:150284ms step_avg:98.94ms
step:1530/1770 train_time:150387ms step_avg:98.94ms
step:1531/1770 train_time:150491ms step_avg:98.94ms
step:1532/1770 train_time:150594ms step_avg:98.95ms
step:1533/1770 train_time:150697ms step_avg:98.95ms
step:1534/1770 train_time:150802ms step_avg:98.95ms
step:1535/1770 train_time:150904ms step_avg:98.95ms
step:1536/1770 train_time:151007ms step_avg:98.96ms
step:1537/1770 train_time:151111ms step_avg:98.96ms
step:1538/1770 train_time:151215ms step_avg:98.96ms
step:1539/1770 train_time:151318ms step_avg:98.97ms
step:1540/1770 train_time:151424ms step_avg:98.97ms
step:1541/1770 train_time:151529ms step_avg:98.97ms
step:1542/1770 train_time:151633ms step_avg:98.98ms
step:1543/1770 train_time:151735ms step_avg:98.98ms
step:1544/1770 train_time:151840ms step_avg:98.98ms
step:1545/1770 train_time:151943ms step_avg:98.99ms
step:1546/1770 train_time:152046ms step_avg:98.99ms
step:1547/1770 train_time:152150ms step_avg:98.99ms
step:1548/1770 train_time:152253ms step_avg:98.99ms
step:1549/1770 train_time:152356ms step_avg:99.00ms
step:1550/1770 train_time:152460ms step_avg:99.00ms
step:1551/1770 train_time:152563ms step_avg:99.00ms
step:1552/1770 train_time:152667ms step_avg:99.01ms
step:1553/1770 train_time:152770ms step_avg:99.01ms
step:1554/1770 train_time:152873ms step_avg:99.01ms
step:1555/1770 train_time:152978ms step_avg:99.01ms
step:1556/1770 train_time:153080ms step_avg:99.02ms
step:1557/1770 train_time:153184ms step_avg:99.02ms
step:1558/1770 train_time:153287ms step_avg:99.02ms
step:1559/1770 train_time:153390ms step_avg:99.03ms
step:1560/1770 train_time:153493ms step_avg:99.03ms
step:1561/1770 train_time:153598ms step_avg:99.03ms
step:1562/1770 train_time:153701ms step_avg:99.03ms
step:1563/1770 train_time:153805ms step_avg:99.04ms
step:1564/1770 train_time:153907ms step_avg:99.04ms
step:1565/1770 train_time:154010ms step_avg:99.04ms
step:1566/1770 train_time:154113ms step_avg:99.04ms
step:1567/1770 train_time:154218ms step_avg:99.05ms
step:1568/1770 train_time:154321ms step_avg:99.05ms
step:1569/1770 train_time:154428ms step_avg:99.06ms
step:1570/1770 train_time:154531ms step_avg:99.06ms
step:1571/1770 train_time:154634ms step_avg:99.06ms
step:1572/1770 train_time:154739ms step_avg:99.06ms
step:1573/1770 train_time:154845ms step_avg:99.07ms
step:1574/1770 train_time:154948ms step_avg:99.07ms
step:1575/1770 train_time:155050ms step_avg:99.07ms
step:1576/1770 train_time:155154ms step_avg:99.08ms
step:1577/1770 train_time:155258ms step_avg:99.08ms
step:1578/1770 train_time:155363ms step_avg:99.08ms
step:1579/1770 train_time:155467ms step_avg:99.09ms
step:1580/1770 train_time:155570ms step_avg:99.09ms
step:1581/1770 train_time:155677ms step_avg:99.09ms
step:1582/1770 train_time:155781ms step_avg:99.10ms
step:1583/1770 train_time:155884ms step_avg:99.10ms
step:1584/1770 train_time:155989ms step_avg:99.10ms
step:1585/1770 train_time:156092ms step_avg:99.11ms
step:1586/1770 train_time:156199ms step_avg:99.11ms
step:1587/1770 train_time:156303ms step_avg:99.11ms
step:1588/1770 train_time:156407ms step_avg:99.12ms
step:1589/1770 train_time:156511ms step_avg:99.12ms
step:1590/1770 train_time:156614ms step_avg:99.12ms
step:1591/1770 train_time:156718ms step_avg:99.13ms
step:1592/1770 train_time:156822ms step_avg:99.13ms
step:1593/1770 train_time:156925ms step_avg:99.13ms
step:1594/1770 train_time:157028ms step_avg:99.13ms
step:1595/1770 train_time:157131ms step_avg:99.14ms
step:1596/1770 train_time:157236ms step_avg:99.14ms
step:1597/1770 train_time:157339ms step_avg:99.14ms
step:1598/1770 train_time:157442ms step_avg:99.14ms
step:1599/1770 train_time:157546ms step_avg:99.15ms
step:1600/1770 train_time:157652ms step_avg:99.15ms
step:1601/1770 train_time:157756ms step_avg:99.16ms
step:1602/1770 train_time:157862ms step_avg:99.16ms
step:1603/1770 train_time:157965ms step_avg:99.16ms
step:1604/1770 train_time:158067ms step_avg:99.16ms
step:1605/1770 train_time:158169ms step_avg:99.17ms
step:1606/1770 train_time:158272ms step_avg:99.17ms
step:1607/1770 train_time:158378ms step_avg:99.17ms
step:1608/1770 train_time:158482ms step_avg:99.18ms
step:1609/1770 train_time:158586ms step_avg:99.18ms
step:1610/1770 train_time:158690ms step_avg:99.18ms
step:1611/1770 train_time:158795ms step_avg:99.19ms
step:1612/1770 train_time:158900ms step_avg:99.19ms
step:1613/1770 train_time:159003ms step_avg:99.19ms
step:1614/1770 train_time:159106ms step_avg:99.19ms
step:1615/1770 train_time:159209ms step_avg:99.20ms
step:1616/1770 train_time:159312ms step_avg:99.20ms
step:1617/1770 train_time:159418ms step_avg:99.20ms
step:1618/1770 train_time:159522ms step_avg:99.21ms
step:1619/1770 train_time:159626ms step_avg:99.21ms
step:1620/1770 train_time:159729ms step_avg:99.21ms
step:1621/1770 train_time:159832ms step_avg:99.21ms
step:1622/1770 train_time:159937ms step_avg:99.22ms
step:1623/1770 train_time:160043ms step_avg:99.22ms
step:1624/1770 train_time:160146ms step_avg:99.22ms
step:1625/1770 train_time:160249ms step_avg:99.23ms
step:1625/1770 val_loss:3.3105 train_time:160351ms step_avg:99.29ms
step:1626/1770 train_time:160372ms step_avg:99.24ms
step:1627/1770 train_time:160463ms step_avg:99.24ms
step:1628/1770 train_time:160566ms step_avg:99.24ms
step:1629/1770 train_time:160669ms step_avg:99.24ms
step:1630/1770 train_time:160772ms step_avg:99.24ms
step:1631/1770 train_time:160875ms step_avg:99.24ms
step:1632/1770 train_time:160977ms step_avg:99.25ms
step:1633/1770 train_time:161081ms step_avg:99.25ms
step:1634/1770 train_time:161183ms step_avg:99.25ms
step:1635/1770 train_time:161286ms step_avg:99.25ms
step:1636/1770 train_time:161392ms step_avg:99.26ms
step:1637/1770 train_time:161496ms step_avg:99.26ms
step:1638/1770 train_time:161599ms step_avg:99.26ms
step:1639/1770 train_time:161702ms step_avg:99.26ms
step:1640/1770 train_time:161807ms step_avg:99.27ms
step:1641/1770 train_time:161910ms step_avg:99.27ms
step:1642/1770 train_time:162014ms step_avg:99.27ms
step:1643/1770 train_time:162118ms step_avg:99.28ms
step:1644/1770 train_time:162223ms step_avg:99.28ms
step:1645/1770 train_time:162326ms step_avg:99.28ms
step:1646/1770 train_time:162431ms step_avg:99.29ms
step:1647/1770 train_time:162535ms step_avg:99.29ms
step:1648/1770 train_time:162638ms step_avg:99.29ms
step:1649/1770 train_time:162741ms step_avg:99.29ms
step:1650/1770 train_time:162844ms step_avg:99.30ms
step:1651/1770 train_time:162947ms step_avg:99.30ms
step:1652/1770 train_time:163051ms step_avg:99.30ms
step:1653/1770 train_time:163155ms step_avg:99.30ms
step:1654/1770 train_time:163262ms step_avg:99.31ms
step:1655/1770 train_time:163368ms step_avg:99.31ms
step:1656/1770 train_time:163471ms step_avg:99.31ms
step:1657/1770 train_time:163577ms step_avg:99.32ms
step:1658/1770 train_time:163680ms step_avg:99.32ms
step:1659/1770 train_time:163785ms step_avg:99.32ms
step:1660/1770 train_time:163888ms step_avg:99.33ms
step:1661/1770 train_time:163993ms step_avg:99.33ms
step:1662/1770 train_time:164096ms step_avg:99.33ms
step:1663/1770 train_time:164198ms step_avg:99.33ms
step:1664/1770 train_time:164302ms step_avg:99.34ms
step:1665/1770 train_time:164407ms step_avg:99.34ms
step:1666/1770 train_time:164510ms step_avg:99.34ms
step:1667/1770 train_time:164614ms step_avg:99.34ms
step:1668/1770 train_time:164718ms step_avg:99.35ms
step:1669/1770 train_time:164820ms step_avg:99.35ms
step:1670/1770 train_time:164923ms step_avg:99.35ms
step:1671/1770 train_time:165026ms step_avg:99.35ms
step:1672/1770 train_time:165130ms step_avg:99.36ms
step:1673/1770 train_time:165235ms step_avg:99.36ms
step:1674/1770 train_time:165340ms step_avg:99.36ms
step:1675/1770 train_time:165442ms step_avg:99.36ms
step:1676/1770 train_time:165547ms step_avg:99.37ms
step:1677/1770 train_time:165654ms step_avg:99.37ms
step:1678/1770 train_time:165756ms step_avg:99.37ms
step:1679/1770 train_time:165860ms step_avg:99.38ms
step:1680/1770 train_time:165963ms step_avg:99.38ms
step:1681/1770 train_time:166067ms step_avg:99.38ms
step:1682/1770 train_time:166172ms step_avg:99.39ms
step:1683/1770 train_time:166276ms step_avg:99.39ms
step:1684/1770 train_time:166378ms step_avg:99.39ms
step:1685/1770 train_time:166482ms step_avg:99.39ms
step:1686/1770 train_time:166586ms step_avg:99.39ms
step:1687/1770 train_time:166691ms step_avg:99.40ms
step:1688/1770 train_time:166794ms step_avg:99.40ms
step:1689/1770 train_time:166897ms step_avg:99.40ms
step:1690/1770 train_time:167000ms step_avg:99.40ms
step:1691/1770 train_time:167104ms step_avg:99.41ms
step:1692/1770 train_time:167208ms step_avg:99.41ms
step:1693/1770 train_time:167312ms step_avg:99.41ms
step:1694/1770 train_time:167415ms step_avg:99.42ms
step:1695/1770 train_time:167518ms step_avg:99.42ms
step:1696/1770 train_time:167624ms step_avg:99.42ms
step:1697/1770 train_time:167729ms step_avg:99.42ms
step:1698/1770 train_time:167834ms step_avg:99.43ms
step:1699/1770 train_time:167937ms step_avg:99.43ms
step:1700/1770 train_time:168040ms step_avg:99.43ms
step:1701/1770 train_time:168142ms step_avg:99.43ms
step:1702/1770 train_time:168246ms step_avg:99.44ms
step:1703/1770 train_time:168348ms step_avg:99.44ms
step:1704/1770 train_time:168453ms step_avg:99.44ms
step:1705/1770 train_time:168556ms step_avg:99.44ms
step:1706/1770 train_time:168658ms step_avg:99.44ms
step:1707/1770 train_time:168762ms step_avg:99.45ms
step:1708/1770 train_time:168866ms step_avg:99.45ms
step:1709/1770 train_time:168971ms step_avg:99.45ms
step:1710/1770 train_time:169079ms step_avg:99.46ms
step:1711/1770 train_time:169185ms step_avg:99.46ms
step:1712/1770 train_time:169290ms step_avg:99.47ms
step:1713/1770 train_time:169393ms step_avg:99.47ms
step:1714/1770 train_time:169497ms step_avg:99.47ms
step:1715/1770 train_time:169600ms step_avg:99.47ms
step:1716/1770 train_time:169705ms step_avg:99.48ms
step:1717/1770 train_time:169809ms step_avg:99.48ms
step:1718/1770 train_time:169914ms step_avg:99.48ms
step:1719/1770 train_time:170020ms step_avg:99.48ms
step:1720/1770 train_time:170124ms step_avg:99.49ms
step:1721/1770 train_time:170228ms step_avg:99.49ms
step:1722/1770 train_time:170335ms step_avg:99.49ms
step:1723/1770 train_time:170441ms step_avg:99.50ms
step:1724/1770 train_time:170547ms step_avg:99.50ms
step:1725/1770 train_time:170654ms step_avg:99.51ms
step:1726/1770 train_time:170759ms step_avg:99.51ms
step:1727/1770 train_time:170863ms step_avg:99.51ms
step:1728/1770 train_time:170969ms step_avg:99.52ms
step:1729/1770 train_time:171073ms step_avg:99.52ms
step:1730/1770 train_time:171178ms step_avg:99.52ms
step:1731/1770 train_time:171284ms step_avg:99.53ms
step:1732/1770 train_time:171387ms step_avg:99.53ms
step:1733/1770 train_time:171493ms step_avg:99.53ms
step:1734/1770 train_time:171597ms step_avg:99.53ms
step:1735/1770 train_time:171701ms step_avg:99.54ms
step:1736/1770 train_time:171805ms step_avg:99.54ms
step:1737/1770 train_time:171910ms step_avg:99.54ms
step:1738/1770 train_time:172014ms step_avg:99.55ms
step:1739/1770 train_time:172119ms step_avg:99.55ms
step:1740/1770 train_time:172222ms step_avg:99.55ms
step:1741/1770 train_time:172328ms step_avg:99.55ms
step:1742/1770 train_time:172435ms step_avg:99.56ms
step:1743/1770 train_time:172540ms step_avg:99.56ms
step:1744/1770 train_time:172645ms step_avg:99.56ms
step:1745/1770 train_time:172748ms step_avg:99.57ms
step:1746/1770 train_time:172855ms step_avg:99.57ms
step:1747/1770 train_time:172958ms step_avg:99.57ms
step:1748/1770 train_time:173064ms step_avg:99.58ms
step:1749/1770 train_time:173170ms step_avg:99.58ms
step:1750/1770 train_time:173273ms step_avg:99.58ms
step:1750/1770 val_loss:3.2837 train_time:173376ms step_avg:99.64ms
step:1751/1770 train_time:173397ms step_avg:99.60ms
step:1752/1770 train_time:173486ms step_avg:99.59ms
step:1753/1770 train_time:173590ms step_avg:99.59ms
step:1754/1770 train_time:173695ms step_avg:99.60ms
step:1755/1770 train_time:173799ms step_avg:99.60ms
step:1756/1770 train_time:173903ms step_avg:99.60ms
step:1757/1770 train_time:174007ms step_avg:99.60ms
step:1758/1770 train_time:174111ms step_avg:99.61ms
step:1759/1770 train_time:174215ms step_avg:99.61ms
step:1760/1770 train_time:174320ms step_avg:99.61ms
step:1761/1770 train_time:174426ms step_avg:99.62ms
step:1762/1770 train_time:174535ms step_avg:99.62ms
step:1763/1770 train_time:174638ms step_avg:99.62ms
step:1764/1770 train_time:174742ms step_avg:99.62ms
step:1765/1770 train_time:174847ms step_avg:99.63ms
step:1766/1770 train_time:174955ms step_avg:99.63ms
step:1767/1770 train_time:175058ms step_avg:99.63ms
step:1768/1770 train_time:175162ms step_avg:99.64ms
step:1769/1770 train_time:175265ms step_avg:99.64ms
step:1770/1770 train_time:175369ms step_avg:99.64ms
step:1770/1770 val_loss:3.2804 train_time:175473ms step_avg:99.70ms
peak memory allocated: 28840 MiB reserved: 32232 MiB
