import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 08:00:20 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24150ms step_avg:nanms
step:2/1770 train_time:24568ms step_avg:nanms
step:3/1770 train_time:24663ms step_avg:nanms
step:4/1770 train_time:24756ms step_avg:nanms
step:5/1770 train_time:24851ms step_avg:nanms
step:6/1770 train_time:24945ms step_avg:nanms
step:7/1770 train_time:25039ms step_avg:nanms
step:8/1770 train_time:25133ms step_avg:nanms
step:9/1770 train_time:25227ms step_avg:nanms
step:10/1770 train_time:25320ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:94.16ms
step:14/1770 train_time:377ms step_avg:94.29ms
step:15/1770 train_time:472ms step_avg:94.33ms
step:16/1770 train_time:567ms step_avg:94.42ms
step:17/1770 train_time:661ms step_avg:94.45ms
step:18/1770 train_time:755ms step_avg:94.41ms
step:19/1770 train_time:849ms step_avg:94.35ms
step:20/1770 train_time:943ms step_avg:94.33ms
step:21/1770 train_time:1037ms step_avg:94.27ms
step:22/1770 train_time:1131ms step_avg:94.27ms
step:23/1770 train_time:1225ms step_avg:94.24ms
step:24/1770 train_time:1319ms step_avg:94.24ms
step:25/1770 train_time:1413ms step_avg:94.20ms
step:26/1770 train_time:1507ms step_avg:94.17ms
step:27/1770 train_time:1601ms step_avg:94.17ms
step:28/1770 train_time:1695ms step_avg:94.17ms
step:29/1770 train_time:1790ms step_avg:94.19ms
step:30/1770 train_time:1884ms step_avg:94.21ms
step:31/1770 train_time:1978ms step_avg:94.20ms
step:32/1770 train_time:2072ms step_avg:94.18ms
step:33/1770 train_time:2166ms step_avg:94.17ms
step:34/1770 train_time:2260ms step_avg:94.18ms
step:35/1770 train_time:2354ms step_avg:94.16ms
step:36/1770 train_time:2447ms step_avg:94.13ms
step:37/1770 train_time:2542ms step_avg:94.13ms
step:38/1770 train_time:2635ms step_avg:94.12ms
step:39/1770 train_time:2729ms step_avg:94.11ms
step:40/1770 train_time:2824ms step_avg:94.12ms
step:41/1770 train_time:2918ms step_avg:94.13ms
step:42/1770 train_time:3012ms step_avg:94.13ms
step:43/1770 train_time:3106ms step_avg:94.13ms
step:44/1770 train_time:3201ms step_avg:94.14ms
step:45/1770 train_time:3295ms step_avg:94.14ms
step:46/1770 train_time:3389ms step_avg:94.13ms
step:47/1770 train_time:3483ms step_avg:94.15ms
step:48/1770 train_time:3577ms step_avg:94.14ms
step:49/1770 train_time:3671ms step_avg:94.14ms
step:50/1770 train_time:3766ms step_avg:94.14ms
step:51/1770 train_time:3860ms step_avg:94.15ms
step:52/1770 train_time:3954ms step_avg:94.14ms
step:53/1770 train_time:4048ms step_avg:94.14ms
step:54/1770 train_time:4143ms step_avg:94.17ms
step:55/1770 train_time:4237ms step_avg:94.17ms
step:56/1770 train_time:4331ms step_avg:94.16ms
step:57/1770 train_time:4425ms step_avg:94.15ms
step:58/1770 train_time:4519ms step_avg:94.15ms
step:59/1770 train_time:4613ms step_avg:94.14ms
step:60/1770 train_time:4707ms step_avg:94.14ms
step:61/1770 train_time:4802ms step_avg:94.16ms
step:62/1770 train_time:4896ms step_avg:94.16ms
step:63/1770 train_time:4990ms step_avg:94.16ms
step:64/1770 train_time:5085ms step_avg:94.16ms
step:65/1770 train_time:5179ms step_avg:94.16ms
step:66/1770 train_time:5272ms step_avg:94.15ms
step:67/1770 train_time:5367ms step_avg:94.15ms
step:68/1770 train_time:5461ms step_avg:94.16ms
step:69/1770 train_time:5555ms step_avg:94.16ms
step:70/1770 train_time:5650ms step_avg:94.16ms
step:71/1770 train_time:5743ms step_avg:94.15ms
step:72/1770 train_time:5837ms step_avg:94.15ms
step:73/1770 train_time:5931ms step_avg:94.15ms
step:74/1770 train_time:6025ms step_avg:94.15ms
step:75/1770 train_time:6120ms step_avg:94.15ms
step:76/1770 train_time:6214ms step_avg:94.14ms
step:77/1770 train_time:6308ms step_avg:94.15ms
step:78/1770 train_time:6402ms step_avg:94.15ms
step:79/1770 train_time:6496ms step_avg:94.15ms
step:80/1770 train_time:6590ms step_avg:94.14ms
step:81/1770 train_time:6685ms step_avg:94.15ms
step:82/1770 train_time:6779ms step_avg:94.16ms
step:83/1770 train_time:6874ms step_avg:94.16ms
step:84/1770 train_time:6968ms step_avg:94.16ms
step:85/1770 train_time:7062ms step_avg:94.16ms
step:86/1770 train_time:7156ms step_avg:94.16ms
step:87/1770 train_time:7250ms step_avg:94.15ms
step:88/1770 train_time:7344ms step_avg:94.16ms
step:89/1770 train_time:7439ms step_avg:94.16ms
step:90/1770 train_time:7532ms step_avg:94.16ms
step:91/1770 train_time:7626ms step_avg:94.15ms
step:92/1770 train_time:7721ms step_avg:94.15ms
step:93/1770 train_time:7814ms step_avg:94.15ms
step:94/1770 train_time:7908ms step_avg:94.14ms
step:95/1770 train_time:8002ms step_avg:94.14ms
step:96/1770 train_time:8096ms step_avg:94.14ms
step:97/1770 train_time:8190ms step_avg:94.13ms
step:98/1770 train_time:8284ms step_avg:94.14ms
step:99/1770 train_time:8378ms step_avg:94.14ms
step:100/1770 train_time:8472ms step_avg:94.14ms
step:101/1770 train_time:8567ms step_avg:94.14ms
step:102/1770 train_time:8661ms step_avg:94.14ms
step:103/1770 train_time:8755ms step_avg:94.13ms
step:104/1770 train_time:8848ms step_avg:94.13ms
step:105/1770 train_time:8943ms step_avg:94.14ms
step:106/1770 train_time:9037ms step_avg:94.14ms
step:107/1770 train_time:9131ms step_avg:94.14ms
step:108/1770 train_time:9225ms step_avg:94.14ms
step:109/1770 train_time:9320ms step_avg:94.14ms
step:110/1770 train_time:9414ms step_avg:94.14ms
step:111/1770 train_time:9508ms step_avg:94.13ms
step:112/1770 train_time:9602ms step_avg:94.14ms
step:113/1770 train_time:9696ms step_avg:94.13ms
step:114/1770 train_time:9789ms step_avg:94.13ms
step:115/1770 train_time:9884ms step_avg:94.13ms
step:116/1770 train_time:9978ms step_avg:94.13ms
step:117/1770 train_time:10072ms step_avg:94.13ms
step:118/1770 train_time:10166ms step_avg:94.13ms
step:119/1770 train_time:10261ms step_avg:94.14ms
step:120/1770 train_time:10355ms step_avg:94.14ms
step:121/1770 train_time:10449ms step_avg:94.14ms
step:122/1770 train_time:10544ms step_avg:94.14ms
step:123/1770 train_time:10638ms step_avg:94.14ms
step:124/1770 train_time:10732ms step_avg:94.14ms
step:125/1770 train_time:10826ms step_avg:94.14ms
step:125/1770 val_loss:4.6470 train_time:10919ms step_avg:94.95ms
step:126/1770 train_time:10940ms step_avg:94.31ms
step:127/1770 train_time:11019ms step_avg:94.18ms
step:128/1770 train_time:11118ms step_avg:94.22ms
step:129/1770 train_time:11218ms step_avg:94.27ms
step:130/1770 train_time:11313ms step_avg:94.27ms
step:131/1770 train_time:11407ms step_avg:94.27ms
step:132/1770 train_time:11501ms step_avg:94.27ms
step:133/1770 train_time:11594ms step_avg:94.26ms
step:134/1770 train_time:11688ms step_avg:94.26ms
step:135/1770 train_time:11783ms step_avg:94.27ms
step:136/1770 train_time:11878ms step_avg:94.27ms
step:137/1770 train_time:11972ms step_avg:94.27ms
step:138/1770 train_time:12067ms step_avg:94.27ms
step:139/1770 train_time:12163ms step_avg:94.29ms
step:140/1770 train_time:12258ms step_avg:94.30ms
step:141/1770 train_time:12353ms step_avg:94.30ms
step:142/1770 train_time:12447ms step_avg:94.29ms
step:143/1770 train_time:12542ms step_avg:94.30ms
step:144/1770 train_time:12636ms step_avg:94.30ms
step:145/1770 train_time:12730ms step_avg:94.30ms
step:146/1770 train_time:12825ms step_avg:94.30ms
step:147/1770 train_time:12920ms step_avg:94.30ms
step:148/1770 train_time:13014ms step_avg:94.31ms
step:149/1770 train_time:13109ms step_avg:94.31ms
step:150/1770 train_time:13204ms step_avg:94.31ms
step:151/1770 train_time:13300ms step_avg:94.33ms
step:152/1770 train_time:13395ms step_avg:94.33ms
step:153/1770 train_time:13489ms step_avg:94.33ms
step:154/1770 train_time:13584ms step_avg:94.33ms
step:155/1770 train_time:13679ms step_avg:94.34ms
step:156/1770 train_time:13773ms step_avg:94.34ms
step:157/1770 train_time:13868ms step_avg:94.34ms
step:158/1770 train_time:13963ms step_avg:94.35ms
step:159/1770 train_time:14058ms step_avg:94.35ms
step:160/1770 train_time:14152ms step_avg:94.35ms
step:161/1770 train_time:14247ms step_avg:94.35ms
step:162/1770 train_time:14343ms step_avg:94.36ms
step:163/1770 train_time:14438ms step_avg:94.37ms
step:164/1770 train_time:14532ms step_avg:94.36ms
step:165/1770 train_time:14627ms step_avg:94.37ms
step:166/1770 train_time:14722ms step_avg:94.37ms
step:167/1770 train_time:14816ms step_avg:94.37ms
step:168/1770 train_time:14910ms step_avg:94.37ms
step:169/1770 train_time:15005ms step_avg:94.37ms
step:170/1770 train_time:15100ms step_avg:94.38ms
step:171/1770 train_time:15195ms step_avg:94.38ms
step:172/1770 train_time:15290ms step_avg:94.38ms
step:173/1770 train_time:15386ms step_avg:94.39ms
step:174/1770 train_time:15481ms step_avg:94.39ms
step:175/1770 train_time:15575ms step_avg:94.39ms
step:176/1770 train_time:15670ms step_avg:94.40ms
step:177/1770 train_time:15765ms step_avg:94.40ms
step:178/1770 train_time:15860ms step_avg:94.40ms
step:179/1770 train_time:15954ms step_avg:94.40ms
step:180/1770 train_time:16048ms step_avg:94.40ms
step:181/1770 train_time:16143ms step_avg:94.40ms
step:182/1770 train_time:16238ms step_avg:94.41ms
step:183/1770 train_time:16332ms step_avg:94.40ms
step:184/1770 train_time:16427ms step_avg:94.41ms
step:185/1770 train_time:16522ms step_avg:94.41ms
step:186/1770 train_time:16617ms step_avg:94.41ms
step:187/1770 train_time:16711ms step_avg:94.41ms
step:188/1770 train_time:16806ms step_avg:94.42ms
step:189/1770 train_time:16901ms step_avg:94.42ms
step:190/1770 train_time:16996ms step_avg:94.42ms
step:191/1770 train_time:17090ms step_avg:94.42ms
step:192/1770 train_time:17185ms step_avg:94.42ms
step:193/1770 train_time:17279ms step_avg:94.42ms
step:194/1770 train_time:17374ms step_avg:94.42ms
step:195/1770 train_time:17469ms step_avg:94.42ms
step:196/1770 train_time:17563ms step_avg:94.43ms
step:197/1770 train_time:17658ms step_avg:94.43ms
step:198/1770 train_time:17752ms step_avg:94.42ms
step:199/1770 train_time:17846ms step_avg:94.42ms
step:200/1770 train_time:17942ms step_avg:94.43ms
step:201/1770 train_time:18036ms step_avg:94.43ms
step:202/1770 train_time:18130ms step_avg:94.43ms
step:203/1770 train_time:18225ms step_avg:94.43ms
step:204/1770 train_time:18320ms step_avg:94.43ms
step:205/1770 train_time:18414ms step_avg:94.43ms
step:206/1770 train_time:18509ms step_avg:94.43ms
step:207/1770 train_time:18604ms step_avg:94.43ms
step:208/1770 train_time:18698ms step_avg:94.43ms
step:209/1770 train_time:18792ms step_avg:94.43ms
step:210/1770 train_time:18887ms step_avg:94.43ms
step:211/1770 train_time:18981ms step_avg:94.43ms
step:212/1770 train_time:19076ms step_avg:94.44ms
step:213/1770 train_time:19170ms step_avg:94.43ms
step:214/1770 train_time:19265ms step_avg:94.44ms
step:215/1770 train_time:19360ms step_avg:94.44ms
step:216/1770 train_time:19455ms step_avg:94.44ms
step:217/1770 train_time:19549ms step_avg:94.44ms
step:218/1770 train_time:19644ms step_avg:94.44ms
step:219/1770 train_time:19739ms step_avg:94.45ms
step:220/1770 train_time:19833ms step_avg:94.44ms
step:221/1770 train_time:19928ms step_avg:94.45ms
step:222/1770 train_time:20023ms step_avg:94.45ms
step:223/1770 train_time:20118ms step_avg:94.45ms
step:224/1770 train_time:20212ms step_avg:94.45ms
step:225/1770 train_time:20308ms step_avg:94.45ms
step:226/1770 train_time:20403ms step_avg:94.46ms
step:227/1770 train_time:20497ms step_avg:94.46ms
step:228/1770 train_time:20592ms step_avg:94.46ms
step:229/1770 train_time:20686ms step_avg:94.46ms
step:230/1770 train_time:20781ms step_avg:94.46ms
step:231/1770 train_time:20876ms step_avg:94.46ms
step:232/1770 train_time:20971ms step_avg:94.46ms
step:233/1770 train_time:21065ms step_avg:94.46ms
step:234/1770 train_time:21160ms step_avg:94.47ms
step:235/1770 train_time:21255ms step_avg:94.46ms
step:236/1770 train_time:21349ms step_avg:94.46ms
step:237/1770 train_time:21444ms step_avg:94.47ms
step:238/1770 train_time:21539ms step_avg:94.47ms
step:239/1770 train_time:21633ms step_avg:94.47ms
step:240/1770 train_time:21727ms step_avg:94.47ms
step:241/1770 train_time:21823ms step_avg:94.47ms
step:242/1770 train_time:21917ms step_avg:94.47ms
step:243/1770 train_time:22011ms step_avg:94.47ms
step:244/1770 train_time:22106ms step_avg:94.47ms
step:245/1770 train_time:22200ms step_avg:94.47ms
step:246/1770 train_time:22295ms step_avg:94.47ms
step:247/1770 train_time:22389ms step_avg:94.47ms
step:248/1770 train_time:22484ms step_avg:94.47ms
step:249/1770 train_time:22579ms step_avg:94.47ms
step:250/1770 train_time:22674ms step_avg:94.48ms
step:250/1770 val_loss:4.1109 train_time:22767ms step_avg:94.86ms
step:251/1770 train_time:22789ms step_avg:94.56ms
step:252/1770 train_time:22870ms step_avg:94.50ms
step:253/1770 train_time:22968ms step_avg:94.52ms
step:254/1770 train_time:23064ms step_avg:94.52ms
step:255/1770 train_time:23159ms step_avg:94.52ms
step:256/1770 train_time:23252ms step_avg:94.52ms
step:257/1770 train_time:23347ms step_avg:94.52ms
step:258/1770 train_time:23441ms step_avg:94.52ms
step:259/1770 train_time:23535ms step_avg:94.52ms
step:260/1770 train_time:23630ms step_avg:94.52ms
step:261/1770 train_time:23725ms step_avg:94.52ms
step:262/1770 train_time:23819ms step_avg:94.52ms
step:263/1770 train_time:23915ms step_avg:94.53ms
step:264/1770 train_time:24011ms step_avg:94.53ms
step:265/1770 train_time:24107ms step_avg:94.54ms
step:266/1770 train_time:24203ms step_avg:94.54ms
step:267/1770 train_time:24298ms step_avg:94.54ms
step:268/1770 train_time:24393ms step_avg:94.55ms
step:269/1770 train_time:24488ms step_avg:94.55ms
step:270/1770 train_time:24583ms step_avg:94.55ms
step:271/1770 train_time:24677ms step_avg:94.55ms
step:272/1770 train_time:24772ms step_avg:94.55ms
step:273/1770 train_time:24867ms step_avg:94.55ms
step:274/1770 train_time:24963ms step_avg:94.56ms
step:275/1770 train_time:25058ms step_avg:94.56ms
step:276/1770 train_time:25153ms step_avg:94.56ms
step:277/1770 train_time:25249ms step_avg:94.57ms
step:278/1770 train_time:25345ms step_avg:94.57ms
step:279/1770 train_time:25440ms step_avg:94.57ms
step:280/1770 train_time:25534ms step_avg:94.57ms
step:281/1770 train_time:25630ms step_avg:94.58ms
step:282/1770 train_time:25726ms step_avg:94.58ms
step:283/1770 train_time:25821ms step_avg:94.58ms
step:284/1770 train_time:25916ms step_avg:94.58ms
step:285/1770 train_time:26012ms step_avg:94.59ms
step:286/1770 train_time:26108ms step_avg:94.59ms
step:287/1770 train_time:26203ms step_avg:94.60ms
step:288/1770 train_time:26299ms step_avg:94.60ms
step:289/1770 train_time:26394ms step_avg:94.60ms
step:290/1770 train_time:26489ms step_avg:94.60ms
step:291/1770 train_time:26584ms step_avg:94.61ms
step:292/1770 train_time:26680ms step_avg:94.61ms
step:293/1770 train_time:26774ms step_avg:94.61ms
step:294/1770 train_time:26870ms step_avg:94.61ms
step:295/1770 train_time:26965ms step_avg:94.61ms
step:296/1770 train_time:27060ms step_avg:94.62ms
step:297/1770 train_time:27155ms step_avg:94.62ms
step:298/1770 train_time:27251ms step_avg:94.62ms
step:299/1770 train_time:27347ms step_avg:94.63ms
step:300/1770 train_time:27443ms step_avg:94.63ms
step:301/1770 train_time:27537ms step_avg:94.63ms
step:302/1770 train_time:27632ms step_avg:94.63ms
step:303/1770 train_time:27728ms step_avg:94.64ms
step:304/1770 train_time:27824ms step_avg:94.64ms
step:305/1770 train_time:27919ms step_avg:94.64ms
step:306/1770 train_time:28013ms step_avg:94.64ms
step:307/1770 train_time:28108ms step_avg:94.64ms
step:308/1770 train_time:28204ms step_avg:94.64ms
step:309/1770 train_time:28299ms step_avg:94.64ms
step:310/1770 train_time:28394ms step_avg:94.65ms
step:311/1770 train_time:28489ms step_avg:94.65ms
step:312/1770 train_time:28585ms step_avg:94.65ms
step:313/1770 train_time:28680ms step_avg:94.65ms
step:314/1770 train_time:28775ms step_avg:94.66ms
step:315/1770 train_time:28870ms step_avg:94.66ms
step:316/1770 train_time:28966ms step_avg:94.66ms
step:317/1770 train_time:29061ms step_avg:94.66ms
step:318/1770 train_time:29156ms step_avg:94.66ms
step:319/1770 train_time:29251ms step_avg:94.66ms
step:320/1770 train_time:29347ms step_avg:94.67ms
step:321/1770 train_time:29444ms step_avg:94.67ms
step:322/1770 train_time:29539ms step_avg:94.68ms
step:323/1770 train_time:29634ms step_avg:94.68ms
step:324/1770 train_time:29730ms step_avg:94.68ms
step:325/1770 train_time:29825ms step_avg:94.68ms
step:326/1770 train_time:29920ms step_avg:94.68ms
step:327/1770 train_time:30015ms step_avg:94.68ms
step:328/1770 train_time:30110ms step_avg:94.69ms
step:329/1770 train_time:30205ms step_avg:94.69ms
step:330/1770 train_time:30301ms step_avg:94.69ms
step:331/1770 train_time:30395ms step_avg:94.69ms
step:332/1770 train_time:30491ms step_avg:94.69ms
step:333/1770 train_time:30586ms step_avg:94.69ms
step:334/1770 train_time:30682ms step_avg:94.70ms
step:335/1770 train_time:30777ms step_avg:94.70ms
step:336/1770 train_time:30872ms step_avg:94.70ms
step:337/1770 train_time:30968ms step_avg:94.70ms
step:338/1770 train_time:31064ms step_avg:94.71ms
step:339/1770 train_time:31159ms step_avg:94.71ms
step:340/1770 train_time:31253ms step_avg:94.71ms
step:341/1770 train_time:31348ms step_avg:94.71ms
step:342/1770 train_time:31444ms step_avg:94.71ms
step:343/1770 train_time:31539ms step_avg:94.71ms
step:344/1770 train_time:31634ms step_avg:94.71ms
step:345/1770 train_time:31730ms step_avg:94.72ms
step:346/1770 train_time:31825ms step_avg:94.72ms
step:347/1770 train_time:31920ms step_avg:94.72ms
step:348/1770 train_time:32015ms step_avg:94.72ms
step:349/1770 train_time:32110ms step_avg:94.72ms
step:350/1770 train_time:32206ms step_avg:94.72ms
step:351/1770 train_time:32301ms step_avg:94.72ms
step:352/1770 train_time:32395ms step_avg:94.72ms
step:353/1770 train_time:32491ms step_avg:94.73ms
step:354/1770 train_time:32586ms step_avg:94.73ms
step:355/1770 train_time:32681ms step_avg:94.73ms
step:356/1770 train_time:32776ms step_avg:94.73ms
step:357/1770 train_time:32870ms step_avg:94.73ms
step:358/1770 train_time:32966ms step_avg:94.73ms
step:359/1770 train_time:33062ms step_avg:94.73ms
step:360/1770 train_time:33157ms step_avg:94.73ms
step:361/1770 train_time:33252ms step_avg:94.73ms
step:362/1770 train_time:33348ms step_avg:94.74ms
step:363/1770 train_time:33444ms step_avg:94.74ms
step:364/1770 train_time:33539ms step_avg:94.74ms
step:365/1770 train_time:33634ms step_avg:94.74ms
step:366/1770 train_time:33729ms step_avg:94.74ms
step:367/1770 train_time:33824ms step_avg:94.74ms
step:368/1770 train_time:33919ms step_avg:94.75ms
step:369/1770 train_time:34014ms step_avg:94.75ms
step:370/1770 train_time:34109ms step_avg:94.75ms
step:371/1770 train_time:34205ms step_avg:94.75ms
step:372/1770 train_time:34301ms step_avg:94.75ms
step:373/1770 train_time:34396ms step_avg:94.75ms
step:374/1770 train_time:34491ms step_avg:94.75ms
step:375/1770 train_time:34587ms step_avg:94.76ms
step:375/1770 val_loss:3.9013 train_time:34680ms step_avg:95.01ms
step:376/1770 train_time:34701ms step_avg:94.81ms
step:377/1770 train_time:34785ms step_avg:94.78ms
step:378/1770 train_time:34882ms step_avg:94.79ms
step:379/1770 train_time:34978ms step_avg:94.79ms
step:380/1770 train_time:35072ms step_avg:94.79ms
step:381/1770 train_time:35167ms step_avg:94.79ms
step:382/1770 train_time:35262ms step_avg:94.79ms
step:383/1770 train_time:35357ms step_avg:94.79ms
step:384/1770 train_time:35451ms step_avg:94.79ms
step:385/1770 train_time:35546ms step_avg:94.79ms
step:386/1770 train_time:35641ms step_avg:94.79ms
step:387/1770 train_time:35737ms step_avg:94.79ms
step:388/1770 train_time:35832ms step_avg:94.79ms
step:389/1770 train_time:35927ms step_avg:94.80ms
step:390/1770 train_time:36023ms step_avg:94.80ms
step:391/1770 train_time:36119ms step_avg:94.80ms
step:392/1770 train_time:36214ms step_avg:94.80ms
step:393/1770 train_time:36309ms step_avg:94.80ms
step:394/1770 train_time:36404ms step_avg:94.80ms
step:395/1770 train_time:36499ms step_avg:94.80ms
step:396/1770 train_time:36596ms step_avg:94.81ms
step:397/1770 train_time:36693ms step_avg:94.81ms
step:398/1770 train_time:36790ms step_avg:94.82ms
step:399/1770 train_time:36887ms step_avg:94.82ms
step:400/1770 train_time:36984ms step_avg:94.83ms
step:401/1770 train_time:37082ms step_avg:94.84ms
step:402/1770 train_time:37179ms step_avg:94.85ms
step:403/1770 train_time:37277ms step_avg:94.85ms
step:404/1770 train_time:37374ms step_avg:94.86ms
step:405/1770 train_time:37470ms step_avg:94.86ms
step:406/1770 train_time:37567ms step_avg:94.87ms
step:407/1770 train_time:37665ms step_avg:94.87ms
step:408/1770 train_time:37762ms step_avg:94.88ms
step:409/1770 train_time:37859ms step_avg:94.89ms
step:410/1770 train_time:37957ms step_avg:94.89ms
step:411/1770 train_time:38053ms step_avg:94.90ms
step:412/1770 train_time:38150ms step_avg:94.90ms
step:413/1770 train_time:38249ms step_avg:94.91ms
step:414/1770 train_time:38345ms step_avg:94.91ms
step:415/1770 train_time:38442ms step_avg:94.92ms
step:416/1770 train_time:38540ms step_avg:94.93ms
step:417/1770 train_time:38637ms step_avg:94.93ms
step:418/1770 train_time:38734ms step_avg:94.94ms
step:419/1770 train_time:38831ms step_avg:94.94ms
step:420/1770 train_time:38927ms step_avg:94.94ms
step:421/1770 train_time:39025ms step_avg:94.95ms
step:422/1770 train_time:39123ms step_avg:94.96ms
step:423/1770 train_time:39220ms step_avg:94.96ms
step:424/1770 train_time:39318ms step_avg:94.97ms
step:425/1770 train_time:39415ms step_avg:94.98ms
step:426/1770 train_time:39511ms step_avg:94.98ms
step:427/1770 train_time:39608ms step_avg:94.98ms
step:428/1770 train_time:39705ms step_avg:94.99ms
step:429/1770 train_time:39802ms step_avg:94.99ms
step:430/1770 train_time:39900ms step_avg:95.00ms
step:431/1770 train_time:39997ms step_avg:95.00ms
step:432/1770 train_time:40094ms step_avg:95.01ms
step:433/1770 train_time:40190ms step_avg:95.01ms
step:434/1770 train_time:40288ms step_avg:95.02ms
step:435/1770 train_time:40385ms step_avg:95.02ms
step:436/1770 train_time:40483ms step_avg:95.03ms
step:437/1770 train_time:40582ms step_avg:95.04ms
step:438/1770 train_time:40680ms step_avg:95.05ms
step:439/1770 train_time:40777ms step_avg:95.05ms
step:440/1770 train_time:40873ms step_avg:95.05ms
step:441/1770 train_time:40970ms step_avg:95.06ms
step:442/1770 train_time:41067ms step_avg:95.06ms
step:443/1770 train_time:41164ms step_avg:95.07ms
step:444/1770 train_time:41262ms step_avg:95.07ms
step:445/1770 train_time:41359ms step_avg:95.08ms
step:446/1770 train_time:41457ms step_avg:95.08ms
step:447/1770 train_time:41554ms step_avg:95.09ms
step:448/1770 train_time:41651ms step_avg:95.09ms
step:449/1770 train_time:41747ms step_avg:95.10ms
step:450/1770 train_time:41845ms step_avg:95.10ms
step:451/1770 train_time:41943ms step_avg:95.11ms
step:452/1770 train_time:42040ms step_avg:95.11ms
step:453/1770 train_time:42138ms step_avg:95.12ms
step:454/1770 train_time:42235ms step_avg:95.12ms
step:455/1770 train_time:42331ms step_avg:95.13ms
step:456/1770 train_time:42428ms step_avg:95.13ms
step:457/1770 train_time:42525ms step_avg:95.13ms
step:458/1770 train_time:42622ms step_avg:95.14ms
step:459/1770 train_time:42720ms step_avg:95.14ms
step:460/1770 train_time:42817ms step_avg:95.15ms
step:461/1770 train_time:42913ms step_avg:95.15ms
step:462/1770 train_time:43009ms step_avg:95.15ms
step:463/1770 train_time:43106ms step_avg:95.16ms
step:464/1770 train_time:43203ms step_avg:95.16ms
step:465/1770 train_time:43301ms step_avg:95.17ms
step:466/1770 train_time:43399ms step_avg:95.17ms
step:467/1770 train_time:43496ms step_avg:95.18ms
step:468/1770 train_time:43593ms step_avg:95.18ms
step:469/1770 train_time:43690ms step_avg:95.19ms
step:470/1770 train_time:43787ms step_avg:95.19ms
step:471/1770 train_time:43884ms step_avg:95.19ms
step:472/1770 train_time:43981ms step_avg:95.20ms
step:473/1770 train_time:44079ms step_avg:95.20ms
step:474/1770 train_time:44175ms step_avg:95.21ms
step:475/1770 train_time:44272ms step_avg:95.21ms
step:476/1770 train_time:44369ms step_avg:95.21ms
step:477/1770 train_time:44466ms step_avg:95.22ms
step:478/1770 train_time:44563ms step_avg:95.22ms
step:479/1770 train_time:44661ms step_avg:95.23ms
step:480/1770 train_time:44759ms step_avg:95.23ms
step:481/1770 train_time:44856ms step_avg:95.24ms
step:482/1770 train_time:44952ms step_avg:95.24ms
step:483/1770 train_time:45049ms step_avg:95.24ms
step:484/1770 train_time:45147ms step_avg:95.25ms
step:485/1770 train_time:45245ms step_avg:95.25ms
step:486/1770 train_time:45342ms step_avg:95.26ms
step:487/1770 train_time:45440ms step_avg:95.26ms
step:488/1770 train_time:45537ms step_avg:95.27ms
step:489/1770 train_time:45634ms step_avg:95.27ms
step:490/1770 train_time:45731ms step_avg:95.27ms
step:491/1770 train_time:45828ms step_avg:95.28ms
step:492/1770 train_time:45925ms step_avg:95.28ms
step:493/1770 train_time:46023ms step_avg:95.28ms
step:494/1770 train_time:46120ms step_avg:95.29ms
step:495/1770 train_time:46217ms step_avg:95.29ms
step:496/1770 train_time:46313ms step_avg:95.30ms
step:497/1770 train_time:46410ms step_avg:95.30ms
step:498/1770 train_time:46507ms step_avg:95.30ms
step:499/1770 train_time:46605ms step_avg:95.31ms
step:500/1770 train_time:46704ms step_avg:95.31ms
step:500/1770 val_loss:3.7486 train_time:46800ms step_avg:95.51ms
step:501/1770 train_time:46821ms step_avg:95.36ms
step:502/1770 train_time:46905ms step_avg:95.34ms
step:503/1770 train_time:47006ms step_avg:95.35ms
step:504/1770 train_time:47103ms step_avg:95.35ms
step:505/1770 train_time:47200ms step_avg:95.35ms
step:506/1770 train_time:47297ms step_avg:95.36ms
step:507/1770 train_time:47393ms step_avg:95.36ms
step:508/1770 train_time:47490ms step_avg:95.36ms
step:509/1770 train_time:47587ms step_avg:95.36ms
step:510/1770 train_time:47683ms step_avg:95.37ms
step:511/1770 train_time:47780ms step_avg:95.37ms
step:512/1770 train_time:47878ms step_avg:95.37ms
step:513/1770 train_time:47976ms step_avg:95.38ms
step:514/1770 train_time:48073ms step_avg:95.38ms
step:515/1770 train_time:48170ms step_avg:95.39ms
step:516/1770 train_time:48266ms step_avg:95.39ms
step:517/1770 train_time:48364ms step_avg:95.39ms
step:518/1770 train_time:48460ms step_avg:95.39ms
step:519/1770 train_time:48558ms step_avg:95.40ms
step:520/1770 train_time:48655ms step_avg:95.40ms
step:521/1770 train_time:48752ms step_avg:95.40ms
step:522/1770 train_time:48848ms step_avg:95.41ms
step:523/1770 train_time:48945ms step_avg:95.41ms
step:524/1770 train_time:49043ms step_avg:95.41ms
step:525/1770 train_time:49141ms step_avg:95.42ms
step:526/1770 train_time:49238ms step_avg:95.42ms
step:527/1770 train_time:49337ms step_avg:95.43ms
step:528/1770 train_time:49434ms step_avg:95.43ms
step:529/1770 train_time:49530ms step_avg:95.43ms
step:530/1770 train_time:49627ms step_avg:95.44ms
step:531/1770 train_time:49724ms step_avg:95.44ms
step:532/1770 train_time:49822ms step_avg:95.44ms
step:533/1770 train_time:49920ms step_avg:95.45ms
step:534/1770 train_time:50018ms step_avg:95.45ms
step:535/1770 train_time:50116ms step_avg:95.46ms
step:536/1770 train_time:50213ms step_avg:95.46ms
step:537/1770 train_time:50310ms step_avg:95.47ms
step:538/1770 train_time:50408ms step_avg:95.47ms
step:539/1770 train_time:50505ms step_avg:95.47ms
step:540/1770 train_time:50603ms step_avg:95.48ms
step:541/1770 train_time:50701ms step_avg:95.48ms
step:542/1770 train_time:50798ms step_avg:95.49ms
step:543/1770 train_time:50896ms step_avg:95.49ms
step:544/1770 train_time:50993ms step_avg:95.49ms
step:545/1770 train_time:51089ms step_avg:95.49ms
step:546/1770 train_time:51187ms step_avg:95.50ms
step:547/1770 train_time:51284ms step_avg:95.50ms
step:548/1770 train_time:51382ms step_avg:95.51ms
step:549/1770 train_time:51480ms step_avg:95.51ms
step:550/1770 train_time:51578ms step_avg:95.51ms
step:551/1770 train_time:51675ms step_avg:95.52ms
step:552/1770 train_time:51773ms step_avg:95.52ms
step:553/1770 train_time:51870ms step_avg:95.53ms
step:554/1770 train_time:51967ms step_avg:95.53ms
step:555/1770 train_time:52065ms step_avg:95.53ms
step:556/1770 train_time:52163ms step_avg:95.54ms
step:557/1770 train_time:52260ms step_avg:95.54ms
step:558/1770 train_time:52358ms step_avg:95.54ms
step:559/1770 train_time:52456ms step_avg:95.55ms
step:560/1770 train_time:52554ms step_avg:95.55ms
step:561/1770 train_time:52651ms step_avg:95.55ms
step:562/1770 train_time:52748ms step_avg:95.56ms
step:563/1770 train_time:52845ms step_avg:95.56ms
step:564/1770 train_time:52943ms step_avg:95.57ms
step:565/1770 train_time:53041ms step_avg:95.57ms
step:566/1770 train_time:53139ms step_avg:95.57ms
step:567/1770 train_time:53237ms step_avg:95.58ms
step:568/1770 train_time:53335ms step_avg:95.58ms
step:569/1770 train_time:53432ms step_avg:95.58ms
step:570/1770 train_time:53529ms step_avg:95.59ms
step:571/1770 train_time:53627ms step_avg:95.59ms
step:572/1770 train_time:53724ms step_avg:95.59ms
step:573/1770 train_time:53822ms step_avg:95.60ms
step:574/1770 train_time:53920ms step_avg:95.60ms
step:575/1770 train_time:54017ms step_avg:95.61ms
step:576/1770 train_time:54115ms step_avg:95.61ms
step:577/1770 train_time:54211ms step_avg:95.61ms
step:578/1770 train_time:54308ms step_avg:95.61ms
step:579/1770 train_time:54405ms step_avg:95.62ms
step:580/1770 train_time:54503ms step_avg:95.62ms
step:581/1770 train_time:54602ms step_avg:95.62ms
step:582/1770 train_time:54700ms step_avg:95.63ms
step:583/1770 train_time:54798ms step_avg:95.63ms
step:584/1770 train_time:54895ms step_avg:95.64ms
step:585/1770 train_time:54993ms step_avg:95.64ms
step:586/1770 train_time:55090ms step_avg:95.64ms
step:587/1770 train_time:55187ms step_avg:95.65ms
step:588/1770 train_time:55285ms step_avg:95.65ms
step:589/1770 train_time:55382ms step_avg:95.65ms
step:590/1770 train_time:55480ms step_avg:95.66ms
step:591/1770 train_time:55578ms step_avg:95.66ms
step:592/1770 train_time:55676ms step_avg:95.66ms
step:593/1770 train_time:55773ms step_avg:95.67ms
step:594/1770 train_time:55870ms step_avg:95.67ms
step:595/1770 train_time:55968ms step_avg:95.67ms
step:596/1770 train_time:56065ms step_avg:95.67ms
step:597/1770 train_time:56162ms step_avg:95.68ms
step:598/1770 train_time:56261ms step_avg:95.68ms
step:599/1770 train_time:56359ms step_avg:95.69ms
step:600/1770 train_time:56457ms step_avg:95.69ms
step:601/1770 train_time:56554ms step_avg:95.69ms
step:602/1770 train_time:56651ms step_avg:95.69ms
step:603/1770 train_time:56748ms step_avg:95.70ms
step:604/1770 train_time:56846ms step_avg:95.70ms
step:605/1770 train_time:56943ms step_avg:95.70ms
step:606/1770 train_time:57041ms step_avg:95.71ms
step:607/1770 train_time:57139ms step_avg:95.71ms
step:608/1770 train_time:57237ms step_avg:95.71ms
step:609/1770 train_time:57335ms step_avg:95.72ms
step:610/1770 train_time:57432ms step_avg:95.72ms
step:611/1770 train_time:57529ms step_avg:95.72ms
step:612/1770 train_time:57626ms step_avg:95.73ms
step:613/1770 train_time:57724ms step_avg:95.73ms
step:614/1770 train_time:57822ms step_avg:95.73ms
step:615/1770 train_time:57920ms step_avg:95.74ms
step:616/1770 train_time:58018ms step_avg:95.74ms
step:617/1770 train_time:58116ms step_avg:95.74ms
step:618/1770 train_time:58213ms step_avg:95.74ms
step:619/1770 train_time:58310ms step_avg:95.75ms
step:620/1770 train_time:58407ms step_avg:95.75ms
step:621/1770 train_time:58504ms step_avg:95.75ms
step:622/1770 train_time:58602ms step_avg:95.76ms
step:623/1770 train_time:58699ms step_avg:95.76ms
step:624/1770 train_time:58797ms step_avg:95.76ms
step:625/1770 train_time:58894ms step_avg:95.76ms
step:625/1770 val_loss:3.6624 train_time:58990ms step_avg:95.92ms
step:626/1770 train_time:59012ms step_avg:95.80ms
step:627/1770 train_time:59096ms step_avg:95.78ms
step:628/1770 train_time:59195ms step_avg:95.78ms
step:629/1770 train_time:59293ms step_avg:95.79ms
step:630/1770 train_time:59390ms step_avg:95.79ms
step:631/1770 train_time:59487ms step_avg:95.79ms
step:632/1770 train_time:59584ms step_avg:95.79ms
step:633/1770 train_time:59681ms step_avg:95.80ms
step:634/1770 train_time:59778ms step_avg:95.80ms
step:635/1770 train_time:59876ms step_avg:95.80ms
step:636/1770 train_time:59974ms step_avg:95.80ms
step:637/1770 train_time:60071ms step_avg:95.81ms
step:638/1770 train_time:60169ms step_avg:95.81ms
step:639/1770 train_time:60266ms step_avg:95.81ms
step:640/1770 train_time:60363ms step_avg:95.81ms
step:641/1770 train_time:60461ms step_avg:95.82ms
step:642/1770 train_time:60559ms step_avg:95.82ms
step:643/1770 train_time:60657ms step_avg:95.82ms
step:644/1770 train_time:60755ms step_avg:95.83ms
step:645/1770 train_time:60852ms step_avg:95.83ms
step:646/1770 train_time:60949ms step_avg:95.83ms
step:647/1770 train_time:61046ms step_avg:95.83ms
step:648/1770 train_time:61144ms step_avg:95.84ms
step:649/1770 train_time:61242ms step_avg:95.84ms
step:650/1770 train_time:61340ms step_avg:95.84ms
step:651/1770 train_time:61437ms step_avg:95.85ms
step:652/1770 train_time:61535ms step_avg:95.85ms
step:653/1770 train_time:61632ms step_avg:95.85ms
step:654/1770 train_time:61729ms step_avg:95.85ms
step:655/1770 train_time:61826ms step_avg:95.85ms
step:656/1770 train_time:61923ms step_avg:95.86ms
step:657/1770 train_time:62021ms step_avg:95.86ms
step:658/1770 train_time:62121ms step_avg:95.87ms
step:659/1770 train_time:62221ms step_avg:95.87ms
step:660/1770 train_time:62320ms step_avg:95.88ms
step:661/1770 train_time:62420ms step_avg:95.88ms
step:662/1770 train_time:62520ms step_avg:95.89ms
step:663/1770 train_time:62619ms step_avg:95.89ms
step:664/1770 train_time:62719ms step_avg:95.90ms
step:665/1770 train_time:62819ms step_avg:95.91ms
step:666/1770 train_time:62919ms step_avg:95.91ms
step:667/1770 train_time:63020ms step_avg:95.92ms
step:668/1770 train_time:63120ms step_avg:95.93ms
step:669/1770 train_time:63219ms step_avg:95.93ms
step:670/1770 train_time:63320ms step_avg:95.94ms
step:671/1770 train_time:63420ms step_avg:95.95ms
step:672/1770 train_time:63519ms step_avg:95.95ms
step:673/1770 train_time:63619ms step_avg:95.96ms
step:674/1770 train_time:63719ms step_avg:95.96ms
step:675/1770 train_time:63818ms step_avg:95.97ms
step:676/1770 train_time:63918ms step_avg:95.97ms
step:677/1770 train_time:64017ms step_avg:95.98ms
step:678/1770 train_time:64117ms step_avg:95.98ms
step:679/1770 train_time:64218ms step_avg:95.99ms
step:680/1770 train_time:64318ms step_avg:96.00ms
step:681/1770 train_time:64418ms step_avg:96.00ms
step:682/1770 train_time:64519ms step_avg:96.01ms
step:683/1770 train_time:64618ms step_avg:96.02ms
step:684/1770 train_time:64718ms step_avg:96.02ms
step:685/1770 train_time:64818ms step_avg:96.03ms
step:686/1770 train_time:64918ms step_avg:96.03ms
step:687/1770 train_time:65018ms step_avg:96.04ms
step:688/1770 train_time:65117ms step_avg:96.04ms
step:689/1770 train_time:65216ms step_avg:96.05ms
step:690/1770 train_time:65316ms step_avg:96.05ms
step:691/1770 train_time:65416ms step_avg:96.06ms
step:692/1770 train_time:65516ms step_avg:96.06ms
step:693/1770 train_time:65616ms step_avg:96.07ms
step:694/1770 train_time:65716ms step_avg:96.08ms
step:695/1770 train_time:65817ms step_avg:96.08ms
step:696/1770 train_time:65917ms step_avg:96.09ms
step:697/1770 train_time:66017ms step_avg:96.09ms
step:698/1770 train_time:66117ms step_avg:96.10ms
step:699/1770 train_time:66217ms step_avg:96.11ms
step:700/1770 train_time:66316ms step_avg:96.11ms
step:701/1770 train_time:66416ms step_avg:96.12ms
step:702/1770 train_time:66515ms step_avg:96.12ms
step:703/1770 train_time:66615ms step_avg:96.12ms
step:704/1770 train_time:66714ms step_avg:96.13ms
step:705/1770 train_time:66813ms step_avg:96.13ms
step:706/1770 train_time:66912ms step_avg:96.14ms
step:707/1770 train_time:67011ms step_avg:96.14ms
step:708/1770 train_time:67110ms step_avg:96.15ms
step:709/1770 train_time:67209ms step_avg:96.15ms
step:710/1770 train_time:67307ms step_avg:96.15ms
step:711/1770 train_time:67406ms step_avg:96.16ms
step:712/1770 train_time:67506ms step_avg:96.16ms
step:713/1770 train_time:67604ms step_avg:96.17ms
step:714/1770 train_time:67703ms step_avg:96.17ms
step:715/1770 train_time:67803ms step_avg:96.17ms
step:716/1770 train_time:67902ms step_avg:96.18ms
step:717/1770 train_time:68002ms step_avg:96.18ms
step:718/1770 train_time:68101ms step_avg:96.19ms
step:719/1770 train_time:68201ms step_avg:96.19ms
step:720/1770 train_time:68301ms step_avg:96.20ms
step:721/1770 train_time:68401ms step_avg:96.20ms
step:722/1770 train_time:68501ms step_avg:96.21ms
step:723/1770 train_time:68601ms step_avg:96.21ms
step:724/1770 train_time:68701ms step_avg:96.22ms
step:725/1770 train_time:68800ms step_avg:96.22ms
step:726/1770 train_time:68900ms step_avg:96.23ms
step:727/1770 train_time:68999ms step_avg:96.23ms
step:728/1770 train_time:69100ms step_avg:96.24ms
step:729/1770 train_time:69199ms step_avg:96.24ms
step:730/1770 train_time:69299ms step_avg:96.25ms
step:731/1770 train_time:69399ms step_avg:96.25ms
step:732/1770 train_time:69499ms step_avg:96.26ms
step:733/1770 train_time:69599ms step_avg:96.26ms
step:734/1770 train_time:69698ms step_avg:96.27ms
step:735/1770 train_time:69797ms step_avg:96.27ms
step:736/1770 train_time:69896ms step_avg:96.28ms
step:737/1770 train_time:69996ms step_avg:96.28ms
step:738/1770 train_time:70095ms step_avg:96.28ms
step:739/1770 train_time:70195ms step_avg:96.29ms
step:740/1770 train_time:70295ms step_avg:96.29ms
step:741/1770 train_time:70395ms step_avg:96.30ms
step:742/1770 train_time:70494ms step_avg:96.30ms
step:743/1770 train_time:70593ms step_avg:96.31ms
step:744/1770 train_time:70692ms step_avg:96.31ms
step:745/1770 train_time:70790ms step_avg:96.31ms
step:746/1770 train_time:70889ms step_avg:96.32ms
step:747/1770 train_time:70988ms step_avg:96.32ms
step:748/1770 train_time:71087ms step_avg:96.32ms
step:749/1770 train_time:71185ms step_avg:96.33ms
step:750/1770 train_time:71284ms step_avg:96.33ms
step:750/1770 val_loss:3.5987 train_time:71382ms step_avg:96.46ms
step:751/1770 train_time:71403ms step_avg:96.36ms
step:752/1770 train_time:71490ms step_avg:96.35ms
step:753/1770 train_time:71591ms step_avg:96.35ms
step:754/1770 train_time:71690ms step_avg:96.36ms
step:755/1770 train_time:71789ms step_avg:96.36ms
step:756/1770 train_time:71889ms step_avg:96.37ms
step:757/1770 train_time:71988ms step_avg:96.37ms
step:758/1770 train_time:72087ms step_avg:96.37ms
step:759/1770 train_time:72186ms step_avg:96.38ms
step:760/1770 train_time:72285ms step_avg:96.38ms
step:761/1770 train_time:72385ms step_avg:96.38ms
step:762/1770 train_time:72486ms step_avg:96.39ms
step:763/1770 train_time:72587ms step_avg:96.40ms
step:764/1770 train_time:72687ms step_avg:96.40ms
step:765/1770 train_time:72787ms step_avg:96.41ms
step:766/1770 train_time:72886ms step_avg:96.41ms
step:767/1770 train_time:72986ms step_avg:96.42ms
step:768/1770 train_time:73086ms step_avg:96.42ms
step:769/1770 train_time:73186ms step_avg:96.42ms
step:770/1770 train_time:73285ms step_avg:96.43ms
step:771/1770 train_time:73385ms step_avg:96.43ms
step:772/1770 train_time:73485ms step_avg:96.44ms
step:773/1770 train_time:73585ms step_avg:96.44ms
step:774/1770 train_time:73685ms step_avg:96.45ms
step:775/1770 train_time:73785ms step_avg:96.45ms
step:776/1770 train_time:73885ms step_avg:96.46ms
step:777/1770 train_time:73984ms step_avg:96.46ms
step:778/1770 train_time:74083ms step_avg:96.46ms
step:779/1770 train_time:74183ms step_avg:96.47ms
step:780/1770 train_time:74282ms step_avg:96.47ms
step:781/1770 train_time:74382ms step_avg:96.47ms
step:782/1770 train_time:74481ms step_avg:96.48ms
step:783/1770 train_time:74581ms step_avg:96.48ms
step:784/1770 train_time:74680ms step_avg:96.49ms
step:785/1770 train_time:74780ms step_avg:96.49ms
step:786/1770 train_time:74880ms step_avg:96.49ms
step:787/1770 train_time:74979ms step_avg:96.50ms
step:788/1770 train_time:75078ms step_avg:96.50ms
step:789/1770 train_time:75177ms step_avg:96.50ms
step:790/1770 train_time:75276ms step_avg:96.51ms
step:791/1770 train_time:75375ms step_avg:96.51ms
step:792/1770 train_time:75475ms step_avg:96.51ms
step:793/1770 train_time:75574ms step_avg:96.52ms
step:794/1770 train_time:75673ms step_avg:96.52ms
step:795/1770 train_time:75772ms step_avg:96.52ms
step:796/1770 train_time:75871ms step_avg:96.53ms
step:797/1770 train_time:75971ms step_avg:96.53ms
step:798/1770 train_time:76071ms step_avg:96.54ms
step:799/1770 train_time:76171ms step_avg:96.54ms
step:800/1770 train_time:76270ms step_avg:96.54ms
step:801/1770 train_time:76370ms step_avg:96.55ms
step:802/1770 train_time:76469ms step_avg:96.55ms
step:803/1770 train_time:76570ms step_avg:96.56ms
step:804/1770 train_time:76670ms step_avg:96.56ms
step:805/1770 train_time:76770ms step_avg:96.57ms
step:806/1770 train_time:76870ms step_avg:96.57ms
step:807/1770 train_time:76970ms step_avg:96.57ms
step:808/1770 train_time:77069ms step_avg:96.58ms
step:809/1770 train_time:77170ms step_avg:96.58ms
step:810/1770 train_time:77269ms step_avg:96.59ms
step:811/1770 train_time:77369ms step_avg:96.59ms
step:812/1770 train_time:77468ms step_avg:96.59ms
step:813/1770 train_time:77568ms step_avg:96.60ms
step:814/1770 train_time:77668ms step_avg:96.60ms
step:815/1770 train_time:77767ms step_avg:96.61ms
step:816/1770 train_time:77867ms step_avg:96.61ms
step:817/1770 train_time:77967ms step_avg:96.61ms
step:818/1770 train_time:78067ms step_avg:96.62ms
step:819/1770 train_time:78167ms step_avg:96.62ms
step:820/1770 train_time:78266ms step_avg:96.63ms
step:821/1770 train_time:78367ms step_avg:96.63ms
step:822/1770 train_time:78467ms step_avg:96.63ms
step:823/1770 train_time:78567ms step_avg:96.64ms
step:824/1770 train_time:78666ms step_avg:96.64ms
step:825/1770 train_time:78766ms step_avg:96.65ms
step:826/1770 train_time:78866ms step_avg:96.65ms
step:827/1770 train_time:78967ms step_avg:96.65ms
step:828/1770 train_time:79066ms step_avg:96.66ms
step:829/1770 train_time:79166ms step_avg:96.66ms
step:830/1770 train_time:79266ms step_avg:96.67ms
step:831/1770 train_time:79366ms step_avg:96.67ms
step:832/1770 train_time:79466ms step_avg:96.67ms
step:833/1770 train_time:79565ms step_avg:96.68ms
step:834/1770 train_time:79665ms step_avg:96.68ms
step:835/1770 train_time:79765ms step_avg:96.68ms
step:836/1770 train_time:79865ms step_avg:96.69ms
step:837/1770 train_time:79965ms step_avg:96.69ms
step:838/1770 train_time:80064ms step_avg:96.70ms
step:839/1770 train_time:80164ms step_avg:96.70ms
step:840/1770 train_time:80264ms step_avg:96.70ms
step:841/1770 train_time:80364ms step_avg:96.71ms
step:842/1770 train_time:80464ms step_avg:96.71ms
step:843/1770 train_time:80565ms step_avg:96.72ms
step:844/1770 train_time:80664ms step_avg:96.72ms
step:845/1770 train_time:80764ms step_avg:96.72ms
step:846/1770 train_time:80864ms step_avg:96.73ms
step:847/1770 train_time:80964ms step_avg:96.73ms
step:848/1770 train_time:81064ms step_avg:96.73ms
step:849/1770 train_time:81163ms step_avg:96.74ms
step:850/1770 train_time:81263ms step_avg:96.74ms
step:851/1770 train_time:81362ms step_avg:96.74ms
step:852/1770 train_time:81462ms step_avg:96.75ms
step:853/1770 train_time:81562ms step_avg:96.75ms
step:854/1770 train_time:81662ms step_avg:96.76ms
step:855/1770 train_time:81761ms step_avg:96.76ms
step:856/1770 train_time:81861ms step_avg:96.76ms
step:857/1770 train_time:81961ms step_avg:96.77ms
step:858/1770 train_time:82060ms step_avg:96.77ms
step:859/1770 train_time:82160ms step_avg:96.77ms
step:860/1770 train_time:82259ms step_avg:96.77ms
step:861/1770 train_time:82357ms step_avg:96.78ms
step:862/1770 train_time:82456ms step_avg:96.78ms
step:863/1770 train_time:82555ms step_avg:96.78ms
step:864/1770 train_time:82653ms step_avg:96.78ms
step:865/1770 train_time:82752ms step_avg:96.79ms
step:866/1770 train_time:82852ms step_avg:96.79ms
step:867/1770 train_time:82952ms step_avg:96.79ms
step:868/1770 train_time:83051ms step_avg:96.80ms
step:869/1770 train_time:83150ms step_avg:96.80ms
step:870/1770 train_time:83249ms step_avg:96.80ms
step:871/1770 train_time:83349ms step_avg:96.80ms
step:872/1770 train_time:83449ms step_avg:96.81ms
step:873/1770 train_time:83549ms step_avg:96.81ms
step:874/1770 train_time:83649ms step_avg:96.82ms
step:875/1770 train_time:83748ms step_avg:96.82ms
step:875/1770 val_loss:3.5490 train_time:83846ms step_avg:96.93ms
step:876/1770 train_time:83868ms step_avg:96.84ms
step:877/1770 train_time:83954ms step_avg:96.83ms
step:878/1770 train_time:84053ms step_avg:96.84ms
step:879/1770 train_time:84153ms step_avg:96.84ms
step:880/1770 train_time:84251ms step_avg:96.84ms
step:881/1770 train_time:84350ms step_avg:96.84ms
step:882/1770 train_time:84449ms step_avg:96.84ms
step:883/1770 train_time:84548ms step_avg:96.85ms
step:884/1770 train_time:84647ms step_avg:96.85ms
step:885/1770 train_time:84746ms step_avg:96.85ms
step:886/1770 train_time:84846ms step_avg:96.86ms
step:887/1770 train_time:84948ms step_avg:96.86ms
step:888/1770 train_time:85048ms step_avg:96.87ms
step:889/1770 train_time:85149ms step_avg:96.87ms
step:890/1770 train_time:85249ms step_avg:96.87ms
step:891/1770 train_time:85349ms step_avg:96.88ms
step:892/1770 train_time:85448ms step_avg:96.88ms
step:893/1770 train_time:85547ms step_avg:96.88ms
step:894/1770 train_time:85646ms step_avg:96.89ms
step:895/1770 train_time:85746ms step_avg:96.89ms
step:896/1770 train_time:85845ms step_avg:96.89ms
step:897/1770 train_time:85945ms step_avg:96.89ms
step:898/1770 train_time:86046ms step_avg:96.90ms
step:899/1770 train_time:86147ms step_avg:96.90ms
step:900/1770 train_time:86247ms step_avg:96.91ms
step:901/1770 train_time:86348ms step_avg:96.91ms
step:902/1770 train_time:86448ms step_avg:96.91ms
step:903/1770 train_time:86547ms step_avg:96.92ms
step:904/1770 train_time:86647ms step_avg:96.92ms
step:905/1770 train_time:86746ms step_avg:96.92ms
step:906/1770 train_time:86846ms step_avg:96.93ms
step:907/1770 train_time:86946ms step_avg:96.93ms
step:908/1770 train_time:87046ms step_avg:96.93ms
step:909/1770 train_time:87146ms step_avg:96.94ms
step:910/1770 train_time:87247ms step_avg:96.94ms
step:911/1770 train_time:87347ms step_avg:96.94ms
step:912/1770 train_time:87448ms step_avg:96.95ms
step:913/1770 train_time:87547ms step_avg:96.95ms
step:914/1770 train_time:87647ms step_avg:96.95ms
step:915/1770 train_time:87747ms step_avg:96.96ms
step:916/1770 train_time:87847ms step_avg:96.96ms
step:917/1770 train_time:87947ms step_avg:96.96ms
step:918/1770 train_time:88047ms step_avg:96.97ms
step:919/1770 train_time:88148ms step_avg:96.97ms
step:920/1770 train_time:88250ms step_avg:96.98ms
step:921/1770 train_time:88352ms step_avg:96.98ms
step:922/1770 train_time:88453ms step_avg:96.99ms
step:923/1770 train_time:88554ms step_avg:96.99ms
step:924/1770 train_time:88654ms step_avg:97.00ms
step:925/1770 train_time:88754ms step_avg:97.00ms
step:926/1770 train_time:88855ms step_avg:97.00ms
step:927/1770 train_time:88956ms step_avg:97.01ms
step:928/1770 train_time:89055ms step_avg:97.01ms
step:929/1770 train_time:89156ms step_avg:97.01ms
step:930/1770 train_time:89256ms step_avg:97.02ms
step:931/1770 train_time:89356ms step_avg:97.02ms
step:932/1770 train_time:89456ms step_avg:97.02ms
step:933/1770 train_time:89556ms step_avg:97.03ms
step:934/1770 train_time:89656ms step_avg:97.03ms
step:935/1770 train_time:89757ms step_avg:97.03ms
step:936/1770 train_time:89857ms step_avg:97.04ms
step:937/1770 train_time:89958ms step_avg:97.04ms
step:938/1770 train_time:90059ms step_avg:97.05ms
step:939/1770 train_time:90159ms step_avg:97.05ms
step:940/1770 train_time:90260ms step_avg:97.05ms
step:941/1770 train_time:90360ms step_avg:97.06ms
step:942/1770 train_time:90462ms step_avg:97.06ms
step:943/1770 train_time:90564ms step_avg:97.07ms
step:944/1770 train_time:90665ms step_avg:97.07ms
step:945/1770 train_time:90766ms step_avg:97.08ms
step:946/1770 train_time:90869ms step_avg:97.08ms
step:947/1770 train_time:90971ms step_avg:97.09ms
step:948/1770 train_time:91072ms step_avg:97.09ms
step:949/1770 train_time:91174ms step_avg:97.10ms
step:950/1770 train_time:91275ms step_avg:97.10ms
step:951/1770 train_time:91376ms step_avg:97.11ms
step:952/1770 train_time:91476ms step_avg:97.11ms
step:953/1770 train_time:91577ms step_avg:97.11ms
step:954/1770 train_time:91677ms step_avg:97.12ms
step:955/1770 train_time:91777ms step_avg:97.12ms
step:956/1770 train_time:91877ms step_avg:97.12ms
step:957/1770 train_time:91977ms step_avg:97.13ms
step:958/1770 train_time:92078ms step_avg:97.13ms
step:959/1770 train_time:92179ms step_avg:97.13ms
step:960/1770 train_time:92279ms step_avg:97.14ms
step:961/1770 train_time:92380ms step_avg:97.14ms
step:962/1770 train_time:92481ms step_avg:97.14ms
step:963/1770 train_time:92582ms step_avg:97.15ms
step:964/1770 train_time:92682ms step_avg:97.15ms
step:965/1770 train_time:92783ms step_avg:97.15ms
step:966/1770 train_time:92884ms step_avg:97.16ms
step:967/1770 train_time:92985ms step_avg:97.16ms
step:968/1770 train_time:93086ms step_avg:97.17ms
step:969/1770 train_time:93188ms step_avg:97.17ms
step:970/1770 train_time:93289ms step_avg:97.18ms
step:971/1770 train_time:93391ms step_avg:97.18ms
step:972/1770 train_time:93492ms step_avg:97.19ms
step:973/1770 train_time:93593ms step_avg:97.19ms
step:974/1770 train_time:93694ms step_avg:97.19ms
step:975/1770 train_time:93795ms step_avg:97.20ms
step:976/1770 train_time:93896ms step_avg:97.20ms
step:977/1770 train_time:93997ms step_avg:97.20ms
step:978/1770 train_time:94097ms step_avg:97.21ms
step:979/1770 train_time:94198ms step_avg:97.21ms
step:980/1770 train_time:94298ms step_avg:97.21ms
step:981/1770 train_time:94398ms step_avg:97.22ms
step:982/1770 train_time:94498ms step_avg:97.22ms
step:983/1770 train_time:94598ms step_avg:97.22ms
step:984/1770 train_time:94699ms step_avg:97.23ms
step:985/1770 train_time:94800ms step_avg:97.23ms
step:986/1770 train_time:94901ms step_avg:97.23ms
step:987/1770 train_time:95003ms step_avg:97.24ms
step:988/1770 train_time:95105ms step_avg:97.24ms
step:989/1770 train_time:95209ms step_avg:97.25ms
step:990/1770 train_time:95310ms step_avg:97.25ms
step:991/1770 train_time:95410ms step_avg:97.26ms
step:992/1770 train_time:95512ms step_avg:97.26ms
step:993/1770 train_time:95612ms step_avg:97.27ms
step:994/1770 train_time:95713ms step_avg:97.27ms
step:995/1770 train_time:95814ms step_avg:97.27ms
step:996/1770 train_time:95914ms step_avg:97.28ms
step:997/1770 train_time:96015ms step_avg:97.28ms
step:998/1770 train_time:96116ms step_avg:97.28ms
step:999/1770 train_time:96216ms step_avg:97.29ms
step:1000/1770 train_time:96317ms step_avg:97.29ms
step:1000/1770 val_loss:3.5112 train_time:96415ms step_avg:97.39ms
step:1001/1770 train_time:96437ms step_avg:97.31ms
step:1002/1770 train_time:96530ms step_avg:97.31ms
step:1003/1770 train_time:96632ms step_avg:97.31ms
step:1004/1770 train_time:96732ms step_avg:97.32ms
step:1005/1770 train_time:96832ms step_avg:97.32ms
step:1006/1770 train_time:96932ms step_avg:97.32ms
step:1007/1770 train_time:97032ms step_avg:97.32ms
step:1008/1770 train_time:97132ms step_avg:97.33ms
step:1009/1770 train_time:97232ms step_avg:97.33ms
step:1010/1770 train_time:97331ms step_avg:97.33ms
step:1011/1770 train_time:97433ms step_avg:97.34ms
step:1012/1770 train_time:97535ms step_avg:97.34ms
step:1013/1770 train_time:97635ms step_avg:97.34ms
step:1014/1770 train_time:97736ms step_avg:97.35ms
step:1015/1770 train_time:97837ms step_avg:97.35ms
step:1016/1770 train_time:97938ms step_avg:97.35ms
step:1017/1770 train_time:98039ms step_avg:97.36ms
step:1018/1770 train_time:98142ms step_avg:97.36ms
step:1019/1770 train_time:98243ms step_avg:97.37ms
step:1020/1770 train_time:98345ms step_avg:97.37ms
step:1021/1770 train_time:98447ms step_avg:97.38ms
step:1022/1770 train_time:98548ms step_avg:97.38ms
step:1023/1770 train_time:98648ms step_avg:97.38ms
step:1024/1770 train_time:98750ms step_avg:97.39ms
step:1025/1770 train_time:98851ms step_avg:97.39ms
step:1026/1770 train_time:98952ms step_avg:97.39ms
step:1027/1770 train_time:99051ms step_avg:97.40ms
step:1028/1770 train_time:99152ms step_avg:97.40ms
step:1029/1770 train_time:99252ms step_avg:97.40ms
step:1030/1770 train_time:99353ms step_avg:97.40ms
step:1031/1770 train_time:99453ms step_avg:97.41ms
step:1032/1770 train_time:99554ms step_avg:97.41ms
step:1033/1770 train_time:99655ms step_avg:97.41ms
step:1034/1770 train_time:99756ms step_avg:97.42ms
step:1035/1770 train_time:99857ms step_avg:97.42ms
step:1036/1770 train_time:99958ms step_avg:97.42ms
step:1037/1770 train_time:100060ms step_avg:97.43ms
step:1038/1770 train_time:100162ms step_avg:97.43ms
step:1039/1770 train_time:100264ms step_avg:97.44ms
step:1040/1770 train_time:100365ms step_avg:97.44ms
step:1041/1770 train_time:100466ms step_avg:97.45ms
step:1042/1770 train_time:100567ms step_avg:97.45ms
step:1043/1770 train_time:100669ms step_avg:97.45ms
step:1044/1770 train_time:100769ms step_avg:97.46ms
step:1045/1770 train_time:100870ms step_avg:97.46ms
step:1046/1770 train_time:100971ms step_avg:97.46ms
step:1047/1770 train_time:101072ms step_avg:97.47ms
step:1048/1770 train_time:101173ms step_avg:97.47ms
step:1049/1770 train_time:101274ms step_avg:97.47ms
step:1050/1770 train_time:101374ms step_avg:97.48ms
step:1051/1770 train_time:101475ms step_avg:97.48ms
step:1052/1770 train_time:101576ms step_avg:97.48ms
step:1053/1770 train_time:101677ms step_avg:97.49ms
step:1054/1770 train_time:101780ms step_avg:97.49ms
step:1055/1770 train_time:101881ms step_avg:97.49ms
step:1056/1770 train_time:101983ms step_avg:97.50ms
step:1057/1770 train_time:102085ms step_avg:97.50ms
step:1058/1770 train_time:102187ms step_avg:97.51ms
step:1059/1770 train_time:102288ms step_avg:97.51ms
step:1060/1770 train_time:102389ms step_avg:97.51ms
step:1061/1770 train_time:102491ms step_avg:97.52ms
step:1062/1770 train_time:102591ms step_avg:97.52ms
step:1063/1770 train_time:102694ms step_avg:97.52ms
step:1064/1770 train_time:102794ms step_avg:97.53ms
step:1065/1770 train_time:102894ms step_avg:97.53ms
step:1066/1770 train_time:102995ms step_avg:97.53ms
step:1067/1770 train_time:103096ms step_avg:97.54ms
step:1068/1770 train_time:103197ms step_avg:97.54ms
step:1069/1770 train_time:103298ms step_avg:97.54ms
step:1070/1770 train_time:103401ms step_avg:97.55ms
step:1071/1770 train_time:103503ms step_avg:97.55ms
step:1072/1770 train_time:103604ms step_avg:97.56ms
step:1073/1770 train_time:103706ms step_avg:97.56ms
step:1074/1770 train_time:103808ms step_avg:97.56ms
step:1075/1770 train_time:103909ms step_avg:97.57ms
step:1076/1770 train_time:104011ms step_avg:97.57ms
step:1077/1770 train_time:104112ms step_avg:97.57ms
step:1078/1770 train_time:104212ms step_avg:97.58ms
step:1079/1770 train_time:104312ms step_avg:97.58ms
step:1080/1770 train_time:104412ms step_avg:97.58ms
step:1081/1770 train_time:104512ms step_avg:97.58ms
step:1082/1770 train_time:104612ms step_avg:97.59ms
step:1083/1770 train_time:104712ms step_avg:97.59ms
step:1084/1770 train_time:104813ms step_avg:97.59ms
step:1085/1770 train_time:104914ms step_avg:97.59ms
step:1086/1770 train_time:105014ms step_avg:97.60ms
step:1087/1770 train_time:105114ms step_avg:97.60ms
step:1088/1770 train_time:105216ms step_avg:97.60ms
step:1089/1770 train_time:105317ms step_avg:97.61ms
step:1090/1770 train_time:105418ms step_avg:97.61ms
step:1091/1770 train_time:105519ms step_avg:97.61ms
step:1092/1770 train_time:105620ms step_avg:97.62ms
step:1093/1770 train_time:105722ms step_avg:97.62ms
step:1094/1770 train_time:105823ms step_avg:97.62ms
step:1095/1770 train_time:105926ms step_avg:97.63ms
step:1096/1770 train_time:106028ms step_avg:97.63ms
step:1097/1770 train_time:106128ms step_avg:97.63ms
step:1098/1770 train_time:106229ms step_avg:97.64ms
step:1099/1770 train_time:106330ms step_avg:97.64ms
step:1100/1770 train_time:106431ms step_avg:97.64ms
step:1101/1770 train_time:106531ms step_avg:97.65ms
step:1102/1770 train_time:106632ms step_avg:97.65ms
step:1103/1770 train_time:106732ms step_avg:97.65ms
step:1104/1770 train_time:106834ms step_avg:97.65ms
step:1105/1770 train_time:106934ms step_avg:97.66ms
step:1106/1770 train_time:107034ms step_avg:97.66ms
step:1107/1770 train_time:107134ms step_avg:97.66ms
step:1108/1770 train_time:107236ms step_avg:97.66ms
step:1109/1770 train_time:107336ms step_avg:97.67ms
step:1110/1770 train_time:107437ms step_avg:97.67ms
step:1111/1770 train_time:107538ms step_avg:97.67ms
step:1112/1770 train_time:107639ms step_avg:97.68ms
step:1113/1770 train_time:107741ms step_avg:97.68ms
step:1114/1770 train_time:107843ms step_avg:97.68ms
step:1115/1770 train_time:107945ms step_avg:97.69ms
step:1116/1770 train_time:108047ms step_avg:97.69ms
step:1117/1770 train_time:108149ms step_avg:97.70ms
step:1118/1770 train_time:108250ms step_avg:97.70ms
step:1119/1770 train_time:108352ms step_avg:97.70ms
step:1120/1770 train_time:108452ms step_avg:97.70ms
step:1121/1770 train_time:108552ms step_avg:97.71ms
step:1122/1770 train_time:108652ms step_avg:97.71ms
step:1123/1770 train_time:108753ms step_avg:97.71ms
step:1124/1770 train_time:108854ms step_avg:97.71ms
step:1125/1770 train_time:108955ms step_avg:97.72ms
step:1125/1770 val_loss:3.4703 train_time:109054ms step_avg:97.81ms
step:1126/1770 train_time:109076ms step_avg:97.74ms
step:1127/1770 train_time:109168ms step_avg:97.73ms
step:1128/1770 train_time:109269ms step_avg:97.74ms
step:1129/1770 train_time:109369ms step_avg:97.74ms
step:1130/1770 train_time:109470ms step_avg:97.74ms
step:1131/1770 train_time:109571ms step_avg:97.74ms
step:1132/1770 train_time:109672ms step_avg:97.75ms
step:1133/1770 train_time:109772ms step_avg:97.75ms
step:1134/1770 train_time:109873ms step_avg:97.75ms
step:1135/1770 train_time:109974ms step_avg:97.75ms
step:1136/1770 train_time:110076ms step_avg:97.76ms
step:1137/1770 train_time:110179ms step_avg:97.76ms
step:1138/1770 train_time:110281ms step_avg:97.77ms
step:1139/1770 train_time:110383ms step_avg:97.77ms
step:1140/1770 train_time:110484ms step_avg:97.77ms
step:1141/1770 train_time:110584ms step_avg:97.78ms
step:1142/1770 train_time:110685ms step_avg:97.78ms
step:1143/1770 train_time:110785ms step_avg:97.78ms
step:1144/1770 train_time:110886ms step_avg:97.78ms
step:1145/1770 train_time:110987ms step_avg:97.79ms
step:1146/1770 train_time:111089ms step_avg:97.79ms
step:1147/1770 train_time:111190ms step_avg:97.79ms
step:1148/1770 train_time:111291ms step_avg:97.80ms
step:1149/1770 train_time:111392ms step_avg:97.80ms
step:1150/1770 train_time:111492ms step_avg:97.80ms
step:1151/1770 train_time:111594ms step_avg:97.80ms
step:1152/1770 train_time:111696ms step_avg:97.81ms
step:1153/1770 train_time:111798ms step_avg:97.81ms
step:1154/1770 train_time:111900ms step_avg:97.81ms
step:1155/1770 train_time:112002ms step_avg:97.82ms
step:1156/1770 train_time:112103ms step_avg:97.82ms
step:1157/1770 train_time:112207ms step_avg:97.83ms
step:1158/1770 train_time:112308ms step_avg:97.83ms
step:1159/1770 train_time:112409ms step_avg:97.83ms
step:1160/1770 train_time:112510ms step_avg:97.83ms
step:1161/1770 train_time:112610ms step_avg:97.84ms
step:1162/1770 train_time:112711ms step_avg:97.84ms
step:1163/1770 train_time:112811ms step_avg:97.84ms
step:1164/1770 train_time:112912ms step_avg:97.84ms
step:1165/1770 train_time:113012ms step_avg:97.85ms
step:1166/1770 train_time:113114ms step_avg:97.85ms
step:1167/1770 train_time:113216ms step_avg:97.85ms
step:1168/1770 train_time:113319ms step_avg:97.86ms
step:1169/1770 train_time:113420ms step_avg:97.86ms
step:1170/1770 train_time:113521ms step_avg:97.86ms
step:1171/1770 train_time:113623ms step_avg:97.87ms
step:1172/1770 train_time:113724ms step_avg:97.87ms
step:1173/1770 train_time:113825ms step_avg:97.87ms
step:1174/1770 train_time:113927ms step_avg:97.88ms
step:1175/1770 train_time:114028ms step_avg:97.88ms
step:1176/1770 train_time:114128ms step_avg:97.88ms
step:1177/1770 train_time:114229ms step_avg:97.88ms
step:1178/1770 train_time:114330ms step_avg:97.88ms
step:1179/1770 train_time:114430ms step_avg:97.89ms
step:1180/1770 train_time:114530ms step_avg:97.89ms
step:1181/1770 train_time:114632ms step_avg:97.89ms
step:1182/1770 train_time:114732ms step_avg:97.89ms
step:1183/1770 train_time:114834ms step_avg:97.90ms
step:1184/1770 train_time:114937ms step_avg:97.90ms
step:1185/1770 train_time:115040ms step_avg:97.91ms
step:1186/1770 train_time:115143ms step_avg:97.91ms
step:1187/1770 train_time:115248ms step_avg:97.92ms
step:1188/1770 train_time:115350ms step_avg:97.92ms
step:1189/1770 train_time:115451ms step_avg:97.92ms
step:1190/1770 train_time:115552ms step_avg:97.93ms
step:1191/1770 train_time:115655ms step_avg:97.93ms
step:1192/1770 train_time:115757ms step_avg:97.93ms
step:1193/1770 train_time:115860ms step_avg:97.94ms
step:1194/1770 train_time:115963ms step_avg:97.94ms
step:1195/1770 train_time:116066ms step_avg:97.95ms
step:1196/1770 train_time:116170ms step_avg:97.95ms
step:1197/1770 train_time:116271ms step_avg:97.95ms
step:1198/1770 train_time:116373ms step_avg:97.96ms
step:1199/1770 train_time:116475ms step_avg:97.96ms
step:1200/1770 train_time:116577ms step_avg:97.96ms
step:1201/1770 train_time:116680ms step_avg:97.97ms
step:1202/1770 train_time:116782ms step_avg:97.97ms
step:1203/1770 train_time:116884ms step_avg:97.98ms
step:1204/1770 train_time:116987ms step_avg:97.98ms
step:1205/1770 train_time:117089ms step_avg:97.98ms
step:1206/1770 train_time:117192ms step_avg:97.99ms
step:1207/1770 train_time:117293ms step_avg:97.99ms
step:1208/1770 train_time:117394ms step_avg:97.99ms
step:1209/1770 train_time:117496ms step_avg:98.00ms
step:1210/1770 train_time:117598ms step_avg:98.00ms
step:1211/1770 train_time:117701ms step_avg:98.00ms
step:1212/1770 train_time:117805ms step_avg:98.01ms
step:1213/1770 train_time:117907ms step_avg:98.01ms
step:1214/1770 train_time:118009ms step_avg:98.01ms
step:1215/1770 train_time:118111ms step_avg:98.02ms
step:1216/1770 train_time:118215ms step_avg:98.02ms
step:1217/1770 train_time:118317ms step_avg:98.03ms
step:1218/1770 train_time:118419ms step_avg:98.03ms
step:1219/1770 train_time:118521ms step_avg:98.03ms
step:1220/1770 train_time:118624ms step_avg:98.04ms
step:1221/1770 train_time:118726ms step_avg:98.04ms
step:1222/1770 train_time:118829ms step_avg:98.04ms
step:1223/1770 train_time:118930ms step_avg:98.05ms
step:1224/1770 train_time:119034ms step_avg:98.05ms
step:1225/1770 train_time:119137ms step_avg:98.05ms
step:1226/1770 train_time:119239ms step_avg:98.06ms
step:1227/1770 train_time:119345ms step_avg:98.06ms
step:1228/1770 train_time:119448ms step_avg:98.07ms
step:1229/1770 train_time:119550ms step_avg:98.07ms
step:1230/1770 train_time:119652ms step_avg:98.08ms
step:1231/1770 train_time:119754ms step_avg:98.08ms
step:1232/1770 train_time:119856ms step_avg:98.08ms
step:1233/1770 train_time:119958ms step_avg:98.08ms
step:1234/1770 train_time:120061ms step_avg:98.09ms
step:1235/1770 train_time:120163ms step_avg:98.09ms
step:1236/1770 train_time:120266ms step_avg:98.10ms
step:1237/1770 train_time:120368ms step_avg:98.10ms
step:1238/1770 train_time:120470ms step_avg:98.10ms
step:1239/1770 train_time:120572ms step_avg:98.11ms
step:1240/1770 train_time:120674ms step_avg:98.11ms
step:1241/1770 train_time:120777ms step_avg:98.11ms
step:1242/1770 train_time:120880ms step_avg:98.12ms
step:1243/1770 train_time:120983ms step_avg:98.12ms
step:1244/1770 train_time:121085ms step_avg:98.12ms
step:1245/1770 train_time:121186ms step_avg:98.13ms
step:1246/1770 train_time:121289ms step_avg:98.13ms
step:1247/1770 train_time:121390ms step_avg:98.13ms
step:1248/1770 train_time:121493ms step_avg:98.14ms
step:1249/1770 train_time:121594ms step_avg:98.14ms
step:1250/1770 train_time:121696ms step_avg:98.14ms
step:1250/1770 val_loss:3.4233 train_time:121798ms step_avg:98.22ms
step:1251/1770 train_time:121820ms step_avg:98.16ms
step:1252/1770 train_time:121907ms step_avg:98.15ms
step:1253/1770 train_time:122010ms step_avg:98.16ms
step:1254/1770 train_time:122112ms step_avg:98.16ms
step:1255/1770 train_time:122216ms step_avg:98.17ms
step:1256/1770 train_time:122318ms step_avg:98.17ms
step:1257/1770 train_time:122420ms step_avg:98.17ms
step:1258/1770 train_time:122523ms step_avg:98.18ms
step:1259/1770 train_time:122625ms step_avg:98.18ms
step:1260/1770 train_time:122727ms step_avg:98.18ms
step:1261/1770 train_time:122830ms step_avg:98.19ms
step:1262/1770 train_time:122934ms step_avg:98.19ms
step:1263/1770 train_time:123035ms step_avg:98.19ms
step:1264/1770 train_time:123139ms step_avg:98.20ms
step:1265/1770 train_time:123242ms step_avg:98.20ms
step:1266/1770 train_time:123344ms step_avg:98.20ms
step:1267/1770 train_time:123447ms step_avg:98.21ms
step:1268/1770 train_time:123550ms step_avg:98.21ms
step:1269/1770 train_time:123651ms step_avg:98.21ms
step:1270/1770 train_time:123753ms step_avg:98.22ms
step:1271/1770 train_time:123856ms step_avg:98.22ms
step:1272/1770 train_time:123958ms step_avg:98.22ms
step:1273/1770 train_time:124062ms step_avg:98.23ms
step:1274/1770 train_time:124164ms step_avg:98.23ms
step:1275/1770 train_time:124266ms step_avg:98.23ms
step:1276/1770 train_time:124368ms step_avg:98.24ms
step:1277/1770 train_time:124469ms step_avg:98.24ms
step:1278/1770 train_time:124572ms step_avg:98.24ms
step:1279/1770 train_time:124675ms step_avg:98.25ms
step:1280/1770 train_time:124778ms step_avg:98.25ms
step:1281/1770 train_time:124880ms step_avg:98.25ms
step:1282/1770 train_time:124983ms step_avg:98.26ms
step:1283/1770 train_time:125086ms step_avg:98.26ms
step:1284/1770 train_time:125189ms step_avg:98.26ms
step:1285/1770 train_time:125291ms step_avg:98.27ms
step:1286/1770 train_time:125394ms step_avg:98.27ms
step:1287/1770 train_time:125497ms step_avg:98.28ms
step:1288/1770 train_time:125600ms step_avg:98.28ms
step:1289/1770 train_time:125703ms step_avg:98.28ms
step:1290/1770 train_time:125804ms step_avg:98.28ms
step:1291/1770 train_time:125906ms step_avg:98.29ms
step:1292/1770 train_time:126008ms step_avg:98.29ms
step:1293/1770 train_time:126110ms step_avg:98.29ms
step:1294/1770 train_time:126211ms step_avg:98.30ms
step:1295/1770 train_time:126314ms step_avg:98.30ms
step:1296/1770 train_time:126416ms step_avg:98.30ms
step:1297/1770 train_time:126518ms step_avg:98.30ms
step:1298/1770 train_time:126621ms step_avg:98.31ms
step:1299/1770 train_time:126723ms step_avg:98.31ms
step:1300/1770 train_time:126825ms step_avg:98.31ms
step:1301/1770 train_time:126928ms step_avg:98.32ms
step:1302/1770 train_time:127030ms step_avg:98.32ms
step:1303/1770 train_time:127132ms step_avg:98.32ms
step:1304/1770 train_time:127234ms step_avg:98.33ms
step:1305/1770 train_time:127336ms step_avg:98.33ms
step:1306/1770 train_time:127438ms step_avg:98.33ms
step:1307/1770 train_time:127541ms step_avg:98.34ms
step:1308/1770 train_time:127643ms step_avg:98.34ms
step:1309/1770 train_time:127746ms step_avg:98.34ms
step:1310/1770 train_time:127848ms step_avg:98.34ms
step:1311/1770 train_time:127949ms step_avg:98.35ms
step:1312/1770 train_time:128051ms step_avg:98.35ms
step:1313/1770 train_time:128153ms step_avg:98.35ms
step:1314/1770 train_time:128256ms step_avg:98.36ms
step:1315/1770 train_time:128358ms step_avg:98.36ms
step:1316/1770 train_time:128461ms step_avg:98.36ms
step:1317/1770 train_time:128564ms step_avg:98.37ms
step:1318/1770 train_time:128670ms step_avg:98.37ms
step:1319/1770 train_time:128773ms step_avg:98.38ms
step:1320/1770 train_time:128875ms step_avg:98.38ms
step:1321/1770 train_time:128978ms step_avg:98.38ms
step:1322/1770 train_time:129081ms step_avg:98.39ms
step:1323/1770 train_time:129185ms step_avg:98.39ms
step:1324/1770 train_time:129288ms step_avg:98.39ms
step:1325/1770 train_time:129391ms step_avg:98.40ms
step:1326/1770 train_time:129493ms step_avg:98.40ms
step:1327/1770 train_time:129598ms step_avg:98.40ms
step:1328/1770 train_time:129700ms step_avg:98.41ms
step:1329/1770 train_time:129803ms step_avg:98.41ms
step:1330/1770 train_time:129904ms step_avg:98.41ms
step:1331/1770 train_time:130006ms step_avg:98.41ms
step:1332/1770 train_time:130108ms step_avg:98.42ms
step:1333/1770 train_time:130210ms step_avg:98.42ms
step:1334/1770 train_time:130312ms step_avg:98.42ms
step:1335/1770 train_time:130413ms step_avg:98.42ms
step:1336/1770 train_time:130515ms step_avg:98.43ms
step:1337/1770 train_time:130617ms step_avg:98.43ms
step:1338/1770 train_time:130720ms step_avg:98.43ms
step:1339/1770 train_time:130824ms step_avg:98.44ms
step:1340/1770 train_time:130928ms step_avg:98.44ms
step:1341/1770 train_time:131030ms step_avg:98.44ms
step:1342/1770 train_time:131133ms step_avg:98.45ms
step:1343/1770 train_time:131236ms step_avg:98.45ms
step:1344/1770 train_time:131340ms step_avg:98.46ms
step:1345/1770 train_time:131442ms step_avg:98.46ms
step:1346/1770 train_time:131543ms step_avg:98.46ms
step:1347/1770 train_time:131645ms step_avg:98.46ms
step:1348/1770 train_time:131750ms step_avg:98.47ms
step:1349/1770 train_time:131852ms step_avg:98.47ms
step:1350/1770 train_time:131955ms step_avg:98.47ms
step:1351/1770 train_time:132057ms step_avg:98.48ms
step:1352/1770 train_time:132160ms step_avg:98.48ms
step:1353/1770 train_time:132263ms step_avg:98.48ms
step:1354/1770 train_time:132365ms step_avg:98.49ms
step:1355/1770 train_time:132467ms step_avg:98.49ms
step:1356/1770 train_time:132568ms step_avg:98.49ms
step:1357/1770 train_time:132670ms step_avg:98.49ms
step:1358/1770 train_time:132773ms step_avg:98.50ms
step:1359/1770 train_time:132875ms step_avg:98.50ms
step:1360/1770 train_time:132977ms step_avg:98.50ms
step:1361/1770 train_time:133080ms step_avg:98.50ms
step:1362/1770 train_time:133183ms step_avg:98.51ms
step:1363/1770 train_time:133287ms step_avg:98.51ms
step:1364/1770 train_time:133389ms step_avg:98.51ms
step:1365/1770 train_time:133490ms step_avg:98.52ms
step:1366/1770 train_time:133592ms step_avg:98.52ms
step:1367/1770 train_time:133695ms step_avg:98.52ms
step:1368/1770 train_time:133797ms step_avg:98.53ms
step:1369/1770 train_time:133900ms step_avg:98.53ms
step:1370/1770 train_time:134003ms step_avg:98.53ms
step:1371/1770 train_time:134105ms step_avg:98.53ms
step:1372/1770 train_time:134207ms step_avg:98.54ms
step:1373/1770 train_time:134310ms step_avg:98.54ms
step:1374/1770 train_time:134413ms step_avg:98.54ms
step:1375/1770 train_time:134515ms step_avg:98.55ms
step:1375/1770 val_loss:3.3794 train_time:134617ms step_avg:98.62ms
step:1376/1770 train_time:134638ms step_avg:98.56ms
step:1377/1770 train_time:134730ms step_avg:98.56ms
step:1378/1770 train_time:134831ms step_avg:98.56ms
step:1379/1770 train_time:134933ms step_avg:98.56ms
step:1380/1770 train_time:135035ms step_avg:98.57ms
step:1381/1770 train_time:135137ms step_avg:98.57ms
step:1382/1770 train_time:135239ms step_avg:98.57ms
step:1383/1770 train_time:135341ms step_avg:98.57ms
step:1384/1770 train_time:135443ms step_avg:98.58ms
step:1385/1770 train_time:135545ms step_avg:98.58ms
step:1386/1770 train_time:135648ms step_avg:98.58ms
step:1387/1770 train_time:135750ms step_avg:98.58ms
step:1388/1770 train_time:135852ms step_avg:98.59ms
step:1389/1770 train_time:135955ms step_avg:98.59ms
step:1390/1770 train_time:136057ms step_avg:98.59ms
step:1391/1770 train_time:136159ms step_avg:98.59ms
step:1392/1770 train_time:136262ms step_avg:98.60ms
step:1393/1770 train_time:136364ms step_avg:98.60ms
step:1394/1770 train_time:136466ms step_avg:98.60ms
step:1395/1770 train_time:136568ms step_avg:98.61ms
step:1396/1770 train_time:136672ms step_avg:98.61ms
step:1397/1770 train_time:136774ms step_avg:98.61ms
step:1398/1770 train_time:136878ms step_avg:98.62ms
step:1399/1770 train_time:136981ms step_avg:98.62ms
step:1400/1770 train_time:137083ms step_avg:98.62ms
step:1401/1770 train_time:137186ms step_avg:98.62ms
step:1402/1770 train_time:137288ms step_avg:98.63ms
step:1403/1770 train_time:137390ms step_avg:98.63ms
step:1404/1770 train_time:137493ms step_avg:98.63ms
step:1405/1770 train_time:137596ms step_avg:98.63ms
step:1406/1770 train_time:137698ms step_avg:98.64ms
step:1407/1770 train_time:137800ms step_avg:98.64ms
step:1408/1770 train_time:137903ms step_avg:98.64ms
step:1409/1770 train_time:138005ms step_avg:98.65ms
step:1410/1770 train_time:138107ms step_avg:98.65ms
step:1411/1770 train_time:138210ms step_avg:98.65ms
step:1412/1770 train_time:138312ms step_avg:98.65ms
step:1413/1770 train_time:138413ms step_avg:98.66ms
step:1414/1770 train_time:138517ms step_avg:98.66ms
step:1415/1770 train_time:138620ms step_avg:98.66ms
step:1416/1770 train_time:138723ms step_avg:98.67ms
step:1417/1770 train_time:138825ms step_avg:98.67ms
step:1418/1770 train_time:138927ms step_avg:98.67ms
step:1419/1770 train_time:139029ms step_avg:98.67ms
step:1420/1770 train_time:139130ms step_avg:98.67ms
step:1421/1770 train_time:139232ms step_avg:98.68ms
step:1422/1770 train_time:139335ms step_avg:98.68ms
step:1423/1770 train_time:139437ms step_avg:98.68ms
step:1424/1770 train_time:139541ms step_avg:98.69ms
step:1425/1770 train_time:139643ms step_avg:98.69ms
step:1426/1770 train_time:139745ms step_avg:98.69ms
step:1427/1770 train_time:139847ms step_avg:98.69ms
step:1428/1770 train_time:139951ms step_avg:98.70ms
step:1429/1770 train_time:140053ms step_avg:98.70ms
step:1430/1770 train_time:140155ms step_avg:98.70ms
step:1431/1770 train_time:140259ms step_avg:98.70ms
step:1432/1770 train_time:140360ms step_avg:98.71ms
step:1433/1770 train_time:140462ms step_avg:98.71ms
step:1434/1770 train_time:140563ms step_avg:98.71ms
step:1435/1770 train_time:140665ms step_avg:98.71ms
step:1436/1770 train_time:140769ms step_avg:98.72ms
step:1437/1770 train_time:140871ms step_avg:98.72ms
step:1438/1770 train_time:140973ms step_avg:98.72ms
step:1439/1770 train_time:141076ms step_avg:98.72ms
step:1440/1770 train_time:141177ms step_avg:98.73ms
step:1441/1770 train_time:141283ms step_avg:98.73ms
step:1442/1770 train_time:141386ms step_avg:98.73ms
step:1443/1770 train_time:141488ms step_avg:98.74ms
step:1444/1770 train_time:141591ms step_avg:98.74ms
step:1445/1770 train_time:141694ms step_avg:98.74ms
step:1446/1770 train_time:141798ms step_avg:98.74ms
step:1447/1770 train_time:141901ms step_avg:98.75ms
step:1448/1770 train_time:142004ms step_avg:98.75ms
step:1449/1770 train_time:142108ms step_avg:98.75ms
step:1450/1770 train_time:142211ms step_avg:98.76ms
step:1451/1770 train_time:142314ms step_avg:98.76ms
step:1452/1770 train_time:142418ms step_avg:98.76ms
step:1453/1770 train_time:142521ms step_avg:98.77ms
step:1454/1770 train_time:142624ms step_avg:98.77ms
step:1455/1770 train_time:142729ms step_avg:98.77ms
step:1456/1770 train_time:142834ms step_avg:98.78ms
step:1457/1770 train_time:142939ms step_avg:98.78ms
step:1458/1770 train_time:143043ms step_avg:98.79ms
step:1459/1770 train_time:143147ms step_avg:98.79ms
step:1460/1770 train_time:143250ms step_avg:98.79ms
step:1461/1770 train_time:143353ms step_avg:98.80ms
step:1462/1770 train_time:143456ms step_avg:98.80ms
step:1463/1770 train_time:143561ms step_avg:98.80ms
step:1464/1770 train_time:143666ms step_avg:98.81ms
step:1465/1770 train_time:143769ms step_avg:98.81ms
step:1466/1770 train_time:143873ms step_avg:98.81ms
step:1467/1770 train_time:143979ms step_avg:98.82ms
step:1468/1770 train_time:144082ms step_avg:98.82ms
step:1469/1770 train_time:144186ms step_avg:98.82ms
step:1470/1770 train_time:144288ms step_avg:98.83ms
step:1471/1770 train_time:144390ms step_avg:98.83ms
step:1472/1770 train_time:144493ms step_avg:98.83ms
step:1473/1770 train_time:144597ms step_avg:98.84ms
step:1474/1770 train_time:144702ms step_avg:98.84ms
step:1475/1770 train_time:144805ms step_avg:98.84ms
step:1476/1770 train_time:144908ms step_avg:98.85ms
step:1477/1770 train_time:145013ms step_avg:98.85ms
step:1478/1770 train_time:145117ms step_avg:98.85ms
step:1479/1770 train_time:145221ms step_avg:98.86ms
step:1480/1770 train_time:145324ms step_avg:98.86ms
step:1481/1770 train_time:145431ms step_avg:98.87ms
step:1482/1770 train_time:145534ms step_avg:98.87ms
step:1483/1770 train_time:145637ms step_avg:98.87ms
step:1484/1770 train_time:145741ms step_avg:98.87ms
step:1485/1770 train_time:145845ms step_avg:98.88ms
step:1486/1770 train_time:145947ms step_avg:98.88ms
step:1487/1770 train_time:146051ms step_avg:98.88ms
step:1488/1770 train_time:146155ms step_avg:98.89ms
step:1489/1770 train_time:146260ms step_avg:98.89ms
step:1490/1770 train_time:146363ms step_avg:98.89ms
step:1491/1770 train_time:146466ms step_avg:98.90ms
step:1492/1770 train_time:146570ms step_avg:98.90ms
step:1493/1770 train_time:146675ms step_avg:98.90ms
step:1494/1770 train_time:146782ms step_avg:98.91ms
step:1495/1770 train_time:146884ms step_avg:98.91ms
step:1496/1770 train_time:146987ms step_avg:98.91ms
step:1497/1770 train_time:147090ms step_avg:98.92ms
step:1498/1770 train_time:147193ms step_avg:98.92ms
step:1499/1770 train_time:147297ms step_avg:98.92ms
step:1500/1770 train_time:147400ms step_avg:98.93ms
step:1500/1770 val_loss:3.3412 train_time:147502ms step_avg:98.99ms
step:1501/1770 train_time:147523ms step_avg:98.94ms
step:1502/1770 train_time:147611ms step_avg:98.93ms
step:1503/1770 train_time:147713ms step_avg:98.94ms
step:1504/1770 train_time:147816ms step_avg:98.94ms
step:1505/1770 train_time:147921ms step_avg:98.94ms
step:1506/1770 train_time:148024ms step_avg:98.95ms
step:1507/1770 train_time:148128ms step_avg:98.95ms
step:1508/1770 train_time:148233ms step_avg:98.95ms
step:1509/1770 train_time:148336ms step_avg:98.96ms
step:1510/1770 train_time:148438ms step_avg:98.96ms
step:1511/1770 train_time:148543ms step_avg:98.96ms
step:1512/1770 train_time:148649ms step_avg:98.97ms
step:1513/1770 train_time:148754ms step_avg:98.97ms
step:1514/1770 train_time:148857ms step_avg:98.97ms
step:1515/1770 train_time:148960ms step_avg:98.98ms
step:1516/1770 train_time:149064ms step_avg:98.98ms
step:1517/1770 train_time:149167ms step_avg:98.98ms
step:1518/1770 train_time:149272ms step_avg:98.99ms
step:1519/1770 train_time:149374ms step_avg:98.99ms
step:1520/1770 train_time:149478ms step_avg:98.99ms
step:1521/1770 train_time:149581ms step_avg:98.99ms
step:1522/1770 train_time:149686ms step_avg:99.00ms
step:1523/1770 train_time:149791ms step_avg:99.00ms
step:1524/1770 train_time:149894ms step_avg:99.01ms
step:1525/1770 train_time:149997ms step_avg:99.01ms
step:1526/1770 train_time:150099ms step_avg:99.01ms
step:1527/1770 train_time:150202ms step_avg:99.01ms
step:1528/1770 train_time:150308ms step_avg:99.02ms
step:1529/1770 train_time:150411ms step_avg:99.02ms
step:1530/1770 train_time:150514ms step_avg:99.02ms
step:1531/1770 train_time:150617ms step_avg:99.02ms
step:1532/1770 train_time:150721ms step_avg:99.03ms
step:1533/1770 train_time:150825ms step_avg:99.03ms
step:1534/1770 train_time:150930ms step_avg:99.04ms
step:1535/1770 train_time:151033ms step_avg:99.04ms
step:1536/1770 train_time:151136ms step_avg:99.04ms
step:1537/1770 train_time:151239ms step_avg:99.04ms
step:1538/1770 train_time:151343ms step_avg:99.05ms
step:1539/1770 train_time:151448ms step_avg:99.05ms
step:1540/1770 train_time:151554ms step_avg:99.05ms
step:1541/1770 train_time:151659ms step_avg:99.06ms
step:1542/1770 train_time:151762ms step_avg:99.06ms
step:1543/1770 train_time:151865ms step_avg:99.06ms
step:1544/1770 train_time:151970ms step_avg:99.07ms
step:1545/1770 train_time:152075ms step_avg:99.07ms
step:1546/1770 train_time:152178ms step_avg:99.07ms
step:1547/1770 train_time:152281ms step_avg:99.08ms
step:1548/1770 train_time:152384ms step_avg:99.08ms
step:1549/1770 train_time:152489ms step_avg:99.08ms
step:1550/1770 train_time:152593ms step_avg:99.09ms
step:1551/1770 train_time:152696ms step_avg:99.09ms
step:1552/1770 train_time:152801ms step_avg:99.09ms
step:1553/1770 train_time:152905ms step_avg:99.10ms
step:1554/1770 train_time:153010ms step_avg:99.10ms
step:1555/1770 train_time:153114ms step_avg:99.10ms
step:1556/1770 train_time:153216ms step_avg:99.10ms
step:1557/1770 train_time:153319ms step_avg:99.11ms
step:1558/1770 train_time:153423ms step_avg:99.11ms
step:1559/1770 train_time:153528ms step_avg:99.11ms
step:1560/1770 train_time:153631ms step_avg:99.12ms
step:1561/1770 train_time:153737ms step_avg:99.12ms
step:1562/1770 train_time:153840ms step_avg:99.12ms
step:1563/1770 train_time:153944ms step_avg:99.13ms
step:1564/1770 train_time:154046ms step_avg:99.13ms
step:1565/1770 train_time:154150ms step_avg:99.13ms
step:1566/1770 train_time:154253ms step_avg:99.13ms
step:1567/1770 train_time:154357ms step_avg:99.14ms
step:1568/1770 train_time:154459ms step_avg:99.14ms
step:1569/1770 train_time:154566ms step_avg:99.14ms
step:1570/1770 train_time:154670ms step_avg:99.15ms
step:1571/1770 train_time:154773ms step_avg:99.15ms
step:1572/1770 train_time:154877ms step_avg:99.15ms
step:1573/1770 train_time:154982ms step_avg:99.16ms
step:1574/1770 train_time:155085ms step_avg:99.16ms
step:1575/1770 train_time:155188ms step_avg:99.16ms
step:1576/1770 train_time:155291ms step_avg:99.16ms
step:1577/1770 train_time:155396ms step_avg:99.17ms
step:1578/1770 train_time:155501ms step_avg:99.17ms
step:1579/1770 train_time:155604ms step_avg:99.17ms
step:1580/1770 train_time:155709ms step_avg:99.18ms
step:1581/1770 train_time:155815ms step_avg:99.18ms
step:1582/1770 train_time:155920ms step_avg:99.19ms
step:1583/1770 train_time:156024ms step_avg:99.19ms
step:1584/1770 train_time:156128ms step_avg:99.19ms
step:1585/1770 train_time:156233ms step_avg:99.20ms
step:1586/1770 train_time:156339ms step_avg:99.20ms
step:1587/1770 train_time:156443ms step_avg:99.20ms
step:1588/1770 train_time:156547ms step_avg:99.21ms
step:1589/1770 train_time:156653ms step_avg:99.21ms
step:1590/1770 train_time:156755ms step_avg:99.21ms
step:1591/1770 train_time:156858ms step_avg:99.21ms
step:1592/1770 train_time:156963ms step_avg:99.22ms
step:1593/1770 train_time:157066ms step_avg:99.22ms
step:1594/1770 train_time:157169ms step_avg:99.22ms
step:1595/1770 train_time:157273ms step_avg:99.23ms
step:1596/1770 train_time:157377ms step_avg:99.23ms
step:1597/1770 train_time:157480ms step_avg:99.23ms
step:1598/1770 train_time:157584ms step_avg:99.23ms
step:1599/1770 train_time:157690ms step_avg:99.24ms
step:1600/1770 train_time:157796ms step_avg:99.24ms
step:1601/1770 train_time:157899ms step_avg:99.25ms
step:1602/1770 train_time:158004ms step_avg:99.25ms
step:1603/1770 train_time:158107ms step_avg:99.25ms
step:1604/1770 train_time:158210ms step_avg:99.25ms
step:1605/1770 train_time:158312ms step_avg:99.26ms
step:1606/1770 train_time:158416ms step_avg:99.26ms
step:1607/1770 train_time:158523ms step_avg:99.26ms
step:1608/1770 train_time:158626ms step_avg:99.27ms
step:1609/1770 train_time:158730ms step_avg:99.27ms
step:1610/1770 train_time:158835ms step_avg:99.27ms
step:1611/1770 train_time:158940ms step_avg:99.28ms
step:1612/1770 train_time:159045ms step_avg:99.28ms
step:1613/1770 train_time:159148ms step_avg:99.28ms
step:1614/1770 train_time:159251ms step_avg:99.28ms
step:1615/1770 train_time:159355ms step_avg:99.29ms
step:1616/1770 train_time:159458ms step_avg:99.29ms
step:1617/1770 train_time:159566ms step_avg:99.29ms
step:1618/1770 train_time:159669ms step_avg:99.30ms
step:1619/1770 train_time:159773ms step_avg:99.30ms
step:1620/1770 train_time:159877ms step_avg:99.30ms
step:1621/1770 train_time:159980ms step_avg:99.30ms
step:1622/1770 train_time:160085ms step_avg:99.31ms
step:1623/1770 train_time:160193ms step_avg:99.31ms
step:1624/1770 train_time:160296ms step_avg:99.32ms
step:1625/1770 train_time:160398ms step_avg:99.32ms
step:1625/1770 val_loss:3.3069 train_time:160500ms step_avg:99.38ms
step:1626/1770 train_time:160521ms step_avg:99.33ms
step:1627/1770 train_time:160611ms step_avg:99.33ms
step:1628/1770 train_time:160714ms step_avg:99.33ms
step:1629/1770 train_time:160817ms step_avg:99.33ms
step:1630/1770 train_time:160920ms step_avg:99.33ms
step:1631/1770 train_time:161023ms step_avg:99.34ms
step:1632/1770 train_time:161126ms step_avg:99.34ms
step:1633/1770 train_time:161229ms step_avg:99.34ms
step:1634/1770 train_time:161332ms step_avg:99.34ms
step:1635/1770 train_time:161436ms step_avg:99.34ms
step:1636/1770 train_time:161540ms step_avg:99.35ms
step:1637/1770 train_time:161645ms step_avg:99.35ms
step:1638/1770 train_time:161748ms step_avg:99.35ms
step:1639/1770 train_time:161852ms step_avg:99.36ms
step:1640/1770 train_time:161956ms step_avg:99.36ms
step:1641/1770 train_time:162060ms step_avg:99.36ms
step:1642/1770 train_time:162163ms step_avg:99.36ms
step:1643/1770 train_time:162266ms step_avg:99.37ms
step:1644/1770 train_time:162371ms step_avg:99.37ms
step:1645/1770 train_time:162473ms step_avg:99.37ms
step:1646/1770 train_time:162579ms step_avg:99.38ms
step:1647/1770 train_time:162683ms step_avg:99.38ms
step:1648/1770 train_time:162785ms step_avg:99.38ms
step:1649/1770 train_time:162888ms step_avg:99.38ms
step:1650/1770 train_time:162992ms step_avg:99.39ms
step:1651/1770 train_time:163096ms step_avg:99.39ms
step:1652/1770 train_time:163199ms step_avg:99.39ms
step:1653/1770 train_time:163303ms step_avg:99.39ms
step:1654/1770 train_time:163409ms step_avg:99.40ms
step:1655/1770 train_time:163515ms step_avg:99.40ms
step:1656/1770 train_time:163619ms step_avg:99.40ms
step:1657/1770 train_time:163724ms step_avg:99.41ms
step:1658/1770 train_time:163827ms step_avg:99.41ms
step:1659/1770 train_time:163931ms step_avg:99.41ms
step:1660/1770 train_time:164036ms step_avg:99.42ms
step:1661/1770 train_time:164141ms step_avg:99.42ms
step:1662/1770 train_time:164245ms step_avg:99.42ms
step:1663/1770 train_time:164347ms step_avg:99.42ms
step:1664/1770 train_time:164450ms step_avg:99.43ms
step:1665/1770 train_time:164554ms step_avg:99.43ms
step:1666/1770 train_time:164658ms step_avg:99.43ms
step:1667/1770 train_time:164761ms step_avg:99.43ms
step:1668/1770 train_time:164863ms step_avg:99.44ms
step:1669/1770 train_time:164966ms step_avg:99.44ms
step:1670/1770 train_time:165069ms step_avg:99.44ms
step:1671/1770 train_time:165173ms step_avg:99.44ms
step:1672/1770 train_time:165279ms step_avg:99.45ms
step:1673/1770 train_time:165384ms step_avg:99.45ms
step:1674/1770 train_time:165486ms step_avg:99.45ms
step:1675/1770 train_time:165590ms step_avg:99.45ms
step:1676/1770 train_time:165694ms step_avg:99.46ms
step:1677/1770 train_time:165803ms step_avg:99.46ms
step:1678/1770 train_time:165905ms step_avg:99.46ms
step:1679/1770 train_time:166008ms step_avg:99.47ms
step:1680/1770 train_time:166112ms step_avg:99.47ms
step:1681/1770 train_time:166217ms step_avg:99.47ms
step:1682/1770 train_time:166323ms step_avg:99.48ms
step:1683/1770 train_time:166426ms step_avg:99.48ms
step:1684/1770 train_time:166528ms step_avg:99.48ms
step:1685/1770 train_time:166633ms step_avg:99.48ms
step:1686/1770 train_time:166739ms step_avg:99.49ms
step:1687/1770 train_time:166844ms step_avg:99.49ms
step:1688/1770 train_time:166947ms step_avg:99.49ms
step:1689/1770 train_time:167050ms step_avg:99.49ms
step:1690/1770 train_time:167154ms step_avg:99.50ms
step:1691/1770 train_time:167258ms step_avg:99.50ms
step:1692/1770 train_time:167361ms step_avg:99.50ms
step:1693/1770 train_time:167466ms step_avg:99.50ms
step:1694/1770 train_time:167569ms step_avg:99.51ms
step:1695/1770 train_time:167673ms step_avg:99.51ms
step:1696/1770 train_time:167778ms step_avg:99.51ms
step:1697/1770 train_time:167883ms step_avg:99.52ms
step:1698/1770 train_time:167987ms step_avg:99.52ms
step:1699/1770 train_time:168091ms step_avg:99.52ms
step:1700/1770 train_time:168195ms step_avg:99.52ms
step:1701/1770 train_time:168298ms step_avg:99.53ms
step:1702/1770 train_time:168401ms step_avg:99.53ms
step:1703/1770 train_time:168504ms step_avg:99.53ms
step:1704/1770 train_time:168607ms step_avg:99.53ms
step:1705/1770 train_time:168710ms step_avg:99.53ms
step:1706/1770 train_time:168814ms step_avg:99.54ms
step:1707/1770 train_time:168919ms step_avg:99.54ms
step:1708/1770 train_time:169023ms step_avg:99.54ms
step:1709/1770 train_time:169127ms step_avg:99.54ms
step:1710/1770 train_time:169234ms step_avg:99.55ms
step:1711/1770 train_time:169341ms step_avg:99.55ms
step:1712/1770 train_time:169445ms step_avg:99.56ms
step:1713/1770 train_time:169549ms step_avg:99.56ms
step:1714/1770 train_time:169653ms step_avg:99.56ms
step:1715/1770 train_time:169757ms step_avg:99.56ms
step:1716/1770 train_time:169861ms step_avg:99.57ms
step:1717/1770 train_time:169964ms step_avg:99.57ms
step:1718/1770 train_time:170069ms step_avg:99.57ms
step:1719/1770 train_time:170175ms step_avg:99.58ms
step:1720/1770 train_time:170280ms step_avg:99.58ms
step:1721/1770 train_time:170383ms step_avg:99.58ms
step:1722/1770 train_time:170490ms step_avg:99.59ms
step:1723/1770 train_time:170595ms step_avg:99.59ms
step:1724/1770 train_time:170702ms step_avg:99.59ms
step:1725/1770 train_time:170809ms step_avg:99.60ms
step:1726/1770 train_time:170916ms step_avg:99.60ms
step:1727/1770 train_time:171020ms step_avg:99.60ms
step:1728/1770 train_time:171126ms step_avg:99.61ms
step:1729/1770 train_time:171230ms step_avg:99.61ms
step:1730/1770 train_time:171336ms step_avg:99.61ms
step:1731/1770 train_time:171442ms step_avg:99.62ms
step:1732/1770 train_time:171545ms step_avg:99.62ms
step:1733/1770 train_time:171650ms step_avg:99.62ms
step:1734/1770 train_time:171755ms step_avg:99.63ms
step:1735/1770 train_time:171860ms step_avg:99.63ms
step:1736/1770 train_time:171963ms step_avg:99.63ms
step:1737/1770 train_time:172068ms step_avg:99.63ms
step:1738/1770 train_time:172173ms step_avg:99.64ms
step:1739/1770 train_time:172277ms step_avg:99.64ms
step:1740/1770 train_time:172381ms step_avg:99.64ms
step:1741/1770 train_time:172488ms step_avg:99.65ms
step:1742/1770 train_time:172595ms step_avg:99.65ms
step:1743/1770 train_time:172700ms step_avg:99.65ms
step:1744/1770 train_time:172804ms step_avg:99.66ms
step:1745/1770 train_time:172908ms step_avg:99.66ms
step:1746/1770 train_time:173015ms step_avg:99.66ms
step:1747/1770 train_time:173118ms step_avg:99.66ms
step:1748/1770 train_time:173224ms step_avg:99.67ms
step:1749/1770 train_time:173330ms step_avg:99.67ms
step:1750/1770 train_time:173434ms step_avg:99.67ms
step:1750/1770 val_loss:3.2803 train_time:173538ms step_avg:99.73ms
step:1751/1770 train_time:173559ms step_avg:99.69ms
step:1752/1770 train_time:173649ms step_avg:99.68ms
step:1753/1770 train_time:173754ms step_avg:99.69ms
step:1754/1770 train_time:173859ms step_avg:99.69ms
step:1755/1770 train_time:173963ms step_avg:99.69ms
step:1756/1770 train_time:174067ms step_avg:99.69ms
step:1757/1770 train_time:174172ms step_avg:99.70ms
step:1758/1770 train_time:174276ms step_avg:99.70ms
step:1759/1770 train_time:174380ms step_avg:99.70ms
step:1760/1770 train_time:174485ms step_avg:99.71ms
step:1761/1770 train_time:174592ms step_avg:99.71ms
step:1762/1770 train_time:174700ms step_avg:99.71ms
step:1763/1770 train_time:174802ms step_avg:99.72ms
step:1764/1770 train_time:174907ms step_avg:99.72ms
step:1765/1770 train_time:175012ms step_avg:99.72ms
step:1766/1770 train_time:175120ms step_avg:99.73ms
step:1767/1770 train_time:175223ms step_avg:99.73ms
step:1768/1770 train_time:175328ms step_avg:99.73ms
step:1769/1770 train_time:175431ms step_avg:99.73ms
step:1770/1770 train_time:175534ms step_avg:99.74ms
step:1770/1770 val_loss:3.2773 train_time:175640ms step_avg:99.80ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
