import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 08:13:38 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23622ms step_avg:nanms
step:2/1770 train_time:24051ms step_avg:nanms
step:3/1770 train_time:24148ms step_avg:nanms
step:4/1770 train_time:24241ms step_avg:nanms
step:5/1770 train_time:24334ms step_avg:nanms
step:6/1770 train_time:24428ms step_avg:nanms
step:7/1770 train_time:24522ms step_avg:nanms
step:8/1770 train_time:24615ms step_avg:nanms
step:9/1770 train_time:24709ms step_avg:nanms
step:10/1770 train_time:24803ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:93.88ms
step:14/1770 train_time:377ms step_avg:94.13ms
step:15/1770 train_time:471ms step_avg:94.11ms
step:16/1770 train_time:564ms step_avg:94.07ms
step:17/1770 train_time:658ms step_avg:94.01ms
step:18/1770 train_time:752ms step_avg:94.01ms
step:19/1770 train_time:846ms step_avg:93.96ms
step:20/1770 train_time:940ms step_avg:93.99ms
step:21/1770 train_time:1034ms step_avg:94.03ms
step:22/1770 train_time:1128ms step_avg:94.03ms
step:23/1770 train_time:1222ms step_avg:94.01ms
step:24/1770 train_time:1316ms step_avg:94.03ms
step:25/1770 train_time:1411ms step_avg:94.06ms
step:26/1770 train_time:1505ms step_avg:94.06ms
step:27/1770 train_time:1599ms step_avg:94.07ms
step:28/1770 train_time:1693ms step_avg:94.08ms
step:29/1770 train_time:1787ms step_avg:94.05ms
step:30/1770 train_time:1881ms step_avg:94.03ms
step:31/1770 train_time:1974ms step_avg:94.02ms
step:32/1770 train_time:2069ms step_avg:94.03ms
step:33/1770 train_time:2162ms step_avg:94.00ms
step:34/1770 train_time:2256ms step_avg:94.01ms
step:35/1770 train_time:2350ms step_avg:94.00ms
step:36/1770 train_time:2444ms step_avg:94.00ms
step:37/1770 train_time:2538ms step_avg:94.01ms
step:38/1770 train_time:2633ms step_avg:94.05ms
step:39/1770 train_time:2727ms step_avg:94.04ms
step:40/1770 train_time:2821ms step_avg:94.04ms
step:41/1770 train_time:2915ms step_avg:94.04ms
step:42/1770 train_time:3009ms step_avg:94.03ms
step:43/1770 train_time:3103ms step_avg:94.02ms
step:44/1770 train_time:3197ms step_avg:94.04ms
step:45/1770 train_time:3292ms step_avg:94.06ms
step:46/1770 train_time:3386ms step_avg:94.06ms
step:47/1770 train_time:3480ms step_avg:94.04ms
step:48/1770 train_time:3574ms step_avg:94.05ms
step:49/1770 train_time:3667ms step_avg:94.04ms
step:50/1770 train_time:3761ms step_avg:94.03ms
step:51/1770 train_time:3856ms step_avg:94.04ms
step:52/1770 train_time:3949ms step_avg:94.03ms
step:53/1770 train_time:4043ms step_avg:94.03ms
step:54/1770 train_time:4137ms step_avg:94.03ms
step:55/1770 train_time:4232ms step_avg:94.04ms
step:56/1770 train_time:4326ms step_avg:94.03ms
step:57/1770 train_time:4420ms step_avg:94.04ms
step:58/1770 train_time:4514ms step_avg:94.04ms
step:59/1770 train_time:4607ms step_avg:94.03ms
step:60/1770 train_time:4702ms step_avg:94.03ms
step:61/1770 train_time:4796ms step_avg:94.05ms
step:62/1770 train_time:4890ms step_avg:94.04ms
step:63/1770 train_time:4984ms step_avg:94.04ms
step:64/1770 train_time:5078ms step_avg:94.03ms
step:65/1770 train_time:5172ms step_avg:94.03ms
step:66/1770 train_time:5266ms step_avg:94.03ms
step:67/1770 train_time:5360ms step_avg:94.03ms
step:68/1770 train_time:5454ms step_avg:94.04ms
step:69/1770 train_time:5548ms step_avg:94.03ms
step:70/1770 train_time:5641ms step_avg:94.02ms
step:71/1770 train_time:5736ms step_avg:94.03ms
step:72/1770 train_time:5830ms step_avg:94.03ms
step:73/1770 train_time:5924ms step_avg:94.02ms
step:74/1770 train_time:6018ms step_avg:94.04ms
step:75/1770 train_time:6112ms step_avg:94.04ms
step:76/1770 train_time:6206ms step_avg:94.04ms
step:77/1770 train_time:6300ms step_avg:94.03ms
step:78/1770 train_time:6395ms step_avg:94.04ms
step:79/1770 train_time:6489ms step_avg:94.04ms
step:80/1770 train_time:6583ms step_avg:94.04ms
step:81/1770 train_time:6677ms step_avg:94.04ms
step:82/1770 train_time:6771ms step_avg:94.04ms
step:83/1770 train_time:6865ms step_avg:94.04ms
step:84/1770 train_time:6959ms step_avg:94.04ms
step:85/1770 train_time:7053ms step_avg:94.05ms
step:86/1770 train_time:7147ms step_avg:94.05ms
step:87/1770 train_time:7241ms step_avg:94.04ms
step:88/1770 train_time:7336ms step_avg:94.05ms
step:89/1770 train_time:7430ms step_avg:94.05ms
step:90/1770 train_time:7524ms step_avg:94.05ms
step:91/1770 train_time:7618ms step_avg:94.05ms
step:92/1770 train_time:7712ms step_avg:94.05ms
step:93/1770 train_time:7806ms step_avg:94.05ms
step:94/1770 train_time:7900ms step_avg:94.05ms
step:95/1770 train_time:7994ms step_avg:94.05ms
step:96/1770 train_time:8089ms step_avg:94.05ms
step:97/1770 train_time:8183ms step_avg:94.05ms
step:98/1770 train_time:8277ms step_avg:94.06ms
step:99/1770 train_time:8372ms step_avg:94.06ms
step:100/1770 train_time:8466ms step_avg:94.06ms
step:101/1770 train_time:8559ms step_avg:94.06ms
step:102/1770 train_time:8654ms step_avg:94.06ms
step:103/1770 train_time:8748ms step_avg:94.06ms
step:104/1770 train_time:8842ms step_avg:94.06ms
step:105/1770 train_time:8936ms step_avg:94.06ms
step:106/1770 train_time:9030ms step_avg:94.06ms
step:107/1770 train_time:9124ms step_avg:94.06ms
step:108/1770 train_time:9219ms step_avg:94.07ms
step:109/1770 train_time:9313ms step_avg:94.07ms
step:110/1770 train_time:9407ms step_avg:94.07ms
step:111/1770 train_time:9501ms step_avg:94.07ms
step:112/1770 train_time:9595ms step_avg:94.07ms
step:113/1770 train_time:9689ms step_avg:94.07ms
step:114/1770 train_time:9783ms step_avg:94.06ms
step:115/1770 train_time:9877ms step_avg:94.07ms
step:116/1770 train_time:9971ms step_avg:94.07ms
step:117/1770 train_time:10065ms step_avg:94.06ms
step:118/1770 train_time:10159ms step_avg:94.07ms
step:119/1770 train_time:10254ms step_avg:94.07ms
step:120/1770 train_time:10347ms step_avg:94.07ms
step:121/1770 train_time:10441ms step_avg:94.06ms
step:122/1770 train_time:10535ms step_avg:94.07ms
step:123/1770 train_time:10630ms step_avg:94.07ms
step:124/1770 train_time:10724ms step_avg:94.07ms
step:125/1770 train_time:10818ms step_avg:94.07ms
step:125/1770 val_loss:4.6622 train_time:10911ms step_avg:94.88ms
step:126/1770 train_time:10934ms step_avg:94.26ms
step:127/1770 train_time:11009ms step_avg:94.10ms
step:128/1770 train_time:11106ms step_avg:94.12ms
step:129/1770 train_time:11202ms step_avg:94.14ms
step:130/1770 train_time:11297ms step_avg:94.14ms
step:131/1770 train_time:11391ms step_avg:94.14ms
step:132/1770 train_time:11485ms step_avg:94.14ms
step:133/1770 train_time:11579ms step_avg:94.14ms
step:134/1770 train_time:11673ms step_avg:94.14ms
step:135/1770 train_time:11768ms step_avg:94.14ms
step:136/1770 train_time:11862ms step_avg:94.14ms
step:137/1770 train_time:11957ms step_avg:94.15ms
step:138/1770 train_time:12052ms step_avg:94.16ms
step:139/1770 train_time:12148ms step_avg:94.17ms
step:140/1770 train_time:12242ms step_avg:94.17ms
step:141/1770 train_time:12336ms step_avg:94.17ms
step:142/1770 train_time:12432ms step_avg:94.18ms
step:143/1770 train_time:12526ms step_avg:94.18ms
step:144/1770 train_time:12620ms step_avg:94.18ms
step:145/1770 train_time:12715ms step_avg:94.18ms
step:146/1770 train_time:12809ms step_avg:94.18ms
step:147/1770 train_time:12903ms step_avg:94.18ms
step:148/1770 train_time:12997ms step_avg:94.18ms
step:149/1770 train_time:13092ms step_avg:94.19ms
step:150/1770 train_time:13187ms step_avg:94.19ms
step:151/1770 train_time:13281ms step_avg:94.19ms
step:152/1770 train_time:13376ms step_avg:94.20ms
step:153/1770 train_time:13471ms step_avg:94.20ms
step:154/1770 train_time:13566ms step_avg:94.21ms
step:155/1770 train_time:13660ms step_avg:94.21ms
step:156/1770 train_time:13755ms step_avg:94.21ms
step:157/1770 train_time:13849ms step_avg:94.21ms
step:158/1770 train_time:13944ms step_avg:94.21ms
step:159/1770 train_time:14038ms step_avg:94.21ms
step:160/1770 train_time:14133ms step_avg:94.22ms
step:161/1770 train_time:14228ms step_avg:94.22ms
step:162/1770 train_time:14322ms step_avg:94.22ms
step:163/1770 train_time:14417ms step_avg:94.23ms
step:164/1770 train_time:14512ms step_avg:94.23ms
step:165/1770 train_time:14606ms step_avg:94.23ms
step:166/1770 train_time:14701ms step_avg:94.23ms
step:167/1770 train_time:14795ms step_avg:94.24ms
step:168/1770 train_time:14891ms step_avg:94.25ms
step:169/1770 train_time:14985ms step_avg:94.25ms
step:170/1770 train_time:15079ms step_avg:94.25ms
step:171/1770 train_time:15174ms step_avg:94.25ms
step:172/1770 train_time:15269ms step_avg:94.25ms
step:173/1770 train_time:15363ms step_avg:94.25ms
step:174/1770 train_time:15457ms step_avg:94.25ms
step:175/1770 train_time:15553ms step_avg:94.26ms
step:176/1770 train_time:15648ms step_avg:94.27ms
step:177/1770 train_time:15742ms step_avg:94.27ms
step:178/1770 train_time:15837ms step_avg:94.27ms
step:179/1770 train_time:15932ms step_avg:94.27ms
step:180/1770 train_time:16027ms step_avg:94.27ms
step:181/1770 train_time:16121ms step_avg:94.27ms
step:182/1770 train_time:16215ms step_avg:94.27ms
step:183/1770 train_time:16310ms step_avg:94.28ms
step:184/1770 train_time:16405ms step_avg:94.28ms
step:185/1770 train_time:16499ms step_avg:94.28ms
step:186/1770 train_time:16594ms step_avg:94.29ms
step:187/1770 train_time:16690ms step_avg:94.29ms
step:188/1770 train_time:16784ms step_avg:94.29ms
step:189/1770 train_time:16878ms step_avg:94.29ms
step:190/1770 train_time:16973ms step_avg:94.30ms
step:191/1770 train_time:17068ms step_avg:94.30ms
step:192/1770 train_time:17163ms step_avg:94.30ms
step:193/1770 train_time:17257ms step_avg:94.30ms
step:194/1770 train_time:17352ms step_avg:94.30ms
step:195/1770 train_time:17447ms step_avg:94.31ms
step:196/1770 train_time:17541ms step_avg:94.31ms
step:197/1770 train_time:17635ms step_avg:94.31ms
step:198/1770 train_time:17730ms step_avg:94.31ms
step:199/1770 train_time:17825ms step_avg:94.31ms
step:200/1770 train_time:17919ms step_avg:94.31ms
step:201/1770 train_time:18014ms step_avg:94.31ms
step:202/1770 train_time:18109ms step_avg:94.32ms
step:203/1770 train_time:18203ms step_avg:94.32ms
step:204/1770 train_time:18297ms step_avg:94.32ms
step:205/1770 train_time:18392ms step_avg:94.32ms
step:206/1770 train_time:18487ms step_avg:94.32ms
step:207/1770 train_time:18581ms step_avg:94.32ms
step:208/1770 train_time:18676ms step_avg:94.32ms
step:209/1770 train_time:18771ms step_avg:94.33ms
step:210/1770 train_time:18865ms step_avg:94.33ms
step:211/1770 train_time:18960ms step_avg:94.33ms
step:212/1770 train_time:19055ms step_avg:94.33ms
step:213/1770 train_time:19149ms step_avg:94.33ms
step:214/1770 train_time:19243ms step_avg:94.33ms
step:215/1770 train_time:19338ms step_avg:94.33ms
step:216/1770 train_time:19433ms step_avg:94.33ms
step:217/1770 train_time:19527ms step_avg:94.34ms
step:218/1770 train_time:19622ms step_avg:94.34ms
step:219/1770 train_time:19717ms step_avg:94.34ms
step:220/1770 train_time:19812ms step_avg:94.34ms
step:221/1770 train_time:19906ms step_avg:94.34ms
step:222/1770 train_time:20001ms step_avg:94.34ms
step:223/1770 train_time:20095ms step_avg:94.34ms
step:224/1770 train_time:20189ms step_avg:94.34ms
step:225/1770 train_time:20284ms step_avg:94.34ms
step:226/1770 train_time:20378ms step_avg:94.34ms
step:227/1770 train_time:20473ms step_avg:94.35ms
step:228/1770 train_time:20567ms step_avg:94.35ms
step:229/1770 train_time:20661ms step_avg:94.34ms
step:230/1770 train_time:20756ms step_avg:94.35ms
step:231/1770 train_time:20850ms step_avg:94.35ms
step:232/1770 train_time:20945ms step_avg:94.35ms
step:233/1770 train_time:21039ms step_avg:94.35ms
step:234/1770 train_time:21134ms step_avg:94.35ms
step:235/1770 train_time:21229ms step_avg:94.35ms
step:236/1770 train_time:21323ms step_avg:94.35ms
step:237/1770 train_time:21417ms step_avg:94.35ms
step:238/1770 train_time:21513ms step_avg:94.35ms
step:239/1770 train_time:21608ms step_avg:94.36ms
step:240/1770 train_time:21702ms step_avg:94.36ms
step:241/1770 train_time:21797ms step_avg:94.36ms
step:242/1770 train_time:21892ms step_avg:94.36ms
step:243/1770 train_time:21986ms step_avg:94.36ms
step:244/1770 train_time:22081ms step_avg:94.36ms
step:245/1770 train_time:22176ms step_avg:94.36ms
step:246/1770 train_time:22270ms step_avg:94.36ms
step:247/1770 train_time:22364ms step_avg:94.36ms
step:248/1770 train_time:22458ms step_avg:94.36ms
step:249/1770 train_time:22553ms step_avg:94.36ms
step:250/1770 train_time:22648ms step_avg:94.36ms
step:250/1770 val_loss:4.1073 train_time:22740ms step_avg:94.75ms
step:251/1770 train_time:22762ms step_avg:94.45ms
step:252/1770 train_time:22846ms step_avg:94.40ms
step:253/1770 train_time:22944ms step_avg:94.42ms
step:254/1770 train_time:23039ms step_avg:94.42ms
step:255/1770 train_time:23134ms step_avg:94.42ms
step:256/1770 train_time:23228ms step_avg:94.42ms
step:257/1770 train_time:23323ms step_avg:94.42ms
step:258/1770 train_time:23418ms step_avg:94.43ms
step:259/1770 train_time:23512ms step_avg:94.43ms
step:260/1770 train_time:23606ms step_avg:94.42ms
step:261/1770 train_time:23701ms step_avg:94.43ms
step:262/1770 train_time:23799ms step_avg:94.44ms
step:263/1770 train_time:23895ms step_avg:94.45ms
step:264/1770 train_time:23990ms step_avg:94.45ms
step:265/1770 train_time:24085ms step_avg:94.45ms
step:266/1770 train_time:24180ms step_avg:94.45ms
step:267/1770 train_time:24275ms step_avg:94.46ms
step:268/1770 train_time:24370ms step_avg:94.46ms
step:269/1770 train_time:24464ms step_avg:94.46ms
step:270/1770 train_time:24559ms step_avg:94.46ms
step:271/1770 train_time:24654ms step_avg:94.46ms
step:272/1770 train_time:24749ms step_avg:94.46ms
step:273/1770 train_time:24844ms step_avg:94.46ms
step:274/1770 train_time:24940ms step_avg:94.47ms
step:275/1770 train_time:25036ms step_avg:94.47ms
step:276/1770 train_time:25131ms step_avg:94.48ms
step:277/1770 train_time:25225ms step_avg:94.48ms
step:278/1770 train_time:25321ms step_avg:94.48ms
step:279/1770 train_time:25416ms step_avg:94.48ms
step:280/1770 train_time:25511ms step_avg:94.48ms
step:281/1770 train_time:25606ms step_avg:94.49ms
step:282/1770 train_time:25701ms step_avg:94.49ms
step:283/1770 train_time:25796ms step_avg:94.49ms
step:284/1770 train_time:25891ms step_avg:94.49ms
step:285/1770 train_time:25986ms step_avg:94.49ms
step:286/1770 train_time:26081ms step_avg:94.50ms
step:287/1770 train_time:26177ms step_avg:94.50ms
step:288/1770 train_time:26273ms step_avg:94.51ms
step:289/1770 train_time:26367ms step_avg:94.51ms
step:290/1770 train_time:26462ms step_avg:94.51ms
step:291/1770 train_time:26558ms step_avg:94.51ms
step:292/1770 train_time:26653ms step_avg:94.52ms
step:293/1770 train_time:26748ms step_avg:94.52ms
step:294/1770 train_time:26843ms step_avg:94.52ms
step:295/1770 train_time:26939ms step_avg:94.52ms
step:296/1770 train_time:27035ms step_avg:94.53ms
step:297/1770 train_time:27129ms step_avg:94.53ms
step:298/1770 train_time:27225ms step_avg:94.53ms
step:299/1770 train_time:27320ms step_avg:94.53ms
step:300/1770 train_time:27416ms step_avg:94.54ms
step:301/1770 train_time:27510ms step_avg:94.54ms
step:302/1770 train_time:27605ms step_avg:94.54ms
step:303/1770 train_time:27700ms step_avg:94.54ms
step:304/1770 train_time:27796ms step_avg:94.54ms
step:305/1770 train_time:27890ms step_avg:94.54ms
step:306/1770 train_time:27985ms step_avg:94.54ms
step:307/1770 train_time:28081ms step_avg:94.55ms
step:308/1770 train_time:28176ms step_avg:94.55ms
step:309/1770 train_time:28272ms step_avg:94.55ms
step:310/1770 train_time:28366ms step_avg:94.55ms
step:311/1770 train_time:28462ms step_avg:94.56ms
step:312/1770 train_time:28557ms step_avg:94.56ms
step:313/1770 train_time:28652ms step_avg:94.56ms
step:314/1770 train_time:28747ms step_avg:94.56ms
step:315/1770 train_time:28842ms step_avg:94.56ms
step:316/1770 train_time:28937ms step_avg:94.57ms
step:317/1770 train_time:29033ms step_avg:94.57ms
step:318/1770 train_time:29127ms step_avg:94.57ms
step:319/1770 train_time:29222ms step_avg:94.57ms
step:320/1770 train_time:29318ms step_avg:94.57ms
step:321/1770 train_time:29413ms step_avg:94.57ms
step:322/1770 train_time:29507ms step_avg:94.57ms
step:323/1770 train_time:29602ms step_avg:94.58ms
step:324/1770 train_time:29698ms step_avg:94.58ms
step:325/1770 train_time:29793ms step_avg:94.58ms
step:326/1770 train_time:29888ms step_avg:94.58ms
step:327/1770 train_time:29984ms step_avg:94.59ms
step:328/1770 train_time:30079ms step_avg:94.59ms
step:329/1770 train_time:30174ms step_avg:94.59ms
step:330/1770 train_time:30269ms step_avg:94.59ms
step:331/1770 train_time:30364ms step_avg:94.59ms
step:332/1770 train_time:30460ms step_avg:94.59ms
step:333/1770 train_time:30555ms step_avg:94.60ms
step:334/1770 train_time:30650ms step_avg:94.60ms
step:335/1770 train_time:30744ms step_avg:94.60ms
step:336/1770 train_time:30840ms step_avg:94.60ms
step:337/1770 train_time:30935ms step_avg:94.60ms
step:338/1770 train_time:31030ms step_avg:94.60ms
step:339/1770 train_time:31125ms step_avg:94.61ms
step:340/1770 train_time:31221ms step_avg:94.61ms
step:341/1770 train_time:31317ms step_avg:94.61ms
step:342/1770 train_time:31411ms step_avg:94.61ms
step:343/1770 train_time:31505ms step_avg:94.61ms
step:344/1770 train_time:31601ms step_avg:94.61ms
step:345/1770 train_time:31696ms step_avg:94.61ms
step:346/1770 train_time:31791ms step_avg:94.62ms
step:347/1770 train_time:31885ms step_avg:94.61ms
step:348/1770 train_time:31980ms step_avg:94.62ms
step:349/1770 train_time:32076ms step_avg:94.62ms
step:350/1770 train_time:32170ms step_avg:94.62ms
step:351/1770 train_time:32265ms step_avg:94.62ms
step:352/1770 train_time:32361ms step_avg:94.62ms
step:353/1770 train_time:32456ms step_avg:94.63ms
step:354/1770 train_time:32552ms step_avg:94.63ms
step:355/1770 train_time:32647ms step_avg:94.63ms
step:356/1770 train_time:32742ms step_avg:94.63ms
step:357/1770 train_time:32838ms step_avg:94.63ms
step:358/1770 train_time:32933ms step_avg:94.64ms
step:359/1770 train_time:33027ms step_avg:94.63ms
step:360/1770 train_time:33123ms step_avg:94.64ms
step:361/1770 train_time:33217ms step_avg:94.64ms
step:362/1770 train_time:33312ms step_avg:94.64ms
step:363/1770 train_time:33406ms step_avg:94.64ms
step:364/1770 train_time:33502ms step_avg:94.64ms
step:365/1770 train_time:33597ms step_avg:94.64ms
step:366/1770 train_time:33692ms step_avg:94.64ms
step:367/1770 train_time:33787ms step_avg:94.64ms
step:368/1770 train_time:33882ms step_avg:94.64ms
step:369/1770 train_time:33978ms step_avg:94.65ms
step:370/1770 train_time:34072ms step_avg:94.65ms
step:371/1770 train_time:34167ms step_avg:94.64ms
step:372/1770 train_time:34262ms step_avg:94.65ms
step:373/1770 train_time:34358ms step_avg:94.65ms
step:374/1770 train_time:34453ms step_avg:94.65ms
step:375/1770 train_time:34547ms step_avg:94.65ms
step:375/1770 val_loss:3.8990 train_time:34640ms step_avg:94.91ms
step:376/1770 train_time:34662ms step_avg:94.70ms
step:377/1770 train_time:34745ms step_avg:94.67ms
step:378/1770 train_time:34842ms step_avg:94.68ms
step:379/1770 train_time:34938ms step_avg:94.68ms
step:380/1770 train_time:35033ms step_avg:94.68ms
step:381/1770 train_time:35127ms step_avg:94.68ms
step:382/1770 train_time:35222ms step_avg:94.68ms
step:383/1770 train_time:35317ms step_avg:94.68ms
step:384/1770 train_time:35411ms step_avg:94.68ms
step:385/1770 train_time:35505ms step_avg:94.68ms
step:386/1770 train_time:35600ms step_avg:94.68ms
step:387/1770 train_time:35696ms step_avg:94.68ms
step:388/1770 train_time:35792ms step_avg:94.69ms
step:389/1770 train_time:35887ms step_avg:94.69ms
step:390/1770 train_time:35983ms step_avg:94.69ms
step:391/1770 train_time:36078ms step_avg:94.69ms
step:392/1770 train_time:36173ms step_avg:94.69ms
step:393/1770 train_time:36267ms step_avg:94.69ms
step:394/1770 train_time:36362ms step_avg:94.69ms
step:395/1770 train_time:36457ms step_avg:94.69ms
step:396/1770 train_time:36554ms step_avg:94.70ms
step:397/1770 train_time:36650ms step_avg:94.70ms
step:398/1770 train_time:36747ms step_avg:94.71ms
step:399/1770 train_time:36844ms step_avg:94.71ms
step:400/1770 train_time:36941ms step_avg:94.72ms
step:401/1770 train_time:37038ms step_avg:94.73ms
step:402/1770 train_time:37136ms step_avg:94.73ms
step:403/1770 train_time:37232ms step_avg:94.74ms
step:404/1770 train_time:37329ms step_avg:94.74ms
step:405/1770 train_time:37426ms step_avg:94.75ms
step:406/1770 train_time:37523ms step_avg:94.76ms
step:407/1770 train_time:37621ms step_avg:94.76ms
step:408/1770 train_time:37718ms step_avg:94.77ms
step:409/1770 train_time:37815ms step_avg:94.77ms
step:410/1770 train_time:37911ms step_avg:94.78ms
step:411/1770 train_time:38008ms step_avg:94.78ms
step:412/1770 train_time:38105ms step_avg:94.79ms
step:413/1770 train_time:38202ms step_avg:94.79ms
step:414/1770 train_time:38299ms step_avg:94.80ms
step:415/1770 train_time:38396ms step_avg:94.81ms
step:416/1770 train_time:38493ms step_avg:94.81ms
step:417/1770 train_time:38589ms step_avg:94.81ms
step:418/1770 train_time:38686ms step_avg:94.82ms
step:419/1770 train_time:38782ms step_avg:94.82ms
step:420/1770 train_time:38880ms step_avg:94.83ms
step:421/1770 train_time:38977ms step_avg:94.83ms
step:422/1770 train_time:39074ms step_avg:94.84ms
step:423/1770 train_time:39170ms step_avg:94.84ms
step:424/1770 train_time:39267ms step_avg:94.85ms
step:425/1770 train_time:39364ms step_avg:94.85ms
step:426/1770 train_time:39462ms step_avg:94.86ms
step:427/1770 train_time:39559ms step_avg:94.87ms
step:428/1770 train_time:39657ms step_avg:94.87ms
step:429/1770 train_time:39753ms step_avg:94.88ms
step:430/1770 train_time:39850ms step_avg:94.88ms
step:431/1770 train_time:39947ms step_avg:94.89ms
step:432/1770 train_time:40044ms step_avg:94.89ms
step:433/1770 train_time:40142ms step_avg:94.90ms
step:434/1770 train_time:40239ms step_avg:94.90ms
step:435/1770 train_time:40336ms step_avg:94.91ms
step:436/1770 train_time:40433ms step_avg:94.91ms
step:437/1770 train_time:40529ms step_avg:94.92ms
step:438/1770 train_time:40626ms step_avg:94.92ms
step:439/1770 train_time:40723ms step_avg:94.93ms
step:440/1770 train_time:40821ms step_avg:94.93ms
step:441/1770 train_time:40918ms step_avg:94.94ms
step:442/1770 train_time:41015ms step_avg:94.94ms
step:443/1770 train_time:41112ms step_avg:94.95ms
step:444/1770 train_time:41208ms step_avg:94.95ms
step:445/1770 train_time:41305ms step_avg:94.96ms
step:446/1770 train_time:41403ms step_avg:94.96ms
step:447/1770 train_time:41500ms step_avg:94.97ms
step:448/1770 train_time:41597ms step_avg:94.97ms
step:449/1770 train_time:41694ms step_avg:94.98ms
step:450/1770 train_time:41791ms step_avg:94.98ms
step:451/1770 train_time:41887ms step_avg:94.98ms
step:452/1770 train_time:41984ms step_avg:94.99ms
step:453/1770 train_time:42081ms step_avg:94.99ms
step:454/1770 train_time:42178ms step_avg:95.00ms
step:455/1770 train_time:42275ms step_avg:95.00ms
step:456/1770 train_time:42372ms step_avg:95.00ms
step:457/1770 train_time:42469ms step_avg:95.01ms
step:458/1770 train_time:42566ms step_avg:95.01ms
step:459/1770 train_time:42663ms step_avg:95.02ms
step:460/1770 train_time:42760ms step_avg:95.02ms
step:461/1770 train_time:42858ms step_avg:95.03ms
step:462/1770 train_time:42955ms step_avg:95.03ms
step:463/1770 train_time:43052ms step_avg:95.04ms
step:464/1770 train_time:43148ms step_avg:95.04ms
step:465/1770 train_time:43246ms step_avg:95.05ms
step:466/1770 train_time:43343ms step_avg:95.05ms
step:467/1770 train_time:43440ms step_avg:95.05ms
step:468/1770 train_time:43537ms step_avg:95.06ms
step:469/1770 train_time:43634ms step_avg:95.06ms
step:470/1770 train_time:43730ms step_avg:95.07ms
step:471/1770 train_time:43827ms step_avg:95.07ms
step:472/1770 train_time:43924ms step_avg:95.07ms
step:473/1770 train_time:44022ms step_avg:95.08ms
step:474/1770 train_time:44119ms step_avg:95.08ms
step:475/1770 train_time:44217ms step_avg:95.09ms
step:476/1770 train_time:44314ms step_avg:95.09ms
step:477/1770 train_time:44410ms step_avg:95.10ms
step:478/1770 train_time:44507ms step_avg:95.10ms
step:479/1770 train_time:44604ms step_avg:95.10ms
step:480/1770 train_time:44701ms step_avg:95.11ms
step:481/1770 train_time:44799ms step_avg:95.11ms
step:482/1770 train_time:44896ms step_avg:95.12ms
step:483/1770 train_time:44992ms step_avg:95.12ms
step:484/1770 train_time:45089ms step_avg:95.12ms
step:485/1770 train_time:45185ms step_avg:95.13ms
step:486/1770 train_time:45283ms step_avg:95.13ms
step:487/1770 train_time:45381ms step_avg:95.14ms
step:488/1770 train_time:45478ms step_avg:95.14ms
step:489/1770 train_time:45576ms step_avg:95.15ms
step:490/1770 train_time:45672ms step_avg:95.15ms
step:491/1770 train_time:45768ms step_avg:95.15ms
step:492/1770 train_time:45866ms step_avg:95.16ms
step:493/1770 train_time:45963ms step_avg:95.16ms
step:494/1770 train_time:46061ms step_avg:95.17ms
step:495/1770 train_time:46158ms step_avg:95.17ms
step:496/1770 train_time:46255ms step_avg:95.17ms
step:497/1770 train_time:46352ms step_avg:95.18ms
step:498/1770 train_time:46448ms step_avg:95.18ms
step:499/1770 train_time:46545ms step_avg:95.18ms
step:500/1770 train_time:46643ms step_avg:95.19ms
step:500/1770 val_loss:3.7492 train_time:46739ms step_avg:95.39ms
step:501/1770 train_time:46762ms step_avg:95.24ms
step:502/1770 train_time:46844ms step_avg:95.21ms
step:503/1770 train_time:46943ms step_avg:95.22ms
step:504/1770 train_time:47040ms step_avg:95.22ms
step:505/1770 train_time:47137ms step_avg:95.23ms
step:506/1770 train_time:47234ms step_avg:95.23ms
step:507/1770 train_time:47330ms step_avg:95.23ms
step:508/1770 train_time:47427ms step_avg:95.23ms
step:509/1770 train_time:47524ms step_avg:95.24ms
step:510/1770 train_time:47621ms step_avg:95.24ms
step:511/1770 train_time:47718ms step_avg:95.25ms
step:512/1770 train_time:47817ms step_avg:95.25ms
step:513/1770 train_time:47915ms step_avg:95.26ms
step:514/1770 train_time:48012ms step_avg:95.26ms
step:515/1770 train_time:48109ms step_avg:95.26ms
step:516/1770 train_time:48206ms step_avg:95.27ms
step:517/1770 train_time:48303ms step_avg:95.27ms
step:518/1770 train_time:48399ms step_avg:95.27ms
step:519/1770 train_time:48496ms step_avg:95.28ms
step:520/1770 train_time:48593ms step_avg:95.28ms
step:521/1770 train_time:48690ms step_avg:95.28ms
step:522/1770 train_time:48787ms step_avg:95.29ms
step:523/1770 train_time:48884ms step_avg:95.29ms
step:524/1770 train_time:48982ms step_avg:95.30ms
step:525/1770 train_time:49080ms step_avg:95.30ms
step:526/1770 train_time:49178ms step_avg:95.31ms
step:527/1770 train_time:49275ms step_avg:95.31ms
step:528/1770 train_time:49372ms step_avg:95.31ms
step:529/1770 train_time:49469ms step_avg:95.32ms
step:530/1770 train_time:49566ms step_avg:95.32ms
step:531/1770 train_time:49663ms step_avg:95.32ms
step:532/1770 train_time:49761ms step_avg:95.33ms
step:533/1770 train_time:49859ms step_avg:95.33ms
step:534/1770 train_time:49957ms step_avg:95.34ms
step:535/1770 train_time:50054ms step_avg:95.34ms
step:536/1770 train_time:50152ms step_avg:95.35ms
step:537/1770 train_time:50248ms step_avg:95.35ms
step:538/1770 train_time:50346ms step_avg:95.35ms
step:539/1770 train_time:50443ms step_avg:95.36ms
step:540/1770 train_time:50541ms step_avg:95.36ms
step:541/1770 train_time:50639ms step_avg:95.37ms
step:542/1770 train_time:50737ms step_avg:95.37ms
step:543/1770 train_time:50834ms step_avg:95.37ms
step:544/1770 train_time:50932ms step_avg:95.38ms
step:545/1770 train_time:51029ms step_avg:95.38ms
step:546/1770 train_time:51126ms step_avg:95.38ms
step:547/1770 train_time:51224ms step_avg:95.39ms
step:548/1770 train_time:51322ms step_avg:95.39ms
step:549/1770 train_time:51419ms step_avg:95.40ms
step:550/1770 train_time:51517ms step_avg:95.40ms
step:551/1770 train_time:51615ms step_avg:95.41ms
step:552/1770 train_time:51712ms step_avg:95.41ms
step:553/1770 train_time:51809ms step_avg:95.41ms
step:554/1770 train_time:51906ms step_avg:95.42ms
step:555/1770 train_time:52004ms step_avg:95.42ms
step:556/1770 train_time:52102ms step_avg:95.42ms
step:557/1770 train_time:52200ms step_avg:95.43ms
step:558/1770 train_time:52298ms step_avg:95.43ms
step:559/1770 train_time:52395ms step_avg:95.44ms
step:560/1770 train_time:52493ms step_avg:95.44ms
step:561/1770 train_time:52590ms step_avg:95.44ms
step:562/1770 train_time:52687ms step_avg:95.45ms
step:563/1770 train_time:52785ms step_avg:95.45ms
step:564/1770 train_time:52883ms step_avg:95.46ms
step:565/1770 train_time:52981ms step_avg:95.46ms
step:566/1770 train_time:53079ms step_avg:95.46ms
step:567/1770 train_time:53176ms step_avg:95.47ms
step:568/1770 train_time:53273ms step_avg:95.47ms
step:569/1770 train_time:53370ms step_avg:95.47ms
step:570/1770 train_time:53467ms step_avg:95.48ms
step:571/1770 train_time:53565ms step_avg:95.48ms
step:572/1770 train_time:53662ms step_avg:95.48ms
step:573/1770 train_time:53760ms step_avg:95.49ms
step:574/1770 train_time:53859ms step_avg:95.49ms
step:575/1770 train_time:53956ms step_avg:95.50ms
step:576/1770 train_time:54053ms step_avg:95.50ms
step:577/1770 train_time:54150ms step_avg:95.50ms
step:578/1770 train_time:54248ms step_avg:95.51ms
step:579/1770 train_time:54345ms step_avg:95.51ms
step:580/1770 train_time:54443ms step_avg:95.51ms
step:581/1770 train_time:54540ms step_avg:95.52ms
step:582/1770 train_time:54638ms step_avg:95.52ms
step:583/1770 train_time:54736ms step_avg:95.52ms
step:584/1770 train_time:54833ms step_avg:95.53ms
step:585/1770 train_time:54930ms step_avg:95.53ms
step:586/1770 train_time:55027ms step_avg:95.53ms
step:587/1770 train_time:55124ms step_avg:95.54ms
step:588/1770 train_time:55222ms step_avg:95.54ms
step:589/1770 train_time:55320ms step_avg:95.54ms
step:590/1770 train_time:55418ms step_avg:95.55ms
step:591/1770 train_time:55515ms step_avg:95.55ms
step:592/1770 train_time:55612ms step_avg:95.55ms
step:593/1770 train_time:55709ms step_avg:95.56ms
step:594/1770 train_time:55806ms step_avg:95.56ms
step:595/1770 train_time:55904ms step_avg:95.56ms
step:596/1770 train_time:56002ms step_avg:95.57ms
step:597/1770 train_time:56100ms step_avg:95.57ms
step:598/1770 train_time:56198ms step_avg:95.57ms
step:599/1770 train_time:56295ms step_avg:95.58ms
step:600/1770 train_time:56392ms step_avg:95.58ms
step:601/1770 train_time:56489ms step_avg:95.58ms
step:602/1770 train_time:56586ms step_avg:95.59ms
step:603/1770 train_time:56684ms step_avg:95.59ms
step:604/1770 train_time:56782ms step_avg:95.59ms
step:605/1770 train_time:56880ms step_avg:95.60ms
step:606/1770 train_time:56978ms step_avg:95.60ms
step:607/1770 train_time:57075ms step_avg:95.60ms
step:608/1770 train_time:57172ms step_avg:95.61ms
step:609/1770 train_time:57269ms step_avg:95.61ms
step:610/1770 train_time:57366ms step_avg:95.61ms
step:611/1770 train_time:57463ms step_avg:95.61ms
step:612/1770 train_time:57561ms step_avg:95.62ms
step:613/1770 train_time:57659ms step_avg:95.62ms
step:614/1770 train_time:57757ms step_avg:95.62ms
step:615/1770 train_time:57854ms step_avg:95.63ms
step:616/1770 train_time:57951ms step_avg:95.63ms
step:617/1770 train_time:58048ms step_avg:95.63ms
step:618/1770 train_time:58145ms step_avg:95.63ms
step:619/1770 train_time:58242ms step_avg:95.64ms
step:620/1770 train_time:58340ms step_avg:95.64ms
step:621/1770 train_time:58437ms step_avg:95.64ms
step:622/1770 train_time:58535ms step_avg:95.64ms
step:623/1770 train_time:58632ms step_avg:95.65ms
step:624/1770 train_time:58728ms step_avg:95.65ms
step:625/1770 train_time:58826ms step_avg:95.65ms
step:625/1770 val_loss:3.6654 train_time:58921ms step_avg:95.81ms
step:626/1770 train_time:58943ms step_avg:95.69ms
step:627/1770 train_time:59026ms step_avg:95.67ms
step:628/1770 train_time:59125ms step_avg:95.67ms
step:629/1770 train_time:59222ms step_avg:95.67ms
step:630/1770 train_time:59319ms step_avg:95.68ms
step:631/1770 train_time:59416ms step_avg:95.68ms
step:632/1770 train_time:59514ms step_avg:95.68ms
step:633/1770 train_time:59611ms step_avg:95.68ms
step:634/1770 train_time:59708ms step_avg:95.69ms
step:635/1770 train_time:59805ms step_avg:95.69ms
step:636/1770 train_time:59902ms step_avg:95.69ms
step:637/1770 train_time:60001ms step_avg:95.69ms
step:638/1770 train_time:60099ms step_avg:95.70ms
step:639/1770 train_time:60198ms step_avg:95.70ms
step:640/1770 train_time:60297ms step_avg:95.71ms
step:641/1770 train_time:60395ms step_avg:95.71ms
step:642/1770 train_time:60493ms step_avg:95.72ms
step:643/1770 train_time:60590ms step_avg:95.72ms
step:644/1770 train_time:60687ms step_avg:95.72ms
step:645/1770 train_time:60784ms step_avg:95.72ms
step:646/1770 train_time:60881ms step_avg:95.73ms
step:647/1770 train_time:60979ms step_avg:95.73ms
step:648/1770 train_time:61077ms step_avg:95.73ms
step:649/1770 train_time:61175ms step_avg:95.74ms
step:650/1770 train_time:61273ms step_avg:95.74ms
step:651/1770 train_time:61370ms step_avg:95.74ms
step:652/1770 train_time:61467ms step_avg:95.74ms
step:653/1770 train_time:61564ms step_avg:95.74ms
step:654/1770 train_time:61662ms step_avg:95.75ms
step:655/1770 train_time:61759ms step_avg:95.75ms
step:656/1770 train_time:61856ms step_avg:95.75ms
step:657/1770 train_time:61955ms step_avg:95.76ms
step:658/1770 train_time:62054ms step_avg:95.76ms
step:659/1770 train_time:62153ms step_avg:95.77ms
step:660/1770 train_time:62252ms step_avg:95.77ms
step:661/1770 train_time:62351ms step_avg:95.78ms
step:662/1770 train_time:62450ms step_avg:95.78ms
step:663/1770 train_time:62550ms step_avg:95.79ms
step:664/1770 train_time:62648ms step_avg:95.79ms
step:665/1770 train_time:62747ms step_avg:95.80ms
step:666/1770 train_time:62845ms step_avg:95.80ms
step:667/1770 train_time:62944ms step_avg:95.80ms
step:668/1770 train_time:63042ms step_avg:95.81ms
step:669/1770 train_time:63141ms step_avg:95.81ms
step:670/1770 train_time:63241ms step_avg:95.82ms
step:671/1770 train_time:63340ms step_avg:95.83ms
step:672/1770 train_time:63440ms step_avg:95.83ms
step:673/1770 train_time:63540ms step_avg:95.84ms
step:674/1770 train_time:63640ms step_avg:95.84ms
step:675/1770 train_time:63739ms step_avg:95.85ms
step:676/1770 train_time:63839ms step_avg:95.85ms
step:677/1770 train_time:63939ms step_avg:95.86ms
step:678/1770 train_time:64039ms step_avg:95.87ms
step:679/1770 train_time:64139ms step_avg:95.87ms
step:680/1770 train_time:64238ms step_avg:95.88ms
step:681/1770 train_time:64338ms step_avg:95.88ms
step:682/1770 train_time:64437ms step_avg:95.89ms
step:683/1770 train_time:64537ms step_avg:95.89ms
step:684/1770 train_time:64636ms step_avg:95.90ms
step:685/1770 train_time:64736ms step_avg:95.91ms
step:686/1770 train_time:64836ms step_avg:95.91ms
step:687/1770 train_time:64936ms step_avg:95.92ms
step:688/1770 train_time:65036ms step_avg:95.92ms
step:689/1770 train_time:65136ms step_avg:95.93ms
step:690/1770 train_time:65236ms step_avg:95.94ms
step:691/1770 train_time:65336ms step_avg:95.94ms
step:692/1770 train_time:65436ms step_avg:95.95ms
step:693/1770 train_time:65536ms step_avg:95.95ms
step:694/1770 train_time:65636ms step_avg:95.96ms
step:695/1770 train_time:65735ms step_avg:95.96ms
step:696/1770 train_time:65835ms step_avg:95.97ms
step:697/1770 train_time:65935ms step_avg:95.98ms
step:698/1770 train_time:66034ms step_avg:95.98ms
step:699/1770 train_time:66133ms step_avg:95.98ms
step:700/1770 train_time:66232ms step_avg:95.99ms
step:701/1770 train_time:66331ms step_avg:95.99ms
step:702/1770 train_time:66430ms step_avg:96.00ms
step:703/1770 train_time:66528ms step_avg:96.00ms
step:704/1770 train_time:66627ms step_avg:96.00ms
step:705/1770 train_time:66725ms step_avg:96.01ms
step:706/1770 train_time:66824ms step_avg:96.01ms
step:707/1770 train_time:66923ms step_avg:96.02ms
step:708/1770 train_time:67022ms step_avg:96.02ms
step:709/1770 train_time:67122ms step_avg:96.03ms
step:710/1770 train_time:67221ms step_avg:96.03ms
step:711/1770 train_time:67320ms step_avg:96.03ms
step:712/1770 train_time:67420ms step_avg:96.04ms
step:713/1770 train_time:67519ms step_avg:96.04ms
step:714/1770 train_time:67619ms step_avg:96.05ms
step:715/1770 train_time:67718ms step_avg:96.05ms
step:716/1770 train_time:67818ms step_avg:96.06ms
step:717/1770 train_time:67918ms step_avg:96.07ms
step:718/1770 train_time:68018ms step_avg:96.07ms
step:719/1770 train_time:68118ms step_avg:96.08ms
step:720/1770 train_time:68218ms step_avg:96.08ms
step:721/1770 train_time:68318ms step_avg:96.09ms
step:722/1770 train_time:68418ms step_avg:96.09ms
step:723/1770 train_time:68518ms step_avg:96.10ms
step:724/1770 train_time:68618ms step_avg:96.10ms
step:725/1770 train_time:68718ms step_avg:96.11ms
step:726/1770 train_time:68817ms step_avg:96.11ms
step:727/1770 train_time:68916ms step_avg:96.12ms
step:728/1770 train_time:69016ms step_avg:96.12ms
step:729/1770 train_time:69116ms step_avg:96.13ms
step:730/1770 train_time:69216ms step_avg:96.13ms
step:731/1770 train_time:69316ms step_avg:96.14ms
step:732/1770 train_time:69416ms step_avg:96.14ms
step:733/1770 train_time:69516ms step_avg:96.15ms
step:734/1770 train_time:69616ms step_avg:96.15ms
step:735/1770 train_time:69716ms step_avg:96.16ms
step:736/1770 train_time:69816ms step_avg:96.17ms
step:737/1770 train_time:69916ms step_avg:96.17ms
step:738/1770 train_time:70015ms step_avg:96.17ms
step:739/1770 train_time:70115ms step_avg:96.18ms
step:740/1770 train_time:70215ms step_avg:96.18ms
step:741/1770 train_time:70315ms step_avg:96.19ms
step:742/1770 train_time:70414ms step_avg:96.19ms
step:743/1770 train_time:70513ms step_avg:96.20ms
step:744/1770 train_time:70612ms step_avg:96.20ms
step:745/1770 train_time:70711ms step_avg:96.21ms
step:746/1770 train_time:70811ms step_avg:96.21ms
step:747/1770 train_time:70910ms step_avg:96.21ms
step:748/1770 train_time:71008ms step_avg:96.22ms
step:749/1770 train_time:71106ms step_avg:96.22ms
step:750/1770 train_time:71205ms step_avg:96.22ms
step:750/1770 val_loss:3.5995 train_time:71302ms step_avg:96.35ms
step:751/1770 train_time:71323ms step_avg:96.25ms
step:752/1770 train_time:71411ms step_avg:96.24ms
step:753/1770 train_time:71511ms step_avg:96.25ms
step:754/1770 train_time:71611ms step_avg:96.25ms
step:755/1770 train_time:71709ms step_avg:96.25ms
step:756/1770 train_time:71808ms step_avg:96.26ms
step:757/1770 train_time:71907ms step_avg:96.26ms
step:758/1770 train_time:72006ms step_avg:96.27ms
step:759/1770 train_time:72105ms step_avg:96.27ms
step:760/1770 train_time:72204ms step_avg:96.27ms
step:761/1770 train_time:72305ms step_avg:96.28ms
step:762/1770 train_time:72406ms step_avg:96.28ms
step:763/1770 train_time:72507ms step_avg:96.29ms
step:764/1770 train_time:72608ms step_avg:96.30ms
step:765/1770 train_time:72708ms step_avg:96.30ms
step:766/1770 train_time:72807ms step_avg:96.31ms
step:767/1770 train_time:72907ms step_avg:96.31ms
step:768/1770 train_time:73007ms step_avg:96.31ms
step:769/1770 train_time:73106ms step_avg:96.32ms
step:770/1770 train_time:73206ms step_avg:96.32ms
step:771/1770 train_time:73305ms step_avg:96.33ms
step:772/1770 train_time:73405ms step_avg:96.33ms
step:773/1770 train_time:73504ms step_avg:96.34ms
step:774/1770 train_time:73606ms step_avg:96.34ms
step:775/1770 train_time:73706ms step_avg:96.35ms
step:776/1770 train_time:73806ms step_avg:96.35ms
step:777/1770 train_time:73905ms step_avg:96.36ms
step:778/1770 train_time:74004ms step_avg:96.36ms
step:779/1770 train_time:74104ms step_avg:96.36ms
step:780/1770 train_time:74203ms step_avg:96.37ms
step:781/1770 train_time:74302ms step_avg:96.37ms
step:782/1770 train_time:74402ms step_avg:96.38ms
step:783/1770 train_time:74501ms step_avg:96.38ms
step:784/1770 train_time:74600ms step_avg:96.38ms
step:785/1770 train_time:74699ms step_avg:96.39ms
step:786/1770 train_time:74798ms step_avg:96.39ms
step:787/1770 train_time:74897ms step_avg:96.39ms
step:788/1770 train_time:74996ms step_avg:96.40ms
step:789/1770 train_time:75094ms step_avg:96.40ms
step:790/1770 train_time:75193ms step_avg:96.40ms
step:791/1770 train_time:75292ms step_avg:96.40ms
step:792/1770 train_time:75391ms step_avg:96.41ms
step:793/1770 train_time:75491ms step_avg:96.41ms
step:794/1770 train_time:75590ms step_avg:96.42ms
step:795/1770 train_time:75690ms step_avg:96.42ms
step:796/1770 train_time:75791ms step_avg:96.43ms
step:797/1770 train_time:75891ms step_avg:96.43ms
step:798/1770 train_time:75991ms step_avg:96.43ms
step:799/1770 train_time:76090ms step_avg:96.44ms
step:800/1770 train_time:76190ms step_avg:96.44ms
step:801/1770 train_time:76289ms step_avg:96.45ms
step:802/1770 train_time:76389ms step_avg:96.45ms
step:803/1770 train_time:76488ms step_avg:96.45ms
step:804/1770 train_time:76588ms step_avg:96.46ms
step:805/1770 train_time:76688ms step_avg:96.46ms
step:806/1770 train_time:76789ms step_avg:96.47ms
step:807/1770 train_time:76889ms step_avg:96.47ms
step:808/1770 train_time:76989ms step_avg:96.48ms
step:809/1770 train_time:77089ms step_avg:96.48ms
step:810/1770 train_time:77189ms step_avg:96.49ms
step:811/1770 train_time:77288ms step_avg:96.49ms
step:812/1770 train_time:77388ms step_avg:96.49ms
step:813/1770 train_time:77488ms step_avg:96.50ms
step:814/1770 train_time:77588ms step_avg:96.50ms
step:815/1770 train_time:77688ms step_avg:96.51ms
step:816/1770 train_time:77789ms step_avg:96.51ms
step:817/1770 train_time:77889ms step_avg:96.52ms
step:818/1770 train_time:77989ms step_avg:96.52ms
step:819/1770 train_time:78089ms step_avg:96.53ms
step:820/1770 train_time:78188ms step_avg:96.53ms
step:821/1770 train_time:78288ms step_avg:96.53ms
step:822/1770 train_time:78388ms step_avg:96.54ms
step:823/1770 train_time:78488ms step_avg:96.54ms
step:824/1770 train_time:78588ms step_avg:96.55ms
step:825/1770 train_time:78688ms step_avg:96.55ms
step:826/1770 train_time:78788ms step_avg:96.55ms
step:827/1770 train_time:78888ms step_avg:96.56ms
step:828/1770 train_time:78988ms step_avg:96.56ms
step:829/1770 train_time:79088ms step_avg:96.57ms
step:830/1770 train_time:79188ms step_avg:96.57ms
step:831/1770 train_time:79288ms step_avg:96.57ms
step:832/1770 train_time:79388ms step_avg:96.58ms
step:833/1770 train_time:79488ms step_avg:96.58ms
step:834/1770 train_time:79588ms step_avg:96.59ms
step:835/1770 train_time:79688ms step_avg:96.59ms
step:836/1770 train_time:79788ms step_avg:96.60ms
step:837/1770 train_time:79888ms step_avg:96.60ms
step:838/1770 train_time:79988ms step_avg:96.60ms
step:839/1770 train_time:80087ms step_avg:96.61ms
step:840/1770 train_time:80188ms step_avg:96.61ms
step:841/1770 train_time:80288ms step_avg:96.62ms
step:842/1770 train_time:80388ms step_avg:96.62ms
step:843/1770 train_time:80487ms step_avg:96.62ms
step:844/1770 train_time:80588ms step_avg:96.63ms
step:845/1770 train_time:80688ms step_avg:96.63ms
step:846/1770 train_time:80788ms step_avg:96.64ms
step:847/1770 train_time:80888ms step_avg:96.64ms
step:848/1770 train_time:80988ms step_avg:96.64ms
step:849/1770 train_time:81088ms step_avg:96.65ms
step:850/1770 train_time:81188ms step_avg:96.65ms
step:851/1770 train_time:81288ms step_avg:96.66ms
step:852/1770 train_time:81388ms step_avg:96.66ms
step:853/1770 train_time:81488ms step_avg:96.66ms
step:854/1770 train_time:81588ms step_avg:96.67ms
step:855/1770 train_time:81688ms step_avg:96.67ms
step:856/1770 train_time:81788ms step_avg:96.68ms
step:857/1770 train_time:81887ms step_avg:96.68ms
step:858/1770 train_time:81986ms step_avg:96.68ms
step:859/1770 train_time:82086ms step_avg:96.69ms
step:860/1770 train_time:82187ms step_avg:96.69ms
step:861/1770 train_time:82286ms step_avg:96.69ms
step:862/1770 train_time:82385ms step_avg:96.70ms
step:863/1770 train_time:82485ms step_avg:96.70ms
step:864/1770 train_time:82585ms step_avg:96.70ms
step:865/1770 train_time:82685ms step_avg:96.71ms
step:866/1770 train_time:82785ms step_avg:96.71ms
step:867/1770 train_time:82886ms step_avg:96.72ms
step:868/1770 train_time:82986ms step_avg:96.72ms
step:869/1770 train_time:83086ms step_avg:96.72ms
step:870/1770 train_time:83187ms step_avg:96.73ms
step:871/1770 train_time:83286ms step_avg:96.73ms
step:872/1770 train_time:83386ms step_avg:96.74ms
step:873/1770 train_time:83486ms step_avg:96.74ms
step:874/1770 train_time:83586ms step_avg:96.74ms
step:875/1770 train_time:83686ms step_avg:96.75ms
step:875/1770 val_loss:3.5505 train_time:83784ms step_avg:96.86ms
step:876/1770 train_time:83805ms step_avg:96.77ms
step:877/1770 train_time:83892ms step_avg:96.76ms
step:878/1770 train_time:83991ms step_avg:96.76ms
step:879/1770 train_time:84091ms step_avg:96.77ms
step:880/1770 train_time:84190ms step_avg:96.77ms
step:881/1770 train_time:84289ms step_avg:96.77ms
step:882/1770 train_time:84388ms step_avg:96.78ms
step:883/1770 train_time:84487ms step_avg:96.78ms
step:884/1770 train_time:84587ms step_avg:96.78ms
step:885/1770 train_time:84686ms step_avg:96.78ms
step:886/1770 train_time:84787ms step_avg:96.79ms
step:887/1770 train_time:84888ms step_avg:96.79ms
step:888/1770 train_time:84990ms step_avg:96.80ms
step:889/1770 train_time:85090ms step_avg:96.80ms
step:890/1770 train_time:85190ms step_avg:96.81ms
step:891/1770 train_time:85290ms step_avg:96.81ms
step:892/1770 train_time:85389ms step_avg:96.81ms
step:893/1770 train_time:85489ms step_avg:96.82ms
step:894/1770 train_time:85588ms step_avg:96.82ms
step:895/1770 train_time:85688ms step_avg:96.82ms
step:896/1770 train_time:85788ms step_avg:96.83ms
step:897/1770 train_time:85889ms step_avg:96.83ms
step:898/1770 train_time:85989ms step_avg:96.83ms
step:899/1770 train_time:86089ms step_avg:96.84ms
step:900/1770 train_time:86190ms step_avg:96.84ms
step:901/1770 train_time:86291ms step_avg:96.85ms
step:902/1770 train_time:86391ms step_avg:96.85ms
step:903/1770 train_time:86490ms step_avg:96.85ms
step:904/1770 train_time:86589ms step_avg:96.86ms
step:905/1770 train_time:86689ms step_avg:96.86ms
step:906/1770 train_time:86789ms step_avg:96.86ms
step:907/1770 train_time:86889ms step_avg:96.87ms
step:908/1770 train_time:86990ms step_avg:96.87ms
step:909/1770 train_time:87090ms step_avg:96.87ms
step:910/1770 train_time:87191ms step_avg:96.88ms
step:911/1770 train_time:87290ms step_avg:96.88ms
step:912/1770 train_time:87391ms step_avg:96.89ms
step:913/1770 train_time:87491ms step_avg:96.89ms
step:914/1770 train_time:87590ms step_avg:96.89ms
step:915/1770 train_time:87690ms step_avg:96.89ms
step:916/1770 train_time:87790ms step_avg:96.90ms
step:917/1770 train_time:87889ms step_avg:96.90ms
step:918/1770 train_time:87989ms step_avg:96.90ms
step:919/1770 train_time:88089ms step_avg:96.91ms
step:920/1770 train_time:88191ms step_avg:96.91ms
step:921/1770 train_time:88294ms step_avg:96.92ms
step:922/1770 train_time:88395ms step_avg:96.92ms
step:923/1770 train_time:88495ms step_avg:96.93ms
step:924/1770 train_time:88595ms step_avg:96.93ms
step:925/1770 train_time:88696ms step_avg:96.94ms
step:926/1770 train_time:88797ms step_avg:96.94ms
step:927/1770 train_time:88897ms step_avg:96.94ms
step:928/1770 train_time:88997ms step_avg:96.95ms
step:929/1770 train_time:89097ms step_avg:96.95ms
step:930/1770 train_time:89197ms step_avg:96.95ms
step:931/1770 train_time:89297ms step_avg:96.96ms
step:932/1770 train_time:89397ms step_avg:96.96ms
step:933/1770 train_time:89497ms step_avg:96.96ms
step:934/1770 train_time:89597ms step_avg:96.97ms
step:935/1770 train_time:89697ms step_avg:96.97ms
step:936/1770 train_time:89797ms step_avg:96.97ms
step:937/1770 train_time:89897ms step_avg:96.98ms
step:938/1770 train_time:89998ms step_avg:96.98ms
step:939/1770 train_time:90099ms step_avg:96.98ms
step:940/1770 train_time:90198ms step_avg:96.99ms
step:941/1770 train_time:90298ms step_avg:96.99ms
step:942/1770 train_time:90398ms step_avg:96.99ms
step:943/1770 train_time:90499ms step_avg:97.00ms
step:944/1770 train_time:90599ms step_avg:97.00ms
step:945/1770 train_time:90699ms step_avg:97.00ms
step:946/1770 train_time:90800ms step_avg:97.01ms
step:947/1770 train_time:90900ms step_avg:97.01ms
step:948/1770 train_time:90999ms step_avg:97.01ms
step:949/1770 train_time:91100ms step_avg:97.02ms
step:950/1770 train_time:91200ms step_avg:97.02ms
step:951/1770 train_time:91301ms step_avg:97.03ms
step:952/1770 train_time:91401ms step_avg:97.03ms
step:953/1770 train_time:91502ms step_avg:97.03ms
step:954/1770 train_time:91602ms step_avg:97.04ms
step:955/1770 train_time:91703ms step_avg:97.04ms
step:956/1770 train_time:91804ms step_avg:97.04ms
step:957/1770 train_time:91905ms step_avg:97.05ms
step:958/1770 train_time:92006ms step_avg:97.05ms
step:959/1770 train_time:92108ms step_avg:97.06ms
step:960/1770 train_time:92209ms step_avg:97.06ms
step:961/1770 train_time:92310ms step_avg:97.07ms
step:962/1770 train_time:92412ms step_avg:97.07ms
step:963/1770 train_time:92513ms step_avg:97.08ms
step:964/1770 train_time:92614ms step_avg:97.08ms
step:965/1770 train_time:92715ms step_avg:97.08ms
step:966/1770 train_time:92815ms step_avg:97.09ms
step:967/1770 train_time:92916ms step_avg:97.09ms
step:968/1770 train_time:93017ms step_avg:97.09ms
step:969/1770 train_time:93118ms step_avg:97.10ms
step:970/1770 train_time:93218ms step_avg:97.10ms
step:971/1770 train_time:93318ms step_avg:97.11ms
step:972/1770 train_time:93418ms step_avg:97.11ms
step:973/1770 train_time:93518ms step_avg:97.11ms
step:974/1770 train_time:93618ms step_avg:97.11ms
step:975/1770 train_time:93719ms step_avg:97.12ms
step:976/1770 train_time:93819ms step_avg:97.12ms
step:977/1770 train_time:93919ms step_avg:97.12ms
step:978/1770 train_time:94019ms step_avg:97.13ms
step:979/1770 train_time:94119ms step_avg:97.13ms
step:980/1770 train_time:94220ms step_avg:97.13ms
step:981/1770 train_time:94320ms step_avg:97.14ms
step:982/1770 train_time:94421ms step_avg:97.14ms
step:983/1770 train_time:94522ms step_avg:97.14ms
step:984/1770 train_time:94623ms step_avg:97.15ms
step:985/1770 train_time:94724ms step_avg:97.15ms
step:986/1770 train_time:94824ms step_avg:97.16ms
step:987/1770 train_time:94925ms step_avg:97.16ms
step:988/1770 train_time:95027ms step_avg:97.16ms
step:989/1770 train_time:95130ms step_avg:97.17ms
step:990/1770 train_time:95231ms step_avg:97.17ms
step:991/1770 train_time:95333ms step_avg:97.18ms
step:992/1770 train_time:95434ms step_avg:97.18ms
step:993/1770 train_time:95535ms step_avg:97.19ms
step:994/1770 train_time:95636ms step_avg:97.19ms
step:995/1770 train_time:95736ms step_avg:97.19ms
step:996/1770 train_time:95836ms step_avg:97.20ms
step:997/1770 train_time:95937ms step_avg:97.20ms
step:998/1770 train_time:96037ms step_avg:97.20ms
step:999/1770 train_time:96137ms step_avg:97.21ms
step:1000/1770 train_time:96238ms step_avg:97.21ms
step:1000/1770 val_loss:3.5123 train_time:96336ms step_avg:97.31ms
step:1001/1770 train_time:96357ms step_avg:97.23ms
step:1002/1770 train_time:96448ms step_avg:97.23ms
step:1003/1770 train_time:96549ms step_avg:97.23ms
step:1004/1770 train_time:96649ms step_avg:97.23ms
step:1005/1770 train_time:96748ms step_avg:97.23ms
step:1006/1770 train_time:96848ms step_avg:97.24ms
step:1007/1770 train_time:96948ms step_avg:97.24ms
step:1008/1770 train_time:97048ms step_avg:97.24ms
step:1009/1770 train_time:97147ms step_avg:97.24ms
step:1010/1770 train_time:97247ms step_avg:97.25ms
step:1011/1770 train_time:97349ms step_avg:97.25ms
step:1012/1770 train_time:97449ms step_avg:97.25ms
step:1013/1770 train_time:97549ms step_avg:97.26ms
step:1014/1770 train_time:97650ms step_avg:97.26ms
step:1015/1770 train_time:97749ms step_avg:97.26ms
step:1016/1770 train_time:97850ms step_avg:97.27ms
step:1017/1770 train_time:97950ms step_avg:97.27ms
step:1018/1770 train_time:98050ms step_avg:97.27ms
step:1019/1770 train_time:98150ms step_avg:97.27ms
step:1020/1770 train_time:98250ms step_avg:97.28ms
step:1021/1770 train_time:98351ms step_avg:97.28ms
step:1022/1770 train_time:98453ms step_avg:97.29ms
step:1023/1770 train_time:98554ms step_avg:97.29ms
step:1024/1770 train_time:98655ms step_avg:97.29ms
step:1025/1770 train_time:98757ms step_avg:97.30ms
step:1026/1770 train_time:98859ms step_avg:97.30ms
step:1027/1770 train_time:98960ms step_avg:97.31ms
step:1028/1770 train_time:99061ms step_avg:97.31ms
step:1029/1770 train_time:99162ms step_avg:97.31ms
step:1030/1770 train_time:99263ms step_avg:97.32ms
step:1031/1770 train_time:99364ms step_avg:97.32ms
step:1032/1770 train_time:99465ms step_avg:97.32ms
step:1033/1770 train_time:99567ms step_avg:97.33ms
step:1034/1770 train_time:99667ms step_avg:97.33ms
step:1035/1770 train_time:99767ms step_avg:97.33ms
step:1036/1770 train_time:99868ms step_avg:97.34ms
step:1037/1770 train_time:99968ms step_avg:97.34ms
step:1038/1770 train_time:100068ms step_avg:97.34ms
step:1039/1770 train_time:100168ms step_avg:97.35ms
step:1040/1770 train_time:100268ms step_avg:97.35ms
step:1041/1770 train_time:100369ms step_avg:97.35ms
step:1042/1770 train_time:100469ms step_avg:97.35ms
step:1043/1770 train_time:100570ms step_avg:97.36ms
step:1044/1770 train_time:100670ms step_avg:97.36ms
step:1045/1770 train_time:100769ms step_avg:97.36ms
step:1046/1770 train_time:100870ms step_avg:97.37ms
step:1047/1770 train_time:100970ms step_avg:97.37ms
step:1048/1770 train_time:101071ms step_avg:97.37ms
step:1049/1770 train_time:101171ms step_avg:97.37ms
step:1050/1770 train_time:101271ms step_avg:97.38ms
step:1051/1770 train_time:101372ms step_avg:97.38ms
step:1052/1770 train_time:101474ms step_avg:97.38ms
step:1053/1770 train_time:101575ms step_avg:97.39ms
step:1054/1770 train_time:101677ms step_avg:97.39ms
step:1055/1770 train_time:101779ms step_avg:97.40ms
step:1056/1770 train_time:101881ms step_avg:97.40ms
step:1057/1770 train_time:101983ms step_avg:97.40ms
step:1058/1770 train_time:102085ms step_avg:97.41ms
step:1059/1770 train_time:102185ms step_avg:97.41ms
step:1060/1770 train_time:102286ms step_avg:97.42ms
step:1061/1770 train_time:102387ms step_avg:97.42ms
step:1062/1770 train_time:102488ms step_avg:97.42ms
step:1063/1770 train_time:102590ms step_avg:97.43ms
step:1064/1770 train_time:102691ms step_avg:97.43ms
step:1065/1770 train_time:102791ms step_avg:97.43ms
step:1066/1770 train_time:102891ms step_avg:97.44ms
step:1067/1770 train_time:102992ms step_avg:97.44ms
step:1068/1770 train_time:103094ms step_avg:97.44ms
step:1069/1770 train_time:103196ms step_avg:97.45ms
step:1070/1770 train_time:103297ms step_avg:97.45ms
step:1071/1770 train_time:103399ms step_avg:97.45ms
step:1072/1770 train_time:103501ms step_avg:97.46ms
step:1073/1770 train_time:103602ms step_avg:97.46ms
step:1074/1770 train_time:103704ms step_avg:97.47ms
step:1075/1770 train_time:103804ms step_avg:97.47ms
step:1076/1770 train_time:103906ms step_avg:97.47ms
step:1077/1770 train_time:104007ms step_avg:97.48ms
step:1078/1770 train_time:104107ms step_avg:97.48ms
step:1079/1770 train_time:104207ms step_avg:97.48ms
step:1080/1770 train_time:104309ms step_avg:97.49ms
step:1081/1770 train_time:104409ms step_avg:97.49ms
step:1082/1770 train_time:104509ms step_avg:97.49ms
step:1083/1770 train_time:104609ms step_avg:97.49ms
step:1084/1770 train_time:104710ms step_avg:97.49ms
step:1085/1770 train_time:104810ms step_avg:97.50ms
step:1086/1770 train_time:104910ms step_avg:97.50ms
step:1087/1770 train_time:105010ms step_avg:97.50ms
step:1088/1770 train_time:105110ms step_avg:97.51ms
step:1089/1770 train_time:105211ms step_avg:97.51ms
step:1090/1770 train_time:105313ms step_avg:97.51ms
step:1091/1770 train_time:105413ms step_avg:97.51ms
step:1092/1770 train_time:105514ms step_avg:97.52ms
step:1093/1770 train_time:105615ms step_avg:97.52ms
step:1094/1770 train_time:105717ms step_avg:97.53ms
step:1095/1770 train_time:105819ms step_avg:97.53ms
step:1096/1770 train_time:105920ms step_avg:97.53ms
step:1097/1770 train_time:106022ms step_avg:97.54ms
step:1098/1770 train_time:106123ms step_avg:97.54ms
step:1099/1770 train_time:106224ms step_avg:97.54ms
step:1100/1770 train_time:106325ms step_avg:97.55ms
step:1101/1770 train_time:106426ms step_avg:97.55ms
step:1102/1770 train_time:106527ms step_avg:97.55ms
step:1103/1770 train_time:106627ms step_avg:97.55ms
step:1104/1770 train_time:106728ms step_avg:97.56ms
step:1105/1770 train_time:106828ms step_avg:97.56ms
step:1106/1770 train_time:106929ms step_avg:97.56ms
step:1107/1770 train_time:107029ms step_avg:97.57ms
step:1108/1770 train_time:107130ms step_avg:97.57ms
step:1109/1770 train_time:107230ms step_avg:97.57ms
step:1110/1770 train_time:107331ms step_avg:97.57ms
step:1111/1770 train_time:107432ms step_avg:97.58ms
step:1112/1770 train_time:107534ms step_avg:97.58ms
step:1113/1770 train_time:107635ms step_avg:97.58ms
step:1114/1770 train_time:107737ms step_avg:97.59ms
step:1115/1770 train_time:107839ms step_avg:97.59ms
step:1116/1770 train_time:107941ms step_avg:97.60ms
step:1117/1770 train_time:108042ms step_avg:97.60ms
step:1118/1770 train_time:108144ms step_avg:97.60ms
step:1119/1770 train_time:108245ms step_avg:97.61ms
step:1120/1770 train_time:108346ms step_avg:97.61ms
step:1121/1770 train_time:108446ms step_avg:97.61ms
step:1122/1770 train_time:108547ms step_avg:97.61ms
step:1123/1770 train_time:108647ms step_avg:97.62ms
step:1124/1770 train_time:108747ms step_avg:97.62ms
step:1125/1770 train_time:108848ms step_avg:97.62ms
step:1125/1770 val_loss:3.4741 train_time:108947ms step_avg:97.71ms
step:1126/1770 train_time:108968ms step_avg:97.64ms
step:1127/1770 train_time:109058ms step_avg:97.63ms
step:1128/1770 train_time:109159ms step_avg:97.64ms
step:1129/1770 train_time:109259ms step_avg:97.64ms
step:1130/1770 train_time:109360ms step_avg:97.64ms
step:1131/1770 train_time:109461ms step_avg:97.65ms
step:1132/1770 train_time:109561ms step_avg:97.65ms
step:1133/1770 train_time:109662ms step_avg:97.65ms
step:1134/1770 train_time:109762ms step_avg:97.65ms
step:1135/1770 train_time:109863ms step_avg:97.66ms
step:1136/1770 train_time:109964ms step_avg:97.66ms
step:1137/1770 train_time:110066ms step_avg:97.66ms
step:1138/1770 train_time:110166ms step_avg:97.66ms
step:1139/1770 train_time:110268ms step_avg:97.67ms
step:1140/1770 train_time:110370ms step_avg:97.67ms
step:1141/1770 train_time:110471ms step_avg:97.68ms
step:1142/1770 train_time:110573ms step_avg:97.68ms
step:1143/1770 train_time:110674ms step_avg:97.68ms
step:1144/1770 train_time:110775ms step_avg:97.69ms
step:1145/1770 train_time:110876ms step_avg:97.69ms
step:1146/1770 train_time:110978ms step_avg:97.69ms
step:1147/1770 train_time:111079ms step_avg:97.69ms
step:1148/1770 train_time:111180ms step_avg:97.70ms
step:1149/1770 train_time:111281ms step_avg:97.70ms
step:1150/1770 train_time:111381ms step_avg:97.70ms
step:1151/1770 train_time:111482ms step_avg:97.71ms
step:1152/1770 train_time:111583ms step_avg:97.71ms
step:1153/1770 train_time:111684ms step_avg:97.71ms
step:1154/1770 train_time:111785ms step_avg:97.71ms
step:1155/1770 train_time:111886ms step_avg:97.72ms
step:1156/1770 train_time:111987ms step_avg:97.72ms
step:1157/1770 train_time:112091ms step_avg:97.73ms
step:1158/1770 train_time:112193ms step_avg:97.73ms
step:1159/1770 train_time:112294ms step_avg:97.73ms
step:1160/1770 train_time:112395ms step_avg:97.73ms
step:1161/1770 train_time:112495ms step_avg:97.74ms
step:1162/1770 train_time:112596ms step_avg:97.74ms
step:1163/1770 train_time:112697ms step_avg:97.74ms
step:1164/1770 train_time:112797ms step_avg:97.74ms
step:1165/1770 train_time:112897ms step_avg:97.75ms
step:1166/1770 train_time:112999ms step_avg:97.75ms
step:1167/1770 train_time:113099ms step_avg:97.75ms
step:1168/1770 train_time:113201ms step_avg:97.76ms
step:1169/1770 train_time:113301ms step_avg:97.76ms
step:1170/1770 train_time:113401ms step_avg:97.76ms
step:1171/1770 train_time:113502ms step_avg:97.76ms
step:1172/1770 train_time:113602ms step_avg:97.76ms
step:1173/1770 train_time:113702ms step_avg:97.77ms
step:1174/1770 train_time:113803ms step_avg:97.77ms
step:1175/1770 train_time:113904ms step_avg:97.77ms
step:1176/1770 train_time:114005ms step_avg:97.77ms
step:1177/1770 train_time:114106ms step_avg:97.78ms
step:1178/1770 train_time:114208ms step_avg:97.78ms
step:1179/1770 train_time:114309ms step_avg:97.78ms
step:1180/1770 train_time:114411ms step_avg:97.79ms
step:1181/1770 train_time:114514ms step_avg:97.79ms
step:1182/1770 train_time:114615ms step_avg:97.79ms
step:1183/1770 train_time:114718ms step_avg:97.80ms
step:1184/1770 train_time:114821ms step_avg:97.80ms
step:1185/1770 train_time:114922ms step_avg:97.81ms
step:1186/1770 train_time:115025ms step_avg:97.81ms
step:1187/1770 train_time:115129ms step_avg:97.82ms
step:1188/1770 train_time:115231ms step_avg:97.82ms
step:1189/1770 train_time:115333ms step_avg:97.82ms
step:1190/1770 train_time:115435ms step_avg:97.83ms
step:1191/1770 train_time:115539ms step_avg:97.83ms
step:1192/1770 train_time:115640ms step_avg:97.83ms
step:1193/1770 train_time:115743ms step_avg:97.84ms
step:1194/1770 train_time:115845ms step_avg:97.84ms
step:1195/1770 train_time:115948ms step_avg:97.85ms
step:1196/1770 train_time:116051ms step_avg:97.85ms
step:1197/1770 train_time:116153ms step_avg:97.85ms
step:1198/1770 train_time:116255ms step_avg:97.86ms
step:1199/1770 train_time:116358ms step_avg:97.86ms
step:1200/1770 train_time:116460ms step_avg:97.87ms
step:1201/1770 train_time:116563ms step_avg:97.87ms
step:1202/1770 train_time:116664ms step_avg:97.87ms
step:1203/1770 train_time:116767ms step_avg:97.88ms
step:1204/1770 train_time:116869ms step_avg:97.88ms
step:1205/1770 train_time:116971ms step_avg:97.88ms
step:1206/1770 train_time:117074ms step_avg:97.89ms
step:1207/1770 train_time:117176ms step_avg:97.89ms
step:1208/1770 train_time:117278ms step_avg:97.89ms
step:1209/1770 train_time:117379ms step_avg:97.90ms
step:1210/1770 train_time:117481ms step_avg:97.90ms
step:1211/1770 train_time:117583ms step_avg:97.90ms
step:1212/1770 train_time:117687ms step_avg:97.91ms
step:1213/1770 train_time:117789ms step_avg:97.91ms
step:1214/1770 train_time:117891ms step_avg:97.92ms
step:1215/1770 train_time:117993ms step_avg:97.92ms
step:1216/1770 train_time:118099ms step_avg:97.93ms
step:1217/1770 train_time:118200ms step_avg:97.93ms
step:1218/1770 train_time:118302ms step_avg:97.93ms
step:1219/1770 train_time:118405ms step_avg:97.94ms
step:1220/1770 train_time:118508ms step_avg:97.94ms
step:1221/1770 train_time:118610ms step_avg:97.94ms
step:1222/1770 train_time:118713ms step_avg:97.95ms
step:1223/1770 train_time:118816ms step_avg:97.95ms
step:1224/1770 train_time:118919ms step_avg:97.96ms
step:1225/1770 train_time:119021ms step_avg:97.96ms
step:1226/1770 train_time:119124ms step_avg:97.96ms
step:1227/1770 train_time:119228ms step_avg:97.97ms
step:1228/1770 train_time:119333ms step_avg:97.97ms
step:1229/1770 train_time:119435ms step_avg:97.98ms
step:1230/1770 train_time:119538ms step_avg:97.98ms
step:1231/1770 train_time:119640ms step_avg:97.99ms
step:1232/1770 train_time:119741ms step_avg:97.99ms
step:1233/1770 train_time:119842ms step_avg:97.99ms
step:1234/1770 train_time:119945ms step_avg:97.99ms
step:1235/1770 train_time:120047ms step_avg:98.00ms
step:1236/1770 train_time:120149ms step_avg:98.00ms
step:1237/1770 train_time:120252ms step_avg:98.00ms
step:1238/1770 train_time:120355ms step_avg:98.01ms
step:1239/1770 train_time:120458ms step_avg:98.01ms
step:1240/1770 train_time:120559ms step_avg:98.02ms
step:1241/1770 train_time:120661ms step_avg:98.02ms
step:1242/1770 train_time:120762ms step_avg:98.02ms
step:1243/1770 train_time:120864ms step_avg:98.02ms
step:1244/1770 train_time:120966ms step_avg:98.03ms
step:1245/1770 train_time:121067ms step_avg:98.03ms
step:1246/1770 train_time:121169ms step_avg:98.03ms
step:1247/1770 train_time:121272ms step_avg:98.04ms
step:1248/1770 train_time:121375ms step_avg:98.04ms
step:1249/1770 train_time:121477ms step_avg:98.04ms
step:1250/1770 train_time:121579ms step_avg:98.05ms
step:1250/1770 val_loss:3.4251 train_time:121680ms step_avg:98.13ms
step:1251/1770 train_time:121701ms step_avg:98.07ms
step:1252/1770 train_time:121788ms step_avg:98.06ms
step:1253/1770 train_time:121891ms step_avg:98.06ms
step:1254/1770 train_time:121993ms step_avg:98.07ms
step:1255/1770 train_time:122098ms step_avg:98.07ms
step:1256/1770 train_time:122199ms step_avg:98.07ms
step:1257/1770 train_time:122301ms step_avg:98.08ms
step:1258/1770 train_time:122403ms step_avg:98.08ms
step:1259/1770 train_time:122505ms step_avg:98.08ms
step:1260/1770 train_time:122608ms step_avg:98.09ms
step:1261/1770 train_time:122712ms step_avg:98.09ms
step:1262/1770 train_time:122815ms step_avg:98.10ms
step:1263/1770 train_time:122917ms step_avg:98.10ms
step:1264/1770 train_time:123020ms step_avg:98.10ms
step:1265/1770 train_time:123121ms step_avg:98.10ms
step:1266/1770 train_time:123224ms step_avg:98.11ms
step:1267/1770 train_time:123327ms step_avg:98.11ms
step:1268/1770 train_time:123430ms step_avg:98.12ms
step:1269/1770 train_time:123532ms step_avg:98.12ms
step:1270/1770 train_time:123634ms step_avg:98.12ms
step:1271/1770 train_time:123737ms step_avg:98.13ms
step:1272/1770 train_time:123838ms step_avg:98.13ms
step:1273/1770 train_time:123940ms step_avg:98.13ms
step:1274/1770 train_time:124042ms step_avg:98.13ms
step:1275/1770 train_time:124144ms step_avg:98.14ms
step:1276/1770 train_time:124246ms step_avg:98.14ms
step:1277/1770 train_time:124347ms step_avg:98.14ms
step:1278/1770 train_time:124452ms step_avg:98.15ms
step:1279/1770 train_time:124555ms step_avg:98.15ms
step:1280/1770 train_time:124658ms step_avg:98.16ms
step:1281/1770 train_time:124759ms step_avg:98.16ms
step:1282/1770 train_time:124863ms step_avg:98.16ms
step:1283/1770 train_time:124965ms step_avg:98.17ms
step:1284/1770 train_time:125068ms step_avg:98.17ms
step:1285/1770 train_time:125170ms step_avg:98.17ms
step:1286/1770 train_time:125274ms step_avg:98.18ms
step:1287/1770 train_time:125378ms step_avg:98.18ms
step:1288/1770 train_time:125480ms step_avg:98.18ms
step:1289/1770 train_time:125583ms step_avg:98.19ms
step:1290/1770 train_time:125684ms step_avg:98.19ms
step:1291/1770 train_time:125787ms step_avg:98.19ms
step:1292/1770 train_time:125890ms step_avg:98.20ms
step:1293/1770 train_time:125993ms step_avg:98.20ms
step:1294/1770 train_time:126095ms step_avg:98.20ms
step:1295/1770 train_time:126198ms step_avg:98.21ms
step:1296/1770 train_time:126299ms step_avg:98.21ms
step:1297/1770 train_time:126400ms step_avg:98.21ms
step:1298/1770 train_time:126502ms step_avg:98.22ms
step:1299/1770 train_time:126603ms step_avg:98.22ms
step:1300/1770 train_time:126704ms step_avg:98.22ms
step:1301/1770 train_time:126807ms step_avg:98.22ms
step:1302/1770 train_time:126909ms step_avg:98.23ms
step:1303/1770 train_time:127012ms step_avg:98.23ms
step:1304/1770 train_time:127115ms step_avg:98.23ms
step:1305/1770 train_time:127218ms step_avg:98.24ms
step:1306/1770 train_time:127320ms step_avg:98.24ms
step:1307/1770 train_time:127421ms step_avg:98.24ms
step:1308/1770 train_time:127524ms step_avg:98.25ms
step:1309/1770 train_time:127626ms step_avg:98.25ms
step:1310/1770 train_time:127728ms step_avg:98.25ms
step:1311/1770 train_time:127829ms step_avg:98.25ms
step:1312/1770 train_time:127931ms step_avg:98.26ms
step:1313/1770 train_time:128034ms step_avg:98.26ms
step:1314/1770 train_time:128137ms step_avg:98.26ms
step:1315/1770 train_time:128239ms step_avg:98.27ms
step:1316/1770 train_time:128341ms step_avg:98.27ms
step:1317/1770 train_time:128444ms step_avg:98.27ms
step:1318/1770 train_time:128549ms step_avg:98.28ms
step:1319/1770 train_time:128651ms step_avg:98.28ms
step:1320/1770 train_time:128753ms step_avg:98.29ms
step:1321/1770 train_time:128856ms step_avg:98.29ms
step:1322/1770 train_time:128958ms step_avg:98.29ms
step:1323/1770 train_time:129060ms step_avg:98.29ms
step:1324/1770 train_time:129163ms step_avg:98.30ms
step:1325/1770 train_time:129267ms step_avg:98.30ms
step:1326/1770 train_time:129369ms step_avg:98.30ms
step:1327/1770 train_time:129475ms step_avg:98.31ms
step:1328/1770 train_time:129577ms step_avg:98.31ms
step:1329/1770 train_time:129679ms step_avg:98.32ms
step:1330/1770 train_time:129780ms step_avg:98.32ms
step:1331/1770 train_time:129882ms step_avg:98.32ms
step:1332/1770 train_time:129983ms step_avg:98.32ms
step:1333/1770 train_time:130085ms step_avg:98.33ms
step:1334/1770 train_time:130187ms step_avg:98.33ms
step:1335/1770 train_time:130290ms step_avg:98.33ms
step:1336/1770 train_time:130393ms step_avg:98.34ms
step:1337/1770 train_time:130496ms step_avg:98.34ms
step:1338/1770 train_time:130598ms step_avg:98.34ms
step:1339/1770 train_time:130701ms step_avg:98.35ms
step:1340/1770 train_time:130805ms step_avg:98.35ms
step:1341/1770 train_time:130906ms step_avg:98.35ms
step:1342/1770 train_time:131009ms step_avg:98.35ms
step:1343/1770 train_time:131111ms step_avg:98.36ms
step:1344/1770 train_time:131215ms step_avg:98.36ms
step:1345/1770 train_time:131316ms step_avg:98.36ms
step:1346/1770 train_time:131419ms step_avg:98.37ms
step:1347/1770 train_time:131521ms step_avg:98.37ms
step:1348/1770 train_time:131625ms step_avg:98.37ms
step:1349/1770 train_time:131728ms step_avg:98.38ms
step:1350/1770 train_time:131831ms step_avg:98.38ms
step:1351/1770 train_time:131934ms step_avg:98.38ms
step:1352/1770 train_time:132036ms step_avg:98.39ms
step:1353/1770 train_time:132139ms step_avg:98.39ms
step:1354/1770 train_time:132241ms step_avg:98.39ms
step:1355/1770 train_time:132342ms step_avg:98.40ms
step:1356/1770 train_time:132444ms step_avg:98.40ms
step:1357/1770 train_time:132546ms step_avg:98.40ms
step:1358/1770 train_time:132650ms step_avg:98.41ms
step:1359/1770 train_time:132753ms step_avg:98.41ms
step:1360/1770 train_time:132856ms step_avg:98.41ms
step:1361/1770 train_time:132958ms step_avg:98.41ms
step:1362/1770 train_time:133060ms step_avg:98.42ms
step:1363/1770 train_time:133163ms step_avg:98.42ms
step:1364/1770 train_time:133265ms step_avg:98.42ms
step:1365/1770 train_time:133367ms step_avg:98.43ms
step:1366/1770 train_time:133469ms step_avg:98.43ms
step:1367/1770 train_time:133573ms step_avg:98.43ms
step:1368/1770 train_time:133676ms step_avg:98.44ms
step:1369/1770 train_time:133779ms step_avg:98.44ms
step:1370/1770 train_time:133882ms step_avg:98.44ms
step:1371/1770 train_time:133984ms step_avg:98.45ms
step:1372/1770 train_time:134087ms step_avg:98.45ms
step:1373/1770 train_time:134190ms step_avg:98.45ms
step:1374/1770 train_time:134293ms step_avg:98.46ms
step:1375/1770 train_time:134396ms step_avg:98.46ms
step:1375/1770 val_loss:3.3809 train_time:134497ms step_avg:98.53ms
step:1376/1770 train_time:134518ms step_avg:98.48ms
step:1377/1770 train_time:134605ms step_avg:98.47ms
step:1378/1770 train_time:134706ms step_avg:98.47ms
step:1379/1770 train_time:134809ms step_avg:98.47ms
step:1380/1770 train_time:134911ms step_avg:98.48ms
step:1381/1770 train_time:135014ms step_avg:98.48ms
step:1382/1770 train_time:135115ms step_avg:98.48ms
step:1383/1770 train_time:135218ms step_avg:98.48ms
step:1384/1770 train_time:135320ms step_avg:98.49ms
step:1385/1770 train_time:135422ms step_avg:98.49ms
step:1386/1770 train_time:135525ms step_avg:98.49ms
step:1387/1770 train_time:135628ms step_avg:98.50ms
step:1388/1770 train_time:135731ms step_avg:98.50ms
step:1389/1770 train_time:135834ms step_avg:98.50ms
step:1390/1770 train_time:135936ms step_avg:98.50ms
step:1391/1770 train_time:136037ms step_avg:98.51ms
step:1392/1770 train_time:136139ms step_avg:98.51ms
step:1393/1770 train_time:136240ms step_avg:98.51ms
step:1394/1770 train_time:136342ms step_avg:98.51ms
step:1395/1770 train_time:136445ms step_avg:98.52ms
step:1396/1770 train_time:136548ms step_avg:98.52ms
step:1397/1770 train_time:136651ms step_avg:98.52ms
step:1398/1770 train_time:136754ms step_avg:98.53ms
step:1399/1770 train_time:136856ms step_avg:98.53ms
step:1400/1770 train_time:136958ms step_avg:98.53ms
step:1401/1770 train_time:137060ms step_avg:98.53ms
step:1402/1770 train_time:137162ms step_avg:98.54ms
step:1403/1770 train_time:137264ms step_avg:98.54ms
step:1404/1770 train_time:137367ms step_avg:98.54ms
step:1405/1770 train_time:137469ms step_avg:98.54ms
step:1406/1770 train_time:137572ms step_avg:98.55ms
step:1407/1770 train_time:137675ms step_avg:98.55ms
step:1408/1770 train_time:137777ms step_avg:98.55ms
step:1409/1770 train_time:137880ms step_avg:98.56ms
step:1410/1770 train_time:137981ms step_avg:98.56ms
step:1411/1770 train_time:138084ms step_avg:98.56ms
step:1412/1770 train_time:138186ms step_avg:98.56ms
step:1413/1770 train_time:138288ms step_avg:98.57ms
step:1414/1770 train_time:138391ms step_avg:98.57ms
step:1415/1770 train_time:138494ms step_avg:98.57ms
step:1416/1770 train_time:138598ms step_avg:98.58ms
step:1417/1770 train_time:138700ms step_avg:98.58ms
step:1418/1770 train_time:138802ms step_avg:98.58ms
step:1419/1770 train_time:138905ms step_avg:98.58ms
step:1420/1770 train_time:139007ms step_avg:98.59ms
step:1421/1770 train_time:139110ms step_avg:98.59ms
step:1422/1770 train_time:139212ms step_avg:98.59ms
step:1423/1770 train_time:139315ms step_avg:98.59ms
step:1424/1770 train_time:139417ms step_avg:98.60ms
step:1425/1770 train_time:139519ms step_avg:98.60ms
step:1426/1770 train_time:139622ms step_avg:98.60ms
step:1427/1770 train_time:139724ms step_avg:98.61ms
step:1428/1770 train_time:139827ms step_avg:98.61ms
step:1429/1770 train_time:139930ms step_avg:98.61ms
step:1430/1770 train_time:140032ms step_avg:98.61ms
step:1431/1770 train_time:140135ms step_avg:98.62ms
step:1432/1770 train_time:140237ms step_avg:98.62ms
step:1433/1770 train_time:140338ms step_avg:98.62ms
step:1434/1770 train_time:140439ms step_avg:98.62ms
step:1435/1770 train_time:140541ms step_avg:98.63ms
step:1436/1770 train_time:140645ms step_avg:98.63ms
step:1437/1770 train_time:140746ms step_avg:98.63ms
step:1438/1770 train_time:140848ms step_avg:98.63ms
step:1439/1770 train_time:140950ms step_avg:98.64ms
step:1440/1770 train_time:141053ms step_avg:98.64ms
step:1441/1770 train_time:141158ms step_avg:98.64ms
step:1442/1770 train_time:141260ms step_avg:98.65ms
step:1443/1770 train_time:141363ms step_avg:98.65ms
step:1444/1770 train_time:141465ms step_avg:98.65ms
step:1445/1770 train_time:141568ms step_avg:98.65ms
step:1446/1770 train_time:141672ms step_avg:98.66ms
step:1447/1770 train_time:141776ms step_avg:98.66ms
step:1448/1770 train_time:141878ms step_avg:98.66ms
step:1449/1770 train_time:141983ms step_avg:98.67ms
step:1450/1770 train_time:142085ms step_avg:98.67ms
step:1451/1770 train_time:142190ms step_avg:98.67ms
step:1452/1770 train_time:142295ms step_avg:98.68ms
step:1453/1770 train_time:142398ms step_avg:98.68ms
step:1454/1770 train_time:142500ms step_avg:98.68ms
step:1455/1770 train_time:142604ms step_avg:98.69ms
step:1456/1770 train_time:142708ms step_avg:98.69ms
step:1457/1770 train_time:142813ms step_avg:98.70ms
step:1458/1770 train_time:142918ms step_avg:98.70ms
step:1459/1770 train_time:143022ms step_avg:98.70ms
step:1460/1770 train_time:143124ms step_avg:98.71ms
step:1461/1770 train_time:143228ms step_avg:98.71ms
step:1462/1770 train_time:143332ms step_avg:98.71ms
step:1463/1770 train_time:143436ms step_avg:98.72ms
step:1464/1770 train_time:143541ms step_avg:98.72ms
step:1465/1770 train_time:143643ms step_avg:98.72ms
step:1466/1770 train_time:143747ms step_avg:98.73ms
step:1467/1770 train_time:143852ms step_avg:98.73ms
step:1468/1770 train_time:143956ms step_avg:98.74ms
step:1469/1770 train_time:144058ms step_avg:98.74ms
step:1470/1770 train_time:144161ms step_avg:98.74ms
step:1471/1770 train_time:144264ms step_avg:98.74ms
step:1472/1770 train_time:144367ms step_avg:98.75ms
step:1473/1770 train_time:144472ms step_avg:98.75ms
step:1474/1770 train_time:144576ms step_avg:98.75ms
step:1475/1770 train_time:144679ms step_avg:98.76ms
step:1476/1770 train_time:144782ms step_avg:98.76ms
step:1477/1770 train_time:144887ms step_avg:98.76ms
step:1478/1770 train_time:144991ms step_avg:98.77ms
step:1479/1770 train_time:145095ms step_avg:98.77ms
step:1480/1770 train_time:145199ms step_avg:98.77ms
step:1481/1770 train_time:145306ms step_avg:98.78ms
step:1482/1770 train_time:145408ms step_avg:98.78ms
step:1483/1770 train_time:145512ms step_avg:98.79ms
step:1484/1770 train_time:145615ms step_avg:98.79ms
step:1485/1770 train_time:145719ms step_avg:98.79ms
step:1486/1770 train_time:145821ms step_avg:98.79ms
step:1487/1770 train_time:145925ms step_avg:98.80ms
step:1488/1770 train_time:146029ms step_avg:98.80ms
step:1489/1770 train_time:146134ms step_avg:98.81ms
step:1490/1770 train_time:146238ms step_avg:98.81ms
step:1491/1770 train_time:146341ms step_avg:98.81ms
step:1492/1770 train_time:146445ms step_avg:98.82ms
step:1493/1770 train_time:146551ms step_avg:98.82ms
step:1494/1770 train_time:146658ms step_avg:98.83ms
step:1495/1770 train_time:146760ms step_avg:98.83ms
step:1496/1770 train_time:146863ms step_avg:98.83ms
step:1497/1770 train_time:146967ms step_avg:98.83ms
step:1498/1770 train_time:147070ms step_avg:98.84ms
step:1499/1770 train_time:147174ms step_avg:98.84ms
step:1500/1770 train_time:147277ms step_avg:98.84ms
step:1500/1770 val_loss:3.3431 train_time:147378ms step_avg:98.91ms
step:1501/1770 train_time:147403ms step_avg:98.86ms
step:1502/1770 train_time:147492ms step_avg:98.85ms
step:1503/1770 train_time:147594ms step_avg:98.86ms
step:1504/1770 train_time:147697ms step_avg:98.86ms
step:1505/1770 train_time:147802ms step_avg:98.86ms
step:1506/1770 train_time:147906ms step_avg:98.87ms
step:1507/1770 train_time:148010ms step_avg:98.87ms
step:1508/1770 train_time:148115ms step_avg:98.87ms
step:1509/1770 train_time:148217ms step_avg:98.88ms
step:1510/1770 train_time:148320ms step_avg:98.88ms
step:1511/1770 train_time:148426ms step_avg:98.88ms
step:1512/1770 train_time:148530ms step_avg:98.89ms
step:1513/1770 train_time:148633ms step_avg:98.89ms
step:1514/1770 train_time:148736ms step_avg:98.89ms
step:1515/1770 train_time:148840ms step_avg:98.90ms
step:1516/1770 train_time:148944ms step_avg:98.90ms
step:1517/1770 train_time:149047ms step_avg:98.90ms
step:1518/1770 train_time:149152ms step_avg:98.91ms
step:1519/1770 train_time:149254ms step_avg:98.91ms
step:1520/1770 train_time:149358ms step_avg:98.91ms
step:1521/1770 train_time:149462ms step_avg:98.92ms
step:1522/1770 train_time:149565ms step_avg:98.92ms
step:1523/1770 train_time:149670ms step_avg:98.92ms
step:1524/1770 train_time:149773ms step_avg:98.93ms
step:1525/1770 train_time:149876ms step_avg:98.93ms
step:1526/1770 train_time:149978ms step_avg:98.93ms
step:1527/1770 train_time:150082ms step_avg:98.93ms
step:1528/1770 train_time:150188ms step_avg:98.94ms
step:1529/1770 train_time:150291ms step_avg:98.94ms
step:1530/1770 train_time:150394ms step_avg:98.94ms
step:1531/1770 train_time:150497ms step_avg:98.95ms
step:1532/1770 train_time:150601ms step_avg:98.95ms
step:1533/1770 train_time:150706ms step_avg:98.95ms
step:1534/1770 train_time:150810ms step_avg:98.96ms
step:1535/1770 train_time:150912ms step_avg:98.96ms
step:1536/1770 train_time:151015ms step_avg:98.96ms
step:1537/1770 train_time:151118ms step_avg:98.96ms
step:1538/1770 train_time:151224ms step_avg:98.97ms
step:1539/1770 train_time:151328ms step_avg:98.97ms
step:1540/1770 train_time:151434ms step_avg:98.98ms
step:1541/1770 train_time:151538ms step_avg:98.98ms
step:1542/1770 train_time:151641ms step_avg:98.98ms
step:1543/1770 train_time:151744ms step_avg:98.99ms
step:1544/1770 train_time:151850ms step_avg:98.99ms
step:1545/1770 train_time:151953ms step_avg:98.99ms
step:1546/1770 train_time:152056ms step_avg:99.00ms
step:1547/1770 train_time:152160ms step_avg:99.00ms
step:1548/1770 train_time:152263ms step_avg:99.00ms
step:1549/1770 train_time:152367ms step_avg:99.00ms
step:1550/1770 train_time:152470ms step_avg:99.01ms
step:1551/1770 train_time:152573ms step_avg:99.01ms
step:1552/1770 train_time:152679ms step_avg:99.01ms
step:1553/1770 train_time:152782ms step_avg:99.02ms
step:1554/1770 train_time:152885ms step_avg:99.02ms
step:1555/1770 train_time:152989ms step_avg:99.02ms
step:1556/1770 train_time:153092ms step_avg:99.02ms
step:1557/1770 train_time:153194ms step_avg:99.03ms
step:1558/1770 train_time:153298ms step_avg:99.03ms
step:1559/1770 train_time:153402ms step_avg:99.03ms
step:1560/1770 train_time:153505ms step_avg:99.04ms
step:1561/1770 train_time:153611ms step_avg:99.04ms
step:1562/1770 train_time:153714ms step_avg:99.04ms
step:1563/1770 train_time:153817ms step_avg:99.05ms
step:1564/1770 train_time:153920ms step_avg:99.05ms
step:1565/1770 train_time:154025ms step_avg:99.05ms
step:1566/1770 train_time:154128ms step_avg:99.05ms
step:1567/1770 train_time:154232ms step_avg:99.06ms
step:1568/1770 train_time:154335ms step_avg:99.06ms
step:1569/1770 train_time:154441ms step_avg:99.06ms
step:1570/1770 train_time:154544ms step_avg:99.07ms
step:1571/1770 train_time:154649ms step_avg:99.07ms
step:1572/1770 train_time:154753ms step_avg:99.07ms
step:1573/1770 train_time:154858ms step_avg:99.08ms
step:1574/1770 train_time:154961ms step_avg:99.08ms
step:1575/1770 train_time:155064ms step_avg:99.08ms
step:1576/1770 train_time:155167ms step_avg:99.09ms
step:1577/1770 train_time:155272ms step_avg:99.09ms
step:1578/1770 train_time:155377ms step_avg:99.09ms
step:1579/1770 train_time:155480ms step_avg:99.10ms
step:1580/1770 train_time:155583ms step_avg:99.10ms
step:1581/1770 train_time:155690ms step_avg:99.10ms
step:1582/1770 train_time:155794ms step_avg:99.11ms
step:1583/1770 train_time:155897ms step_avg:99.11ms
step:1584/1770 train_time:156001ms step_avg:99.11ms
step:1585/1770 train_time:156106ms step_avg:99.11ms
step:1586/1770 train_time:156213ms step_avg:99.12ms
step:1587/1770 train_time:156316ms step_avg:99.12ms
step:1588/1770 train_time:156420ms step_avg:99.13ms
step:1589/1770 train_time:156526ms step_avg:99.13ms
step:1590/1770 train_time:156629ms step_avg:99.13ms
step:1591/1770 train_time:156732ms step_avg:99.13ms
step:1592/1770 train_time:156836ms step_avg:99.14ms
step:1593/1770 train_time:156939ms step_avg:99.14ms
step:1594/1770 train_time:157042ms step_avg:99.14ms
step:1595/1770 train_time:157146ms step_avg:99.15ms
step:1596/1770 train_time:157251ms step_avg:99.15ms
step:1597/1770 train_time:157354ms step_avg:99.15ms
step:1598/1770 train_time:157457ms step_avg:99.15ms
step:1599/1770 train_time:157562ms step_avg:99.16ms
step:1600/1770 train_time:157669ms step_avg:99.16ms
step:1601/1770 train_time:157772ms step_avg:99.17ms
step:1602/1770 train_time:157876ms step_avg:99.17ms
step:1603/1770 train_time:157979ms step_avg:99.17ms
step:1604/1770 train_time:158082ms step_avg:99.17ms
step:1605/1770 train_time:158185ms step_avg:99.18ms
step:1606/1770 train_time:158290ms step_avg:99.18ms
step:1607/1770 train_time:158397ms step_avg:99.18ms
step:1608/1770 train_time:158500ms step_avg:99.19ms
step:1609/1770 train_time:158604ms step_avg:99.19ms
step:1610/1770 train_time:158710ms step_avg:99.19ms
step:1611/1770 train_time:158815ms step_avg:99.20ms
step:1612/1770 train_time:158920ms step_avg:99.20ms
step:1613/1770 train_time:159024ms step_avg:99.20ms
step:1614/1770 train_time:159128ms step_avg:99.21ms
step:1615/1770 train_time:159231ms step_avg:99.21ms
step:1616/1770 train_time:159334ms step_avg:99.21ms
step:1617/1770 train_time:159440ms step_avg:99.22ms
step:1618/1770 train_time:159545ms step_avg:99.22ms
step:1619/1770 train_time:159649ms step_avg:99.22ms
step:1620/1770 train_time:159753ms step_avg:99.23ms
step:1621/1770 train_time:159856ms step_avg:99.23ms
step:1622/1770 train_time:159960ms step_avg:99.23ms
step:1623/1770 train_time:160067ms step_avg:99.24ms
step:1624/1770 train_time:160170ms step_avg:99.24ms
step:1625/1770 train_time:160273ms step_avg:99.24ms
step:1625/1770 val_loss:3.3084 train_time:160375ms step_avg:99.30ms
step:1626/1770 train_time:160396ms step_avg:99.26ms
step:1627/1770 train_time:160485ms step_avg:99.25ms
step:1628/1770 train_time:160588ms step_avg:99.25ms
step:1629/1770 train_time:160691ms step_avg:99.25ms
step:1630/1770 train_time:160794ms step_avg:99.26ms
step:1631/1770 train_time:160897ms step_avg:99.26ms
step:1632/1770 train_time:161000ms step_avg:99.26ms
step:1633/1770 train_time:161103ms step_avg:99.26ms
step:1634/1770 train_time:161206ms step_avg:99.26ms
step:1635/1770 train_time:161309ms step_avg:99.27ms
step:1636/1770 train_time:161414ms step_avg:99.27ms
step:1637/1770 train_time:161519ms step_avg:99.27ms
step:1638/1770 train_time:161622ms step_avg:99.28ms
step:1639/1770 train_time:161725ms step_avg:99.28ms
step:1640/1770 train_time:161828ms step_avg:99.28ms
step:1641/1770 train_time:161931ms step_avg:99.28ms
step:1642/1770 train_time:162035ms step_avg:99.29ms
step:1643/1770 train_time:162139ms step_avg:99.29ms
step:1644/1770 train_time:162244ms step_avg:99.29ms
step:1645/1770 train_time:162347ms step_avg:99.29ms
step:1646/1770 train_time:162452ms step_avg:99.30ms
step:1647/1770 train_time:162557ms step_avg:99.30ms
step:1648/1770 train_time:162661ms step_avg:99.30ms
step:1649/1770 train_time:162764ms step_avg:99.31ms
step:1650/1770 train_time:162867ms step_avg:99.31ms
step:1651/1770 train_time:162970ms step_avg:99.31ms
step:1652/1770 train_time:163073ms step_avg:99.31ms
step:1653/1770 train_time:163177ms step_avg:99.32ms
step:1654/1770 train_time:163284ms step_avg:99.32ms
step:1655/1770 train_time:163390ms step_avg:99.33ms
step:1656/1770 train_time:163493ms step_avg:99.33ms
step:1657/1770 train_time:163599ms step_avg:99.33ms
step:1658/1770 train_time:163703ms step_avg:99.33ms
step:1659/1770 train_time:163808ms step_avg:99.34ms
step:1660/1770 train_time:163912ms step_avg:99.34ms
step:1661/1770 train_time:164016ms step_avg:99.34ms
step:1662/1770 train_time:164120ms step_avg:99.35ms
step:1663/1770 train_time:164222ms step_avg:99.35ms
step:1664/1770 train_time:164325ms step_avg:99.35ms
step:1665/1770 train_time:164428ms step_avg:99.35ms
step:1666/1770 train_time:164531ms step_avg:99.35ms
step:1667/1770 train_time:164635ms step_avg:99.36ms
step:1668/1770 train_time:164739ms step_avg:99.36ms
step:1669/1770 train_time:164842ms step_avg:99.36ms
step:1670/1770 train_time:164945ms step_avg:99.36ms
step:1671/1770 train_time:165050ms step_avg:99.37ms
step:1672/1770 train_time:165153ms step_avg:99.37ms
step:1673/1770 train_time:165258ms step_avg:99.37ms
step:1674/1770 train_time:165362ms step_avg:99.38ms
step:1675/1770 train_time:165464ms step_avg:99.38ms
step:1676/1770 train_time:165568ms step_avg:99.38ms
step:1677/1770 train_time:165676ms step_avg:99.39ms
step:1678/1770 train_time:165778ms step_avg:99.39ms
step:1679/1770 train_time:165882ms step_avg:99.39ms
step:1680/1770 train_time:165985ms step_avg:99.39ms
step:1681/1770 train_time:166089ms step_avg:99.40ms
step:1682/1770 train_time:166194ms step_avg:99.40ms
step:1683/1770 train_time:166298ms step_avg:99.40ms
step:1684/1770 train_time:166401ms step_avg:99.40ms
step:1685/1770 train_time:166504ms step_avg:99.41ms
step:1686/1770 train_time:166608ms step_avg:99.41ms
step:1687/1770 train_time:166713ms step_avg:99.41ms
step:1688/1770 train_time:166817ms step_avg:99.41ms
step:1689/1770 train_time:166921ms step_avg:99.42ms
step:1690/1770 train_time:167024ms step_avg:99.42ms
step:1691/1770 train_time:167127ms step_avg:99.42ms
step:1692/1770 train_time:167230ms step_avg:99.42ms
step:1693/1770 train_time:167336ms step_avg:99.43ms
step:1694/1770 train_time:167439ms step_avg:99.43ms
step:1695/1770 train_time:167543ms step_avg:99.43ms
step:1696/1770 train_time:167648ms step_avg:99.44ms
step:1697/1770 train_time:167754ms step_avg:99.44ms
step:1698/1770 train_time:167858ms step_avg:99.44ms
step:1699/1770 train_time:167961ms step_avg:99.44ms
step:1700/1770 train_time:168064ms step_avg:99.45ms
step:1701/1770 train_time:168167ms step_avg:99.45ms
step:1702/1770 train_time:168272ms step_avg:99.45ms
step:1703/1770 train_time:168375ms step_avg:99.45ms
step:1704/1770 train_time:168480ms step_avg:99.46ms
step:1705/1770 train_time:168584ms step_avg:99.46ms
step:1706/1770 train_time:168686ms step_avg:99.46ms
step:1707/1770 train_time:168790ms step_avg:99.46ms
step:1708/1770 train_time:168894ms step_avg:99.47ms
step:1709/1770 train_time:169002ms step_avg:99.47ms
step:1710/1770 train_time:169109ms step_avg:99.48ms
step:1711/1770 train_time:169215ms step_avg:99.48ms
step:1712/1770 train_time:169320ms step_avg:99.48ms
step:1713/1770 train_time:169423ms step_avg:99.49ms
step:1714/1770 train_time:169527ms step_avg:99.49ms
step:1715/1770 train_time:169631ms step_avg:99.49ms
step:1716/1770 train_time:169735ms step_avg:99.49ms
step:1717/1770 train_time:169839ms step_avg:99.50ms
step:1718/1770 train_time:169944ms step_avg:99.50ms
step:1719/1770 train_time:170049ms step_avg:99.50ms
step:1720/1770 train_time:170154ms step_avg:99.51ms
step:1721/1770 train_time:170258ms step_avg:99.51ms
step:1722/1770 train_time:170365ms step_avg:99.51ms
step:1723/1770 train_time:170470ms step_avg:99.52ms
step:1724/1770 train_time:170577ms step_avg:99.52ms
step:1725/1770 train_time:170684ms step_avg:99.52ms
step:1726/1770 train_time:170790ms step_avg:99.53ms
step:1727/1770 train_time:170893ms step_avg:99.53ms
step:1728/1770 train_time:171000ms step_avg:99.53ms
step:1729/1770 train_time:171104ms step_avg:99.54ms
step:1730/1770 train_time:171209ms step_avg:99.54ms
step:1731/1770 train_time:171315ms step_avg:99.54ms
step:1732/1770 train_time:171419ms step_avg:99.55ms
step:1733/1770 train_time:171525ms step_avg:99.55ms
step:1734/1770 train_time:171628ms step_avg:99.55ms
step:1735/1770 train_time:171733ms step_avg:99.56ms
step:1736/1770 train_time:171837ms step_avg:99.56ms
step:1737/1770 train_time:171943ms step_avg:99.56ms
step:1738/1770 train_time:172046ms step_avg:99.56ms
step:1739/1770 train_time:172150ms step_avg:99.57ms
step:1740/1770 train_time:172254ms step_avg:99.57ms
step:1741/1770 train_time:172362ms step_avg:99.57ms
step:1742/1770 train_time:172468ms step_avg:99.58ms
step:1743/1770 train_time:172574ms step_avg:99.58ms
step:1744/1770 train_time:172680ms step_avg:99.58ms
step:1745/1770 train_time:172784ms step_avg:99.59ms
step:1746/1770 train_time:172891ms step_avg:99.59ms
step:1747/1770 train_time:172993ms step_avg:99.59ms
step:1748/1770 train_time:173100ms step_avg:99.60ms
step:1749/1770 train_time:173205ms step_avg:99.60ms
step:1750/1770 train_time:173309ms step_avg:99.60ms
step:1750/1770 val_loss:3.2818 train_time:173411ms step_avg:99.66ms
step:1751/1770 train_time:173432ms step_avg:99.62ms
step:1752/1770 train_time:173523ms step_avg:99.61ms
step:1753/1770 train_time:173626ms step_avg:99.61ms
step:1754/1770 train_time:173731ms step_avg:99.62ms
step:1755/1770 train_time:173835ms step_avg:99.62ms
step:1756/1770 train_time:173940ms step_avg:99.62ms
step:1757/1770 train_time:174044ms step_avg:99.62ms
step:1758/1770 train_time:174148ms step_avg:99.63ms
step:1759/1770 train_time:174253ms step_avg:99.63ms
step:1760/1770 train_time:174358ms step_avg:99.63ms
step:1761/1770 train_time:174466ms step_avg:99.64ms
step:1762/1770 train_time:174574ms step_avg:99.64ms
step:1763/1770 train_time:174677ms step_avg:99.64ms
step:1764/1770 train_time:174783ms step_avg:99.65ms
step:1765/1770 train_time:174887ms step_avg:99.65ms
step:1766/1770 train_time:174995ms step_avg:99.66ms
step:1767/1770 train_time:175098ms step_avg:99.66ms
step:1768/1770 train_time:175203ms step_avg:99.66ms
step:1769/1770 train_time:175306ms step_avg:99.66ms
step:1770/1770 train_time:175410ms step_avg:99.66ms
step:1770/1770 val_loss:3.2786 train_time:175515ms step_avg:99.72ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
