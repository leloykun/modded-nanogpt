import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 07:55:55 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23563ms step_avg:nanms
step:2/1770 train_time:23975ms step_avg:nanms
step:3/1770 train_time:24070ms step_avg:nanms
step:4/1770 train_time:24163ms step_avg:nanms
step:5/1770 train_time:24257ms step_avg:nanms
step:6/1770 train_time:24350ms step_avg:nanms
step:7/1770 train_time:24444ms step_avg:nanms
step:8/1770 train_time:24538ms step_avg:nanms
step:9/1770 train_time:24631ms step_avg:nanms
step:10/1770 train_time:24725ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:93.95ms
step:14/1770 train_time:376ms step_avg:94.01ms
step:15/1770 train_time:470ms step_avg:94.03ms
step:16/1770 train_time:564ms step_avg:94.06ms
step:17/1770 train_time:658ms step_avg:94.07ms
step:18/1770 train_time:753ms step_avg:94.08ms
step:19/1770 train_time:846ms step_avg:94.05ms
step:20/1770 train_time:940ms step_avg:94.02ms
step:21/1770 train_time:1034ms step_avg:94.00ms
step:22/1770 train_time:1128ms step_avg:93.98ms
step:23/1770 train_time:1222ms step_avg:94.00ms
step:24/1770 train_time:1316ms step_avg:93.97ms
step:25/1770 train_time:1410ms step_avg:94.00ms
step:26/1770 train_time:1504ms step_avg:94.02ms
step:27/1770 train_time:1598ms step_avg:94.00ms
step:28/1770 train_time:1692ms step_avg:94.03ms
step:29/1770 train_time:1787ms step_avg:94.05ms
step:30/1770 train_time:1881ms step_avg:94.04ms
step:31/1770 train_time:1974ms step_avg:94.02ms
step:32/1770 train_time:2069ms step_avg:94.03ms
step:33/1770 train_time:2162ms step_avg:94.01ms
step:34/1770 train_time:2256ms step_avg:93.99ms
step:35/1770 train_time:2350ms step_avg:94.00ms
step:36/1770 train_time:2445ms step_avg:94.04ms
step:37/1770 train_time:2539ms step_avg:94.02ms
step:38/1770 train_time:2633ms step_avg:94.02ms
step:39/1770 train_time:2726ms step_avg:94.01ms
step:40/1770 train_time:2820ms step_avg:94.00ms
step:41/1770 train_time:2914ms step_avg:94.00ms
step:42/1770 train_time:3008ms step_avg:93.99ms
step:43/1770 train_time:3101ms step_avg:93.98ms
step:44/1770 train_time:3196ms step_avg:93.99ms
step:45/1770 train_time:3290ms step_avg:94.00ms
step:46/1770 train_time:3385ms step_avg:94.01ms
step:47/1770 train_time:3478ms step_avg:94.01ms
step:48/1770 train_time:3572ms step_avg:94.00ms
step:49/1770 train_time:3666ms step_avg:94.00ms
step:50/1770 train_time:3760ms step_avg:94.00ms
step:51/1770 train_time:3854ms step_avg:94.01ms
step:52/1770 train_time:3949ms step_avg:94.01ms
step:53/1770 train_time:4043ms step_avg:94.01ms
step:54/1770 train_time:4137ms step_avg:94.03ms
step:55/1770 train_time:4231ms step_avg:94.02ms
step:56/1770 train_time:4326ms step_avg:94.05ms
step:57/1770 train_time:4420ms step_avg:94.04ms
step:58/1770 train_time:4514ms step_avg:94.04ms
step:59/1770 train_time:4609ms step_avg:94.06ms
step:60/1770 train_time:4703ms step_avg:94.06ms
step:61/1770 train_time:4797ms step_avg:94.05ms
step:62/1770 train_time:4891ms step_avg:94.05ms
step:63/1770 train_time:4985ms step_avg:94.05ms
step:64/1770 train_time:5078ms step_avg:94.04ms
step:65/1770 train_time:5172ms step_avg:94.04ms
step:66/1770 train_time:5267ms step_avg:94.06ms
step:67/1770 train_time:5361ms step_avg:94.06ms
step:68/1770 train_time:5456ms step_avg:94.06ms
step:69/1770 train_time:5550ms step_avg:94.07ms
step:70/1770 train_time:5644ms step_avg:94.07ms
step:71/1770 train_time:5738ms step_avg:94.06ms
step:72/1770 train_time:5832ms step_avg:94.06ms
step:73/1770 train_time:5926ms step_avg:94.06ms
step:74/1770 train_time:6019ms step_avg:94.05ms
step:75/1770 train_time:6113ms step_avg:94.04ms
step:76/1770 train_time:6206ms step_avg:94.04ms
step:77/1770 train_time:6300ms step_avg:94.03ms
step:78/1770 train_time:6394ms step_avg:94.03ms
step:79/1770 train_time:6488ms step_avg:94.03ms
step:80/1770 train_time:6583ms step_avg:94.04ms
step:81/1770 train_time:6676ms step_avg:94.03ms
step:82/1770 train_time:6770ms step_avg:94.03ms
step:83/1770 train_time:6865ms step_avg:94.03ms
step:84/1770 train_time:6958ms step_avg:94.03ms
step:85/1770 train_time:7053ms step_avg:94.03ms
step:86/1770 train_time:7147ms step_avg:94.04ms
step:87/1770 train_time:7240ms step_avg:94.02ms
step:88/1770 train_time:7334ms step_avg:94.03ms
step:89/1770 train_time:7428ms step_avg:94.03ms
step:90/1770 train_time:7522ms step_avg:94.02ms
step:91/1770 train_time:7616ms step_avg:94.02ms
step:92/1770 train_time:7710ms step_avg:94.02ms
step:93/1770 train_time:7804ms step_avg:94.03ms
step:94/1770 train_time:7898ms step_avg:94.03ms
step:95/1770 train_time:7992ms step_avg:94.03ms
step:96/1770 train_time:8087ms step_avg:94.03ms
step:97/1770 train_time:8181ms step_avg:94.03ms
step:98/1770 train_time:8275ms step_avg:94.03ms
step:99/1770 train_time:8369ms step_avg:94.03ms
step:100/1770 train_time:8463ms step_avg:94.03ms
step:101/1770 train_time:8557ms step_avg:94.04ms
step:102/1770 train_time:8651ms step_avg:94.03ms
step:103/1770 train_time:8745ms step_avg:94.04ms
step:104/1770 train_time:8839ms step_avg:94.03ms
step:105/1770 train_time:8933ms step_avg:94.03ms
step:106/1770 train_time:9027ms step_avg:94.03ms
step:107/1770 train_time:9121ms step_avg:94.03ms
step:108/1770 train_time:9215ms step_avg:94.03ms
step:109/1770 train_time:9309ms step_avg:94.03ms
step:110/1770 train_time:9403ms step_avg:94.03ms
step:111/1770 train_time:9497ms step_avg:94.03ms
step:112/1770 train_time:9591ms step_avg:94.03ms
step:113/1770 train_time:9686ms step_avg:94.04ms
step:114/1770 train_time:9780ms step_avg:94.04ms
step:115/1770 train_time:9874ms step_avg:94.04ms
step:116/1770 train_time:9968ms step_avg:94.04ms
step:117/1770 train_time:10062ms step_avg:94.04ms
step:118/1770 train_time:10156ms step_avg:94.04ms
step:119/1770 train_time:10250ms step_avg:94.04ms
step:120/1770 train_time:10344ms step_avg:94.04ms
step:121/1770 train_time:10438ms step_avg:94.04ms
step:122/1770 train_time:10532ms step_avg:94.04ms
step:123/1770 train_time:10628ms step_avg:94.05ms
step:124/1770 train_time:10721ms step_avg:94.04ms
step:125/1770 train_time:10814ms step_avg:94.04ms
step:125/1770 val_loss:4.6489 train_time:10907ms step_avg:94.85ms
step:126/1770 train_time:10937ms step_avg:94.29ms
step:127/1770 train_time:11006ms step_avg:94.07ms
step:128/1770 train_time:11103ms step_avg:94.09ms
step:129/1770 train_time:11200ms step_avg:94.12ms
step:130/1770 train_time:11295ms step_avg:94.12ms
step:131/1770 train_time:11389ms step_avg:94.12ms
step:132/1770 train_time:11483ms step_avg:94.12ms
step:133/1770 train_time:11576ms step_avg:94.11ms
step:134/1770 train_time:11670ms step_avg:94.12ms
step:135/1770 train_time:11764ms step_avg:94.12ms
step:136/1770 train_time:11859ms step_avg:94.12ms
step:137/1770 train_time:11954ms step_avg:94.12ms
step:138/1770 train_time:12048ms step_avg:94.12ms
step:139/1770 train_time:12143ms step_avg:94.14ms
step:140/1770 train_time:12239ms step_avg:94.15ms
step:141/1770 train_time:12334ms step_avg:94.15ms
step:142/1770 train_time:12428ms step_avg:94.15ms
step:143/1770 train_time:12523ms step_avg:94.16ms
step:144/1770 train_time:12618ms step_avg:94.16ms
step:145/1770 train_time:12712ms step_avg:94.16ms
step:146/1770 train_time:12807ms step_avg:94.17ms
step:147/1770 train_time:12901ms step_avg:94.17ms
step:148/1770 train_time:12996ms step_avg:94.17ms
step:149/1770 train_time:13090ms step_avg:94.17ms
step:150/1770 train_time:13185ms step_avg:94.18ms
step:151/1770 train_time:13281ms step_avg:94.19ms
step:152/1770 train_time:13375ms step_avg:94.19ms
step:153/1770 train_time:13470ms step_avg:94.19ms
step:154/1770 train_time:13565ms step_avg:94.20ms
step:155/1770 train_time:13659ms step_avg:94.20ms
step:156/1770 train_time:13753ms step_avg:94.20ms
step:157/1770 train_time:13848ms step_avg:94.20ms
step:158/1770 train_time:13943ms step_avg:94.21ms
step:159/1770 train_time:14038ms step_avg:94.21ms
step:160/1770 train_time:14132ms step_avg:94.21ms
step:161/1770 train_time:14226ms step_avg:94.21ms
step:162/1770 train_time:14321ms step_avg:94.22ms
step:163/1770 train_time:14417ms step_avg:94.23ms
step:164/1770 train_time:14510ms step_avg:94.22ms
step:165/1770 train_time:14605ms step_avg:94.22ms
step:166/1770 train_time:14700ms step_avg:94.23ms
step:167/1770 train_time:14794ms step_avg:94.23ms
step:168/1770 train_time:14888ms step_avg:94.23ms
step:169/1770 train_time:14983ms step_avg:94.23ms
step:170/1770 train_time:15078ms step_avg:94.24ms
step:171/1770 train_time:15172ms step_avg:94.24ms
step:172/1770 train_time:15267ms step_avg:94.24ms
step:173/1770 train_time:15363ms step_avg:94.25ms
step:174/1770 train_time:15459ms step_avg:94.26ms
step:175/1770 train_time:15553ms step_avg:94.26ms
step:176/1770 train_time:15647ms step_avg:94.26ms
step:177/1770 train_time:15743ms step_avg:94.27ms
step:178/1770 train_time:15837ms step_avg:94.27ms
step:179/1770 train_time:15931ms step_avg:94.27ms
step:180/1770 train_time:16026ms step_avg:94.27ms
step:181/1770 train_time:16121ms step_avg:94.27ms
step:182/1770 train_time:16215ms step_avg:94.28ms
step:183/1770 train_time:16310ms step_avg:94.28ms
step:184/1770 train_time:16405ms step_avg:94.28ms
step:185/1770 train_time:16500ms step_avg:94.29ms
step:186/1770 train_time:16594ms step_avg:94.28ms
step:187/1770 train_time:16689ms step_avg:94.29ms
step:188/1770 train_time:16784ms step_avg:94.29ms
step:189/1770 train_time:16880ms step_avg:94.30ms
step:190/1770 train_time:16974ms step_avg:94.30ms
step:191/1770 train_time:17068ms step_avg:94.30ms
step:192/1770 train_time:17163ms step_avg:94.30ms
step:193/1770 train_time:17258ms step_avg:94.31ms
step:194/1770 train_time:17352ms step_avg:94.31ms
step:195/1770 train_time:17448ms step_avg:94.31ms
step:196/1770 train_time:17542ms step_avg:94.31ms
step:197/1770 train_time:17637ms step_avg:94.31ms
step:198/1770 train_time:17731ms step_avg:94.31ms
step:199/1770 train_time:17826ms step_avg:94.32ms
step:200/1770 train_time:17921ms step_avg:94.32ms
step:201/1770 train_time:18017ms step_avg:94.33ms
step:202/1770 train_time:18110ms step_avg:94.32ms
step:203/1770 train_time:18205ms step_avg:94.33ms
step:204/1770 train_time:18300ms step_avg:94.33ms
step:205/1770 train_time:18394ms step_avg:94.33ms
step:206/1770 train_time:18488ms step_avg:94.33ms
step:207/1770 train_time:18583ms step_avg:94.33ms
step:208/1770 train_time:18678ms step_avg:94.33ms
step:209/1770 train_time:18773ms step_avg:94.33ms
step:210/1770 train_time:18867ms step_avg:94.34ms
step:211/1770 train_time:18962ms step_avg:94.34ms
step:212/1770 train_time:19058ms step_avg:94.34ms
step:213/1770 train_time:19152ms step_avg:94.34ms
step:214/1770 train_time:19246ms step_avg:94.34ms
step:215/1770 train_time:19341ms step_avg:94.35ms
step:216/1770 train_time:19435ms step_avg:94.35ms
step:217/1770 train_time:19530ms step_avg:94.35ms
step:218/1770 train_time:19625ms step_avg:94.35ms
step:219/1770 train_time:19720ms step_avg:94.35ms
step:220/1770 train_time:19814ms step_avg:94.35ms
step:221/1770 train_time:19909ms step_avg:94.36ms
step:222/1770 train_time:20004ms step_avg:94.36ms
step:223/1770 train_time:20100ms step_avg:94.36ms
step:224/1770 train_time:20194ms step_avg:94.36ms
step:225/1770 train_time:20288ms step_avg:94.36ms
step:226/1770 train_time:20384ms step_avg:94.37ms
step:227/1770 train_time:20479ms step_avg:94.37ms
step:228/1770 train_time:20573ms step_avg:94.37ms
step:229/1770 train_time:20667ms step_avg:94.37ms
step:230/1770 train_time:20763ms step_avg:94.38ms
step:231/1770 train_time:20859ms step_avg:94.38ms
step:232/1770 train_time:20952ms step_avg:94.38ms
step:233/1770 train_time:21047ms step_avg:94.38ms
step:234/1770 train_time:21142ms step_avg:94.38ms
step:235/1770 train_time:21237ms step_avg:94.39ms
step:236/1770 train_time:21332ms step_avg:94.39ms
step:237/1770 train_time:21426ms step_avg:94.39ms
step:238/1770 train_time:21521ms step_avg:94.39ms
step:239/1770 train_time:21615ms step_avg:94.39ms
step:240/1770 train_time:21709ms step_avg:94.39ms
step:241/1770 train_time:21805ms step_avg:94.39ms
step:242/1770 train_time:21900ms step_avg:94.40ms
step:243/1770 train_time:21994ms step_avg:94.40ms
step:244/1770 train_time:22090ms step_avg:94.40ms
step:245/1770 train_time:22184ms step_avg:94.40ms
step:246/1770 train_time:22279ms step_avg:94.40ms
step:247/1770 train_time:22374ms step_avg:94.40ms
step:248/1770 train_time:22468ms step_avg:94.40ms
step:249/1770 train_time:22563ms step_avg:94.41ms
step:250/1770 train_time:22659ms step_avg:94.41ms
step:250/1770 val_loss:4.1177 train_time:22751ms step_avg:94.80ms
step:251/1770 train_time:22776ms step_avg:94.51ms
step:252/1770 train_time:22855ms step_avg:94.44ms
step:253/1770 train_time:22951ms step_avg:94.45ms
step:254/1770 train_time:23045ms step_avg:94.45ms
step:255/1770 train_time:23140ms step_avg:94.45ms
step:256/1770 train_time:23234ms step_avg:94.45ms
step:257/1770 train_time:23328ms step_avg:94.45ms
step:258/1770 train_time:23422ms step_avg:94.45ms
step:259/1770 train_time:23517ms step_avg:94.45ms
step:260/1770 train_time:23611ms step_avg:94.44ms
step:261/1770 train_time:23705ms step_avg:94.44ms
step:262/1770 train_time:23801ms step_avg:94.45ms
step:263/1770 train_time:23899ms step_avg:94.46ms
step:264/1770 train_time:23995ms step_avg:94.47ms
step:265/1770 train_time:24090ms step_avg:94.47ms
step:266/1770 train_time:24185ms step_avg:94.47ms
step:267/1770 train_time:24280ms step_avg:94.48ms
step:268/1770 train_time:24375ms step_avg:94.48ms
step:269/1770 train_time:24470ms step_avg:94.48ms
step:270/1770 train_time:24565ms step_avg:94.48ms
step:271/1770 train_time:24660ms step_avg:94.48ms
step:272/1770 train_time:24756ms step_avg:94.49ms
step:273/1770 train_time:24850ms step_avg:94.49ms
step:274/1770 train_time:24946ms step_avg:94.49ms
step:275/1770 train_time:25042ms step_avg:94.50ms
step:276/1770 train_time:25137ms step_avg:94.50ms
step:277/1770 train_time:25232ms step_avg:94.50ms
step:278/1770 train_time:25327ms step_avg:94.50ms
step:279/1770 train_time:25422ms step_avg:94.50ms
step:280/1770 train_time:25517ms step_avg:94.51ms
step:281/1770 train_time:25611ms step_avg:94.51ms
step:282/1770 train_time:25706ms step_avg:94.51ms
step:283/1770 train_time:25802ms step_avg:94.51ms
step:284/1770 train_time:25897ms step_avg:94.52ms
step:285/1770 train_time:25992ms step_avg:94.52ms
step:286/1770 train_time:26087ms step_avg:94.52ms
step:287/1770 train_time:26183ms step_avg:94.52ms
step:288/1770 train_time:26278ms step_avg:94.53ms
step:289/1770 train_time:26373ms step_avg:94.53ms
step:290/1770 train_time:26468ms step_avg:94.53ms
step:291/1770 train_time:26563ms step_avg:94.53ms
step:292/1770 train_time:26658ms step_avg:94.53ms
step:293/1770 train_time:26753ms step_avg:94.53ms
step:294/1770 train_time:26848ms step_avg:94.54ms
step:295/1770 train_time:26944ms step_avg:94.54ms
step:296/1770 train_time:27039ms step_avg:94.54ms
step:297/1770 train_time:27135ms step_avg:94.55ms
step:298/1770 train_time:27230ms step_avg:94.55ms
step:299/1770 train_time:27325ms step_avg:94.55ms
step:300/1770 train_time:27421ms step_avg:94.55ms
step:301/1770 train_time:27516ms step_avg:94.56ms
step:302/1770 train_time:27611ms step_avg:94.56ms
step:303/1770 train_time:27706ms step_avg:94.56ms
step:304/1770 train_time:27801ms step_avg:94.56ms
step:305/1770 train_time:27896ms step_avg:94.56ms
step:306/1770 train_time:27991ms step_avg:94.56ms
step:307/1770 train_time:28086ms step_avg:94.57ms
step:308/1770 train_time:28182ms step_avg:94.57ms
step:309/1770 train_time:28276ms step_avg:94.57ms
step:310/1770 train_time:28371ms step_avg:94.57ms
step:311/1770 train_time:28467ms step_avg:94.57ms
step:312/1770 train_time:28563ms step_avg:94.58ms
step:313/1770 train_time:28659ms step_avg:94.58ms
step:314/1770 train_time:28753ms step_avg:94.58ms
step:315/1770 train_time:28848ms step_avg:94.58ms
step:316/1770 train_time:28944ms step_avg:94.59ms
step:317/1770 train_time:29039ms step_avg:94.59ms
step:318/1770 train_time:29134ms step_avg:94.59ms
step:319/1770 train_time:29229ms step_avg:94.59ms
step:320/1770 train_time:29324ms step_avg:94.59ms
step:321/1770 train_time:29419ms step_avg:94.59ms
step:322/1770 train_time:29514ms step_avg:94.60ms
step:323/1770 train_time:29608ms step_avg:94.60ms
step:324/1770 train_time:29704ms step_avg:94.60ms
step:325/1770 train_time:29800ms step_avg:94.60ms
step:326/1770 train_time:29896ms step_avg:94.61ms
step:327/1770 train_time:29991ms step_avg:94.61ms
step:328/1770 train_time:30086ms step_avg:94.61ms
step:329/1770 train_time:30181ms step_avg:94.61ms
step:330/1770 train_time:30276ms step_avg:94.61ms
step:331/1770 train_time:30372ms step_avg:94.62ms
step:332/1770 train_time:30466ms step_avg:94.62ms
step:333/1770 train_time:30562ms step_avg:94.62ms
step:334/1770 train_time:30657ms step_avg:94.62ms
step:335/1770 train_time:30752ms step_avg:94.62ms
step:336/1770 train_time:30847ms step_avg:94.62ms
step:337/1770 train_time:30943ms step_avg:94.63ms
step:338/1770 train_time:31038ms step_avg:94.63ms
step:339/1770 train_time:31132ms step_avg:94.63ms
step:340/1770 train_time:31227ms step_avg:94.63ms
step:341/1770 train_time:31323ms step_avg:94.63ms
step:342/1770 train_time:31418ms step_avg:94.63ms
step:343/1770 train_time:31514ms step_avg:94.64ms
step:344/1770 train_time:31609ms step_avg:94.64ms
step:345/1770 train_time:31705ms step_avg:94.64ms
step:346/1770 train_time:31800ms step_avg:94.64ms
step:347/1770 train_time:31895ms step_avg:94.65ms
step:348/1770 train_time:31991ms step_avg:94.65ms
step:349/1770 train_time:32086ms step_avg:94.65ms
step:350/1770 train_time:32182ms step_avg:94.65ms
step:351/1770 train_time:32277ms step_avg:94.65ms
step:352/1770 train_time:32372ms step_avg:94.65ms
step:353/1770 train_time:32467ms step_avg:94.66ms
step:354/1770 train_time:32562ms step_avg:94.66ms
step:355/1770 train_time:32658ms step_avg:94.66ms
step:356/1770 train_time:32753ms step_avg:94.66ms
step:357/1770 train_time:32848ms step_avg:94.66ms
step:358/1770 train_time:32943ms step_avg:94.66ms
step:359/1770 train_time:33039ms step_avg:94.67ms
step:360/1770 train_time:33134ms step_avg:94.67ms
step:361/1770 train_time:33229ms step_avg:94.67ms
step:362/1770 train_time:33325ms step_avg:94.67ms
step:363/1770 train_time:33420ms step_avg:94.67ms
step:364/1770 train_time:33515ms step_avg:94.67ms
step:365/1770 train_time:33610ms step_avg:94.68ms
step:366/1770 train_time:33705ms step_avg:94.68ms
step:367/1770 train_time:33800ms step_avg:94.68ms
step:368/1770 train_time:33896ms step_avg:94.68ms
step:369/1770 train_time:33991ms step_avg:94.68ms
step:370/1770 train_time:34086ms step_avg:94.68ms
step:371/1770 train_time:34181ms step_avg:94.68ms
step:372/1770 train_time:34278ms step_avg:94.69ms
step:373/1770 train_time:34372ms step_avg:94.69ms
step:374/1770 train_time:34467ms step_avg:94.69ms
step:375/1770 train_time:34562ms step_avg:94.69ms
step:375/1770 val_loss:3.9040 train_time:34655ms step_avg:94.95ms
step:376/1770 train_time:34678ms step_avg:94.75ms
step:377/1770 train_time:34757ms step_avg:94.71ms
step:378/1770 train_time:34853ms step_avg:94.71ms
step:379/1770 train_time:34948ms step_avg:94.71ms
step:380/1770 train_time:35043ms step_avg:94.71ms
step:381/1770 train_time:35138ms step_avg:94.71ms
step:382/1770 train_time:35232ms step_avg:94.71ms
step:383/1770 train_time:35327ms step_avg:94.71ms
step:384/1770 train_time:35422ms step_avg:94.71ms
step:385/1770 train_time:35516ms step_avg:94.71ms
step:386/1770 train_time:35612ms step_avg:94.71ms
step:387/1770 train_time:35709ms step_avg:94.72ms
step:388/1770 train_time:35805ms step_avg:94.72ms
step:389/1770 train_time:35900ms step_avg:94.72ms
step:390/1770 train_time:35995ms step_avg:94.72ms
step:391/1770 train_time:36091ms step_avg:94.73ms
step:392/1770 train_time:36186ms step_avg:94.73ms
step:393/1770 train_time:36281ms step_avg:94.73ms
step:394/1770 train_time:36375ms step_avg:94.73ms
step:395/1770 train_time:36470ms step_avg:94.73ms
step:396/1770 train_time:36568ms step_avg:94.73ms
step:397/1770 train_time:36666ms step_avg:94.74ms
step:398/1770 train_time:36763ms step_avg:94.75ms
step:399/1770 train_time:36860ms step_avg:94.76ms
step:400/1770 train_time:36956ms step_avg:94.76ms
step:401/1770 train_time:37053ms step_avg:94.77ms
step:402/1770 train_time:37150ms step_avg:94.77ms
step:403/1770 train_time:37248ms step_avg:94.78ms
step:404/1770 train_time:37345ms step_avg:94.78ms
step:405/1770 train_time:37441ms step_avg:94.79ms
step:406/1770 train_time:37539ms step_avg:94.80ms
step:407/1770 train_time:37635ms step_avg:94.80ms
step:408/1770 train_time:37732ms step_avg:94.80ms
step:409/1770 train_time:37829ms step_avg:94.81ms
step:410/1770 train_time:37927ms step_avg:94.82ms
step:411/1770 train_time:38024ms step_avg:94.82ms
step:412/1770 train_time:38121ms step_avg:94.83ms
step:413/1770 train_time:38218ms step_avg:94.83ms
step:414/1770 train_time:38314ms step_avg:94.84ms
step:415/1770 train_time:38412ms step_avg:94.84ms
step:416/1770 train_time:38509ms step_avg:94.85ms
step:417/1770 train_time:38606ms step_avg:94.86ms
step:418/1770 train_time:38703ms step_avg:94.86ms
step:419/1770 train_time:38800ms step_avg:94.87ms
step:420/1770 train_time:38897ms step_avg:94.87ms
step:421/1770 train_time:38994ms step_avg:94.88ms
step:422/1770 train_time:39091ms step_avg:94.88ms
step:423/1770 train_time:39189ms step_avg:94.89ms
step:424/1770 train_time:39286ms step_avg:94.89ms
step:425/1770 train_time:39384ms step_avg:94.90ms
step:426/1770 train_time:39481ms step_avg:94.91ms
step:427/1770 train_time:39578ms step_avg:94.91ms
step:428/1770 train_time:39675ms step_avg:94.92ms
step:429/1770 train_time:39773ms step_avg:94.92ms
step:430/1770 train_time:39870ms step_avg:94.93ms
step:431/1770 train_time:39967ms step_avg:94.93ms
step:432/1770 train_time:40064ms step_avg:94.94ms
step:433/1770 train_time:40161ms step_avg:94.94ms
step:434/1770 train_time:40257ms step_avg:94.95ms
step:435/1770 train_time:40354ms step_avg:94.95ms
step:436/1770 train_time:40451ms step_avg:94.96ms
step:437/1770 train_time:40549ms step_avg:94.96ms
step:438/1770 train_time:40647ms step_avg:94.97ms
step:439/1770 train_time:40744ms step_avg:94.97ms
step:440/1770 train_time:40841ms step_avg:94.98ms
step:441/1770 train_time:40937ms step_avg:94.98ms
step:442/1770 train_time:41034ms step_avg:94.99ms
step:443/1770 train_time:41132ms step_avg:94.99ms
step:444/1770 train_time:41229ms step_avg:95.00ms
step:445/1770 train_time:41326ms step_avg:95.00ms
step:446/1770 train_time:41423ms step_avg:95.01ms
step:447/1770 train_time:41519ms step_avg:95.01ms
step:448/1770 train_time:41616ms step_avg:95.01ms
step:449/1770 train_time:41713ms step_avg:95.02ms
step:450/1770 train_time:41811ms step_avg:95.03ms
step:451/1770 train_time:41909ms step_avg:95.03ms
step:452/1770 train_time:42006ms step_avg:95.04ms
step:453/1770 train_time:42104ms step_avg:95.04ms
step:454/1770 train_time:42201ms step_avg:95.05ms
step:455/1770 train_time:42297ms step_avg:95.05ms
step:456/1770 train_time:42394ms step_avg:95.05ms
step:457/1770 train_time:42492ms step_avg:95.06ms
step:458/1770 train_time:42589ms step_avg:95.07ms
step:459/1770 train_time:42687ms step_avg:95.07ms
step:460/1770 train_time:42784ms step_avg:95.08ms
step:461/1770 train_time:42881ms step_avg:95.08ms
step:462/1770 train_time:42977ms step_avg:95.08ms
step:463/1770 train_time:43074ms step_avg:95.09ms
step:464/1770 train_time:43172ms step_avg:95.09ms
step:465/1770 train_time:43269ms step_avg:95.10ms
step:466/1770 train_time:43367ms step_avg:95.10ms
step:467/1770 train_time:43464ms step_avg:95.11ms
step:468/1770 train_time:43561ms step_avg:95.11ms
step:469/1770 train_time:43658ms step_avg:95.12ms
step:470/1770 train_time:43755ms step_avg:95.12ms
step:471/1770 train_time:43853ms step_avg:95.12ms
step:472/1770 train_time:43950ms step_avg:95.13ms
step:473/1770 train_time:44048ms step_avg:95.14ms
step:474/1770 train_time:44146ms step_avg:95.14ms
step:475/1770 train_time:44243ms step_avg:95.15ms
step:476/1770 train_time:44339ms step_avg:95.15ms
step:477/1770 train_time:44436ms step_avg:95.15ms
step:478/1770 train_time:44532ms step_avg:95.15ms
step:479/1770 train_time:44629ms step_avg:95.16ms
step:480/1770 train_time:44726ms step_avg:95.16ms
step:481/1770 train_time:44824ms step_avg:95.17ms
step:482/1770 train_time:44920ms step_avg:95.17ms
step:483/1770 train_time:45017ms step_avg:95.17ms
step:484/1770 train_time:45114ms step_avg:95.18ms
step:485/1770 train_time:45211ms step_avg:95.18ms
step:486/1770 train_time:45308ms step_avg:95.19ms
step:487/1770 train_time:45406ms step_avg:95.19ms
step:488/1770 train_time:45504ms step_avg:95.20ms
step:489/1770 train_time:45601ms step_avg:95.20ms
step:490/1770 train_time:45698ms step_avg:95.20ms
step:491/1770 train_time:45794ms step_avg:95.21ms
step:492/1770 train_time:45892ms step_avg:95.21ms
step:493/1770 train_time:45989ms step_avg:95.21ms
step:494/1770 train_time:46086ms step_avg:95.22ms
step:495/1770 train_time:46183ms step_avg:95.22ms
step:496/1770 train_time:46280ms step_avg:95.23ms
step:497/1770 train_time:46377ms step_avg:95.23ms
step:498/1770 train_time:46474ms step_avg:95.23ms
step:499/1770 train_time:46571ms step_avg:95.24ms
step:500/1770 train_time:46669ms step_avg:95.24ms
step:500/1770 val_loss:3.7556 train_time:46764ms step_avg:95.44ms
step:501/1770 train_time:46788ms step_avg:95.29ms
step:502/1770 train_time:46871ms step_avg:95.27ms
step:503/1770 train_time:46969ms step_avg:95.27ms
step:504/1770 train_time:47066ms step_avg:95.27ms
step:505/1770 train_time:47163ms step_avg:95.28ms
step:506/1770 train_time:47260ms step_avg:95.28ms
step:507/1770 train_time:47357ms step_avg:95.28ms
step:508/1770 train_time:47453ms step_avg:95.29ms
step:509/1770 train_time:47550ms step_avg:95.29ms
step:510/1770 train_time:47646ms step_avg:95.29ms
step:511/1770 train_time:47744ms step_avg:95.30ms
step:512/1770 train_time:47843ms step_avg:95.30ms
step:513/1770 train_time:47941ms step_avg:95.31ms
step:514/1770 train_time:48039ms step_avg:95.31ms
step:515/1770 train_time:48137ms step_avg:95.32ms
step:516/1770 train_time:48233ms step_avg:95.32ms
step:517/1770 train_time:48330ms step_avg:95.32ms
step:518/1770 train_time:48426ms step_avg:95.33ms
step:519/1770 train_time:48523ms step_avg:95.33ms
step:520/1770 train_time:48620ms step_avg:95.33ms
step:521/1770 train_time:48717ms step_avg:95.34ms
step:522/1770 train_time:48815ms step_avg:95.34ms
step:523/1770 train_time:48912ms step_avg:95.34ms
step:524/1770 train_time:49009ms step_avg:95.35ms
step:525/1770 train_time:49106ms step_avg:95.35ms
step:526/1770 train_time:49204ms step_avg:95.36ms
step:527/1770 train_time:49302ms step_avg:95.36ms
step:528/1770 train_time:49399ms step_avg:95.37ms
step:529/1770 train_time:49497ms step_avg:95.37ms
step:530/1770 train_time:49595ms step_avg:95.37ms
step:531/1770 train_time:49692ms step_avg:95.38ms
step:532/1770 train_time:49789ms step_avg:95.38ms
step:533/1770 train_time:49887ms step_avg:95.39ms
step:534/1770 train_time:49984ms step_avg:95.39ms
step:535/1770 train_time:50082ms step_avg:95.39ms
step:536/1770 train_time:50180ms step_avg:95.40ms
step:537/1770 train_time:50278ms step_avg:95.40ms
step:538/1770 train_time:50376ms step_avg:95.41ms
step:539/1770 train_time:50473ms step_avg:95.41ms
step:540/1770 train_time:50571ms step_avg:95.42ms
step:541/1770 train_time:50668ms step_avg:95.42ms
step:542/1770 train_time:50765ms step_avg:95.42ms
step:543/1770 train_time:50862ms step_avg:95.43ms
step:544/1770 train_time:50960ms step_avg:95.43ms
step:545/1770 train_time:51058ms step_avg:95.44ms
step:546/1770 train_time:51157ms step_avg:95.44ms
step:547/1770 train_time:51255ms step_avg:95.45ms
step:548/1770 train_time:51352ms step_avg:95.45ms
step:549/1770 train_time:51448ms step_avg:95.45ms
step:550/1770 train_time:51545ms step_avg:95.45ms
step:551/1770 train_time:51643ms step_avg:95.46ms
step:552/1770 train_time:51740ms step_avg:95.46ms
step:553/1770 train_time:51838ms step_avg:95.47ms
step:554/1770 train_time:51936ms step_avg:95.47ms
step:555/1770 train_time:52033ms step_avg:95.47ms
step:556/1770 train_time:52130ms step_avg:95.48ms
step:557/1770 train_time:52227ms step_avg:95.48ms
step:558/1770 train_time:52325ms step_avg:95.48ms
step:559/1770 train_time:52422ms step_avg:95.49ms
step:560/1770 train_time:52520ms step_avg:95.49ms
step:561/1770 train_time:52618ms step_avg:95.49ms
step:562/1770 train_time:52715ms step_avg:95.50ms
step:563/1770 train_time:52813ms step_avg:95.50ms
step:564/1770 train_time:52909ms step_avg:95.50ms
step:565/1770 train_time:53006ms step_avg:95.51ms
step:566/1770 train_time:53104ms step_avg:95.51ms
step:567/1770 train_time:53201ms step_avg:95.51ms
step:568/1770 train_time:53300ms step_avg:95.52ms
step:569/1770 train_time:53397ms step_avg:95.52ms
step:570/1770 train_time:53495ms step_avg:95.53ms
step:571/1770 train_time:53592ms step_avg:95.53ms
step:572/1770 train_time:53690ms step_avg:95.53ms
step:573/1770 train_time:53787ms step_avg:95.54ms
step:574/1770 train_time:53885ms step_avg:95.54ms
step:575/1770 train_time:53982ms step_avg:95.54ms
step:576/1770 train_time:54080ms step_avg:95.55ms
step:577/1770 train_time:54178ms step_avg:95.55ms
step:578/1770 train_time:54276ms step_avg:95.56ms
step:579/1770 train_time:54374ms step_avg:95.56ms
step:580/1770 train_time:54471ms step_avg:95.56ms
step:581/1770 train_time:54568ms step_avg:95.57ms
step:582/1770 train_time:54665ms step_avg:95.57ms
step:583/1770 train_time:54763ms step_avg:95.57ms
step:584/1770 train_time:54860ms step_avg:95.57ms
step:585/1770 train_time:54958ms step_avg:95.58ms
step:586/1770 train_time:55056ms step_avg:95.58ms
step:587/1770 train_time:55153ms step_avg:95.59ms
step:588/1770 train_time:55251ms step_avg:95.59ms
step:589/1770 train_time:55347ms step_avg:95.59ms
step:590/1770 train_time:55445ms step_avg:95.59ms
step:591/1770 train_time:55542ms step_avg:95.60ms
step:592/1770 train_time:55640ms step_avg:95.60ms
step:593/1770 train_time:55738ms step_avg:95.61ms
step:594/1770 train_time:55836ms step_avg:95.61ms
step:595/1770 train_time:55934ms step_avg:95.61ms
step:596/1770 train_time:56031ms step_avg:95.62ms
step:597/1770 train_time:56128ms step_avg:95.62ms
step:598/1770 train_time:56225ms step_avg:95.62ms
step:599/1770 train_time:56323ms step_avg:95.62ms
step:600/1770 train_time:56420ms step_avg:95.63ms
step:601/1770 train_time:56518ms step_avg:95.63ms
step:602/1770 train_time:56616ms step_avg:95.64ms
step:603/1770 train_time:56713ms step_avg:95.64ms
step:604/1770 train_time:56810ms step_avg:95.64ms
step:605/1770 train_time:56907ms step_avg:95.64ms
step:606/1770 train_time:57005ms step_avg:95.65ms
step:607/1770 train_time:57103ms step_avg:95.65ms
step:608/1770 train_time:57200ms step_avg:95.65ms
step:609/1770 train_time:57298ms step_avg:95.66ms
step:610/1770 train_time:57395ms step_avg:95.66ms
step:611/1770 train_time:57492ms step_avg:95.66ms
step:612/1770 train_time:57590ms step_avg:95.66ms
step:613/1770 train_time:57687ms step_avg:95.67ms
step:614/1770 train_time:57784ms step_avg:95.67ms
step:615/1770 train_time:57882ms step_avg:95.67ms
step:616/1770 train_time:57980ms step_avg:95.68ms
step:617/1770 train_time:58078ms step_avg:95.68ms
step:618/1770 train_time:58176ms step_avg:95.68ms
step:619/1770 train_time:58273ms step_avg:95.69ms
step:620/1770 train_time:58370ms step_avg:95.69ms
step:621/1770 train_time:58467ms step_avg:95.69ms
step:622/1770 train_time:58565ms step_avg:95.69ms
step:623/1770 train_time:58662ms step_avg:95.70ms
step:624/1770 train_time:58760ms step_avg:95.70ms
step:625/1770 train_time:58858ms step_avg:95.70ms
step:625/1770 val_loss:3.6670 train_time:58954ms step_avg:95.86ms
step:626/1770 train_time:58976ms step_avg:95.74ms
step:627/1770 train_time:59060ms step_avg:95.72ms
step:628/1770 train_time:59160ms step_avg:95.73ms
step:629/1770 train_time:59258ms step_avg:95.73ms
step:630/1770 train_time:59355ms step_avg:95.73ms
step:631/1770 train_time:59452ms step_avg:95.74ms
step:632/1770 train_time:59549ms step_avg:95.74ms
step:633/1770 train_time:59647ms step_avg:95.74ms
step:634/1770 train_time:59744ms step_avg:95.74ms
step:635/1770 train_time:59840ms step_avg:95.74ms
step:636/1770 train_time:59937ms step_avg:95.75ms
step:637/1770 train_time:60036ms step_avg:95.75ms
step:638/1770 train_time:60134ms step_avg:95.75ms
step:639/1770 train_time:60232ms step_avg:95.76ms
step:640/1770 train_time:60331ms step_avg:95.76ms
step:641/1770 train_time:60429ms step_avg:95.77ms
step:642/1770 train_time:60527ms step_avg:95.77ms
step:643/1770 train_time:60625ms step_avg:95.77ms
step:644/1770 train_time:60721ms step_avg:95.78ms
step:645/1770 train_time:60818ms step_avg:95.78ms
step:646/1770 train_time:60915ms step_avg:95.78ms
step:647/1770 train_time:61013ms step_avg:95.78ms
step:648/1770 train_time:61111ms step_avg:95.79ms
step:649/1770 train_time:61209ms step_avg:95.79ms
step:650/1770 train_time:61307ms step_avg:95.79ms
step:651/1770 train_time:61405ms step_avg:95.79ms
step:652/1770 train_time:61502ms step_avg:95.80ms
step:653/1770 train_time:61599ms step_avg:95.80ms
step:654/1770 train_time:61696ms step_avg:95.80ms
step:655/1770 train_time:61793ms step_avg:95.80ms
step:656/1770 train_time:61891ms step_avg:95.81ms
step:657/1770 train_time:61989ms step_avg:95.81ms
step:658/1770 train_time:62088ms step_avg:95.82ms
step:659/1770 train_time:62188ms step_avg:95.82ms
step:660/1770 train_time:62289ms step_avg:95.83ms
step:661/1770 train_time:62388ms step_avg:95.83ms
step:662/1770 train_time:62488ms step_avg:95.84ms
step:663/1770 train_time:62587ms step_avg:95.85ms
step:664/1770 train_time:62687ms step_avg:95.85ms
step:665/1770 train_time:62787ms step_avg:95.86ms
step:666/1770 train_time:62886ms step_avg:95.86ms
step:667/1770 train_time:62985ms step_avg:95.87ms
step:668/1770 train_time:63084ms step_avg:95.87ms
step:669/1770 train_time:63183ms step_avg:95.88ms
step:670/1770 train_time:63283ms step_avg:95.88ms
step:671/1770 train_time:63382ms step_avg:95.89ms
step:672/1770 train_time:63481ms step_avg:95.89ms
step:673/1770 train_time:63579ms step_avg:95.90ms
step:674/1770 train_time:63678ms step_avg:95.90ms
step:675/1770 train_time:63777ms step_avg:95.91ms
step:676/1770 train_time:63876ms step_avg:95.91ms
step:677/1770 train_time:63976ms step_avg:95.92ms
step:678/1770 train_time:64076ms step_avg:95.92ms
step:679/1770 train_time:64176ms step_avg:95.93ms
step:680/1770 train_time:64276ms step_avg:95.93ms
step:681/1770 train_time:64375ms step_avg:95.94ms
step:682/1770 train_time:64474ms step_avg:95.94ms
step:683/1770 train_time:64574ms step_avg:95.95ms
step:684/1770 train_time:64674ms step_avg:95.95ms
step:685/1770 train_time:64773ms step_avg:95.96ms
step:686/1770 train_time:64872ms step_avg:95.96ms
step:687/1770 train_time:64972ms step_avg:95.97ms
step:688/1770 train_time:65073ms step_avg:95.98ms
step:689/1770 train_time:65173ms step_avg:95.98ms
step:690/1770 train_time:65273ms step_avg:95.99ms
step:691/1770 train_time:65373ms step_avg:96.00ms
step:692/1770 train_time:65473ms step_avg:96.00ms
step:693/1770 train_time:65573ms step_avg:96.01ms
step:694/1770 train_time:65672ms step_avg:96.01ms
step:695/1770 train_time:65772ms step_avg:96.02ms
step:696/1770 train_time:65871ms step_avg:96.02ms
step:697/1770 train_time:65970ms step_avg:96.03ms
step:698/1770 train_time:66070ms step_avg:96.03ms
step:699/1770 train_time:66170ms step_avg:96.04ms
step:700/1770 train_time:66270ms step_avg:96.04ms
step:701/1770 train_time:66370ms step_avg:96.05ms
step:702/1770 train_time:66470ms step_avg:96.06ms
step:703/1770 train_time:66570ms step_avg:96.06ms
step:704/1770 train_time:66669ms step_avg:96.07ms
step:705/1770 train_time:66769ms step_avg:96.07ms
step:706/1770 train_time:66869ms step_avg:96.08ms
step:707/1770 train_time:66969ms step_avg:96.08ms
step:708/1770 train_time:67069ms step_avg:96.09ms
step:709/1770 train_time:67169ms step_avg:96.09ms
step:710/1770 train_time:67269ms step_avg:96.10ms
step:711/1770 train_time:67369ms step_avg:96.10ms
step:712/1770 train_time:67470ms step_avg:96.11ms
step:713/1770 train_time:67570ms step_avg:96.12ms
step:714/1770 train_time:67669ms step_avg:96.12ms
step:715/1770 train_time:67769ms step_avg:96.13ms
step:716/1770 train_time:67869ms step_avg:96.13ms
step:717/1770 train_time:67969ms step_avg:96.14ms
step:718/1770 train_time:68069ms step_avg:96.14ms
step:719/1770 train_time:68168ms step_avg:96.15ms
step:720/1770 train_time:68268ms step_avg:96.15ms
step:721/1770 train_time:68368ms step_avg:96.16ms
step:722/1770 train_time:68467ms step_avg:96.16ms
step:723/1770 train_time:68566ms step_avg:96.17ms
step:724/1770 train_time:68665ms step_avg:96.17ms
step:725/1770 train_time:68763ms step_avg:96.17ms
step:726/1770 train_time:68862ms step_avg:96.18ms
step:727/1770 train_time:68960ms step_avg:96.18ms
step:728/1770 train_time:69059ms step_avg:96.18ms
step:729/1770 train_time:69158ms step_avg:96.19ms
step:730/1770 train_time:69257ms step_avg:96.19ms
step:731/1770 train_time:69356ms step_avg:96.19ms
step:732/1770 train_time:69455ms step_avg:96.20ms
step:733/1770 train_time:69555ms step_avg:96.20ms
step:734/1770 train_time:69656ms step_avg:96.21ms
step:735/1770 train_time:69755ms step_avg:96.21ms
step:736/1770 train_time:69855ms step_avg:96.22ms
step:737/1770 train_time:69954ms step_avg:96.22ms
step:738/1770 train_time:70053ms step_avg:96.23ms
step:739/1770 train_time:70153ms step_avg:96.23ms
step:740/1770 train_time:70253ms step_avg:96.24ms
step:741/1770 train_time:70352ms step_avg:96.24ms
step:742/1770 train_time:70452ms step_avg:96.25ms
step:743/1770 train_time:70552ms step_avg:96.25ms
step:744/1770 train_time:70651ms step_avg:96.26ms
step:745/1770 train_time:70751ms step_avg:96.26ms
step:746/1770 train_time:70851ms step_avg:96.26ms
step:747/1770 train_time:70950ms step_avg:96.27ms
step:748/1770 train_time:71050ms step_avg:96.27ms
step:749/1770 train_time:71149ms step_avg:96.28ms
step:750/1770 train_time:71248ms step_avg:96.28ms
step:750/1770 val_loss:3.6012 train_time:71346ms step_avg:96.41ms
step:751/1770 train_time:71369ms step_avg:96.31ms
step:752/1770 train_time:71454ms step_avg:96.30ms
step:753/1770 train_time:71551ms step_avg:96.30ms
step:754/1770 train_time:71649ms step_avg:96.30ms
step:755/1770 train_time:71748ms step_avg:96.31ms
step:756/1770 train_time:71847ms step_avg:96.31ms
step:757/1770 train_time:71945ms step_avg:96.31ms
step:758/1770 train_time:72044ms step_avg:96.32ms
step:759/1770 train_time:72143ms step_avg:96.32ms
step:760/1770 train_time:72242ms step_avg:96.32ms
step:761/1770 train_time:72343ms step_avg:96.33ms
step:762/1770 train_time:72443ms step_avg:96.33ms
step:763/1770 train_time:72544ms step_avg:96.34ms
step:764/1770 train_time:72644ms step_avg:96.35ms
step:765/1770 train_time:72745ms step_avg:96.35ms
step:766/1770 train_time:72844ms step_avg:96.35ms
step:767/1770 train_time:72943ms step_avg:96.36ms
step:768/1770 train_time:73042ms step_avg:96.36ms
step:769/1770 train_time:73142ms step_avg:96.37ms
step:770/1770 train_time:73241ms step_avg:96.37ms
step:771/1770 train_time:73340ms step_avg:96.37ms
step:772/1770 train_time:73440ms step_avg:96.38ms
step:773/1770 train_time:73540ms step_avg:96.38ms
step:774/1770 train_time:73640ms step_avg:96.39ms
step:775/1770 train_time:73741ms step_avg:96.39ms
step:776/1770 train_time:73841ms step_avg:96.40ms
step:777/1770 train_time:73941ms step_avg:96.40ms
step:778/1770 train_time:74039ms step_avg:96.41ms
step:779/1770 train_time:74138ms step_avg:96.41ms
step:780/1770 train_time:74237ms step_avg:96.41ms
step:781/1770 train_time:74336ms step_avg:96.41ms
step:782/1770 train_time:74434ms step_avg:96.42ms
step:783/1770 train_time:74533ms step_avg:96.42ms
step:784/1770 train_time:74632ms step_avg:96.42ms
step:785/1770 train_time:74731ms step_avg:96.43ms
step:786/1770 train_time:74831ms step_avg:96.43ms
step:787/1770 train_time:74930ms step_avg:96.43ms
step:788/1770 train_time:75029ms step_avg:96.44ms
step:789/1770 train_time:75129ms step_avg:96.44ms
step:790/1770 train_time:75228ms step_avg:96.45ms
step:791/1770 train_time:75327ms step_avg:96.45ms
step:792/1770 train_time:75428ms step_avg:96.45ms
step:793/1770 train_time:75528ms step_avg:96.46ms
step:794/1770 train_time:75627ms step_avg:96.46ms
step:795/1770 train_time:75727ms step_avg:96.47ms
step:796/1770 train_time:75827ms step_avg:96.47ms
step:797/1770 train_time:75927ms step_avg:96.48ms
step:798/1770 train_time:76027ms step_avg:96.48ms
step:799/1770 train_time:76127ms step_avg:96.49ms
step:800/1770 train_time:76226ms step_avg:96.49ms
step:801/1770 train_time:76326ms step_avg:96.49ms
step:802/1770 train_time:76427ms step_avg:96.50ms
step:803/1770 train_time:76525ms step_avg:96.50ms
step:804/1770 train_time:76625ms step_avg:96.51ms
step:805/1770 train_time:76725ms step_avg:96.51ms
step:806/1770 train_time:76825ms step_avg:96.51ms
step:807/1770 train_time:76925ms step_avg:96.52ms
step:808/1770 train_time:77026ms step_avg:96.52ms
step:809/1770 train_time:77126ms step_avg:96.53ms
step:810/1770 train_time:77226ms step_avg:96.53ms
step:811/1770 train_time:77325ms step_avg:96.54ms
step:812/1770 train_time:77425ms step_avg:96.54ms
step:813/1770 train_time:77525ms step_avg:96.54ms
step:814/1770 train_time:77625ms step_avg:96.55ms
step:815/1770 train_time:77726ms step_avg:96.55ms
step:816/1770 train_time:77825ms step_avg:96.56ms
step:817/1770 train_time:77926ms step_avg:96.56ms
step:818/1770 train_time:78025ms step_avg:96.57ms
step:819/1770 train_time:78126ms step_avg:96.57ms
step:820/1770 train_time:78225ms step_avg:96.57ms
step:821/1770 train_time:78325ms step_avg:96.58ms
step:822/1770 train_time:78425ms step_avg:96.58ms
step:823/1770 train_time:78525ms step_avg:96.59ms
step:824/1770 train_time:78625ms step_avg:96.59ms
step:825/1770 train_time:78725ms step_avg:96.60ms
step:826/1770 train_time:78825ms step_avg:96.60ms
step:827/1770 train_time:78925ms step_avg:96.60ms
step:828/1770 train_time:79025ms step_avg:96.61ms
step:829/1770 train_time:79126ms step_avg:96.61ms
step:830/1770 train_time:79227ms step_avg:96.62ms
step:831/1770 train_time:79327ms step_avg:96.62ms
step:832/1770 train_time:79427ms step_avg:96.63ms
step:833/1770 train_time:79526ms step_avg:96.63ms
step:834/1770 train_time:79627ms step_avg:96.63ms
step:835/1770 train_time:79727ms step_avg:96.64ms
step:836/1770 train_time:79827ms step_avg:96.64ms
step:837/1770 train_time:79927ms step_avg:96.65ms
step:838/1770 train_time:80027ms step_avg:96.65ms
step:839/1770 train_time:80126ms step_avg:96.65ms
step:840/1770 train_time:80226ms step_avg:96.66ms
step:841/1770 train_time:80326ms step_avg:96.66ms
step:842/1770 train_time:80426ms step_avg:96.67ms
step:843/1770 train_time:80525ms step_avg:96.67ms
step:844/1770 train_time:80624ms step_avg:96.67ms
step:845/1770 train_time:80724ms step_avg:96.68ms
step:846/1770 train_time:80825ms step_avg:96.68ms
step:847/1770 train_time:80924ms step_avg:96.68ms
step:848/1770 train_time:81024ms step_avg:96.69ms
step:849/1770 train_time:81124ms step_avg:96.69ms
step:850/1770 train_time:81225ms step_avg:96.70ms
step:851/1770 train_time:81325ms step_avg:96.70ms
step:852/1770 train_time:81425ms step_avg:96.70ms
step:853/1770 train_time:81525ms step_avg:96.71ms
step:854/1770 train_time:81624ms step_avg:96.71ms
step:855/1770 train_time:81723ms step_avg:96.71ms
step:856/1770 train_time:81822ms step_avg:96.72ms
step:857/1770 train_time:81922ms step_avg:96.72ms
step:858/1770 train_time:82021ms step_avg:96.72ms
step:859/1770 train_time:82121ms step_avg:96.73ms
step:860/1770 train_time:82221ms step_avg:96.73ms
step:861/1770 train_time:82321ms step_avg:96.73ms
step:862/1770 train_time:82422ms step_avg:96.74ms
step:863/1770 train_time:82521ms step_avg:96.74ms
step:864/1770 train_time:82620ms step_avg:96.75ms
step:865/1770 train_time:82720ms step_avg:96.75ms
step:866/1770 train_time:82820ms step_avg:96.75ms
step:867/1770 train_time:82920ms step_avg:96.76ms
step:868/1770 train_time:83020ms step_avg:96.76ms
step:869/1770 train_time:83119ms step_avg:96.76ms
step:870/1770 train_time:83218ms step_avg:96.77ms
step:871/1770 train_time:83318ms step_avg:96.77ms
step:872/1770 train_time:83418ms step_avg:96.77ms
step:873/1770 train_time:83517ms step_avg:96.78ms
step:874/1770 train_time:83616ms step_avg:96.78ms
step:875/1770 train_time:83715ms step_avg:96.78ms
step:875/1770 val_loss:3.5513 train_time:83812ms step_avg:96.89ms
step:876/1770 train_time:83835ms step_avg:96.81ms
step:877/1770 train_time:83923ms step_avg:96.80ms
step:878/1770 train_time:84023ms step_avg:96.80ms
step:879/1770 train_time:84122ms step_avg:96.80ms
step:880/1770 train_time:84221ms step_avg:96.81ms
step:881/1770 train_time:84320ms step_avg:96.81ms
step:882/1770 train_time:84418ms step_avg:96.81ms
step:883/1770 train_time:84517ms step_avg:96.81ms
step:884/1770 train_time:84615ms step_avg:96.81ms
step:885/1770 train_time:84714ms step_avg:96.82ms
step:886/1770 train_time:84812ms step_avg:96.82ms
step:887/1770 train_time:84912ms step_avg:96.82ms
step:888/1770 train_time:85012ms step_avg:96.82ms
step:889/1770 train_time:85112ms step_avg:96.83ms
step:890/1770 train_time:85212ms step_avg:96.83ms
step:891/1770 train_time:85312ms step_avg:96.84ms
step:892/1770 train_time:85411ms step_avg:96.84ms
step:893/1770 train_time:85512ms step_avg:96.84ms
step:894/1770 train_time:85612ms step_avg:96.85ms
step:895/1770 train_time:85711ms step_avg:96.85ms
step:896/1770 train_time:85810ms step_avg:96.85ms
step:897/1770 train_time:85910ms step_avg:96.85ms
step:898/1770 train_time:86010ms step_avg:96.86ms
step:899/1770 train_time:86110ms step_avg:96.86ms
step:900/1770 train_time:86210ms step_avg:96.86ms
step:901/1770 train_time:86309ms step_avg:96.87ms
step:902/1770 train_time:86410ms step_avg:96.87ms
step:903/1770 train_time:86510ms step_avg:96.88ms
step:904/1770 train_time:86611ms step_avg:96.88ms
step:905/1770 train_time:86710ms step_avg:96.88ms
step:906/1770 train_time:86810ms step_avg:96.89ms
step:907/1770 train_time:86910ms step_avg:96.89ms
step:908/1770 train_time:87010ms step_avg:96.89ms
step:909/1770 train_time:87111ms step_avg:96.90ms
step:910/1770 train_time:87211ms step_avg:96.90ms
step:911/1770 train_time:87310ms step_avg:96.90ms
step:912/1770 train_time:87410ms step_avg:96.91ms
step:913/1770 train_time:87510ms step_avg:96.91ms
step:914/1770 train_time:87610ms step_avg:96.91ms
step:915/1770 train_time:87710ms step_avg:96.92ms
step:916/1770 train_time:87810ms step_avg:96.92ms
step:917/1770 train_time:87910ms step_avg:96.92ms
step:918/1770 train_time:88009ms step_avg:96.93ms
step:919/1770 train_time:88109ms step_avg:96.93ms
step:920/1770 train_time:88211ms step_avg:96.94ms
step:921/1770 train_time:88313ms step_avg:96.94ms
step:922/1770 train_time:88414ms step_avg:96.95ms
step:923/1770 train_time:88514ms step_avg:96.95ms
step:924/1770 train_time:88615ms step_avg:96.95ms
step:925/1770 train_time:88715ms step_avg:96.96ms
step:926/1770 train_time:88816ms step_avg:96.96ms
step:927/1770 train_time:88916ms step_avg:96.96ms
step:928/1770 train_time:89017ms step_avg:96.97ms
step:929/1770 train_time:89117ms step_avg:96.97ms
step:930/1770 train_time:89217ms step_avg:96.98ms
step:931/1770 train_time:89317ms step_avg:96.98ms
step:932/1770 train_time:89418ms step_avg:96.98ms
step:933/1770 train_time:89518ms step_avg:96.99ms
step:934/1770 train_time:89618ms step_avg:96.99ms
step:935/1770 train_time:89719ms step_avg:96.99ms
step:936/1770 train_time:89819ms step_avg:97.00ms
step:937/1770 train_time:89919ms step_avg:97.00ms
step:938/1770 train_time:90020ms step_avg:97.00ms
step:939/1770 train_time:90121ms step_avg:97.01ms
step:940/1770 train_time:90222ms step_avg:97.01ms
step:941/1770 train_time:90323ms step_avg:97.02ms
step:942/1770 train_time:90425ms step_avg:97.02ms
step:943/1770 train_time:90527ms step_avg:97.03ms
step:944/1770 train_time:90628ms step_avg:97.03ms
step:945/1770 train_time:90730ms step_avg:97.04ms
step:946/1770 train_time:90831ms step_avg:97.04ms
step:947/1770 train_time:90933ms step_avg:97.05ms
step:948/1770 train_time:91034ms step_avg:97.05ms
step:949/1770 train_time:91135ms step_avg:97.06ms
step:950/1770 train_time:91236ms step_avg:97.06ms
step:951/1770 train_time:91337ms step_avg:97.06ms
step:952/1770 train_time:91437ms step_avg:97.07ms
step:953/1770 train_time:91538ms step_avg:97.07ms
step:954/1770 train_time:91638ms step_avg:97.07ms
step:955/1770 train_time:91738ms step_avg:97.08ms
step:956/1770 train_time:91839ms step_avg:97.08ms
step:957/1770 train_time:91939ms step_avg:97.08ms
step:958/1770 train_time:92040ms step_avg:97.09ms
step:959/1770 train_time:92141ms step_avg:97.09ms
step:960/1770 train_time:92241ms step_avg:97.10ms
step:961/1770 train_time:92342ms step_avg:97.10ms
step:962/1770 train_time:92443ms step_avg:97.10ms
step:963/1770 train_time:92544ms step_avg:97.11ms
step:964/1770 train_time:92645ms step_avg:97.11ms
step:965/1770 train_time:92747ms step_avg:97.12ms
step:966/1770 train_time:92848ms step_avg:97.12ms
step:967/1770 train_time:92949ms step_avg:97.13ms
step:968/1770 train_time:93050ms step_avg:97.13ms
step:969/1770 train_time:93151ms step_avg:97.13ms
step:970/1770 train_time:93252ms step_avg:97.14ms
step:971/1770 train_time:93354ms step_avg:97.14ms
step:972/1770 train_time:93454ms step_avg:97.15ms
step:973/1770 train_time:93555ms step_avg:97.15ms
step:974/1770 train_time:93655ms step_avg:97.15ms
step:975/1770 train_time:93756ms step_avg:97.16ms
step:976/1770 train_time:93857ms step_avg:97.16ms
step:977/1770 train_time:93958ms step_avg:97.16ms
step:978/1770 train_time:94058ms step_avg:97.17ms
step:979/1770 train_time:94160ms step_avg:97.17ms
step:980/1770 train_time:94260ms step_avg:97.17ms
step:981/1770 train_time:94360ms step_avg:97.18ms
step:982/1770 train_time:94461ms step_avg:97.18ms
step:983/1770 train_time:94561ms step_avg:97.19ms
step:984/1770 train_time:94663ms step_avg:97.19ms
step:985/1770 train_time:94764ms step_avg:97.19ms
step:986/1770 train_time:94867ms step_avg:97.20ms
step:987/1770 train_time:94968ms step_avg:97.20ms
step:988/1770 train_time:95070ms step_avg:97.21ms
step:989/1770 train_time:95172ms step_avg:97.21ms
step:990/1770 train_time:95272ms step_avg:97.22ms
step:991/1770 train_time:95374ms step_avg:97.22ms
step:992/1770 train_time:95474ms step_avg:97.22ms
step:993/1770 train_time:95576ms step_avg:97.23ms
step:994/1770 train_time:95677ms step_avg:97.23ms
step:995/1770 train_time:95778ms step_avg:97.24ms
step:996/1770 train_time:95880ms step_avg:97.24ms
step:997/1770 train_time:95980ms step_avg:97.24ms
step:998/1770 train_time:96080ms step_avg:97.25ms
step:999/1770 train_time:96181ms step_avg:97.25ms
step:1000/1770 train_time:96281ms step_avg:97.25ms
step:1000/1770 val_loss:3.5135 train_time:96379ms step_avg:97.35ms
step:1001/1770 train_time:96401ms step_avg:97.28ms
step:1002/1770 train_time:96488ms step_avg:97.27ms
step:1003/1770 train_time:96590ms step_avg:97.27ms
step:1004/1770 train_time:96691ms step_avg:97.27ms
step:1005/1770 train_time:96791ms step_avg:97.28ms
step:1006/1770 train_time:96891ms step_avg:97.28ms
step:1007/1770 train_time:96992ms step_avg:97.28ms
step:1008/1770 train_time:97092ms step_avg:97.29ms
step:1009/1770 train_time:97193ms step_avg:97.29ms
step:1010/1770 train_time:97295ms step_avg:97.29ms
step:1011/1770 train_time:97398ms step_avg:97.30ms
step:1012/1770 train_time:97499ms step_avg:97.30ms
step:1013/1770 train_time:97600ms step_avg:97.31ms
step:1014/1770 train_time:97701ms step_avg:97.31ms
step:1015/1770 train_time:97802ms step_avg:97.32ms
step:1016/1770 train_time:97903ms step_avg:97.32ms
step:1017/1770 train_time:98004ms step_avg:97.32ms
step:1018/1770 train_time:98104ms step_avg:97.33ms
step:1019/1770 train_time:98204ms step_avg:97.33ms
step:1020/1770 train_time:98305ms step_avg:97.33ms
step:1021/1770 train_time:98405ms step_avg:97.33ms
step:1022/1770 train_time:98508ms step_avg:97.34ms
step:1023/1770 train_time:98610ms step_avg:97.34ms
step:1024/1770 train_time:98711ms step_avg:97.35ms
step:1025/1770 train_time:98812ms step_avg:97.35ms
step:1026/1770 train_time:98914ms step_avg:97.36ms
step:1027/1770 train_time:99016ms step_avg:97.36ms
step:1028/1770 train_time:99118ms step_avg:97.37ms
step:1029/1770 train_time:99218ms step_avg:97.37ms
step:1030/1770 train_time:99319ms step_avg:97.37ms
step:1031/1770 train_time:99419ms step_avg:97.37ms
step:1032/1770 train_time:99519ms step_avg:97.38ms
step:1033/1770 train_time:99621ms step_avg:97.38ms
step:1034/1770 train_time:99721ms step_avg:97.38ms
step:1035/1770 train_time:99822ms step_avg:97.39ms
step:1036/1770 train_time:99922ms step_avg:97.39ms
step:1037/1770 train_time:100022ms step_avg:97.39ms
step:1038/1770 train_time:100122ms step_avg:97.39ms
step:1039/1770 train_time:100223ms step_avg:97.40ms
step:1040/1770 train_time:100323ms step_avg:97.40ms
step:1041/1770 train_time:100423ms step_avg:97.40ms
step:1042/1770 train_time:100524ms step_avg:97.41ms
step:1043/1770 train_time:100625ms step_avg:97.41ms
step:1044/1770 train_time:100725ms step_avg:97.41ms
step:1045/1770 train_time:100826ms step_avg:97.42ms
step:1046/1770 train_time:100926ms step_avg:97.42ms
step:1047/1770 train_time:101026ms step_avg:97.42ms
step:1048/1770 train_time:101127ms step_avg:97.42ms
step:1049/1770 train_time:101228ms step_avg:97.43ms
step:1050/1770 train_time:101329ms step_avg:97.43ms
step:1051/1770 train_time:101431ms step_avg:97.44ms
step:1052/1770 train_time:101533ms step_avg:97.44ms
step:1053/1770 train_time:101635ms step_avg:97.45ms
step:1054/1770 train_time:101736ms step_avg:97.45ms
step:1055/1770 train_time:101837ms step_avg:97.45ms
step:1056/1770 train_time:101937ms step_avg:97.45ms
step:1057/1770 train_time:102038ms step_avg:97.46ms
step:1058/1770 train_time:102139ms step_avg:97.46ms
step:1059/1770 train_time:102240ms step_avg:97.46ms
step:1060/1770 train_time:102341ms step_avg:97.47ms
step:1061/1770 train_time:102443ms step_avg:97.47ms
step:1062/1770 train_time:102544ms step_avg:97.48ms
step:1063/1770 train_time:102647ms step_avg:97.48ms
step:1064/1770 train_time:102748ms step_avg:97.48ms
step:1065/1770 train_time:102850ms step_avg:97.49ms
step:1066/1770 train_time:102951ms step_avg:97.49ms
step:1067/1770 train_time:103053ms step_avg:97.50ms
step:1068/1770 train_time:103155ms step_avg:97.50ms
step:1069/1770 train_time:103256ms step_avg:97.50ms
step:1070/1770 train_time:103359ms step_avg:97.51ms
step:1071/1770 train_time:103460ms step_avg:97.51ms
step:1072/1770 train_time:103561ms step_avg:97.52ms
step:1073/1770 train_time:103661ms step_avg:97.52ms
step:1074/1770 train_time:103762ms step_avg:97.52ms
step:1075/1770 train_time:103862ms step_avg:97.52ms
step:1076/1770 train_time:103963ms step_avg:97.53ms
step:1077/1770 train_time:104064ms step_avg:97.53ms
step:1078/1770 train_time:104165ms step_avg:97.53ms
step:1079/1770 train_time:104266ms step_avg:97.54ms
step:1080/1770 train_time:104368ms step_avg:97.54ms
step:1081/1770 train_time:104471ms step_avg:97.54ms
step:1082/1770 train_time:104574ms step_avg:97.55ms
step:1083/1770 train_time:104676ms step_avg:97.55ms
step:1084/1770 train_time:104778ms step_avg:97.56ms
step:1085/1770 train_time:104879ms step_avg:97.56ms
step:1086/1770 train_time:104980ms step_avg:97.56ms
step:1087/1770 train_time:105080ms step_avg:97.57ms
step:1088/1770 train_time:105180ms step_avg:97.57ms
step:1089/1770 train_time:105281ms step_avg:97.57ms
step:1090/1770 train_time:105382ms step_avg:97.58ms
step:1091/1770 train_time:105484ms step_avg:97.58ms
step:1092/1770 train_time:105584ms step_avg:97.58ms
step:1093/1770 train_time:105685ms step_avg:97.59ms
step:1094/1770 train_time:105787ms step_avg:97.59ms
step:1095/1770 train_time:105889ms step_avg:97.59ms
step:1096/1770 train_time:105990ms step_avg:97.60ms
step:1097/1770 train_time:106091ms step_avg:97.60ms
step:1098/1770 train_time:106193ms step_avg:97.60ms
step:1099/1770 train_time:106295ms step_avg:97.61ms
step:1100/1770 train_time:106398ms step_avg:97.61ms
step:1101/1770 train_time:106499ms step_avg:97.62ms
step:1102/1770 train_time:106600ms step_avg:97.62ms
step:1103/1770 train_time:106700ms step_avg:97.62ms
step:1104/1770 train_time:106801ms step_avg:97.62ms
step:1105/1770 train_time:106902ms step_avg:97.63ms
step:1106/1770 train_time:107003ms step_avg:97.63ms
step:1107/1770 train_time:107103ms step_avg:97.63ms
step:1108/1770 train_time:107204ms step_avg:97.64ms
step:1109/1770 train_time:107305ms step_avg:97.64ms
step:1110/1770 train_time:107406ms step_avg:97.64ms
step:1111/1770 train_time:107508ms step_avg:97.65ms
step:1112/1770 train_time:107610ms step_avg:97.65ms
step:1113/1770 train_time:107712ms step_avg:97.65ms
step:1114/1770 train_time:107813ms step_avg:97.66ms
step:1115/1770 train_time:107915ms step_avg:97.66ms
step:1116/1770 train_time:108017ms step_avg:97.66ms
step:1117/1770 train_time:108119ms step_avg:97.67ms
step:1118/1770 train_time:108220ms step_avg:97.67ms
step:1119/1770 train_time:108321ms step_avg:97.67ms
step:1120/1770 train_time:108421ms step_avg:97.68ms
step:1121/1770 train_time:108521ms step_avg:97.68ms
step:1122/1770 train_time:108621ms step_avg:97.68ms
step:1123/1770 train_time:108722ms step_avg:97.68ms
step:1124/1770 train_time:108823ms step_avg:97.69ms
step:1125/1770 train_time:108924ms step_avg:97.69ms
step:1125/1770 val_loss:3.4727 train_time:109023ms step_avg:97.78ms
step:1126/1770 train_time:109044ms step_avg:97.71ms
step:1127/1770 train_time:109132ms step_avg:97.70ms
step:1128/1770 train_time:109234ms step_avg:97.70ms
step:1129/1770 train_time:109335ms step_avg:97.71ms
step:1130/1770 train_time:109436ms step_avg:97.71ms
step:1131/1770 train_time:109537ms step_avg:97.71ms
step:1132/1770 train_time:109638ms step_avg:97.72ms
step:1133/1770 train_time:109739ms step_avg:97.72ms
step:1134/1770 train_time:109840ms step_avg:97.72ms
step:1135/1770 train_time:109941ms step_avg:97.72ms
step:1136/1770 train_time:110044ms step_avg:97.73ms
step:1137/1770 train_time:110145ms step_avg:97.73ms
step:1138/1770 train_time:110246ms step_avg:97.74ms
step:1139/1770 train_time:110347ms step_avg:97.74ms
step:1140/1770 train_time:110447ms step_avg:97.74ms
step:1141/1770 train_time:110547ms step_avg:97.74ms
step:1142/1770 train_time:110647ms step_avg:97.74ms
step:1143/1770 train_time:110748ms step_avg:97.75ms
step:1144/1770 train_time:110850ms step_avg:97.75ms
step:1145/1770 train_time:110952ms step_avg:97.76ms
step:1146/1770 train_time:111056ms step_avg:97.76ms
step:1147/1770 train_time:111158ms step_avg:97.76ms
step:1148/1770 train_time:111259ms step_avg:97.77ms
step:1149/1770 train_time:111360ms step_avg:97.77ms
step:1150/1770 train_time:111461ms step_avg:97.77ms
step:1151/1770 train_time:111561ms step_avg:97.78ms
step:1152/1770 train_time:111662ms step_avg:97.78ms
step:1153/1770 train_time:111763ms step_avg:97.78ms
step:1154/1770 train_time:111864ms step_avg:97.78ms
step:1155/1770 train_time:111964ms step_avg:97.79ms
step:1156/1770 train_time:112066ms step_avg:97.79ms
step:1157/1770 train_time:112168ms step_avg:97.79ms
step:1158/1770 train_time:112269ms step_avg:97.79ms
step:1159/1770 train_time:112369ms step_avg:97.80ms
step:1160/1770 train_time:112470ms step_avg:97.80ms
step:1161/1770 train_time:112572ms step_avg:97.80ms
step:1162/1770 train_time:112674ms step_avg:97.81ms
step:1163/1770 train_time:112775ms step_avg:97.81ms
step:1164/1770 train_time:112878ms step_avg:97.81ms
step:1165/1770 train_time:112979ms step_avg:97.82ms
step:1166/1770 train_time:113081ms step_avg:97.82ms
step:1167/1770 train_time:113182ms step_avg:97.82ms
step:1168/1770 train_time:113283ms step_avg:97.83ms
step:1169/1770 train_time:113382ms step_avg:97.83ms
step:1170/1770 train_time:113483ms step_avg:97.83ms
step:1171/1770 train_time:113584ms step_avg:97.83ms
step:1172/1770 train_time:113685ms step_avg:97.84ms
step:1173/1770 train_time:113786ms step_avg:97.84ms
step:1174/1770 train_time:113887ms step_avg:97.84ms
step:1175/1770 train_time:113987ms step_avg:97.84ms
step:1176/1770 train_time:114089ms step_avg:97.85ms
step:1177/1770 train_time:114190ms step_avg:97.85ms
step:1178/1770 train_time:114291ms step_avg:97.85ms
step:1179/1770 train_time:114393ms step_avg:97.86ms
step:1180/1770 train_time:114495ms step_avg:97.86ms
step:1181/1770 train_time:114596ms step_avg:97.86ms
step:1182/1770 train_time:114698ms step_avg:97.87ms
step:1183/1770 train_time:114800ms step_avg:97.87ms
step:1184/1770 train_time:114904ms step_avg:97.87ms
step:1185/1770 train_time:115006ms step_avg:97.88ms
step:1186/1770 train_time:115108ms step_avg:97.88ms
step:1187/1770 train_time:115212ms step_avg:97.89ms
step:1188/1770 train_time:115315ms step_avg:97.89ms
step:1189/1770 train_time:115417ms step_avg:97.89ms
step:1190/1770 train_time:115518ms step_avg:97.90ms
step:1191/1770 train_time:115620ms step_avg:97.90ms
step:1192/1770 train_time:115723ms step_avg:97.90ms
step:1193/1770 train_time:115825ms step_avg:97.91ms
step:1194/1770 train_time:115927ms step_avg:97.91ms
step:1195/1770 train_time:116030ms step_avg:97.92ms
step:1196/1770 train_time:116133ms step_avg:97.92ms
step:1197/1770 train_time:116235ms step_avg:97.92ms
step:1198/1770 train_time:116339ms step_avg:97.93ms
step:1199/1770 train_time:116441ms step_avg:97.93ms
step:1200/1770 train_time:116544ms step_avg:97.94ms
step:1201/1770 train_time:116646ms step_avg:97.94ms
step:1202/1770 train_time:116747ms step_avg:97.94ms
step:1203/1770 train_time:116849ms step_avg:97.95ms
step:1204/1770 train_time:116951ms step_avg:97.95ms
step:1205/1770 train_time:117054ms step_avg:97.95ms
step:1206/1770 train_time:117157ms step_avg:97.96ms
step:1207/1770 train_time:117260ms step_avg:97.96ms
step:1208/1770 train_time:117362ms step_avg:97.97ms
step:1209/1770 train_time:117464ms step_avg:97.97ms
step:1210/1770 train_time:117565ms step_avg:97.97ms
step:1211/1770 train_time:117668ms step_avg:97.97ms
step:1212/1770 train_time:117771ms step_avg:97.98ms
step:1213/1770 train_time:117873ms step_avg:97.98ms
step:1214/1770 train_time:117975ms step_avg:97.99ms
step:1215/1770 train_time:118078ms step_avg:97.99ms
step:1216/1770 train_time:118183ms step_avg:98.00ms
step:1217/1770 train_time:118285ms step_avg:98.00ms
step:1218/1770 train_time:118387ms step_avg:98.00ms
step:1219/1770 train_time:118490ms step_avg:98.01ms
step:1220/1770 train_time:118593ms step_avg:98.01ms
step:1221/1770 train_time:118695ms step_avg:98.01ms
step:1222/1770 train_time:118799ms step_avg:98.02ms
step:1223/1770 train_time:118901ms step_avg:98.02ms
step:1224/1770 train_time:119004ms step_avg:98.03ms
step:1225/1770 train_time:119106ms step_avg:98.03ms
step:1226/1770 train_time:119208ms step_avg:98.03ms
step:1227/1770 train_time:119312ms step_avg:98.04ms
step:1228/1770 train_time:119417ms step_avg:98.04ms
step:1229/1770 train_time:119519ms step_avg:98.05ms
step:1230/1770 train_time:119621ms step_avg:98.05ms
step:1231/1770 train_time:119724ms step_avg:98.05ms
step:1232/1770 train_time:119825ms step_avg:98.06ms
step:1233/1770 train_time:119927ms step_avg:98.06ms
step:1234/1770 train_time:120029ms step_avg:98.06ms
step:1235/1770 train_time:120130ms step_avg:98.07ms
step:1236/1770 train_time:120232ms step_avg:98.07ms
step:1237/1770 train_time:120334ms step_avg:98.07ms
step:1238/1770 train_time:120438ms step_avg:98.08ms
step:1239/1770 train_time:120541ms step_avg:98.08ms
step:1240/1770 train_time:120643ms step_avg:98.08ms
step:1241/1770 train_time:120746ms step_avg:98.09ms
step:1242/1770 train_time:120848ms step_avg:98.09ms
step:1243/1770 train_time:120951ms step_avg:98.09ms
step:1244/1770 train_time:121053ms step_avg:98.10ms
step:1245/1770 train_time:121155ms step_avg:98.10ms
step:1246/1770 train_time:121257ms step_avg:98.10ms
step:1247/1770 train_time:121360ms step_avg:98.11ms
step:1248/1770 train_time:121462ms step_avg:98.11ms
step:1249/1770 train_time:121564ms step_avg:98.11ms
step:1250/1770 train_time:121666ms step_avg:98.12ms
step:1250/1770 val_loss:3.4246 train_time:121767ms step_avg:98.20ms
step:1251/1770 train_time:121789ms step_avg:98.14ms
step:1252/1770 train_time:121877ms step_avg:98.13ms
step:1253/1770 train_time:121979ms step_avg:98.13ms
step:1254/1770 train_time:122081ms step_avg:98.14ms
step:1255/1770 train_time:122186ms step_avg:98.14ms
step:1256/1770 train_time:122287ms step_avg:98.14ms
step:1257/1770 train_time:122389ms step_avg:98.15ms
step:1258/1770 train_time:122492ms step_avg:98.15ms
step:1259/1770 train_time:122594ms step_avg:98.15ms
step:1260/1770 train_time:122695ms step_avg:98.16ms
step:1261/1770 train_time:122800ms step_avg:98.16ms
step:1262/1770 train_time:122904ms step_avg:98.17ms
step:1263/1770 train_time:123005ms step_avg:98.17ms
step:1264/1770 train_time:123109ms step_avg:98.17ms
step:1265/1770 train_time:123211ms step_avg:98.18ms
step:1266/1770 train_time:123314ms step_avg:98.18ms
step:1267/1770 train_time:123416ms step_avg:98.18ms
step:1268/1770 train_time:123518ms step_avg:98.19ms
step:1269/1770 train_time:123620ms step_avg:98.19ms
step:1270/1770 train_time:123722ms step_avg:98.19ms
step:1271/1770 train_time:123824ms step_avg:98.20ms
step:1272/1770 train_time:123925ms step_avg:98.20ms
step:1273/1770 train_time:124029ms step_avg:98.20ms
step:1274/1770 train_time:124131ms step_avg:98.21ms
step:1275/1770 train_time:124233ms step_avg:98.21ms
step:1276/1770 train_time:124335ms step_avg:98.21ms
step:1277/1770 train_time:124437ms step_avg:98.21ms
step:1278/1770 train_time:124540ms step_avg:98.22ms
step:1279/1770 train_time:124642ms step_avg:98.22ms
step:1280/1770 train_time:124745ms step_avg:98.22ms
step:1281/1770 train_time:124847ms step_avg:98.23ms
step:1282/1770 train_time:124951ms step_avg:98.23ms
step:1283/1770 train_time:125053ms step_avg:98.24ms
step:1284/1770 train_time:125156ms step_avg:98.24ms
step:1285/1770 train_time:125259ms step_avg:98.24ms
step:1286/1770 train_time:125362ms step_avg:98.25ms
step:1287/1770 train_time:125466ms step_avg:98.25ms
step:1288/1770 train_time:125568ms step_avg:98.25ms
step:1289/1770 train_time:125671ms step_avg:98.26ms
step:1290/1770 train_time:125773ms step_avg:98.26ms
step:1291/1770 train_time:125875ms step_avg:98.26ms
step:1292/1770 train_time:125977ms step_avg:98.27ms
step:1293/1770 train_time:126079ms step_avg:98.27ms
step:1294/1770 train_time:126180ms step_avg:98.27ms
step:1295/1770 train_time:126282ms step_avg:98.27ms
step:1296/1770 train_time:126385ms step_avg:98.28ms
step:1297/1770 train_time:126487ms step_avg:98.28ms
step:1298/1770 train_time:126590ms step_avg:98.28ms
step:1299/1770 train_time:126692ms step_avg:98.29ms
step:1300/1770 train_time:126794ms step_avg:98.29ms
step:1301/1770 train_time:126896ms step_avg:98.29ms
step:1302/1770 train_time:126998ms step_avg:98.30ms
step:1303/1770 train_time:127100ms step_avg:98.30ms
step:1304/1770 train_time:127201ms step_avg:98.30ms
step:1305/1770 train_time:127303ms step_avg:98.30ms
step:1306/1770 train_time:127405ms step_avg:98.31ms
step:1307/1770 train_time:127507ms step_avg:98.31ms
step:1308/1770 train_time:127610ms step_avg:98.31ms
step:1309/1770 train_time:127713ms step_avg:98.32ms
step:1310/1770 train_time:127815ms step_avg:98.32ms
step:1311/1770 train_time:127917ms step_avg:98.32ms
step:1312/1770 train_time:128018ms step_avg:98.32ms
step:1313/1770 train_time:128119ms step_avg:98.33ms
step:1314/1770 train_time:128221ms step_avg:98.33ms
step:1315/1770 train_time:128323ms step_avg:98.33ms
step:1316/1770 train_time:128425ms step_avg:98.33ms
step:1317/1770 train_time:128528ms step_avg:98.34ms
step:1318/1770 train_time:128635ms step_avg:98.34ms
step:1319/1770 train_time:128738ms step_avg:98.35ms
step:1320/1770 train_time:128841ms step_avg:98.35ms
step:1321/1770 train_time:128943ms step_avg:98.35ms
step:1322/1770 train_time:129045ms step_avg:98.36ms
step:1323/1770 train_time:129148ms step_avg:98.36ms
step:1324/1770 train_time:129252ms step_avg:98.37ms
step:1325/1770 train_time:129356ms step_avg:98.37ms
step:1326/1770 train_time:129457ms step_avg:98.37ms
step:1327/1770 train_time:129563ms step_avg:98.38ms
step:1328/1770 train_time:129664ms step_avg:98.38ms
step:1329/1770 train_time:129767ms step_avg:98.38ms
step:1330/1770 train_time:129870ms step_avg:98.39ms
step:1331/1770 train_time:129972ms step_avg:98.39ms
step:1332/1770 train_time:130074ms step_avg:98.39ms
step:1333/1770 train_time:130175ms step_avg:98.39ms
step:1334/1770 train_time:130277ms step_avg:98.40ms
step:1335/1770 train_time:130379ms step_avg:98.40ms
step:1336/1770 train_time:130480ms step_avg:98.40ms
step:1337/1770 train_time:130582ms step_avg:98.40ms
step:1338/1770 train_time:130684ms step_avg:98.41ms
step:1339/1770 train_time:130787ms step_avg:98.41ms
step:1340/1770 train_time:130890ms step_avg:98.41ms
step:1341/1770 train_time:130992ms step_avg:98.42ms
step:1342/1770 train_time:131096ms step_avg:98.42ms
step:1343/1770 train_time:131199ms step_avg:98.42ms
step:1344/1770 train_time:131302ms step_avg:98.43ms
step:1345/1770 train_time:131403ms step_avg:98.43ms
step:1346/1770 train_time:131505ms step_avg:98.43ms
step:1347/1770 train_time:131608ms step_avg:98.43ms
step:1348/1770 train_time:131714ms step_avg:98.44ms
step:1349/1770 train_time:131816ms step_avg:98.44ms
step:1350/1770 train_time:131918ms step_avg:98.45ms
step:1351/1770 train_time:132020ms step_avg:98.45ms
step:1352/1770 train_time:132123ms step_avg:98.45ms
step:1353/1770 train_time:132226ms step_avg:98.46ms
step:1354/1770 train_time:132328ms step_avg:98.46ms
step:1355/1770 train_time:132430ms step_avg:98.46ms
step:1356/1770 train_time:132532ms step_avg:98.46ms
step:1357/1770 train_time:132634ms step_avg:98.47ms
step:1358/1770 train_time:132736ms step_avg:98.47ms
step:1359/1770 train_time:132839ms step_avg:98.47ms
step:1360/1770 train_time:132941ms step_avg:98.47ms
step:1361/1770 train_time:133044ms step_avg:98.48ms
step:1362/1770 train_time:133146ms step_avg:98.48ms
step:1363/1770 train_time:133249ms step_avg:98.48ms
step:1364/1770 train_time:133353ms step_avg:98.49ms
step:1365/1770 train_time:133455ms step_avg:98.49ms
step:1366/1770 train_time:133557ms step_avg:98.49ms
step:1367/1770 train_time:133660ms step_avg:98.50ms
step:1368/1770 train_time:133762ms step_avg:98.50ms
step:1369/1770 train_time:133865ms step_avg:98.50ms
step:1370/1770 train_time:133968ms step_avg:98.51ms
step:1371/1770 train_time:134070ms step_avg:98.51ms
step:1372/1770 train_time:134173ms step_avg:98.51ms
step:1373/1770 train_time:134275ms step_avg:98.51ms
step:1374/1770 train_time:134378ms step_avg:98.52ms
step:1375/1770 train_time:134480ms step_avg:98.52ms
step:1375/1770 val_loss:3.3802 train_time:134581ms step_avg:98.59ms
step:1376/1770 train_time:134603ms step_avg:98.54ms
step:1377/1770 train_time:134690ms step_avg:98.53ms
step:1378/1770 train_time:134791ms step_avg:98.53ms
step:1379/1770 train_time:134893ms step_avg:98.53ms
step:1380/1770 train_time:134994ms step_avg:98.54ms
step:1381/1770 train_time:135098ms step_avg:98.54ms
step:1382/1770 train_time:135200ms step_avg:98.54ms
step:1383/1770 train_time:135303ms step_avg:98.55ms
step:1384/1770 train_time:135405ms step_avg:98.55ms
step:1385/1770 train_time:135508ms step_avg:98.55ms
step:1386/1770 train_time:135611ms step_avg:98.55ms
step:1387/1770 train_time:135714ms step_avg:98.56ms
step:1388/1770 train_time:135816ms step_avg:98.56ms
step:1389/1770 train_time:135919ms step_avg:98.56ms
step:1390/1770 train_time:136021ms step_avg:98.57ms
step:1391/1770 train_time:136123ms step_avg:98.57ms
step:1392/1770 train_time:136225ms step_avg:98.57ms
step:1393/1770 train_time:136327ms step_avg:98.57ms
step:1394/1770 train_time:136429ms step_avg:98.58ms
step:1395/1770 train_time:136532ms step_avg:98.58ms
step:1396/1770 train_time:136636ms step_avg:98.58ms
step:1397/1770 train_time:136739ms step_avg:98.59ms
step:1398/1770 train_time:136841ms step_avg:98.59ms
step:1399/1770 train_time:136944ms step_avg:98.59ms
step:1400/1770 train_time:137047ms step_avg:98.59ms
step:1401/1770 train_time:137148ms step_avg:98.60ms
step:1402/1770 train_time:137250ms step_avg:98.60ms
step:1403/1770 train_time:137352ms step_avg:98.60ms
step:1404/1770 train_time:137455ms step_avg:98.60ms
step:1405/1770 train_time:137557ms step_avg:98.61ms
step:1406/1770 train_time:137659ms step_avg:98.61ms
step:1407/1770 train_time:137761ms step_avg:98.61ms
step:1408/1770 train_time:137864ms step_avg:98.62ms
step:1409/1770 train_time:137966ms step_avg:98.62ms
step:1410/1770 train_time:138069ms step_avg:98.62ms
step:1411/1770 train_time:138170ms step_avg:98.62ms
step:1412/1770 train_time:138272ms step_avg:98.62ms
step:1413/1770 train_time:138373ms step_avg:98.63ms
step:1414/1770 train_time:138476ms step_avg:98.63ms
step:1415/1770 train_time:138578ms step_avg:98.63ms
step:1416/1770 train_time:138682ms step_avg:98.64ms
step:1417/1770 train_time:138786ms step_avg:98.64ms
step:1418/1770 train_time:138888ms step_avg:98.64ms
step:1419/1770 train_time:138990ms step_avg:98.64ms
step:1420/1770 train_time:139093ms step_avg:98.65ms
step:1421/1770 train_time:139195ms step_avg:98.65ms
step:1422/1770 train_time:139297ms step_avg:98.65ms
step:1423/1770 train_time:139399ms step_avg:98.65ms
step:1424/1770 train_time:139502ms step_avg:98.66ms
step:1425/1770 train_time:139604ms step_avg:98.66ms
step:1426/1770 train_time:139706ms step_avg:98.66ms
step:1427/1770 train_time:139808ms step_avg:98.66ms
step:1428/1770 train_time:139912ms step_avg:98.67ms
step:1429/1770 train_time:140014ms step_avg:98.67ms
step:1430/1770 train_time:140116ms step_avg:98.67ms
step:1431/1770 train_time:140219ms step_avg:98.68ms
step:1432/1770 train_time:140320ms step_avg:98.68ms
step:1433/1770 train_time:140422ms step_avg:98.68ms
step:1434/1770 train_time:140524ms step_avg:98.68ms
step:1435/1770 train_time:140626ms step_avg:98.69ms
step:1436/1770 train_time:140729ms step_avg:98.69ms
step:1437/1770 train_time:140832ms step_avg:98.69ms
step:1438/1770 train_time:140934ms step_avg:98.69ms
step:1439/1770 train_time:141038ms step_avg:98.70ms
step:1440/1770 train_time:141140ms step_avg:98.70ms
step:1441/1770 train_time:141245ms step_avg:98.70ms
step:1442/1770 train_time:141347ms step_avg:98.71ms
step:1443/1770 train_time:141448ms step_avg:98.71ms
step:1444/1770 train_time:141551ms step_avg:98.71ms
step:1445/1770 train_time:141653ms step_avg:98.71ms
step:1446/1770 train_time:141756ms step_avg:98.72ms
step:1447/1770 train_time:141861ms step_avg:98.72ms
step:1448/1770 train_time:141964ms step_avg:98.72ms
step:1449/1770 train_time:142068ms step_avg:98.73ms
step:1450/1770 train_time:142171ms step_avg:98.73ms
step:1451/1770 train_time:142274ms step_avg:98.73ms
step:1452/1770 train_time:142377ms step_avg:98.74ms
step:1453/1770 train_time:142481ms step_avg:98.74ms
step:1454/1770 train_time:142585ms step_avg:98.74ms
step:1455/1770 train_time:142689ms step_avg:98.75ms
step:1456/1770 train_time:142793ms step_avg:98.75ms
step:1457/1770 train_time:142897ms step_avg:98.75ms
step:1458/1770 train_time:143001ms step_avg:98.76ms
step:1459/1770 train_time:143106ms step_avg:98.76ms
step:1460/1770 train_time:143209ms step_avg:98.76ms
step:1461/1770 train_time:143313ms step_avg:98.77ms
step:1462/1770 train_time:143416ms step_avg:98.77ms
step:1463/1770 train_time:143519ms step_avg:98.77ms
step:1464/1770 train_time:143625ms step_avg:98.78ms
step:1465/1770 train_time:143729ms step_avg:98.78ms
step:1466/1770 train_time:143833ms step_avg:98.79ms
step:1467/1770 train_time:143939ms step_avg:98.79ms
step:1468/1770 train_time:144042ms step_avg:98.79ms
step:1469/1770 train_time:144145ms step_avg:98.80ms
step:1470/1770 train_time:144248ms step_avg:98.80ms
step:1471/1770 train_time:144351ms step_avg:98.80ms
step:1472/1770 train_time:144454ms step_avg:98.81ms
step:1473/1770 train_time:144558ms step_avg:98.81ms
step:1474/1770 train_time:144663ms step_avg:98.81ms
step:1475/1770 train_time:144766ms step_avg:98.82ms
step:1476/1770 train_time:144869ms step_avg:98.82ms
step:1477/1770 train_time:144975ms step_avg:98.82ms
step:1478/1770 train_time:145079ms step_avg:98.83ms
step:1479/1770 train_time:145182ms step_avg:98.83ms
step:1480/1770 train_time:145287ms step_avg:98.83ms
step:1481/1770 train_time:145394ms step_avg:98.84ms
step:1482/1770 train_time:145496ms step_avg:98.84ms
step:1483/1770 train_time:145600ms step_avg:98.85ms
step:1484/1770 train_time:145704ms step_avg:98.85ms
step:1485/1770 train_time:145807ms step_avg:98.85ms
step:1486/1770 train_time:145909ms step_avg:98.85ms
step:1487/1770 train_time:146013ms step_avg:98.86ms
step:1488/1770 train_time:146118ms step_avg:98.86ms
step:1489/1770 train_time:146222ms step_avg:98.87ms
step:1490/1770 train_time:146326ms step_avg:98.87ms
step:1491/1770 train_time:146428ms step_avg:98.87ms
step:1492/1770 train_time:146532ms step_avg:98.87ms
step:1493/1770 train_time:146638ms step_avg:98.88ms
step:1494/1770 train_time:146745ms step_avg:98.88ms
step:1495/1770 train_time:146848ms step_avg:98.89ms
step:1496/1770 train_time:146951ms step_avg:98.89ms
step:1497/1770 train_time:147054ms step_avg:98.89ms
step:1498/1770 train_time:147156ms step_avg:98.90ms
step:1499/1770 train_time:147260ms step_avg:98.90ms
step:1500/1770 train_time:147363ms step_avg:98.90ms
step:1500/1770 val_loss:3.3422 train_time:147466ms step_avg:98.97ms
step:1501/1770 train_time:147488ms step_avg:98.92ms
step:1502/1770 train_time:147576ms step_avg:98.91ms
step:1503/1770 train_time:147679ms step_avg:98.91ms
step:1504/1770 train_time:147782ms step_avg:98.92ms
step:1505/1770 train_time:147887ms step_avg:98.92ms
step:1506/1770 train_time:147991ms step_avg:98.92ms
step:1507/1770 train_time:148094ms step_avg:98.93ms
step:1508/1770 train_time:148199ms step_avg:98.93ms
step:1509/1770 train_time:148303ms step_avg:98.93ms
step:1510/1770 train_time:148405ms step_avg:98.94ms
step:1511/1770 train_time:148512ms step_avg:98.94ms
step:1512/1770 train_time:148616ms step_avg:98.95ms
step:1513/1770 train_time:148720ms step_avg:98.95ms
step:1514/1770 train_time:148824ms step_avg:98.95ms
step:1515/1770 train_time:148929ms step_avg:98.96ms
step:1516/1770 train_time:149032ms step_avg:98.96ms
step:1517/1770 train_time:149135ms step_avg:98.96ms
step:1518/1770 train_time:149240ms step_avg:98.97ms
step:1519/1770 train_time:149343ms step_avg:98.97ms
step:1520/1770 train_time:149447ms step_avg:98.97ms
step:1521/1770 train_time:149551ms step_avg:98.97ms
step:1522/1770 train_time:149654ms step_avg:98.98ms
step:1523/1770 train_time:149758ms step_avg:98.98ms
step:1524/1770 train_time:149862ms step_avg:98.98ms
step:1525/1770 train_time:149965ms step_avg:98.99ms
step:1526/1770 train_time:150069ms step_avg:98.99ms
step:1527/1770 train_time:150173ms step_avg:98.99ms
step:1528/1770 train_time:150278ms step_avg:99.00ms
step:1529/1770 train_time:150381ms step_avg:99.00ms
step:1530/1770 train_time:150484ms step_avg:99.00ms
step:1531/1770 train_time:150587ms step_avg:99.00ms
step:1532/1770 train_time:150691ms step_avg:99.01ms
step:1533/1770 train_time:150795ms step_avg:99.01ms
step:1534/1770 train_time:150899ms step_avg:99.01ms
step:1535/1770 train_time:151001ms step_avg:99.02ms
step:1536/1770 train_time:151106ms step_avg:99.02ms
step:1537/1770 train_time:151209ms step_avg:99.02ms
step:1538/1770 train_time:151314ms step_avg:99.03ms
step:1539/1770 train_time:151417ms step_avg:99.03ms
step:1540/1770 train_time:151523ms step_avg:99.03ms
step:1541/1770 train_time:151629ms step_avg:99.04ms
step:1542/1770 train_time:151733ms step_avg:99.04ms
step:1543/1770 train_time:151836ms step_avg:99.04ms
step:1544/1770 train_time:151941ms step_avg:99.05ms
step:1545/1770 train_time:152045ms step_avg:99.05ms
step:1546/1770 train_time:152150ms step_avg:99.06ms
step:1547/1770 train_time:152253ms step_avg:99.06ms
step:1548/1770 train_time:152356ms step_avg:99.06ms
step:1549/1770 train_time:152460ms step_avg:99.06ms
step:1550/1770 train_time:152564ms step_avg:99.07ms
step:1551/1770 train_time:152668ms step_avg:99.07ms
step:1552/1770 train_time:152773ms step_avg:99.07ms
step:1553/1770 train_time:152877ms step_avg:99.08ms
step:1554/1770 train_time:152979ms step_avg:99.08ms
step:1555/1770 train_time:153084ms step_avg:99.08ms
step:1556/1770 train_time:153188ms step_avg:99.09ms
step:1557/1770 train_time:153291ms step_avg:99.09ms
step:1558/1770 train_time:153395ms step_avg:99.09ms
step:1559/1770 train_time:153498ms step_avg:99.09ms
step:1560/1770 train_time:153601ms step_avg:99.10ms
step:1561/1770 train_time:153706ms step_avg:99.10ms
step:1562/1770 train_time:153810ms step_avg:99.10ms
step:1563/1770 train_time:153913ms step_avg:99.11ms
step:1564/1770 train_time:154015ms step_avg:99.11ms
step:1565/1770 train_time:154119ms step_avg:99.11ms
step:1566/1770 train_time:154223ms step_avg:99.12ms
step:1567/1770 train_time:154328ms step_avg:99.12ms
step:1568/1770 train_time:154431ms step_avg:99.12ms
step:1569/1770 train_time:154537ms step_avg:99.13ms
step:1570/1770 train_time:154640ms step_avg:99.13ms
step:1571/1770 train_time:154744ms step_avg:99.13ms
step:1572/1770 train_time:154849ms step_avg:99.14ms
step:1573/1770 train_time:154955ms step_avg:99.14ms
step:1574/1770 train_time:155058ms step_avg:99.14ms
step:1575/1770 train_time:155160ms step_avg:99.14ms
step:1576/1770 train_time:155263ms step_avg:99.15ms
step:1577/1770 train_time:155369ms step_avg:99.15ms
step:1578/1770 train_time:155475ms step_avg:99.15ms
step:1579/1770 train_time:155578ms step_avg:99.16ms
step:1580/1770 train_time:155682ms step_avg:99.16ms
step:1581/1770 train_time:155788ms step_avg:99.16ms
step:1582/1770 train_time:155892ms step_avg:99.17ms
step:1583/1770 train_time:155996ms step_avg:99.17ms
step:1584/1770 train_time:156100ms step_avg:99.17ms
step:1585/1770 train_time:156204ms step_avg:99.18ms
step:1586/1770 train_time:156312ms step_avg:99.18ms
step:1587/1770 train_time:156416ms step_avg:99.19ms
step:1588/1770 train_time:156520ms step_avg:99.19ms
step:1589/1770 train_time:156625ms step_avg:99.19ms
step:1590/1770 train_time:156728ms step_avg:99.19ms
step:1591/1770 train_time:156831ms step_avg:99.20ms
step:1592/1770 train_time:156936ms step_avg:99.20ms
step:1593/1770 train_time:157039ms step_avg:99.20ms
step:1594/1770 train_time:157143ms step_avg:99.21ms
step:1595/1770 train_time:157246ms step_avg:99.21ms
step:1596/1770 train_time:157351ms step_avg:99.21ms
step:1597/1770 train_time:157453ms step_avg:99.21ms
step:1598/1770 train_time:157557ms step_avg:99.22ms
step:1599/1770 train_time:157663ms step_avg:99.22ms
step:1600/1770 train_time:157769ms step_avg:99.23ms
step:1601/1770 train_time:157873ms step_avg:99.23ms
step:1602/1770 train_time:157978ms step_avg:99.23ms
step:1603/1770 train_time:158082ms step_avg:99.24ms
step:1604/1770 train_time:158185ms step_avg:99.24ms
step:1605/1770 train_time:158288ms step_avg:99.24ms
step:1606/1770 train_time:158392ms step_avg:99.24ms
step:1607/1770 train_time:158499ms step_avg:99.25ms
step:1608/1770 train_time:158603ms step_avg:99.25ms
step:1609/1770 train_time:158706ms step_avg:99.25ms
step:1610/1770 train_time:158811ms step_avg:99.26ms
step:1611/1770 train_time:158916ms step_avg:99.26ms
step:1612/1770 train_time:159021ms step_avg:99.26ms
step:1613/1770 train_time:159124ms step_avg:99.27ms
step:1614/1770 train_time:159228ms step_avg:99.27ms
step:1615/1770 train_time:159331ms step_avg:99.27ms
step:1616/1770 train_time:159435ms step_avg:99.27ms
step:1617/1770 train_time:159541ms step_avg:99.28ms
step:1618/1770 train_time:159646ms step_avg:99.28ms
step:1619/1770 train_time:159750ms step_avg:99.29ms
step:1620/1770 train_time:159854ms step_avg:99.29ms
step:1621/1770 train_time:159957ms step_avg:99.29ms
step:1622/1770 train_time:160062ms step_avg:99.29ms
step:1623/1770 train_time:160167ms step_avg:99.30ms
step:1624/1770 train_time:160270ms step_avg:99.30ms
step:1625/1770 train_time:160373ms step_avg:99.30ms
step:1625/1770 val_loss:3.3080 train_time:160474ms step_avg:99.36ms
step:1626/1770 train_time:160496ms step_avg:99.32ms
step:1627/1770 train_time:160586ms step_avg:99.31ms
step:1628/1770 train_time:160689ms step_avg:99.31ms
step:1629/1770 train_time:160791ms step_avg:99.32ms
step:1630/1770 train_time:160895ms step_avg:99.32ms
step:1631/1770 train_time:160998ms step_avg:99.32ms
step:1632/1770 train_time:161101ms step_avg:99.32ms
step:1633/1770 train_time:161204ms step_avg:99.32ms
step:1634/1770 train_time:161307ms step_avg:99.33ms
step:1635/1770 train_time:161412ms step_avg:99.33ms
step:1636/1770 train_time:161516ms step_avg:99.33ms
step:1637/1770 train_time:161620ms step_avg:99.34ms
step:1638/1770 train_time:161724ms step_avg:99.34ms
step:1639/1770 train_time:161828ms step_avg:99.34ms
step:1640/1770 train_time:161933ms step_avg:99.35ms
step:1641/1770 train_time:162036ms step_avg:99.35ms
step:1642/1770 train_time:162138ms step_avg:99.35ms
step:1643/1770 train_time:162243ms step_avg:99.35ms
step:1644/1770 train_time:162348ms step_avg:99.36ms
step:1645/1770 train_time:162452ms step_avg:99.36ms
step:1646/1770 train_time:162557ms step_avg:99.36ms
step:1647/1770 train_time:162663ms step_avg:99.37ms
step:1648/1770 train_time:162766ms step_avg:99.37ms
step:1649/1770 train_time:162870ms step_avg:99.37ms
step:1650/1770 train_time:162973ms step_avg:99.37ms
step:1651/1770 train_time:163076ms step_avg:99.38ms
step:1652/1770 train_time:163179ms step_avg:99.38ms
step:1653/1770 train_time:163283ms step_avg:99.38ms
step:1654/1770 train_time:163390ms step_avg:99.39ms
step:1655/1770 train_time:163497ms step_avg:99.39ms
step:1656/1770 train_time:163600ms step_avg:99.39ms
step:1657/1770 train_time:163706ms step_avg:99.40ms
step:1658/1770 train_time:163810ms step_avg:99.40ms
step:1659/1770 train_time:163917ms step_avg:99.40ms
step:1660/1770 train_time:164021ms step_avg:99.41ms
step:1661/1770 train_time:164126ms step_avg:99.41ms
step:1662/1770 train_time:164230ms step_avg:99.41ms
step:1663/1770 train_time:164333ms step_avg:99.41ms
step:1664/1770 train_time:164436ms step_avg:99.42ms
step:1665/1770 train_time:164539ms step_avg:99.42ms
step:1666/1770 train_time:164644ms step_avg:99.42ms
step:1667/1770 train_time:164747ms step_avg:99.42ms
step:1668/1770 train_time:164851ms step_avg:99.43ms
step:1669/1770 train_time:164953ms step_avg:99.43ms
step:1670/1770 train_time:165056ms step_avg:99.43ms
step:1671/1770 train_time:165161ms step_avg:99.43ms
step:1672/1770 train_time:165265ms step_avg:99.44ms
step:1673/1770 train_time:165371ms step_avg:99.44ms
step:1674/1770 train_time:165473ms step_avg:99.44ms
step:1675/1770 train_time:165576ms step_avg:99.44ms
step:1676/1770 train_time:165681ms step_avg:99.45ms
step:1677/1770 train_time:165789ms step_avg:99.45ms
step:1678/1770 train_time:165891ms step_avg:99.46ms
step:1679/1770 train_time:165995ms step_avg:99.46ms
step:1680/1770 train_time:166098ms step_avg:99.46ms
step:1681/1770 train_time:166202ms step_avg:99.46ms
step:1682/1770 train_time:166309ms step_avg:99.47ms
step:1683/1770 train_time:166411ms step_avg:99.47ms
step:1684/1770 train_time:166514ms step_avg:99.47ms
step:1685/1770 train_time:166618ms step_avg:99.47ms
step:1686/1770 train_time:166723ms step_avg:99.48ms
step:1687/1770 train_time:166828ms step_avg:99.48ms
step:1688/1770 train_time:166931ms step_avg:99.48ms
step:1689/1770 train_time:167035ms step_avg:99.48ms
step:1690/1770 train_time:167138ms step_avg:99.49ms
step:1691/1770 train_time:167243ms step_avg:99.49ms
step:1692/1770 train_time:167347ms step_avg:99.49ms
step:1693/1770 train_time:167452ms step_avg:99.50ms
step:1694/1770 train_time:167555ms step_avg:99.50ms
step:1695/1770 train_time:167659ms step_avg:99.50ms
step:1696/1770 train_time:167765ms step_avg:99.50ms
step:1697/1770 train_time:167871ms step_avg:99.51ms
step:1698/1770 train_time:167974ms step_avg:99.51ms
step:1699/1770 train_time:168078ms step_avg:99.51ms
step:1700/1770 train_time:168181ms step_avg:99.52ms
step:1701/1770 train_time:168285ms step_avg:99.52ms
step:1702/1770 train_time:168389ms step_avg:99.52ms
step:1703/1770 train_time:168492ms step_avg:99.52ms
step:1704/1770 train_time:168595ms step_avg:99.52ms
step:1705/1770 train_time:168698ms step_avg:99.53ms
step:1706/1770 train_time:168801ms step_avg:99.53ms
step:1707/1770 train_time:168906ms step_avg:99.53ms
step:1708/1770 train_time:169010ms step_avg:99.53ms
step:1709/1770 train_time:169115ms step_avg:99.54ms
step:1710/1770 train_time:169222ms step_avg:99.54ms
step:1711/1770 train_time:169329ms step_avg:99.55ms
step:1712/1770 train_time:169433ms step_avg:99.55ms
step:1713/1770 train_time:169536ms step_avg:99.55ms
step:1714/1770 train_time:169641ms step_avg:99.55ms
step:1715/1770 train_time:169745ms step_avg:99.56ms
step:1716/1770 train_time:169849ms step_avg:99.56ms
step:1717/1770 train_time:169952ms step_avg:99.56ms
step:1718/1770 train_time:170057ms step_avg:99.57ms
step:1719/1770 train_time:170162ms step_avg:99.57ms
step:1720/1770 train_time:170268ms step_avg:99.57ms
step:1721/1770 train_time:170371ms step_avg:99.57ms
step:1722/1770 train_time:170478ms step_avg:99.58ms
step:1723/1770 train_time:170584ms step_avg:99.58ms
step:1724/1770 train_time:170690ms step_avg:99.59ms
step:1725/1770 train_time:170797ms step_avg:99.59ms
step:1726/1770 train_time:170903ms step_avg:99.59ms
step:1727/1770 train_time:171007ms step_avg:99.60ms
step:1728/1770 train_time:171113ms step_avg:99.60ms
step:1729/1770 train_time:171217ms step_avg:99.60ms
step:1730/1770 train_time:171322ms step_avg:99.61ms
step:1731/1770 train_time:171429ms step_avg:99.61ms
step:1732/1770 train_time:171532ms step_avg:99.61ms
step:1733/1770 train_time:171638ms step_avg:99.62ms
step:1734/1770 train_time:171742ms step_avg:99.62ms
step:1735/1770 train_time:171848ms step_avg:99.62ms
step:1736/1770 train_time:171952ms step_avg:99.62ms
step:1737/1770 train_time:172057ms step_avg:99.63ms
step:1738/1770 train_time:172161ms step_avg:99.63ms
step:1739/1770 train_time:172265ms step_avg:99.63ms
step:1740/1770 train_time:172370ms step_avg:99.64ms
step:1741/1770 train_time:172477ms step_avg:99.64ms
step:1742/1770 train_time:172585ms step_avg:99.64ms
step:1743/1770 train_time:172690ms step_avg:99.65ms
step:1744/1770 train_time:172794ms step_avg:99.65ms
step:1745/1770 train_time:172898ms step_avg:99.65ms
step:1746/1770 train_time:173005ms step_avg:99.66ms
step:1747/1770 train_time:173108ms step_avg:99.66ms
step:1748/1770 train_time:173214ms step_avg:99.66ms
step:1749/1770 train_time:173320ms step_avg:99.67ms
step:1750/1770 train_time:173424ms step_avg:99.67ms
step:1750/1770 val_loss:3.2813 train_time:173527ms step_avg:99.73ms
step:1751/1770 train_time:173548ms step_avg:99.68ms
step:1752/1770 train_time:173638ms step_avg:99.68ms
step:1753/1770 train_time:173742ms step_avg:99.68ms
step:1754/1770 train_time:173847ms step_avg:99.68ms
step:1755/1770 train_time:173951ms step_avg:99.69ms
step:1756/1770 train_time:174056ms step_avg:99.69ms
step:1757/1770 train_time:174160ms step_avg:99.69ms
step:1758/1770 train_time:174264ms step_avg:99.69ms
step:1759/1770 train_time:174370ms step_avg:99.70ms
step:1760/1770 train_time:174474ms step_avg:99.70ms
step:1761/1770 train_time:174581ms step_avg:99.70ms
step:1762/1770 train_time:174689ms step_avg:99.71ms
step:1763/1770 train_time:174792ms step_avg:99.71ms
step:1764/1770 train_time:174897ms step_avg:99.71ms
step:1765/1770 train_time:175002ms step_avg:99.72ms
step:1766/1770 train_time:175111ms step_avg:99.72ms
step:1767/1770 train_time:175214ms step_avg:99.72ms
step:1768/1770 train_time:175318ms step_avg:99.73ms
step:1769/1770 train_time:175422ms step_avg:99.73ms
step:1770/1770 train_time:175526ms step_avg:99.73ms
step:1770/1770 val_loss:3.2783 train_time:175630ms step_avg:99.79ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
