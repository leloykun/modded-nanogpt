import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 08:09:12 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:24176ms step_avg:nanms
step:2/1770 train_time:24584ms step_avg:nanms
step:3/1770 train_time:24682ms step_avg:nanms
step:4/1770 train_time:24774ms step_avg:nanms
step:5/1770 train_time:24868ms step_avg:nanms
step:6/1770 train_time:24962ms step_avg:nanms
step:7/1770 train_time:25055ms step_avg:nanms
step:8/1770 train_time:25149ms step_avg:nanms
step:9/1770 train_time:25243ms step_avg:nanms
step:10/1770 train_time:25336ms step_avg:nanms
step:11/1770 train_time:93ms step_avg:nanms
step:12/1770 train_time:188ms step_avg:nanms
step:13/1770 train_time:283ms step_avg:94.17ms
step:14/1770 train_time:377ms step_avg:94.29ms
step:15/1770 train_time:471ms step_avg:94.13ms
step:16/1770 train_time:565ms step_avg:94.15ms
step:17/1770 train_time:659ms step_avg:94.12ms
step:18/1770 train_time:753ms step_avg:94.10ms
step:19/1770 train_time:847ms step_avg:94.15ms
step:20/1770 train_time:942ms step_avg:94.19ms
step:21/1770 train_time:1036ms step_avg:94.15ms
step:22/1770 train_time:1129ms step_avg:94.12ms
step:23/1770 train_time:1224ms step_avg:94.17ms
step:24/1770 train_time:1318ms step_avg:94.16ms
step:25/1770 train_time:1412ms step_avg:94.12ms
step:26/1770 train_time:1506ms step_avg:94.14ms
step:27/1770 train_time:1600ms step_avg:94.12ms
step:28/1770 train_time:1694ms step_avg:94.12ms
step:29/1770 train_time:1788ms step_avg:94.12ms
step:30/1770 train_time:1882ms step_avg:94.12ms
step:31/1770 train_time:1976ms step_avg:94.11ms
step:32/1770 train_time:2071ms step_avg:94.13ms
step:33/1770 train_time:2165ms step_avg:94.12ms
step:34/1770 train_time:2258ms step_avg:94.10ms
step:35/1770 train_time:2352ms step_avg:94.10ms
step:36/1770 train_time:2446ms step_avg:94.09ms
step:37/1770 train_time:2541ms step_avg:94.11ms
step:38/1770 train_time:2635ms step_avg:94.10ms
step:39/1770 train_time:2729ms step_avg:94.10ms
step:40/1770 train_time:2823ms step_avg:94.09ms
step:41/1770 train_time:2916ms step_avg:94.07ms
step:42/1770 train_time:3011ms step_avg:94.08ms
step:43/1770 train_time:3104ms step_avg:94.07ms
step:44/1770 train_time:3198ms step_avg:94.07ms
step:45/1770 train_time:3292ms step_avg:94.07ms
step:46/1770 train_time:3387ms step_avg:94.08ms
step:47/1770 train_time:3481ms step_avg:94.09ms
step:48/1770 train_time:3575ms step_avg:94.09ms
step:49/1770 train_time:3670ms step_avg:94.10ms
step:50/1770 train_time:3764ms step_avg:94.11ms
step:51/1770 train_time:3858ms step_avg:94.10ms
step:52/1770 train_time:3952ms step_avg:94.09ms
step:53/1770 train_time:4046ms step_avg:94.09ms
step:54/1770 train_time:4140ms step_avg:94.10ms
step:55/1770 train_time:4234ms step_avg:94.09ms
step:56/1770 train_time:4328ms step_avg:94.09ms
step:57/1770 train_time:4423ms step_avg:94.10ms
step:58/1770 train_time:4517ms step_avg:94.11ms
step:59/1770 train_time:4610ms step_avg:94.09ms
step:60/1770 train_time:4705ms step_avg:94.10ms
step:61/1770 train_time:4799ms step_avg:94.11ms
step:62/1770 train_time:4893ms step_avg:94.09ms
step:63/1770 train_time:4987ms step_avg:94.09ms
step:64/1770 train_time:5081ms step_avg:94.09ms
step:65/1770 train_time:5174ms step_avg:94.08ms
step:66/1770 train_time:5268ms step_avg:94.07ms
step:67/1770 train_time:5362ms step_avg:94.08ms
step:68/1770 train_time:5456ms step_avg:94.06ms
step:69/1770 train_time:5549ms step_avg:94.06ms
step:70/1770 train_time:5644ms step_avg:94.06ms
step:71/1770 train_time:5738ms step_avg:94.06ms
step:72/1770 train_time:5832ms step_avg:94.06ms
step:73/1770 train_time:5926ms step_avg:94.07ms
step:74/1770 train_time:6021ms step_avg:94.07ms
step:75/1770 train_time:6114ms step_avg:94.07ms
step:76/1770 train_time:6208ms step_avg:94.07ms
step:77/1770 train_time:6303ms step_avg:94.07ms
step:78/1770 train_time:6397ms step_avg:94.07ms
step:79/1770 train_time:6491ms step_avg:94.07ms
step:80/1770 train_time:6586ms step_avg:94.08ms
step:81/1770 train_time:6680ms step_avg:94.08ms
step:82/1770 train_time:6773ms step_avg:94.07ms
step:83/1770 train_time:6868ms step_avg:94.08ms
step:84/1770 train_time:6963ms step_avg:94.09ms
step:85/1770 train_time:7057ms step_avg:94.10ms
step:86/1770 train_time:7152ms step_avg:94.11ms
step:87/1770 train_time:7246ms step_avg:94.10ms
step:88/1770 train_time:7341ms step_avg:94.11ms
step:89/1770 train_time:7435ms step_avg:94.11ms
step:90/1770 train_time:7528ms step_avg:94.10ms
step:91/1770 train_time:7623ms step_avg:94.11ms
step:92/1770 train_time:7717ms step_avg:94.11ms
step:93/1770 train_time:7810ms step_avg:94.10ms
step:94/1770 train_time:7905ms step_avg:94.11ms
step:95/1770 train_time:7999ms step_avg:94.11ms
step:96/1770 train_time:8093ms step_avg:94.11ms
step:97/1770 train_time:8187ms step_avg:94.11ms
step:98/1770 train_time:8282ms step_avg:94.11ms
step:99/1770 train_time:8376ms step_avg:94.11ms
step:100/1770 train_time:8470ms step_avg:94.11ms
step:101/1770 train_time:8564ms step_avg:94.11ms
step:102/1770 train_time:8658ms step_avg:94.11ms
step:103/1770 train_time:8751ms step_avg:94.10ms
step:104/1770 train_time:8846ms step_avg:94.10ms
step:105/1770 train_time:8940ms step_avg:94.11ms
step:106/1770 train_time:9034ms step_avg:94.10ms
step:107/1770 train_time:9128ms step_avg:94.10ms
step:108/1770 train_time:9222ms step_avg:94.11ms
step:109/1770 train_time:9316ms step_avg:94.10ms
step:110/1770 train_time:9410ms step_avg:94.10ms
step:111/1770 train_time:9504ms step_avg:94.10ms
step:112/1770 train_time:9599ms step_avg:94.10ms
step:113/1770 train_time:9692ms step_avg:94.10ms
step:114/1770 train_time:9786ms step_avg:94.10ms
step:115/1770 train_time:9881ms step_avg:94.10ms
step:116/1770 train_time:9974ms step_avg:94.10ms
step:117/1770 train_time:10068ms step_avg:94.09ms
step:118/1770 train_time:10162ms step_avg:94.09ms
step:119/1770 train_time:10256ms step_avg:94.09ms
step:120/1770 train_time:10350ms step_avg:94.09ms
step:121/1770 train_time:10445ms step_avg:94.10ms
step:122/1770 train_time:10538ms step_avg:94.09ms
step:123/1770 train_time:10632ms step_avg:94.09ms
step:124/1770 train_time:10727ms step_avg:94.09ms
step:125/1770 train_time:10821ms step_avg:94.10ms
step:125/1770 val_loss:4.6350 train_time:10913ms step_avg:94.90ms
step:126/1770 train_time:10936ms step_avg:94.28ms
step:127/1770 train_time:11012ms step_avg:94.12ms
step:128/1770 train_time:11110ms step_avg:94.15ms
step:129/1770 train_time:11207ms step_avg:94.18ms
step:130/1770 train_time:11302ms step_avg:94.19ms
step:131/1770 train_time:11396ms step_avg:94.19ms
step:132/1770 train_time:11490ms step_avg:94.18ms
step:133/1770 train_time:11583ms step_avg:94.17ms
step:134/1770 train_time:11678ms step_avg:94.18ms
step:135/1770 train_time:11772ms step_avg:94.18ms
step:136/1770 train_time:11866ms step_avg:94.18ms
step:137/1770 train_time:11961ms step_avg:94.18ms
step:138/1770 train_time:12055ms step_avg:94.18ms
step:139/1770 train_time:12151ms step_avg:94.19ms
step:140/1770 train_time:12246ms step_avg:94.20ms
step:141/1770 train_time:12341ms step_avg:94.21ms
step:142/1770 train_time:12436ms step_avg:94.21ms
step:143/1770 train_time:12530ms step_avg:94.21ms
step:144/1770 train_time:12625ms step_avg:94.21ms
step:145/1770 train_time:12719ms step_avg:94.22ms
step:146/1770 train_time:12814ms step_avg:94.22ms
step:147/1770 train_time:12908ms step_avg:94.22ms
step:148/1770 train_time:13003ms step_avg:94.23ms
step:149/1770 train_time:13098ms step_avg:94.23ms
step:150/1770 train_time:13193ms step_avg:94.23ms
step:151/1770 train_time:13287ms step_avg:94.24ms
step:152/1770 train_time:13382ms step_avg:94.24ms
step:153/1770 train_time:13477ms step_avg:94.25ms
step:154/1770 train_time:13572ms step_avg:94.25ms
step:155/1770 train_time:13666ms step_avg:94.25ms
step:156/1770 train_time:13760ms step_avg:94.25ms
step:157/1770 train_time:13855ms step_avg:94.25ms
step:158/1770 train_time:13948ms step_avg:94.25ms
step:159/1770 train_time:14044ms step_avg:94.25ms
step:160/1770 train_time:14139ms step_avg:94.26ms
step:161/1770 train_time:14234ms step_avg:94.27ms
step:162/1770 train_time:14329ms step_avg:94.27ms
step:163/1770 train_time:14424ms step_avg:94.27ms
step:164/1770 train_time:14519ms step_avg:94.28ms
step:165/1770 train_time:14614ms step_avg:94.28ms
step:166/1770 train_time:14708ms step_avg:94.28ms
step:167/1770 train_time:14802ms step_avg:94.28ms
step:168/1770 train_time:14897ms step_avg:94.28ms
step:169/1770 train_time:14991ms step_avg:94.28ms
step:170/1770 train_time:15085ms step_avg:94.28ms
step:171/1770 train_time:15180ms step_avg:94.29ms
step:172/1770 train_time:15275ms step_avg:94.29ms
step:173/1770 train_time:15370ms step_avg:94.30ms
step:174/1770 train_time:15466ms step_avg:94.30ms
step:175/1770 train_time:15561ms step_avg:94.31ms
step:176/1770 train_time:15656ms step_avg:94.31ms
step:177/1770 train_time:15751ms step_avg:94.32ms
step:178/1770 train_time:15845ms step_avg:94.31ms
step:179/1770 train_time:15940ms step_avg:94.32ms
step:180/1770 train_time:16034ms step_avg:94.32ms
step:181/1770 train_time:16129ms step_avg:94.32ms
step:182/1770 train_time:16224ms step_avg:94.32ms
step:183/1770 train_time:16319ms step_avg:94.33ms
step:184/1770 train_time:16414ms step_avg:94.33ms
step:185/1770 train_time:16508ms step_avg:94.33ms
step:186/1770 train_time:16603ms step_avg:94.33ms
step:187/1770 train_time:16698ms step_avg:94.34ms
step:188/1770 train_time:16793ms step_avg:94.34ms
step:189/1770 train_time:16887ms step_avg:94.34ms
step:190/1770 train_time:16982ms step_avg:94.34ms
step:191/1770 train_time:17077ms step_avg:94.35ms
step:192/1770 train_time:17171ms step_avg:94.35ms
step:193/1770 train_time:17265ms step_avg:94.35ms
step:194/1770 train_time:17361ms step_avg:94.35ms
step:195/1770 train_time:17456ms step_avg:94.35ms
step:196/1770 train_time:17550ms step_avg:94.36ms
step:197/1770 train_time:17645ms step_avg:94.36ms
step:198/1770 train_time:17741ms step_avg:94.37ms
step:199/1770 train_time:17836ms step_avg:94.37ms
step:200/1770 train_time:17930ms step_avg:94.37ms
step:201/1770 train_time:18024ms step_avg:94.37ms
step:202/1770 train_time:18119ms step_avg:94.37ms
step:203/1770 train_time:18213ms step_avg:94.37ms
step:204/1770 train_time:18308ms step_avg:94.37ms
step:205/1770 train_time:18403ms step_avg:94.37ms
step:206/1770 train_time:18498ms step_avg:94.38ms
step:207/1770 train_time:18592ms step_avg:94.38ms
step:208/1770 train_time:18687ms step_avg:94.38ms
step:209/1770 train_time:18782ms step_avg:94.38ms
step:210/1770 train_time:18876ms step_avg:94.38ms
step:211/1770 train_time:18971ms step_avg:94.38ms
step:212/1770 train_time:19065ms step_avg:94.38ms
step:213/1770 train_time:19160ms step_avg:94.38ms
step:214/1770 train_time:19254ms step_avg:94.38ms
step:215/1770 train_time:19349ms step_avg:94.39ms
step:216/1770 train_time:19443ms step_avg:94.39ms
step:217/1770 train_time:19539ms step_avg:94.39ms
step:218/1770 train_time:19634ms step_avg:94.39ms
step:219/1770 train_time:19728ms step_avg:94.39ms
step:220/1770 train_time:19823ms step_avg:94.39ms
step:221/1770 train_time:19918ms step_avg:94.40ms
step:222/1770 train_time:20013ms step_avg:94.40ms
step:223/1770 train_time:20107ms step_avg:94.40ms
step:224/1770 train_time:20202ms step_avg:94.40ms
step:225/1770 train_time:20297ms step_avg:94.40ms
step:226/1770 train_time:20391ms step_avg:94.40ms
step:227/1770 train_time:20486ms step_avg:94.40ms
step:228/1770 train_time:20581ms step_avg:94.41ms
step:229/1770 train_time:20676ms step_avg:94.41ms
step:230/1770 train_time:20770ms step_avg:94.41ms
step:231/1770 train_time:20865ms step_avg:94.41ms
step:232/1770 train_time:20960ms step_avg:94.41ms
step:233/1770 train_time:21054ms step_avg:94.41ms
step:234/1770 train_time:21148ms step_avg:94.41ms
step:235/1770 train_time:21243ms step_avg:94.41ms
step:236/1770 train_time:21338ms step_avg:94.42ms
step:237/1770 train_time:21432ms step_avg:94.42ms
step:238/1770 train_time:21526ms step_avg:94.41ms
step:239/1770 train_time:21622ms step_avg:94.42ms
step:240/1770 train_time:21717ms step_avg:94.42ms
step:241/1770 train_time:21811ms step_avg:94.42ms
step:242/1770 train_time:21905ms step_avg:94.42ms
step:243/1770 train_time:22001ms step_avg:94.42ms
step:244/1770 train_time:22096ms step_avg:94.43ms
step:245/1770 train_time:22190ms step_avg:94.42ms
step:246/1770 train_time:22284ms step_avg:94.42ms
step:247/1770 train_time:22379ms step_avg:94.43ms
step:248/1770 train_time:22474ms step_avg:94.43ms
step:249/1770 train_time:22568ms step_avg:94.43ms
step:250/1770 train_time:22663ms step_avg:94.43ms
step:250/1770 val_loss:4.0987 train_time:22756ms step_avg:94.82ms
step:251/1770 train_time:22777ms step_avg:94.51ms
step:252/1770 train_time:22860ms step_avg:94.46ms
step:253/1770 train_time:22957ms step_avg:94.47ms
step:254/1770 train_time:23052ms step_avg:94.48ms
step:255/1770 train_time:23146ms step_avg:94.47ms
step:256/1770 train_time:23240ms step_avg:94.47ms
step:257/1770 train_time:23334ms step_avg:94.47ms
step:258/1770 train_time:23428ms step_avg:94.47ms
step:259/1770 train_time:23522ms step_avg:94.47ms
step:260/1770 train_time:23617ms step_avg:94.47ms
step:261/1770 train_time:23711ms step_avg:94.47ms
step:262/1770 train_time:23806ms step_avg:94.47ms
step:263/1770 train_time:23902ms step_avg:94.47ms
step:264/1770 train_time:23997ms step_avg:94.48ms
step:265/1770 train_time:24093ms step_avg:94.48ms
step:266/1770 train_time:24188ms step_avg:94.48ms
step:267/1770 train_time:24282ms step_avg:94.48ms
step:268/1770 train_time:24377ms step_avg:94.48ms
step:269/1770 train_time:24471ms step_avg:94.48ms
step:270/1770 train_time:24566ms step_avg:94.48ms
step:271/1770 train_time:24660ms step_avg:94.48ms
step:272/1770 train_time:24755ms step_avg:94.48ms
step:273/1770 train_time:24851ms step_avg:94.49ms
step:274/1770 train_time:24946ms step_avg:94.49ms
step:275/1770 train_time:25041ms step_avg:94.50ms
step:276/1770 train_time:25137ms step_avg:94.50ms
step:277/1770 train_time:25233ms step_avg:94.51ms
step:278/1770 train_time:25328ms step_avg:94.51ms
step:279/1770 train_time:25423ms step_avg:94.51ms
step:280/1770 train_time:25517ms step_avg:94.51ms
step:281/1770 train_time:25613ms step_avg:94.51ms
step:282/1770 train_time:25707ms step_avg:94.51ms
step:283/1770 train_time:25802ms step_avg:94.51ms
step:284/1770 train_time:25898ms step_avg:94.52ms
step:285/1770 train_time:25994ms step_avg:94.52ms
step:286/1770 train_time:26089ms step_avg:94.53ms
step:287/1770 train_time:26184ms step_avg:94.53ms
step:288/1770 train_time:26279ms step_avg:94.53ms
step:289/1770 train_time:26375ms step_avg:94.53ms
step:290/1770 train_time:26470ms step_avg:94.54ms
step:291/1770 train_time:26565ms step_avg:94.54ms
step:292/1770 train_time:26659ms step_avg:94.54ms
step:293/1770 train_time:26755ms step_avg:94.54ms
step:294/1770 train_time:26850ms step_avg:94.54ms
step:295/1770 train_time:26945ms step_avg:94.54ms
step:296/1770 train_time:27040ms step_avg:94.55ms
step:297/1770 train_time:27136ms step_avg:94.55ms
step:298/1770 train_time:27231ms step_avg:94.55ms
step:299/1770 train_time:27326ms step_avg:94.55ms
step:300/1770 train_time:27421ms step_avg:94.55ms
step:301/1770 train_time:27516ms step_avg:94.56ms
step:302/1770 train_time:27611ms step_avg:94.56ms
step:303/1770 train_time:27705ms step_avg:94.56ms
step:304/1770 train_time:27800ms step_avg:94.56ms
step:305/1770 train_time:27895ms step_avg:94.56ms
step:306/1770 train_time:27991ms step_avg:94.56ms
step:307/1770 train_time:28085ms step_avg:94.56ms
step:308/1770 train_time:28181ms step_avg:94.57ms
step:309/1770 train_time:28276ms step_avg:94.57ms
step:310/1770 train_time:28371ms step_avg:94.57ms
step:311/1770 train_time:28466ms step_avg:94.57ms
step:312/1770 train_time:28560ms step_avg:94.57ms
step:313/1770 train_time:28656ms step_avg:94.57ms
step:314/1770 train_time:28751ms step_avg:94.58ms
step:315/1770 train_time:28846ms step_avg:94.58ms
step:316/1770 train_time:28941ms step_avg:94.58ms
step:317/1770 train_time:29036ms step_avg:94.58ms
step:318/1770 train_time:29131ms step_avg:94.58ms
step:319/1770 train_time:29227ms step_avg:94.58ms
step:320/1770 train_time:29321ms step_avg:94.58ms
step:321/1770 train_time:29416ms step_avg:94.58ms
step:322/1770 train_time:29511ms step_avg:94.59ms
step:323/1770 train_time:29607ms step_avg:94.59ms
step:324/1770 train_time:29701ms step_avg:94.59ms
step:325/1770 train_time:29797ms step_avg:94.59ms
step:326/1770 train_time:29893ms step_avg:94.60ms
step:327/1770 train_time:29987ms step_avg:94.60ms
step:328/1770 train_time:30082ms step_avg:94.60ms
step:329/1770 train_time:30178ms step_avg:94.60ms
step:330/1770 train_time:30273ms step_avg:94.60ms
step:331/1770 train_time:30368ms step_avg:94.60ms
step:332/1770 train_time:30462ms step_avg:94.60ms
step:333/1770 train_time:30558ms step_avg:94.61ms
step:334/1770 train_time:30654ms step_avg:94.61ms
step:335/1770 train_time:30749ms step_avg:94.61ms
step:336/1770 train_time:30844ms step_avg:94.61ms
step:337/1770 train_time:30939ms step_avg:94.61ms
step:338/1770 train_time:31035ms step_avg:94.62ms
step:339/1770 train_time:31130ms step_avg:94.62ms
step:340/1770 train_time:31224ms step_avg:94.62ms
step:341/1770 train_time:31319ms step_avg:94.62ms
step:342/1770 train_time:31415ms step_avg:94.62ms
step:343/1770 train_time:31511ms step_avg:94.63ms
step:344/1770 train_time:31606ms step_avg:94.63ms
step:345/1770 train_time:31700ms step_avg:94.63ms
step:346/1770 train_time:31797ms step_avg:94.63ms
step:347/1770 train_time:31892ms step_avg:94.64ms
step:348/1770 train_time:31987ms step_avg:94.64ms
step:349/1770 train_time:32082ms step_avg:94.64ms
step:350/1770 train_time:32177ms step_avg:94.64ms
step:351/1770 train_time:32272ms step_avg:94.64ms
step:352/1770 train_time:32367ms step_avg:94.64ms
step:353/1770 train_time:32462ms step_avg:94.64ms
step:354/1770 train_time:32558ms step_avg:94.64ms
step:355/1770 train_time:32653ms step_avg:94.65ms
step:356/1770 train_time:32748ms step_avg:94.65ms
step:357/1770 train_time:32843ms step_avg:94.65ms
step:358/1770 train_time:32938ms step_avg:94.65ms
step:359/1770 train_time:33033ms step_avg:94.65ms
step:360/1770 train_time:33128ms step_avg:94.65ms
step:361/1770 train_time:33223ms step_avg:94.65ms
step:362/1770 train_time:33318ms step_avg:94.65ms
step:363/1770 train_time:33414ms step_avg:94.66ms
step:364/1770 train_time:33509ms step_avg:94.66ms
step:365/1770 train_time:33604ms step_avg:94.66ms
step:366/1770 train_time:33699ms step_avg:94.66ms
step:367/1770 train_time:33795ms step_avg:94.66ms
step:368/1770 train_time:33890ms step_avg:94.67ms
step:369/1770 train_time:33986ms step_avg:94.67ms
step:370/1770 train_time:34080ms step_avg:94.67ms
step:371/1770 train_time:34176ms step_avg:94.67ms
step:372/1770 train_time:34271ms step_avg:94.67ms
step:373/1770 train_time:34368ms step_avg:94.68ms
step:374/1770 train_time:34461ms step_avg:94.67ms
step:375/1770 train_time:34556ms step_avg:94.68ms
step:375/1770 val_loss:3.8976 train_time:34651ms step_avg:94.93ms
step:376/1770 train_time:34679ms step_avg:94.75ms
step:377/1770 train_time:34754ms step_avg:94.70ms
step:378/1770 train_time:34851ms step_avg:94.70ms
step:379/1770 train_time:34947ms step_avg:94.71ms
step:380/1770 train_time:35042ms step_avg:94.71ms
step:381/1770 train_time:35136ms step_avg:94.71ms
step:382/1770 train_time:35232ms step_avg:94.71ms
step:383/1770 train_time:35326ms step_avg:94.71ms
step:384/1770 train_time:35421ms step_avg:94.71ms
step:385/1770 train_time:35515ms step_avg:94.71ms
step:386/1770 train_time:35610ms step_avg:94.71ms
step:387/1770 train_time:35705ms step_avg:94.71ms
step:388/1770 train_time:35800ms step_avg:94.71ms
step:389/1770 train_time:35897ms step_avg:94.71ms
step:390/1770 train_time:35992ms step_avg:94.72ms
step:391/1770 train_time:36087ms step_avg:94.72ms
step:392/1770 train_time:36182ms step_avg:94.72ms
step:393/1770 train_time:36277ms step_avg:94.72ms
step:394/1770 train_time:36372ms step_avg:94.72ms
step:395/1770 train_time:36466ms step_avg:94.72ms
step:396/1770 train_time:36562ms step_avg:94.72ms
step:397/1770 train_time:36659ms step_avg:94.73ms
step:398/1770 train_time:36756ms step_avg:94.73ms
step:399/1770 train_time:36854ms step_avg:94.74ms
step:400/1770 train_time:36952ms step_avg:94.75ms
step:401/1770 train_time:37049ms step_avg:94.75ms
step:402/1770 train_time:37146ms step_avg:94.76ms
step:403/1770 train_time:37243ms step_avg:94.76ms
step:404/1770 train_time:37340ms step_avg:94.77ms
step:405/1770 train_time:37437ms step_avg:94.78ms
step:406/1770 train_time:37534ms step_avg:94.78ms
step:407/1770 train_time:37631ms step_avg:94.79ms
step:408/1770 train_time:37728ms step_avg:94.79ms
step:409/1770 train_time:37825ms step_avg:94.80ms
step:410/1770 train_time:37922ms step_avg:94.80ms
step:411/1770 train_time:38019ms step_avg:94.81ms
step:412/1770 train_time:38116ms step_avg:94.82ms
step:413/1770 train_time:38214ms step_avg:94.82ms
step:414/1770 train_time:38311ms step_avg:94.83ms
step:415/1770 train_time:38408ms step_avg:94.83ms
step:416/1770 train_time:38504ms step_avg:94.84ms
step:417/1770 train_time:38601ms step_avg:94.84ms
step:418/1770 train_time:38698ms step_avg:94.85ms
step:419/1770 train_time:38795ms step_avg:94.85ms
step:420/1770 train_time:38893ms step_avg:94.86ms
step:421/1770 train_time:38990ms step_avg:94.87ms
step:422/1770 train_time:39087ms step_avg:94.87ms
step:423/1770 train_time:39183ms step_avg:94.87ms
step:424/1770 train_time:39280ms step_avg:94.88ms
step:425/1770 train_time:39377ms step_avg:94.88ms
step:426/1770 train_time:39474ms step_avg:94.89ms
step:427/1770 train_time:39571ms step_avg:94.89ms
step:428/1770 train_time:39668ms step_avg:94.90ms
step:429/1770 train_time:39765ms step_avg:94.90ms
step:430/1770 train_time:39861ms step_avg:94.91ms
step:431/1770 train_time:39958ms step_avg:94.91ms
step:432/1770 train_time:40056ms step_avg:94.92ms
step:433/1770 train_time:40154ms step_avg:94.93ms
step:434/1770 train_time:40252ms step_avg:94.93ms
step:435/1770 train_time:40350ms step_avg:94.94ms
step:436/1770 train_time:40445ms step_avg:94.94ms
step:437/1770 train_time:40542ms step_avg:94.95ms
step:438/1770 train_time:40639ms step_avg:94.95ms
step:439/1770 train_time:40736ms step_avg:94.96ms
step:440/1770 train_time:40833ms step_avg:94.96ms
step:441/1770 train_time:40931ms step_avg:94.97ms
step:442/1770 train_time:41028ms step_avg:94.97ms
step:443/1770 train_time:41124ms step_avg:94.98ms
step:444/1770 train_time:41221ms step_avg:94.98ms
step:445/1770 train_time:41319ms step_avg:94.99ms
step:446/1770 train_time:41416ms step_avg:94.99ms
step:447/1770 train_time:41513ms step_avg:95.00ms
step:448/1770 train_time:41611ms step_avg:95.00ms
step:449/1770 train_time:41708ms step_avg:95.01ms
step:450/1770 train_time:41804ms step_avg:95.01ms
step:451/1770 train_time:41901ms step_avg:95.01ms
step:452/1770 train_time:41998ms step_avg:95.02ms
step:453/1770 train_time:42095ms step_avg:95.02ms
step:454/1770 train_time:42192ms step_avg:95.03ms
step:455/1770 train_time:42290ms step_avg:95.03ms
step:456/1770 train_time:42386ms step_avg:95.04ms
step:457/1770 train_time:42483ms step_avg:95.04ms
step:458/1770 train_time:42579ms step_avg:95.04ms
step:459/1770 train_time:42677ms step_avg:95.05ms
step:460/1770 train_time:42774ms step_avg:95.05ms
step:461/1770 train_time:42873ms step_avg:95.06ms
step:462/1770 train_time:42970ms step_avg:95.07ms
step:463/1770 train_time:43067ms step_avg:95.07ms
step:464/1770 train_time:43163ms step_avg:95.07ms
step:465/1770 train_time:43260ms step_avg:95.08ms
step:466/1770 train_time:43357ms step_avg:95.08ms
step:467/1770 train_time:43455ms step_avg:95.09ms
step:468/1770 train_time:43552ms step_avg:95.09ms
step:469/1770 train_time:43649ms step_avg:95.10ms
step:470/1770 train_time:43745ms step_avg:95.10ms
step:471/1770 train_time:43842ms step_avg:95.10ms
step:472/1770 train_time:43940ms step_avg:95.11ms
step:473/1770 train_time:44036ms step_avg:95.11ms
step:474/1770 train_time:44134ms step_avg:95.12ms
step:475/1770 train_time:44232ms step_avg:95.12ms
step:476/1770 train_time:44329ms step_avg:95.13ms
step:477/1770 train_time:44426ms step_avg:95.13ms
step:478/1770 train_time:44523ms step_avg:95.13ms
step:479/1770 train_time:44620ms step_avg:95.14ms
step:480/1770 train_time:44717ms step_avg:95.14ms
step:481/1770 train_time:44814ms step_avg:95.15ms
step:482/1770 train_time:44911ms step_avg:95.15ms
step:483/1770 train_time:45008ms step_avg:95.15ms
step:484/1770 train_time:45104ms step_avg:95.16ms
step:485/1770 train_time:45201ms step_avg:95.16ms
step:486/1770 train_time:45298ms step_avg:95.16ms
step:487/1770 train_time:45396ms step_avg:95.17ms
step:488/1770 train_time:45494ms step_avg:95.18ms
step:489/1770 train_time:45592ms step_avg:95.18ms
step:490/1770 train_time:45689ms step_avg:95.19ms
step:491/1770 train_time:45785ms step_avg:95.19ms
step:492/1770 train_time:45882ms step_avg:95.19ms
step:493/1770 train_time:45979ms step_avg:95.19ms
step:494/1770 train_time:46076ms step_avg:95.20ms
step:495/1770 train_time:46174ms step_avg:95.20ms
step:496/1770 train_time:46271ms step_avg:95.21ms
step:497/1770 train_time:46368ms step_avg:95.21ms
step:498/1770 train_time:46465ms step_avg:95.21ms
step:499/1770 train_time:46561ms step_avg:95.22ms
step:500/1770 train_time:46658ms step_avg:95.22ms
step:500/1770 val_loss:3.7509 train_time:46754ms step_avg:95.42ms
step:501/1770 train_time:46775ms step_avg:95.27ms
step:502/1770 train_time:46859ms step_avg:95.24ms
step:503/1770 train_time:46957ms step_avg:95.25ms
step:504/1770 train_time:47055ms step_avg:95.25ms
step:505/1770 train_time:47152ms step_avg:95.26ms
step:506/1770 train_time:47249ms step_avg:95.26ms
step:507/1770 train_time:47345ms step_avg:95.26ms
step:508/1770 train_time:47442ms step_avg:95.26ms
step:509/1770 train_time:47539ms step_avg:95.27ms
step:510/1770 train_time:47635ms step_avg:95.27ms
step:511/1770 train_time:47732ms step_avg:95.27ms
step:512/1770 train_time:47829ms step_avg:95.28ms
step:513/1770 train_time:47927ms step_avg:95.28ms
step:514/1770 train_time:48023ms step_avg:95.28ms
step:515/1770 train_time:48121ms step_avg:95.29ms
step:516/1770 train_time:48218ms step_avg:95.29ms
step:517/1770 train_time:48316ms step_avg:95.30ms
step:518/1770 train_time:48412ms step_avg:95.30ms
step:519/1770 train_time:48508ms step_avg:95.30ms
step:520/1770 train_time:48605ms step_avg:95.30ms
step:521/1770 train_time:48702ms step_avg:95.31ms
step:522/1770 train_time:48800ms step_avg:95.31ms
step:523/1770 train_time:48898ms step_avg:95.32ms
step:524/1770 train_time:48995ms step_avg:95.32ms
step:525/1770 train_time:49092ms step_avg:95.32ms
step:526/1770 train_time:49188ms step_avg:95.33ms
step:527/1770 train_time:49285ms step_avg:95.33ms
step:528/1770 train_time:49383ms step_avg:95.33ms
step:529/1770 train_time:49480ms step_avg:95.34ms
step:530/1770 train_time:49578ms step_avg:95.34ms
step:531/1770 train_time:49677ms step_avg:95.35ms
step:532/1770 train_time:49775ms step_avg:95.35ms
step:533/1770 train_time:49872ms step_avg:95.36ms
step:534/1770 train_time:49969ms step_avg:95.36ms
step:535/1770 train_time:50066ms step_avg:95.36ms
step:536/1770 train_time:50164ms step_avg:95.37ms
step:537/1770 train_time:50261ms step_avg:95.37ms
step:538/1770 train_time:50360ms step_avg:95.38ms
step:539/1770 train_time:50459ms step_avg:95.39ms
step:540/1770 train_time:50557ms step_avg:95.39ms
step:541/1770 train_time:50654ms step_avg:95.39ms
step:542/1770 train_time:50752ms step_avg:95.40ms
step:543/1770 train_time:50849ms step_avg:95.40ms
step:544/1770 train_time:50947ms step_avg:95.41ms
step:545/1770 train_time:51044ms step_avg:95.41ms
step:546/1770 train_time:51142ms step_avg:95.41ms
step:547/1770 train_time:51239ms step_avg:95.42ms
step:548/1770 train_time:51337ms step_avg:95.42ms
step:549/1770 train_time:51435ms step_avg:95.43ms
step:550/1770 train_time:51532ms step_avg:95.43ms
step:551/1770 train_time:51629ms step_avg:95.43ms
step:552/1770 train_time:51726ms step_avg:95.43ms
step:553/1770 train_time:51823ms step_avg:95.44ms
step:554/1770 train_time:51921ms step_avg:95.44ms
step:555/1770 train_time:52019ms step_avg:95.45ms
step:556/1770 train_time:52116ms step_avg:95.45ms
step:557/1770 train_time:52213ms step_avg:95.45ms
step:558/1770 train_time:52310ms step_avg:95.46ms
step:559/1770 train_time:52407ms step_avg:95.46ms
step:560/1770 train_time:52504ms step_avg:95.46ms
step:561/1770 train_time:52602ms step_avg:95.47ms
step:562/1770 train_time:52700ms step_avg:95.47ms
step:563/1770 train_time:52797ms step_avg:95.47ms
step:564/1770 train_time:52895ms step_avg:95.48ms
step:565/1770 train_time:52992ms step_avg:95.48ms
step:566/1770 train_time:53089ms step_avg:95.48ms
step:567/1770 train_time:53187ms step_avg:95.49ms
step:568/1770 train_time:53284ms step_avg:95.49ms
step:569/1770 train_time:53382ms step_avg:95.49ms
step:570/1770 train_time:53479ms step_avg:95.50ms
step:571/1770 train_time:53577ms step_avg:95.50ms
step:572/1770 train_time:53675ms step_avg:95.51ms
step:573/1770 train_time:53773ms step_avg:95.51ms
step:574/1770 train_time:53870ms step_avg:95.51ms
step:575/1770 train_time:53968ms step_avg:95.52ms
step:576/1770 train_time:54065ms step_avg:95.52ms
step:577/1770 train_time:54163ms step_avg:95.53ms
step:578/1770 train_time:54261ms step_avg:95.53ms
step:579/1770 train_time:54358ms step_avg:95.53ms
step:580/1770 train_time:54456ms step_avg:95.54ms
step:581/1770 train_time:54552ms step_avg:95.54ms
step:582/1770 train_time:54649ms step_avg:95.54ms
step:583/1770 train_time:54747ms step_avg:95.54ms
step:584/1770 train_time:54843ms step_avg:95.55ms
step:585/1770 train_time:54941ms step_avg:95.55ms
step:586/1770 train_time:55039ms step_avg:95.55ms
step:587/1770 train_time:55137ms step_avg:95.56ms
step:588/1770 train_time:55234ms step_avg:95.56ms
step:589/1770 train_time:55331ms step_avg:95.56ms
step:590/1770 train_time:55428ms step_avg:95.57ms
step:591/1770 train_time:55525ms step_avg:95.57ms
step:592/1770 train_time:55622ms step_avg:95.57ms
step:593/1770 train_time:55720ms step_avg:95.58ms
step:594/1770 train_time:55819ms step_avg:95.58ms
step:595/1770 train_time:55917ms step_avg:95.58ms
step:596/1770 train_time:56014ms step_avg:95.59ms
step:597/1770 train_time:56112ms step_avg:95.59ms
step:598/1770 train_time:56209ms step_avg:95.59ms
step:599/1770 train_time:56306ms step_avg:95.60ms
step:600/1770 train_time:56404ms step_avg:95.60ms
step:601/1770 train_time:56501ms step_avg:95.60ms
step:602/1770 train_time:56599ms step_avg:95.61ms
step:603/1770 train_time:56696ms step_avg:95.61ms
step:604/1770 train_time:56794ms step_avg:95.61ms
step:605/1770 train_time:56891ms step_avg:95.62ms
step:606/1770 train_time:56988ms step_avg:95.62ms
step:607/1770 train_time:57085ms step_avg:95.62ms
step:608/1770 train_time:57183ms step_avg:95.62ms
step:609/1770 train_time:57281ms step_avg:95.63ms
step:610/1770 train_time:57379ms step_avg:95.63ms
step:611/1770 train_time:57477ms step_avg:95.64ms
step:612/1770 train_time:57575ms step_avg:95.64ms
step:613/1770 train_time:57672ms step_avg:95.64ms
step:614/1770 train_time:57769ms step_avg:95.64ms
step:615/1770 train_time:57866ms step_avg:95.65ms
step:616/1770 train_time:57963ms step_avg:95.65ms
step:617/1770 train_time:58061ms step_avg:95.65ms
step:618/1770 train_time:58159ms step_avg:95.66ms
step:619/1770 train_time:58257ms step_avg:95.66ms
step:620/1770 train_time:58355ms step_avg:95.66ms
step:621/1770 train_time:58453ms step_avg:95.67ms
step:622/1770 train_time:58550ms step_avg:95.67ms
step:623/1770 train_time:58647ms step_avg:95.67ms
step:624/1770 train_time:58745ms step_avg:95.68ms
step:625/1770 train_time:58843ms step_avg:95.68ms
step:625/1770 val_loss:3.6660 train_time:58939ms step_avg:95.84ms
step:626/1770 train_time:58965ms step_avg:95.72ms
step:627/1770 train_time:59047ms step_avg:95.70ms
step:628/1770 train_time:59145ms step_avg:95.70ms
step:629/1770 train_time:59242ms step_avg:95.71ms
step:630/1770 train_time:59339ms step_avg:95.71ms
step:631/1770 train_time:59436ms step_avg:95.71ms
step:632/1770 train_time:59533ms step_avg:95.71ms
step:633/1770 train_time:59631ms step_avg:95.72ms
step:634/1770 train_time:59728ms step_avg:95.72ms
step:635/1770 train_time:59826ms step_avg:95.72ms
step:636/1770 train_time:59924ms step_avg:95.73ms
step:637/1770 train_time:60022ms step_avg:95.73ms
step:638/1770 train_time:60119ms step_avg:95.73ms
step:639/1770 train_time:60217ms step_avg:95.73ms
step:640/1770 train_time:60315ms step_avg:95.74ms
step:641/1770 train_time:60413ms step_avg:95.74ms
step:642/1770 train_time:60510ms step_avg:95.74ms
step:643/1770 train_time:60608ms step_avg:95.75ms
step:644/1770 train_time:60705ms step_avg:95.75ms
step:645/1770 train_time:60802ms step_avg:95.75ms
step:646/1770 train_time:60899ms step_avg:95.75ms
step:647/1770 train_time:60997ms step_avg:95.76ms
step:648/1770 train_time:61096ms step_avg:95.76ms
step:649/1770 train_time:61193ms step_avg:95.76ms
step:650/1770 train_time:61291ms step_avg:95.77ms
step:651/1770 train_time:61389ms step_avg:95.77ms
step:652/1770 train_time:61487ms step_avg:95.77ms
step:653/1770 train_time:61584ms step_avg:95.78ms
step:654/1770 train_time:61681ms step_avg:95.78ms
step:655/1770 train_time:61778ms step_avg:95.78ms
step:656/1770 train_time:61875ms step_avg:95.78ms
step:657/1770 train_time:61972ms step_avg:95.78ms
step:658/1770 train_time:62073ms step_avg:95.79ms
step:659/1770 train_time:62173ms step_avg:95.80ms
step:660/1770 train_time:62272ms step_avg:95.80ms
step:661/1770 train_time:62372ms step_avg:95.81ms
step:662/1770 train_time:62473ms step_avg:95.82ms
step:663/1770 train_time:62573ms step_avg:95.82ms
step:664/1770 train_time:62672ms step_avg:95.83ms
step:665/1770 train_time:62773ms step_avg:95.84ms
step:666/1770 train_time:62872ms step_avg:95.84ms
step:667/1770 train_time:62971ms step_avg:95.85ms
step:668/1770 train_time:63070ms step_avg:95.85ms
step:669/1770 train_time:63170ms step_avg:95.86ms
step:670/1770 train_time:63269ms step_avg:95.86ms
step:671/1770 train_time:63370ms step_avg:95.87ms
step:672/1770 train_time:63468ms step_avg:95.87ms
step:673/1770 train_time:63567ms step_avg:95.88ms
step:674/1770 train_time:63666ms step_avg:95.88ms
step:675/1770 train_time:63766ms step_avg:95.89ms
step:676/1770 train_time:63864ms step_avg:95.89ms
step:677/1770 train_time:63964ms step_avg:95.90ms
step:678/1770 train_time:64063ms step_avg:95.90ms
step:679/1770 train_time:64161ms step_avg:95.91ms
step:680/1770 train_time:64259ms step_avg:95.91ms
step:681/1770 train_time:64358ms step_avg:95.91ms
step:682/1770 train_time:64457ms step_avg:95.92ms
step:683/1770 train_time:64556ms step_avg:95.92ms
step:684/1770 train_time:64655ms step_avg:95.93ms
step:685/1770 train_time:64755ms step_avg:95.93ms
step:686/1770 train_time:64855ms step_avg:95.94ms
step:687/1770 train_time:64954ms step_avg:95.94ms
step:688/1770 train_time:65054ms step_avg:95.95ms
step:689/1770 train_time:65154ms step_avg:95.96ms
step:690/1770 train_time:65254ms step_avg:95.96ms
step:691/1770 train_time:65353ms step_avg:95.97ms
step:692/1770 train_time:65452ms step_avg:95.97ms
step:693/1770 train_time:65551ms step_avg:95.97ms
step:694/1770 train_time:65650ms step_avg:95.98ms
step:695/1770 train_time:65750ms step_avg:95.99ms
step:696/1770 train_time:65850ms step_avg:95.99ms
step:697/1770 train_time:65949ms step_avg:96.00ms
step:698/1770 train_time:66049ms step_avg:96.00ms
step:699/1770 train_time:66149ms step_avg:96.01ms
step:700/1770 train_time:66249ms step_avg:96.01ms
step:701/1770 train_time:66349ms step_avg:96.02ms
step:702/1770 train_time:66448ms step_avg:96.02ms
step:703/1770 train_time:66547ms step_avg:96.03ms
step:704/1770 train_time:66646ms step_avg:96.03ms
step:705/1770 train_time:66745ms step_avg:96.04ms
step:706/1770 train_time:66844ms step_avg:96.04ms
step:707/1770 train_time:66944ms step_avg:96.05ms
step:708/1770 train_time:67043ms step_avg:96.05ms
step:709/1770 train_time:67142ms step_avg:96.05ms
step:710/1770 train_time:67241ms step_avg:96.06ms
step:711/1770 train_time:67339ms step_avg:96.06ms
step:712/1770 train_time:67438ms step_avg:96.07ms
step:713/1770 train_time:67537ms step_avg:96.07ms
step:714/1770 train_time:67635ms step_avg:96.07ms
step:715/1770 train_time:67734ms step_avg:96.08ms
step:716/1770 train_time:67833ms step_avg:96.08ms
step:717/1770 train_time:67933ms step_avg:96.09ms
step:718/1770 train_time:68033ms step_avg:96.09ms
step:719/1770 train_time:68133ms step_avg:96.10ms
step:720/1770 train_time:68233ms step_avg:96.10ms
step:721/1770 train_time:68333ms step_avg:96.11ms
step:722/1770 train_time:68433ms step_avg:96.11ms
step:723/1770 train_time:68533ms step_avg:96.12ms
step:724/1770 train_time:68632ms step_avg:96.12ms
step:725/1770 train_time:68732ms step_avg:96.13ms
step:726/1770 train_time:68831ms step_avg:96.13ms
step:727/1770 train_time:68931ms step_avg:96.14ms
step:728/1770 train_time:69030ms step_avg:96.14ms
step:729/1770 train_time:69130ms step_avg:96.15ms
step:730/1770 train_time:69230ms step_avg:96.15ms
step:731/1770 train_time:69330ms step_avg:96.16ms
step:732/1770 train_time:69430ms step_avg:96.16ms
step:733/1770 train_time:69531ms step_avg:96.17ms
step:734/1770 train_time:69631ms step_avg:96.17ms
step:735/1770 train_time:69731ms step_avg:96.18ms
step:736/1770 train_time:69831ms step_avg:96.19ms
step:737/1770 train_time:69930ms step_avg:96.19ms
step:738/1770 train_time:70030ms step_avg:96.19ms
step:739/1770 train_time:70130ms step_avg:96.20ms
step:740/1770 train_time:70229ms step_avg:96.20ms
step:741/1770 train_time:70330ms step_avg:96.21ms
step:742/1770 train_time:70430ms step_avg:96.22ms
step:743/1770 train_time:70529ms step_avg:96.22ms
step:744/1770 train_time:70629ms step_avg:96.22ms
step:745/1770 train_time:70728ms step_avg:96.23ms
step:746/1770 train_time:70828ms step_avg:96.23ms
step:747/1770 train_time:70927ms step_avg:96.24ms
step:748/1770 train_time:71026ms step_avg:96.24ms
step:749/1770 train_time:71125ms step_avg:96.25ms
step:750/1770 train_time:71224ms step_avg:96.25ms
step:750/1770 val_loss:3.5994 train_time:71321ms step_avg:96.38ms
step:751/1770 train_time:71348ms step_avg:96.29ms
step:752/1770 train_time:71431ms step_avg:96.27ms
step:753/1770 train_time:71530ms step_avg:96.27ms
step:754/1770 train_time:71629ms step_avg:96.28ms
step:755/1770 train_time:71728ms step_avg:96.28ms
step:756/1770 train_time:71828ms step_avg:96.28ms
step:757/1770 train_time:71927ms step_avg:96.29ms
step:758/1770 train_time:72026ms step_avg:96.29ms
step:759/1770 train_time:72124ms step_avg:96.29ms
step:760/1770 train_time:72224ms step_avg:96.30ms
step:761/1770 train_time:72323ms step_avg:96.30ms
step:762/1770 train_time:72423ms step_avg:96.31ms
step:763/1770 train_time:72522ms step_avg:96.31ms
step:764/1770 train_time:72621ms step_avg:96.31ms
step:765/1770 train_time:72720ms step_avg:96.32ms
step:766/1770 train_time:72819ms step_avg:96.32ms
step:767/1770 train_time:72919ms step_avg:96.33ms
step:768/1770 train_time:73017ms step_avg:96.33ms
step:769/1770 train_time:73116ms step_avg:96.33ms
step:770/1770 train_time:73214ms step_avg:96.33ms
step:771/1770 train_time:73313ms step_avg:96.34ms
step:772/1770 train_time:73412ms step_avg:96.34ms
step:773/1770 train_time:73511ms step_avg:96.34ms
step:774/1770 train_time:73611ms step_avg:96.35ms
step:775/1770 train_time:73710ms step_avg:96.35ms
step:776/1770 train_time:73810ms step_avg:96.36ms
step:777/1770 train_time:73910ms step_avg:96.36ms
step:778/1770 train_time:74009ms step_avg:96.37ms
step:779/1770 train_time:74109ms step_avg:96.37ms
step:780/1770 train_time:74209ms step_avg:96.38ms
step:781/1770 train_time:74309ms step_avg:96.38ms
step:782/1770 train_time:74409ms step_avg:96.38ms
step:783/1770 train_time:74508ms step_avg:96.39ms
step:784/1770 train_time:74608ms step_avg:96.39ms
step:785/1770 train_time:74707ms step_avg:96.40ms
step:786/1770 train_time:74807ms step_avg:96.40ms
step:787/1770 train_time:74907ms step_avg:96.40ms
step:788/1770 train_time:75007ms step_avg:96.41ms
step:789/1770 train_time:75107ms step_avg:96.41ms
step:790/1770 train_time:75207ms step_avg:96.42ms
step:791/1770 train_time:75308ms step_avg:96.43ms
step:792/1770 train_time:75409ms step_avg:96.43ms
step:793/1770 train_time:75509ms step_avg:96.44ms
step:794/1770 train_time:75609ms step_avg:96.44ms
step:795/1770 train_time:75709ms step_avg:96.44ms
step:796/1770 train_time:75809ms step_avg:96.45ms
step:797/1770 train_time:75909ms step_avg:96.45ms
step:798/1770 train_time:76010ms step_avg:96.46ms
step:799/1770 train_time:76109ms step_avg:96.46ms
step:800/1770 train_time:76209ms step_avg:96.47ms
step:801/1770 train_time:76309ms step_avg:96.47ms
step:802/1770 train_time:76409ms step_avg:96.48ms
step:803/1770 train_time:76509ms step_avg:96.48ms
step:804/1770 train_time:76609ms step_avg:96.49ms
step:805/1770 train_time:76709ms step_avg:96.49ms
step:806/1770 train_time:76809ms step_avg:96.49ms
step:807/1770 train_time:76909ms step_avg:96.50ms
step:808/1770 train_time:77009ms step_avg:96.50ms
step:809/1770 train_time:77109ms step_avg:96.51ms
step:810/1770 train_time:77210ms step_avg:96.51ms
step:811/1770 train_time:77309ms step_avg:96.52ms
step:812/1770 train_time:77408ms step_avg:96.52ms
step:813/1770 train_time:77508ms step_avg:96.52ms
step:814/1770 train_time:77609ms step_avg:96.53ms
step:815/1770 train_time:77709ms step_avg:96.53ms
step:816/1770 train_time:77809ms step_avg:96.54ms
step:817/1770 train_time:77909ms step_avg:96.54ms
step:818/1770 train_time:78009ms step_avg:96.55ms
step:819/1770 train_time:78109ms step_avg:96.55ms
step:820/1770 train_time:78209ms step_avg:96.55ms
step:821/1770 train_time:78310ms step_avg:96.56ms
step:822/1770 train_time:78409ms step_avg:96.56ms
step:823/1770 train_time:78509ms step_avg:96.57ms
step:824/1770 train_time:78609ms step_avg:96.57ms
step:825/1770 train_time:78709ms step_avg:96.58ms
step:826/1770 train_time:78809ms step_avg:96.58ms
step:827/1770 train_time:78909ms step_avg:96.58ms
step:828/1770 train_time:79010ms step_avg:96.59ms
step:829/1770 train_time:79110ms step_avg:96.59ms
step:830/1770 train_time:79210ms step_avg:96.60ms
step:831/1770 train_time:79309ms step_avg:96.60ms
step:832/1770 train_time:79409ms step_avg:96.60ms
step:833/1770 train_time:79509ms step_avg:96.61ms
step:834/1770 train_time:79609ms step_avg:96.61ms
step:835/1770 train_time:79709ms step_avg:96.62ms
step:836/1770 train_time:79809ms step_avg:96.62ms
step:837/1770 train_time:79909ms step_avg:96.63ms
step:838/1770 train_time:80009ms step_avg:96.63ms
step:839/1770 train_time:80109ms step_avg:96.63ms
step:840/1770 train_time:80209ms step_avg:96.64ms
step:841/1770 train_time:80309ms step_avg:96.64ms
step:842/1770 train_time:80409ms step_avg:96.64ms
step:843/1770 train_time:80509ms step_avg:96.65ms
step:844/1770 train_time:80608ms step_avg:96.65ms
step:845/1770 train_time:80708ms step_avg:96.66ms
step:846/1770 train_time:80808ms step_avg:96.66ms
step:847/1770 train_time:80908ms step_avg:96.66ms
step:848/1770 train_time:81008ms step_avg:96.67ms
step:849/1770 train_time:81109ms step_avg:96.67ms
step:850/1770 train_time:81210ms step_avg:96.68ms
step:851/1770 train_time:81309ms step_avg:96.68ms
step:852/1770 train_time:81409ms step_avg:96.69ms
step:853/1770 train_time:81509ms step_avg:96.69ms
step:854/1770 train_time:81609ms step_avg:96.69ms
step:855/1770 train_time:81709ms step_avg:96.70ms
step:856/1770 train_time:81809ms step_avg:96.70ms
step:857/1770 train_time:81909ms step_avg:96.70ms
step:858/1770 train_time:82009ms step_avg:96.71ms
step:859/1770 train_time:82109ms step_avg:96.71ms
step:860/1770 train_time:82209ms step_avg:96.72ms
step:861/1770 train_time:82309ms step_avg:96.72ms
step:862/1770 train_time:82409ms step_avg:96.72ms
step:863/1770 train_time:82509ms step_avg:96.73ms
step:864/1770 train_time:82608ms step_avg:96.73ms
step:865/1770 train_time:82708ms step_avg:96.73ms
step:866/1770 train_time:82809ms step_avg:96.74ms
step:867/1770 train_time:82909ms step_avg:96.74ms
step:868/1770 train_time:83009ms step_avg:96.75ms
step:869/1770 train_time:83109ms step_avg:96.75ms
step:870/1770 train_time:83209ms step_avg:96.75ms
step:871/1770 train_time:83309ms step_avg:96.76ms
step:872/1770 train_time:83409ms step_avg:96.76ms
step:873/1770 train_time:83509ms step_avg:96.77ms
step:874/1770 train_time:83609ms step_avg:96.77ms
step:875/1770 train_time:83709ms step_avg:96.77ms
step:875/1770 val_loss:3.5509 train_time:83807ms step_avg:96.89ms
step:876/1770 train_time:83830ms step_avg:96.80ms
step:877/1770 train_time:83915ms step_avg:96.79ms
step:878/1770 train_time:84015ms step_avg:96.79ms
step:879/1770 train_time:84114ms step_avg:96.79ms
step:880/1770 train_time:84213ms step_avg:96.80ms
step:881/1770 train_time:84311ms step_avg:96.80ms
step:882/1770 train_time:84411ms step_avg:96.80ms
step:883/1770 train_time:84510ms step_avg:96.80ms
step:884/1770 train_time:84609ms step_avg:96.81ms
step:885/1770 train_time:84708ms step_avg:96.81ms
step:886/1770 train_time:84808ms step_avg:96.81ms
step:887/1770 train_time:84909ms step_avg:96.82ms
step:888/1770 train_time:85010ms step_avg:96.82ms
step:889/1770 train_time:85111ms step_avg:96.83ms
step:890/1770 train_time:85211ms step_avg:96.83ms
step:891/1770 train_time:85311ms step_avg:96.83ms
step:892/1770 train_time:85410ms step_avg:96.84ms
step:893/1770 train_time:85510ms step_avg:96.84ms
step:894/1770 train_time:85609ms step_avg:96.84ms
step:895/1770 train_time:85708ms step_avg:96.85ms
step:896/1770 train_time:85808ms step_avg:96.85ms
step:897/1770 train_time:85908ms step_avg:96.85ms
step:898/1770 train_time:86008ms step_avg:96.86ms
step:899/1770 train_time:86108ms step_avg:96.86ms
step:900/1770 train_time:86207ms step_avg:96.86ms
step:901/1770 train_time:86307ms step_avg:96.86ms
step:902/1770 train_time:86406ms step_avg:96.87ms
step:903/1770 train_time:86505ms step_avg:96.87ms
step:904/1770 train_time:86604ms step_avg:96.87ms
step:905/1770 train_time:86704ms step_avg:96.88ms
step:906/1770 train_time:86803ms step_avg:96.88ms
step:907/1770 train_time:86903ms step_avg:96.88ms
step:908/1770 train_time:87002ms step_avg:96.88ms
step:909/1770 train_time:87100ms step_avg:96.89ms
step:910/1770 train_time:87200ms step_avg:96.89ms
step:911/1770 train_time:87299ms step_avg:96.89ms
step:912/1770 train_time:87398ms step_avg:96.89ms
step:913/1770 train_time:87497ms step_avg:96.90ms
step:914/1770 train_time:87597ms step_avg:96.90ms
step:915/1770 train_time:87697ms step_avg:96.90ms
step:916/1770 train_time:87797ms step_avg:96.91ms
step:917/1770 train_time:87897ms step_avg:96.91ms
step:918/1770 train_time:87996ms step_avg:96.91ms
step:919/1770 train_time:88095ms step_avg:96.91ms
step:920/1770 train_time:88198ms step_avg:96.92ms
step:921/1770 train_time:88299ms step_avg:96.93ms
step:922/1770 train_time:88400ms step_avg:96.93ms
step:923/1770 train_time:88499ms step_avg:96.93ms
step:924/1770 train_time:88600ms step_avg:96.94ms
step:925/1770 train_time:88699ms step_avg:96.94ms
step:926/1770 train_time:88800ms step_avg:96.94ms
step:927/1770 train_time:88900ms step_avg:96.95ms
step:928/1770 train_time:89001ms step_avg:96.95ms
step:929/1770 train_time:89101ms step_avg:96.95ms
step:930/1770 train_time:89201ms step_avg:96.96ms
step:931/1770 train_time:89301ms step_avg:96.96ms
step:932/1770 train_time:89402ms step_avg:96.96ms
step:933/1770 train_time:89502ms step_avg:96.97ms
step:934/1770 train_time:89602ms step_avg:96.97ms
step:935/1770 train_time:89702ms step_avg:96.97ms
step:936/1770 train_time:89801ms step_avg:96.98ms
step:937/1770 train_time:89902ms step_avg:96.98ms
step:938/1770 train_time:90002ms step_avg:96.99ms
step:939/1770 train_time:90102ms step_avg:96.99ms
step:940/1770 train_time:90202ms step_avg:96.99ms
step:941/1770 train_time:90302ms step_avg:96.99ms
step:942/1770 train_time:90403ms step_avg:97.00ms
step:943/1770 train_time:90504ms step_avg:97.00ms
step:944/1770 train_time:90604ms step_avg:97.01ms
step:945/1770 train_time:90704ms step_avg:97.01ms
step:946/1770 train_time:90805ms step_avg:97.01ms
step:947/1770 train_time:90905ms step_avg:97.02ms
step:948/1770 train_time:91005ms step_avg:97.02ms
step:949/1770 train_time:91106ms step_avg:97.02ms
step:950/1770 train_time:91206ms step_avg:97.03ms
step:951/1770 train_time:91308ms step_avg:97.03ms
step:952/1770 train_time:91408ms step_avg:97.04ms
step:953/1770 train_time:91510ms step_avg:97.04ms
step:954/1770 train_time:91611ms step_avg:97.05ms
step:955/1770 train_time:91713ms step_avg:97.05ms
step:956/1770 train_time:91813ms step_avg:97.05ms
step:957/1770 train_time:91915ms step_avg:97.06ms
step:958/1770 train_time:92016ms step_avg:97.06ms
step:959/1770 train_time:92118ms step_avg:97.07ms
step:960/1770 train_time:92218ms step_avg:97.07ms
step:961/1770 train_time:92318ms step_avg:97.07ms
step:962/1770 train_time:92420ms step_avg:97.08ms
step:963/1770 train_time:92520ms step_avg:97.08ms
step:964/1770 train_time:92621ms step_avg:97.09ms
step:965/1770 train_time:92721ms step_avg:97.09ms
step:966/1770 train_time:92821ms step_avg:97.09ms
step:967/1770 train_time:92921ms step_avg:97.10ms
step:968/1770 train_time:93021ms step_avg:97.10ms
step:969/1770 train_time:93122ms step_avg:97.10ms
step:970/1770 train_time:93221ms step_avg:97.11ms
step:971/1770 train_time:93322ms step_avg:97.11ms
step:972/1770 train_time:93421ms step_avg:97.11ms
step:973/1770 train_time:93522ms step_avg:97.11ms
step:974/1770 train_time:93622ms step_avg:97.12ms
step:975/1770 train_time:93722ms step_avg:97.12ms
step:976/1770 train_time:93822ms step_avg:97.12ms
step:977/1770 train_time:93922ms step_avg:97.13ms
step:978/1770 train_time:94022ms step_avg:97.13ms
step:979/1770 train_time:94123ms step_avg:97.13ms
step:980/1770 train_time:94222ms step_avg:97.14ms
step:981/1770 train_time:94323ms step_avg:97.14ms
step:982/1770 train_time:94423ms step_avg:97.14ms
step:983/1770 train_time:94523ms step_avg:97.15ms
step:984/1770 train_time:94624ms step_avg:97.15ms
step:985/1770 train_time:94724ms step_avg:97.15ms
step:986/1770 train_time:94824ms step_avg:97.16ms
step:987/1770 train_time:94924ms step_avg:97.16ms
step:988/1770 train_time:95024ms step_avg:97.16ms
step:989/1770 train_time:95126ms step_avg:97.17ms
step:990/1770 train_time:95226ms step_avg:97.17ms
step:991/1770 train_time:95327ms step_avg:97.17ms
step:992/1770 train_time:95428ms step_avg:97.18ms
step:993/1770 train_time:95529ms step_avg:97.18ms
step:994/1770 train_time:95630ms step_avg:97.18ms
step:995/1770 train_time:95731ms step_avg:97.19ms
step:996/1770 train_time:95833ms step_avg:97.19ms
step:997/1770 train_time:95934ms step_avg:97.20ms
step:998/1770 train_time:96036ms step_avg:97.20ms
step:999/1770 train_time:96137ms step_avg:97.21ms
step:1000/1770 train_time:96239ms step_avg:97.21ms
step:1000/1770 val_loss:3.5141 train_time:96338ms step_avg:97.31ms
step:1001/1770 train_time:96360ms step_avg:97.23ms
step:1002/1770 train_time:96447ms step_avg:97.22ms
step:1003/1770 train_time:96550ms step_avg:97.23ms
step:1004/1770 train_time:96651ms step_avg:97.23ms
step:1005/1770 train_time:96751ms step_avg:97.24ms
step:1006/1770 train_time:96852ms step_avg:97.24ms
step:1007/1770 train_time:96953ms step_avg:97.24ms
step:1008/1770 train_time:97053ms step_avg:97.25ms
step:1009/1770 train_time:97152ms step_avg:97.25ms
step:1010/1770 train_time:97253ms step_avg:97.25ms
step:1011/1770 train_time:97355ms step_avg:97.26ms
step:1012/1770 train_time:97457ms step_avg:97.26ms
step:1013/1770 train_time:97558ms step_avg:97.27ms
step:1014/1770 train_time:97658ms step_avg:97.27ms
step:1015/1770 train_time:97758ms step_avg:97.27ms
step:1016/1770 train_time:97858ms step_avg:97.27ms
step:1017/1770 train_time:97958ms step_avg:97.28ms
step:1018/1770 train_time:98058ms step_avg:97.28ms
step:1019/1770 train_time:98158ms step_avg:97.28ms
step:1020/1770 train_time:98258ms step_avg:97.28ms
step:1021/1770 train_time:98358ms step_avg:97.29ms
step:1022/1770 train_time:98458ms step_avg:97.29ms
step:1023/1770 train_time:98558ms step_avg:97.29ms
step:1024/1770 train_time:98658ms step_avg:97.30ms
step:1025/1770 train_time:98758ms step_avg:97.30ms
step:1026/1770 train_time:98859ms step_avg:97.30ms
step:1027/1770 train_time:98959ms step_avg:97.31ms
step:1028/1770 train_time:99059ms step_avg:97.31ms
step:1029/1770 train_time:99159ms step_avg:97.31ms
step:1030/1770 train_time:99259ms step_avg:97.31ms
step:1031/1770 train_time:99360ms step_avg:97.32ms
step:1032/1770 train_time:99461ms step_avg:97.32ms
step:1033/1770 train_time:99561ms step_avg:97.32ms
step:1034/1770 train_time:99662ms step_avg:97.33ms
step:1035/1770 train_time:99763ms step_avg:97.33ms
step:1036/1770 train_time:99864ms step_avg:97.33ms
step:1037/1770 train_time:99966ms step_avg:97.34ms
step:1038/1770 train_time:100068ms step_avg:97.34ms
step:1039/1770 train_time:100169ms step_avg:97.35ms
step:1040/1770 train_time:100270ms step_avg:97.35ms
step:1041/1770 train_time:100371ms step_avg:97.35ms
step:1042/1770 train_time:100472ms step_avg:97.36ms
step:1043/1770 train_time:100574ms step_avg:97.36ms
step:1044/1770 train_time:100675ms step_avg:97.36ms
step:1045/1770 train_time:100775ms step_avg:97.37ms
step:1046/1770 train_time:100875ms step_avg:97.37ms
step:1047/1770 train_time:100976ms step_avg:97.37ms
step:1048/1770 train_time:101076ms step_avg:97.38ms
step:1049/1770 train_time:101177ms step_avg:97.38ms
step:1050/1770 train_time:101277ms step_avg:97.38ms
step:1051/1770 train_time:101378ms step_avg:97.38ms
step:1052/1770 train_time:101478ms step_avg:97.39ms
step:1053/1770 train_time:101579ms step_avg:97.39ms
step:1054/1770 train_time:101679ms step_avg:97.39ms
step:1055/1770 train_time:101780ms step_avg:97.40ms
step:1056/1770 train_time:101880ms step_avg:97.40ms
step:1057/1770 train_time:101980ms step_avg:97.40ms
step:1058/1770 train_time:102081ms step_avg:97.41ms
step:1059/1770 train_time:102182ms step_avg:97.41ms
step:1060/1770 train_time:102283ms step_avg:97.41ms
step:1061/1770 train_time:102384ms step_avg:97.42ms
step:1062/1770 train_time:102485ms step_avg:97.42ms
step:1063/1770 train_time:102587ms step_avg:97.42ms
step:1064/1770 train_time:102690ms step_avg:97.43ms
step:1065/1770 train_time:102791ms step_avg:97.43ms
step:1066/1770 train_time:102892ms step_avg:97.44ms
step:1067/1770 train_time:102994ms step_avg:97.44ms
step:1068/1770 train_time:103097ms step_avg:97.44ms
step:1069/1770 train_time:103197ms step_avg:97.45ms
step:1070/1770 train_time:103297ms step_avg:97.45ms
step:1071/1770 train_time:103398ms step_avg:97.45ms
step:1072/1770 train_time:103498ms step_avg:97.46ms
step:1073/1770 train_time:103598ms step_avg:97.46ms
step:1074/1770 train_time:103698ms step_avg:97.46ms
step:1075/1770 train_time:103799ms step_avg:97.46ms
step:1076/1770 train_time:103899ms step_avg:97.47ms
step:1077/1770 train_time:103999ms step_avg:97.47ms
step:1078/1770 train_time:104100ms step_avg:97.47ms
step:1079/1770 train_time:104200ms step_avg:97.47ms
step:1080/1770 train_time:104300ms step_avg:97.48ms
step:1081/1770 train_time:104400ms step_avg:97.48ms
step:1082/1770 train_time:104501ms step_avg:97.48ms
step:1083/1770 train_time:104602ms step_avg:97.49ms
step:1084/1770 train_time:104702ms step_avg:97.49ms
step:1085/1770 train_time:104803ms step_avg:97.49ms
step:1086/1770 train_time:104904ms step_avg:97.49ms
step:1087/1770 train_time:105005ms step_avg:97.50ms
step:1088/1770 train_time:105107ms step_avg:97.50ms
step:1089/1770 train_time:105208ms step_avg:97.51ms
step:1090/1770 train_time:105310ms step_avg:97.51ms
step:1091/1770 train_time:105411ms step_avg:97.51ms
step:1092/1770 train_time:105513ms step_avg:97.52ms
step:1093/1770 train_time:105615ms step_avg:97.52ms
step:1094/1770 train_time:105716ms step_avg:97.52ms
step:1095/1770 train_time:105817ms step_avg:97.53ms
step:1096/1770 train_time:105917ms step_avg:97.53ms
step:1097/1770 train_time:106018ms step_avg:97.53ms
step:1098/1770 train_time:106119ms step_avg:97.54ms
step:1099/1770 train_time:106219ms step_avg:97.54ms
step:1100/1770 train_time:106320ms step_avg:97.54ms
step:1101/1770 train_time:106420ms step_avg:97.54ms
step:1102/1770 train_time:106521ms step_avg:97.55ms
step:1103/1770 train_time:106621ms step_avg:97.55ms
step:1104/1770 train_time:106722ms step_avg:97.55ms
step:1105/1770 train_time:106823ms step_avg:97.56ms
step:1106/1770 train_time:106925ms step_avg:97.56ms
step:1107/1770 train_time:107026ms step_avg:97.56ms
step:1108/1770 train_time:107127ms step_avg:97.57ms
step:1109/1770 train_time:107229ms step_avg:97.57ms
step:1110/1770 train_time:107331ms step_avg:97.57ms
step:1111/1770 train_time:107433ms step_avg:97.58ms
step:1112/1770 train_time:107535ms step_avg:97.58ms
step:1113/1770 train_time:107635ms step_avg:97.58ms
step:1114/1770 train_time:107736ms step_avg:97.59ms
step:1115/1770 train_time:107837ms step_avg:97.59ms
step:1116/1770 train_time:107938ms step_avg:97.59ms
step:1117/1770 train_time:108038ms step_avg:97.60ms
step:1118/1770 train_time:108138ms step_avg:97.60ms
step:1119/1770 train_time:108238ms step_avg:97.60ms
step:1120/1770 train_time:108338ms step_avg:97.60ms
step:1121/1770 train_time:108438ms step_avg:97.60ms
step:1122/1770 train_time:108538ms step_avg:97.61ms
step:1123/1770 train_time:108638ms step_avg:97.61ms
step:1124/1770 train_time:108739ms step_avg:97.61ms
step:1125/1770 train_time:108840ms step_avg:97.61ms
step:1125/1770 val_loss:3.4739 train_time:108939ms step_avg:97.70ms
step:1126/1770 train_time:108962ms step_avg:97.64ms
step:1127/1770 train_time:109048ms step_avg:97.63ms
step:1128/1770 train_time:109149ms step_avg:97.63ms
step:1129/1770 train_time:109249ms step_avg:97.63ms
step:1130/1770 train_time:109351ms step_avg:97.63ms
step:1131/1770 train_time:109452ms step_avg:97.64ms
step:1132/1770 train_time:109553ms step_avg:97.64ms
step:1133/1770 train_time:109654ms step_avg:97.64ms
step:1134/1770 train_time:109755ms step_avg:97.65ms
step:1135/1770 train_time:109857ms step_avg:97.65ms
step:1136/1770 train_time:109961ms step_avg:97.66ms
step:1137/1770 train_time:110062ms step_avg:97.66ms
step:1138/1770 train_time:110162ms step_avg:97.66ms
step:1139/1770 train_time:110263ms step_avg:97.66ms
step:1140/1770 train_time:110363ms step_avg:97.67ms
step:1141/1770 train_time:110463ms step_avg:97.67ms
step:1142/1770 train_time:110564ms step_avg:97.67ms
step:1143/1770 train_time:110664ms step_avg:97.67ms
step:1144/1770 train_time:110765ms step_avg:97.68ms
step:1145/1770 train_time:110865ms step_avg:97.68ms
step:1146/1770 train_time:110967ms step_avg:97.68ms
step:1147/1770 train_time:111070ms step_avg:97.69ms
step:1148/1770 train_time:111172ms step_avg:97.69ms
step:1149/1770 train_time:111274ms step_avg:97.69ms
step:1150/1770 train_time:111375ms step_avg:97.70ms
step:1151/1770 train_time:111477ms step_avg:97.70ms
step:1152/1770 train_time:111578ms step_avg:97.70ms
step:1153/1770 train_time:111679ms step_avg:97.71ms
step:1154/1770 train_time:111779ms step_avg:97.71ms
step:1155/1770 train_time:111880ms step_avg:97.71ms
step:1156/1770 train_time:111981ms step_avg:97.71ms
step:1157/1770 train_time:112082ms step_avg:97.72ms
step:1158/1770 train_time:112183ms step_avg:97.72ms
step:1159/1770 train_time:112283ms step_avg:97.72ms
step:1160/1770 train_time:112383ms step_avg:97.72ms
step:1161/1770 train_time:112484ms step_avg:97.73ms
step:1162/1770 train_time:112585ms step_avg:97.73ms
step:1163/1770 train_time:112686ms step_avg:97.73ms
step:1164/1770 train_time:112786ms step_avg:97.74ms
step:1165/1770 train_time:112887ms step_avg:97.74ms
step:1166/1770 train_time:112988ms step_avg:97.74ms
step:1167/1770 train_time:113089ms step_avg:97.74ms
step:1168/1770 train_time:113191ms step_avg:97.75ms
step:1169/1770 train_time:113292ms step_avg:97.75ms
step:1170/1770 train_time:113394ms step_avg:97.75ms
step:1171/1770 train_time:113496ms step_avg:97.76ms
step:1172/1770 train_time:113597ms step_avg:97.76ms
step:1173/1770 train_time:113699ms step_avg:97.76ms
step:1174/1770 train_time:113800ms step_avg:97.77ms
step:1175/1770 train_time:113900ms step_avg:97.77ms
step:1176/1770 train_time:114001ms step_avg:97.77ms
step:1177/1770 train_time:114101ms step_avg:97.77ms
step:1178/1770 train_time:114202ms step_avg:97.78ms
step:1179/1770 train_time:114302ms step_avg:97.78ms
step:1180/1770 train_time:114403ms step_avg:97.78ms
step:1181/1770 train_time:114503ms step_avg:97.78ms
step:1182/1770 train_time:114603ms step_avg:97.78ms
step:1183/1770 train_time:114705ms step_avg:97.79ms
step:1184/1770 train_time:114809ms step_avg:97.79ms
step:1185/1770 train_time:114910ms step_avg:97.80ms
step:1186/1770 train_time:115013ms step_avg:97.80ms
step:1187/1770 train_time:115118ms step_avg:97.81ms
step:1188/1770 train_time:115220ms step_avg:97.81ms
step:1189/1770 train_time:115322ms step_avg:97.81ms
step:1190/1770 train_time:115423ms step_avg:97.82ms
step:1191/1770 train_time:115525ms step_avg:97.82ms
step:1192/1770 train_time:115626ms step_avg:97.82ms
step:1193/1770 train_time:115728ms step_avg:97.83ms
step:1194/1770 train_time:115830ms step_avg:97.83ms
step:1195/1770 train_time:115933ms step_avg:97.83ms
step:1196/1770 train_time:116037ms step_avg:97.84ms
step:1197/1770 train_time:116139ms step_avg:97.84ms
step:1198/1770 train_time:116241ms step_avg:97.85ms
step:1199/1770 train_time:116343ms step_avg:97.85ms
step:1200/1770 train_time:116446ms step_avg:97.85ms
step:1201/1770 train_time:116548ms step_avg:97.86ms
step:1202/1770 train_time:116649ms step_avg:97.86ms
step:1203/1770 train_time:116751ms step_avg:97.86ms
step:1204/1770 train_time:116853ms step_avg:97.87ms
step:1205/1770 train_time:116955ms step_avg:97.87ms
step:1206/1770 train_time:117059ms step_avg:97.88ms
step:1207/1770 train_time:117161ms step_avg:97.88ms
step:1208/1770 train_time:117262ms step_avg:97.88ms
step:1209/1770 train_time:117365ms step_avg:97.89ms
step:1210/1770 train_time:117466ms step_avg:97.89ms
step:1211/1770 train_time:117569ms step_avg:97.89ms
step:1212/1770 train_time:117673ms step_avg:97.90ms
step:1213/1770 train_time:117774ms step_avg:97.90ms
step:1214/1770 train_time:117876ms step_avg:97.90ms
step:1215/1770 train_time:117979ms step_avg:97.91ms
step:1216/1770 train_time:118083ms step_avg:97.91ms
step:1217/1770 train_time:118185ms step_avg:97.92ms
step:1218/1770 train_time:118286ms step_avg:97.92ms
step:1219/1770 train_time:118389ms step_avg:97.92ms
step:1220/1770 train_time:118491ms step_avg:97.93ms
step:1221/1770 train_time:118593ms step_avg:97.93ms
step:1222/1770 train_time:118697ms step_avg:97.93ms
step:1223/1770 train_time:118798ms step_avg:97.94ms
step:1224/1770 train_time:118902ms step_avg:97.94ms
step:1225/1770 train_time:119004ms step_avg:97.95ms
step:1226/1770 train_time:119106ms step_avg:97.95ms
step:1227/1770 train_time:119211ms step_avg:97.95ms
step:1228/1770 train_time:119316ms step_avg:97.96ms
step:1229/1770 train_time:119418ms step_avg:97.96ms
step:1230/1770 train_time:119520ms step_avg:97.97ms
step:1231/1770 train_time:119622ms step_avg:97.97ms
step:1232/1770 train_time:119723ms step_avg:97.97ms
step:1233/1770 train_time:119825ms step_avg:97.98ms
step:1234/1770 train_time:119926ms step_avg:97.98ms
step:1235/1770 train_time:120029ms step_avg:97.98ms
step:1236/1770 train_time:120131ms step_avg:97.99ms
step:1237/1770 train_time:120234ms step_avg:97.99ms
step:1238/1770 train_time:120337ms step_avg:97.99ms
step:1239/1770 train_time:120440ms step_avg:98.00ms
step:1240/1770 train_time:120542ms step_avg:98.00ms
step:1241/1770 train_time:120645ms step_avg:98.01ms
step:1242/1770 train_time:120747ms step_avg:98.01ms
step:1243/1770 train_time:120849ms step_avg:98.01ms
step:1244/1770 train_time:120951ms step_avg:98.02ms
step:1245/1770 train_time:121052ms step_avg:98.02ms
step:1246/1770 train_time:121154ms step_avg:98.02ms
step:1247/1770 train_time:121258ms step_avg:98.03ms
step:1248/1770 train_time:121360ms step_avg:98.03ms
step:1249/1770 train_time:121462ms step_avg:98.03ms
step:1250/1770 train_time:121564ms step_avg:98.04ms
step:1250/1770 val_loss:3.4255 train_time:121665ms step_avg:98.12ms
step:1251/1770 train_time:121687ms step_avg:98.06ms
step:1252/1770 train_time:121774ms step_avg:98.05ms
step:1253/1770 train_time:121876ms step_avg:98.05ms
step:1254/1770 train_time:121978ms step_avg:98.05ms
step:1255/1770 train_time:122082ms step_avg:98.06ms
step:1256/1770 train_time:122184ms step_avg:98.06ms
step:1257/1770 train_time:122285ms step_avg:98.06ms
step:1258/1770 train_time:122387ms step_avg:98.07ms
step:1259/1770 train_time:122489ms step_avg:98.07ms
step:1260/1770 train_time:122590ms step_avg:98.07ms
step:1261/1770 train_time:122693ms step_avg:98.08ms
step:1262/1770 train_time:122796ms step_avg:98.08ms
step:1263/1770 train_time:122898ms step_avg:98.08ms
step:1264/1770 train_time:123001ms step_avg:98.09ms
step:1265/1770 train_time:123103ms step_avg:98.09ms
step:1266/1770 train_time:123206ms step_avg:98.09ms
step:1267/1770 train_time:123308ms step_avg:98.10ms
step:1268/1770 train_time:123411ms step_avg:98.10ms
step:1269/1770 train_time:123512ms step_avg:98.10ms
step:1270/1770 train_time:123614ms step_avg:98.11ms
step:1271/1770 train_time:123716ms step_avg:98.11ms
step:1272/1770 train_time:123818ms step_avg:98.11ms
step:1273/1770 train_time:123921ms step_avg:98.12ms
step:1274/1770 train_time:124022ms step_avg:98.12ms
step:1275/1770 train_time:124125ms step_avg:98.12ms
step:1276/1770 train_time:124228ms step_avg:98.13ms
step:1277/1770 train_time:124330ms step_avg:98.13ms
step:1278/1770 train_time:124433ms step_avg:98.13ms
step:1279/1770 train_time:124535ms step_avg:98.14ms
step:1280/1770 train_time:124638ms step_avg:98.14ms
step:1281/1770 train_time:124739ms step_avg:98.14ms
step:1282/1770 train_time:124843ms step_avg:98.15ms
step:1283/1770 train_time:124946ms step_avg:98.15ms
step:1284/1770 train_time:125048ms step_avg:98.15ms
step:1285/1770 train_time:125149ms step_avg:98.16ms
step:1286/1770 train_time:125253ms step_avg:98.16ms
step:1287/1770 train_time:125357ms step_avg:98.17ms
step:1288/1770 train_time:125460ms step_avg:98.17ms
step:1289/1770 train_time:125563ms step_avg:98.17ms
step:1290/1770 train_time:125665ms step_avg:98.18ms
step:1291/1770 train_time:125767ms step_avg:98.18ms
step:1292/1770 train_time:125869ms step_avg:98.18ms
step:1293/1770 train_time:125971ms step_avg:98.18ms
step:1294/1770 train_time:126072ms step_avg:98.19ms
step:1295/1770 train_time:126174ms step_avg:98.19ms
step:1296/1770 train_time:126276ms step_avg:98.19ms
step:1297/1770 train_time:126377ms step_avg:98.20ms
step:1298/1770 train_time:126479ms step_avg:98.20ms
step:1299/1770 train_time:126581ms step_avg:98.20ms
step:1300/1770 train_time:126684ms step_avg:98.20ms
step:1301/1770 train_time:126786ms step_avg:98.21ms
step:1302/1770 train_time:126888ms step_avg:98.21ms
step:1303/1770 train_time:126990ms step_avg:98.21ms
step:1304/1770 train_time:127092ms step_avg:98.22ms
step:1305/1770 train_time:127194ms step_avg:98.22ms
step:1306/1770 train_time:127295ms step_avg:98.22ms
step:1307/1770 train_time:127397ms step_avg:98.22ms
step:1308/1770 train_time:127498ms step_avg:98.23ms
step:1309/1770 train_time:127600ms step_avg:98.23ms
step:1310/1770 train_time:127703ms step_avg:98.23ms
step:1311/1770 train_time:127805ms step_avg:98.24ms
step:1312/1770 train_time:127908ms step_avg:98.24ms
step:1313/1770 train_time:128010ms step_avg:98.24ms
step:1314/1770 train_time:128113ms step_avg:98.25ms
step:1315/1770 train_time:128215ms step_avg:98.25ms
step:1316/1770 train_time:128317ms step_avg:98.25ms
step:1317/1770 train_time:128419ms step_avg:98.25ms
step:1318/1770 train_time:128525ms step_avg:98.26ms
step:1319/1770 train_time:128627ms step_avg:98.26ms
step:1320/1770 train_time:128729ms step_avg:98.27ms
step:1321/1770 train_time:128831ms step_avg:98.27ms
step:1322/1770 train_time:128933ms step_avg:98.27ms
step:1323/1770 train_time:129036ms step_avg:98.28ms
step:1324/1770 train_time:129140ms step_avg:98.28ms
step:1325/1770 train_time:129244ms step_avg:98.28ms
step:1326/1770 train_time:129346ms step_avg:98.29ms
step:1327/1770 train_time:129451ms step_avg:98.29ms
step:1328/1770 train_time:129552ms step_avg:98.29ms
step:1329/1770 train_time:129655ms step_avg:98.30ms
step:1330/1770 train_time:129756ms step_avg:98.30ms
step:1331/1770 train_time:129858ms step_avg:98.30ms
step:1332/1770 train_time:129960ms step_avg:98.31ms
step:1333/1770 train_time:130063ms step_avg:98.31ms
step:1334/1770 train_time:130165ms step_avg:98.31ms
step:1335/1770 train_time:130268ms step_avg:98.32ms
step:1336/1770 train_time:130369ms step_avg:98.32ms
step:1337/1770 train_time:130471ms step_avg:98.32ms
step:1338/1770 train_time:130572ms step_avg:98.32ms
step:1339/1770 train_time:130674ms step_avg:98.33ms
step:1340/1770 train_time:130777ms step_avg:98.33ms
step:1341/1770 train_time:130879ms step_avg:98.33ms
step:1342/1770 train_time:130982ms step_avg:98.33ms
step:1343/1770 train_time:131085ms step_avg:98.34ms
step:1344/1770 train_time:131187ms step_avg:98.34ms
step:1345/1770 train_time:131289ms step_avg:98.34ms
step:1346/1770 train_time:131391ms step_avg:98.35ms
step:1347/1770 train_time:131493ms step_avg:98.35ms
step:1348/1770 train_time:131598ms step_avg:98.35ms
step:1349/1770 train_time:131700ms step_avg:98.36ms
step:1350/1770 train_time:131803ms step_avg:98.36ms
step:1351/1770 train_time:131905ms step_avg:98.36ms
step:1352/1770 train_time:132007ms step_avg:98.37ms
step:1353/1770 train_time:132110ms step_avg:98.37ms
step:1354/1770 train_time:132212ms step_avg:98.37ms
step:1355/1770 train_time:132314ms step_avg:98.37ms
step:1356/1770 train_time:132416ms step_avg:98.38ms
step:1357/1770 train_time:132518ms step_avg:98.38ms
step:1358/1770 train_time:132621ms step_avg:98.38ms
step:1359/1770 train_time:132723ms step_avg:98.39ms
step:1360/1770 train_time:132827ms step_avg:98.39ms
step:1361/1770 train_time:132930ms step_avg:98.39ms
step:1362/1770 train_time:133032ms step_avg:98.40ms
step:1363/1770 train_time:133135ms step_avg:98.40ms
step:1364/1770 train_time:133237ms step_avg:98.40ms
step:1365/1770 train_time:133339ms step_avg:98.41ms
step:1366/1770 train_time:133442ms step_avg:98.41ms
step:1367/1770 train_time:133545ms step_avg:98.41ms
step:1368/1770 train_time:133648ms step_avg:98.41ms
step:1369/1770 train_time:133751ms step_avg:98.42ms
step:1370/1770 train_time:133853ms step_avg:98.42ms
step:1371/1770 train_time:133955ms step_avg:98.42ms
step:1372/1770 train_time:134057ms step_avg:98.43ms
step:1373/1770 train_time:134159ms step_avg:98.43ms
step:1374/1770 train_time:134262ms step_avg:98.43ms
step:1375/1770 train_time:134366ms step_avg:98.44ms
step:1375/1770 val_loss:3.3809 train_time:134466ms step_avg:98.51ms
step:1376/1770 train_time:134488ms step_avg:98.45ms
step:1377/1770 train_time:134575ms step_avg:98.45ms
step:1378/1770 train_time:134677ms step_avg:98.45ms
step:1379/1770 train_time:134778ms step_avg:98.45ms
step:1380/1770 train_time:134880ms step_avg:98.45ms
step:1381/1770 train_time:134982ms step_avg:98.46ms
step:1382/1770 train_time:135084ms step_avg:98.46ms
step:1383/1770 train_time:135187ms step_avg:98.46ms
step:1384/1770 train_time:135290ms step_avg:98.46ms
step:1385/1770 train_time:135392ms step_avg:98.47ms
step:1386/1770 train_time:135495ms step_avg:98.47ms
step:1387/1770 train_time:135599ms step_avg:98.47ms
step:1388/1770 train_time:135701ms step_avg:98.48ms
step:1389/1770 train_time:135804ms step_avg:98.48ms
step:1390/1770 train_time:135906ms step_avg:98.48ms
step:1391/1770 train_time:136008ms step_avg:98.48ms
step:1392/1770 train_time:136110ms step_avg:98.49ms
step:1393/1770 train_time:136212ms step_avg:98.49ms
step:1394/1770 train_time:136313ms step_avg:98.49ms
step:1395/1770 train_time:136415ms step_avg:98.49ms
step:1396/1770 train_time:136518ms step_avg:98.50ms
step:1397/1770 train_time:136620ms step_avg:98.50ms
step:1398/1770 train_time:136722ms step_avg:98.50ms
step:1399/1770 train_time:136826ms step_avg:98.51ms
step:1400/1770 train_time:136929ms step_avg:98.51ms
step:1401/1770 train_time:137031ms step_avg:98.51ms
step:1402/1770 train_time:137133ms step_avg:98.52ms
step:1403/1770 train_time:137235ms step_avg:98.52ms
step:1404/1770 train_time:137338ms step_avg:98.52ms
step:1405/1770 train_time:137440ms step_avg:98.52ms
step:1406/1770 train_time:137542ms step_avg:98.53ms
step:1407/1770 train_time:137644ms step_avg:98.53ms
step:1408/1770 train_time:137747ms step_avg:98.53ms
step:1409/1770 train_time:137849ms step_avg:98.53ms
step:1410/1770 train_time:137952ms step_avg:98.54ms
step:1411/1770 train_time:138054ms step_avg:98.54ms
step:1412/1770 train_time:138156ms step_avg:98.54ms
step:1413/1770 train_time:138257ms step_avg:98.54ms
step:1414/1770 train_time:138360ms step_avg:98.55ms
step:1415/1770 train_time:138462ms step_avg:98.55ms
step:1416/1770 train_time:138565ms step_avg:98.55ms
step:1417/1770 train_time:138668ms step_avg:98.56ms
step:1418/1770 train_time:138770ms step_avg:98.56ms
step:1419/1770 train_time:138873ms step_avg:98.56ms
step:1420/1770 train_time:138974ms step_avg:98.56ms
step:1421/1770 train_time:139076ms step_avg:98.57ms
step:1422/1770 train_time:139178ms step_avg:98.57ms
step:1423/1770 train_time:139280ms step_avg:98.57ms
step:1424/1770 train_time:139383ms step_avg:98.57ms
step:1425/1770 train_time:139485ms step_avg:98.58ms
step:1426/1770 train_time:139588ms step_avg:98.58ms
step:1427/1770 train_time:139690ms step_avg:98.58ms
step:1428/1770 train_time:139795ms step_avg:98.59ms
step:1429/1770 train_time:139897ms step_avg:98.59ms
step:1430/1770 train_time:140000ms step_avg:98.59ms
step:1431/1770 train_time:140104ms step_avg:98.60ms
step:1432/1770 train_time:140205ms step_avg:98.60ms
step:1433/1770 train_time:140308ms step_avg:98.60ms
step:1434/1770 train_time:140409ms step_avg:98.60ms
step:1435/1770 train_time:140511ms step_avg:98.60ms
step:1436/1770 train_time:140614ms step_avg:98.61ms
step:1437/1770 train_time:140717ms step_avg:98.61ms
step:1438/1770 train_time:140818ms step_avg:98.61ms
step:1439/1770 train_time:140920ms step_avg:98.61ms
step:1440/1770 train_time:141022ms step_avg:98.62ms
step:1441/1770 train_time:141128ms step_avg:98.62ms
step:1442/1770 train_time:141229ms step_avg:98.62ms
step:1443/1770 train_time:141331ms step_avg:98.63ms
step:1444/1770 train_time:141433ms step_avg:98.63ms
step:1445/1770 train_time:141536ms step_avg:98.63ms
step:1446/1770 train_time:141639ms step_avg:98.63ms
step:1447/1770 train_time:141742ms step_avg:98.64ms
step:1448/1770 train_time:141846ms step_avg:98.64ms
step:1449/1770 train_time:141950ms step_avg:98.64ms
step:1450/1770 train_time:142053ms step_avg:98.65ms
step:1451/1770 train_time:142157ms step_avg:98.65ms
step:1452/1770 train_time:142261ms step_avg:98.66ms
step:1453/1770 train_time:142364ms step_avg:98.66ms
step:1454/1770 train_time:142468ms step_avg:98.66ms
step:1455/1770 train_time:142572ms step_avg:98.67ms
step:1456/1770 train_time:142676ms step_avg:98.67ms
step:1457/1770 train_time:142779ms step_avg:98.67ms
step:1458/1770 train_time:142883ms step_avg:98.68ms
step:1459/1770 train_time:142988ms step_avg:98.68ms
step:1460/1770 train_time:143091ms step_avg:98.68ms
step:1461/1770 train_time:143194ms step_avg:98.69ms
step:1462/1770 train_time:143296ms step_avg:98.69ms
step:1463/1770 train_time:143399ms step_avg:98.69ms
step:1464/1770 train_time:143505ms step_avg:98.70ms
step:1465/1770 train_time:143608ms step_avg:98.70ms
step:1466/1770 train_time:143712ms step_avg:98.70ms
step:1467/1770 train_time:143816ms step_avg:98.71ms
step:1468/1770 train_time:143919ms step_avg:98.71ms
step:1469/1770 train_time:144022ms step_avg:98.71ms
step:1470/1770 train_time:144126ms step_avg:98.72ms
step:1471/1770 train_time:144229ms step_avg:98.72ms
step:1472/1770 train_time:144332ms step_avg:98.72ms
step:1473/1770 train_time:144436ms step_avg:98.73ms
step:1474/1770 train_time:144540ms step_avg:98.73ms
step:1475/1770 train_time:144643ms step_avg:98.73ms
step:1476/1770 train_time:144748ms step_avg:98.74ms
step:1477/1770 train_time:144855ms step_avg:98.74ms
step:1478/1770 train_time:144958ms step_avg:98.75ms
step:1479/1770 train_time:145061ms step_avg:98.75ms
step:1480/1770 train_time:145164ms step_avg:98.75ms
step:1481/1770 train_time:145272ms step_avg:98.76ms
step:1482/1770 train_time:145375ms step_avg:98.76ms
step:1483/1770 train_time:145479ms step_avg:98.76ms
step:1484/1770 train_time:145583ms step_avg:98.77ms
step:1485/1770 train_time:145685ms step_avg:98.77ms
step:1486/1770 train_time:145789ms step_avg:98.77ms
step:1487/1770 train_time:145893ms step_avg:98.78ms
step:1488/1770 train_time:145997ms step_avg:98.78ms
step:1489/1770 train_time:146101ms step_avg:98.78ms
step:1490/1770 train_time:146205ms step_avg:98.79ms
step:1491/1770 train_time:146308ms step_avg:98.79ms
step:1492/1770 train_time:146413ms step_avg:98.79ms
step:1493/1770 train_time:146519ms step_avg:98.80ms
step:1494/1770 train_time:146625ms step_avg:98.80ms
step:1495/1770 train_time:146727ms step_avg:98.81ms
step:1496/1770 train_time:146830ms step_avg:98.81ms
step:1497/1770 train_time:146933ms step_avg:98.81ms
step:1498/1770 train_time:147035ms step_avg:98.81ms
step:1499/1770 train_time:147137ms step_avg:98.82ms
step:1500/1770 train_time:147239ms step_avg:98.82ms
step:1500/1770 val_loss:3.3440 train_time:147340ms step_avg:98.89ms
step:1501/1770 train_time:147365ms step_avg:98.84ms
step:1502/1770 train_time:147449ms step_avg:98.83ms
step:1503/1770 train_time:147551ms step_avg:98.83ms
step:1504/1770 train_time:147654ms step_avg:98.83ms
step:1505/1770 train_time:147759ms step_avg:98.84ms
step:1506/1770 train_time:147863ms step_avg:98.84ms
step:1507/1770 train_time:147966ms step_avg:98.84ms
step:1508/1770 train_time:148071ms step_avg:98.85ms
step:1509/1770 train_time:148175ms step_avg:98.85ms
step:1510/1770 train_time:148277ms step_avg:98.85ms
step:1511/1770 train_time:148382ms step_avg:98.86ms
step:1512/1770 train_time:148487ms step_avg:98.86ms
step:1513/1770 train_time:148591ms step_avg:98.86ms
step:1514/1770 train_time:148694ms step_avg:98.87ms
step:1515/1770 train_time:148797ms step_avg:98.87ms
step:1516/1770 train_time:148901ms step_avg:98.87ms
step:1517/1770 train_time:149004ms step_avg:98.87ms
step:1518/1770 train_time:149109ms step_avg:98.88ms
step:1519/1770 train_time:149211ms step_avg:98.88ms
step:1520/1770 train_time:149315ms step_avg:98.88ms
step:1521/1770 train_time:149420ms step_avg:98.89ms
step:1522/1770 train_time:149524ms step_avg:98.89ms
step:1523/1770 train_time:149628ms step_avg:98.89ms
step:1524/1770 train_time:149730ms step_avg:98.90ms
step:1525/1770 train_time:149833ms step_avg:98.90ms
step:1526/1770 train_time:149936ms step_avg:98.90ms
step:1527/1770 train_time:150039ms step_avg:98.91ms
step:1528/1770 train_time:150146ms step_avg:98.91ms
step:1529/1770 train_time:150248ms step_avg:98.91ms
step:1530/1770 train_time:150351ms step_avg:98.92ms
step:1531/1770 train_time:150454ms step_avg:98.92ms
step:1532/1770 train_time:150557ms step_avg:98.92ms
step:1533/1770 train_time:150662ms step_avg:98.92ms
step:1534/1770 train_time:150767ms step_avg:98.93ms
step:1535/1770 train_time:150868ms step_avg:98.93ms
step:1536/1770 train_time:150971ms step_avg:98.93ms
step:1537/1770 train_time:151075ms step_avg:98.94ms
step:1538/1770 train_time:151180ms step_avg:98.94ms
step:1539/1770 train_time:151284ms step_avg:98.94ms
step:1540/1770 train_time:151389ms step_avg:98.95ms
step:1541/1770 train_time:151494ms step_avg:98.95ms
step:1542/1770 train_time:151597ms step_avg:98.95ms
step:1543/1770 train_time:151700ms step_avg:98.96ms
step:1544/1770 train_time:151806ms step_avg:98.96ms
step:1545/1770 train_time:151909ms step_avg:98.96ms
step:1546/1770 train_time:152012ms step_avg:98.97ms
step:1547/1770 train_time:152116ms step_avg:98.97ms
step:1548/1770 train_time:152219ms step_avg:98.97ms
step:1549/1770 train_time:152324ms step_avg:98.98ms
step:1550/1770 train_time:152427ms step_avg:98.98ms
step:1551/1770 train_time:152529ms step_avg:98.98ms
step:1552/1770 train_time:152634ms step_avg:98.98ms
step:1553/1770 train_time:152737ms step_avg:98.99ms
step:1554/1770 train_time:152841ms step_avg:98.99ms
step:1555/1770 train_time:152945ms step_avg:98.99ms
step:1556/1770 train_time:153047ms step_avg:99.00ms
step:1557/1770 train_time:153150ms step_avg:99.00ms
step:1558/1770 train_time:153254ms step_avg:99.00ms
step:1559/1770 train_time:153358ms step_avg:99.00ms
step:1560/1770 train_time:153461ms step_avg:99.01ms
step:1561/1770 train_time:153567ms step_avg:99.01ms
step:1562/1770 train_time:153670ms step_avg:99.01ms
step:1563/1770 train_time:153773ms step_avg:99.02ms
step:1564/1770 train_time:153876ms step_avg:99.02ms
step:1565/1770 train_time:153981ms step_avg:99.02ms
step:1566/1770 train_time:154084ms step_avg:99.03ms
step:1567/1770 train_time:154189ms step_avg:99.03ms
step:1568/1770 train_time:154291ms step_avg:99.03ms
step:1569/1770 train_time:154397ms step_avg:99.04ms
step:1570/1770 train_time:154500ms step_avg:99.04ms
step:1571/1770 train_time:154603ms step_avg:99.04ms
step:1572/1770 train_time:154708ms step_avg:99.04ms
step:1573/1770 train_time:154813ms step_avg:99.05ms
step:1574/1770 train_time:154916ms step_avg:99.05ms
step:1575/1770 train_time:155018ms step_avg:99.05ms
step:1576/1770 train_time:155122ms step_avg:99.06ms
step:1577/1770 train_time:155227ms step_avg:99.06ms
step:1578/1770 train_time:155332ms step_avg:99.06ms
step:1579/1770 train_time:155434ms step_avg:99.07ms
step:1580/1770 train_time:155538ms step_avg:99.07ms
step:1581/1770 train_time:155645ms step_avg:99.07ms
step:1582/1770 train_time:155751ms step_avg:99.08ms
step:1583/1770 train_time:155854ms step_avg:99.08ms
step:1584/1770 train_time:155958ms step_avg:99.08ms
step:1585/1770 train_time:156061ms step_avg:99.09ms
step:1586/1770 train_time:156168ms step_avg:99.09ms
step:1587/1770 train_time:156272ms step_avg:99.09ms
step:1588/1770 train_time:156375ms step_avg:99.10ms
step:1589/1770 train_time:156480ms step_avg:99.10ms
step:1590/1770 train_time:156584ms step_avg:99.10ms
step:1591/1770 train_time:156688ms step_avg:99.11ms
step:1592/1770 train_time:156792ms step_avg:99.11ms
step:1593/1770 train_time:156894ms step_avg:99.11ms
step:1594/1770 train_time:156999ms step_avg:99.12ms
step:1595/1770 train_time:157102ms step_avg:99.12ms
step:1596/1770 train_time:157207ms step_avg:99.12ms
step:1597/1770 train_time:157310ms step_avg:99.12ms
step:1598/1770 train_time:157413ms step_avg:99.13ms
step:1599/1770 train_time:157517ms step_avg:99.13ms
step:1600/1770 train_time:157623ms step_avg:99.13ms
step:1601/1770 train_time:157727ms step_avg:99.14ms
step:1602/1770 train_time:157831ms step_avg:99.14ms
step:1603/1770 train_time:157934ms step_avg:99.14ms
step:1604/1770 train_time:158037ms step_avg:99.14ms
step:1605/1770 train_time:158140ms step_avg:99.15ms
step:1606/1770 train_time:158244ms step_avg:99.15ms
step:1607/1770 train_time:158351ms step_avg:99.16ms
step:1608/1770 train_time:158454ms step_avg:99.16ms
step:1609/1770 train_time:158558ms step_avg:99.16ms
step:1610/1770 train_time:158663ms step_avg:99.16ms
step:1611/1770 train_time:158769ms step_avg:99.17ms
step:1612/1770 train_time:158873ms step_avg:99.17ms
step:1613/1770 train_time:158976ms step_avg:99.17ms
step:1614/1770 train_time:159081ms step_avg:99.18ms
step:1615/1770 train_time:159185ms step_avg:99.18ms
step:1616/1770 train_time:159289ms step_avg:99.18ms
step:1617/1770 train_time:159394ms step_avg:99.19ms
step:1618/1770 train_time:159499ms step_avg:99.19ms
step:1619/1770 train_time:159602ms step_avg:99.19ms
step:1620/1770 train_time:159706ms step_avg:99.20ms
step:1621/1770 train_time:159810ms step_avg:99.20ms
step:1622/1770 train_time:159913ms step_avg:99.20ms
step:1623/1770 train_time:160019ms step_avg:99.21ms
step:1624/1770 train_time:160123ms step_avg:99.21ms
step:1625/1770 train_time:160225ms step_avg:99.21ms
step:1625/1770 val_loss:3.3087 train_time:160327ms step_avg:99.27ms
step:1626/1770 train_time:160349ms step_avg:99.23ms
step:1627/1770 train_time:160437ms step_avg:99.22ms
step:1628/1770 train_time:160539ms step_avg:99.22ms
step:1629/1770 train_time:160643ms step_avg:99.22ms
step:1630/1770 train_time:160746ms step_avg:99.23ms
step:1631/1770 train_time:160848ms step_avg:99.23ms
step:1632/1770 train_time:160951ms step_avg:99.23ms
step:1633/1770 train_time:161055ms step_avg:99.23ms
step:1634/1770 train_time:161157ms step_avg:99.23ms
step:1635/1770 train_time:161261ms step_avg:99.24ms
step:1636/1770 train_time:161366ms step_avg:99.24ms
step:1637/1770 train_time:161470ms step_avg:99.24ms
step:1638/1770 train_time:161573ms step_avg:99.25ms
step:1639/1770 train_time:161677ms step_avg:99.25ms
step:1640/1770 train_time:161781ms step_avg:99.25ms
step:1641/1770 train_time:161885ms step_avg:99.25ms
step:1642/1770 train_time:161987ms step_avg:99.26ms
step:1643/1770 train_time:162090ms step_avg:99.26ms
step:1644/1770 train_time:162195ms step_avg:99.26ms
step:1645/1770 train_time:162298ms step_avg:99.26ms
step:1646/1770 train_time:162404ms step_avg:99.27ms
step:1647/1770 train_time:162510ms step_avg:99.27ms
step:1648/1770 train_time:162612ms step_avg:99.27ms
step:1649/1770 train_time:162714ms step_avg:99.28ms
step:1650/1770 train_time:162818ms step_avg:99.28ms
step:1651/1770 train_time:162921ms step_avg:99.28ms
step:1652/1770 train_time:163026ms step_avg:99.28ms
step:1653/1770 train_time:163129ms step_avg:99.29ms
step:1654/1770 train_time:163236ms step_avg:99.29ms
step:1655/1770 train_time:163341ms step_avg:99.30ms
step:1656/1770 train_time:163445ms step_avg:99.30ms
step:1657/1770 train_time:163550ms step_avg:99.30ms
step:1658/1770 train_time:163654ms step_avg:99.30ms
step:1659/1770 train_time:163758ms step_avg:99.31ms
step:1660/1770 train_time:163862ms step_avg:99.31ms
step:1661/1770 train_time:163968ms step_avg:99.31ms
step:1662/1770 train_time:164072ms step_avg:99.32ms
step:1663/1770 train_time:164174ms step_avg:99.32ms
step:1664/1770 train_time:164277ms step_avg:99.32ms
step:1665/1770 train_time:164381ms step_avg:99.32ms
step:1666/1770 train_time:164486ms step_avg:99.33ms
step:1667/1770 train_time:164589ms step_avg:99.33ms
step:1668/1770 train_time:164692ms step_avg:99.33ms
step:1669/1770 train_time:164794ms step_avg:99.33ms
step:1670/1770 train_time:164896ms step_avg:99.34ms
step:1671/1770 train_time:165000ms step_avg:99.34ms
step:1672/1770 train_time:165106ms step_avg:99.34ms
step:1673/1770 train_time:165210ms step_avg:99.34ms
step:1674/1770 train_time:165313ms step_avg:99.35ms
step:1675/1770 train_time:165416ms step_avg:99.35ms
step:1676/1770 train_time:165520ms step_avg:99.35ms
step:1677/1770 train_time:165628ms step_avg:99.36ms
step:1678/1770 train_time:165730ms step_avg:99.36ms
step:1679/1770 train_time:165833ms step_avg:99.36ms
step:1680/1770 train_time:165936ms step_avg:99.36ms
step:1681/1770 train_time:166040ms step_avg:99.37ms
step:1682/1770 train_time:166146ms step_avg:99.37ms
step:1683/1770 train_time:166249ms step_avg:99.37ms
step:1684/1770 train_time:166352ms step_avg:99.37ms
step:1685/1770 train_time:166455ms step_avg:99.38ms
step:1686/1770 train_time:166560ms step_avg:99.38ms
step:1687/1770 train_time:166666ms step_avg:99.38ms
step:1688/1770 train_time:166768ms step_avg:99.39ms
step:1689/1770 train_time:166871ms step_avg:99.39ms
step:1690/1770 train_time:166974ms step_avg:99.39ms
step:1691/1770 train_time:167077ms step_avg:99.39ms
step:1692/1770 train_time:167181ms step_avg:99.39ms
step:1693/1770 train_time:167286ms step_avg:99.40ms
step:1694/1770 train_time:167389ms step_avg:99.40ms
step:1695/1770 train_time:167492ms step_avg:99.40ms
step:1696/1770 train_time:167598ms step_avg:99.41ms
step:1697/1770 train_time:167703ms step_avg:99.41ms
step:1698/1770 train_time:167807ms step_avg:99.41ms
step:1699/1770 train_time:167909ms step_avg:99.41ms
step:1700/1770 train_time:168012ms step_avg:99.42ms
step:1701/1770 train_time:168115ms step_avg:99.42ms
step:1702/1770 train_time:168219ms step_avg:99.42ms
step:1703/1770 train_time:168322ms step_avg:99.42ms
step:1704/1770 train_time:168427ms step_avg:99.43ms
step:1705/1770 train_time:168530ms step_avg:99.43ms
step:1706/1770 train_time:168632ms step_avg:99.43ms
step:1707/1770 train_time:168736ms step_avg:99.43ms
step:1708/1770 train_time:168841ms step_avg:99.44ms
step:1709/1770 train_time:168946ms step_avg:99.44ms
step:1710/1770 train_time:169053ms step_avg:99.44ms
step:1711/1770 train_time:169158ms step_avg:99.45ms
step:1712/1770 train_time:169263ms step_avg:99.45ms
step:1713/1770 train_time:169366ms step_avg:99.45ms
step:1714/1770 train_time:169470ms step_avg:99.45ms
step:1715/1770 train_time:169573ms step_avg:99.46ms
step:1716/1770 train_time:169678ms step_avg:99.46ms
step:1717/1770 train_time:169782ms step_avg:99.46ms
step:1718/1770 train_time:169888ms step_avg:99.47ms
step:1719/1770 train_time:169993ms step_avg:99.47ms
step:1720/1770 train_time:170097ms step_avg:99.47ms
step:1721/1770 train_time:170201ms step_avg:99.47ms
step:1722/1770 train_time:170308ms step_avg:99.48ms
step:1723/1770 train_time:170413ms step_avg:99.48ms
step:1724/1770 train_time:170520ms step_avg:99.49ms
step:1725/1770 train_time:170626ms step_avg:99.49ms
step:1726/1770 train_time:170732ms step_avg:99.49ms
step:1727/1770 train_time:170835ms step_avg:99.50ms
step:1728/1770 train_time:170941ms step_avg:99.50ms
step:1729/1770 train_time:171045ms step_avg:99.50ms
step:1730/1770 train_time:171151ms step_avg:99.51ms
step:1731/1770 train_time:171258ms step_avg:99.51ms
step:1732/1770 train_time:171362ms step_avg:99.51ms
step:1733/1770 train_time:171468ms step_avg:99.52ms
step:1734/1770 train_time:171571ms step_avg:99.52ms
step:1735/1770 train_time:171676ms step_avg:99.52ms
step:1736/1770 train_time:171779ms step_avg:99.52ms
step:1737/1770 train_time:171886ms step_avg:99.53ms
step:1738/1770 train_time:171989ms step_avg:99.53ms
step:1739/1770 train_time:172093ms step_avg:99.53ms
step:1740/1770 train_time:172197ms step_avg:99.54ms
step:1741/1770 train_time:172305ms step_avg:99.54ms
step:1742/1770 train_time:172412ms step_avg:99.54ms
step:1743/1770 train_time:172517ms step_avg:99.55ms
step:1744/1770 train_time:172622ms step_avg:99.55ms
step:1745/1770 train_time:172726ms step_avg:99.55ms
step:1746/1770 train_time:172833ms step_avg:99.56ms
step:1747/1770 train_time:172936ms step_avg:99.56ms
step:1748/1770 train_time:173042ms step_avg:99.56ms
step:1749/1770 train_time:173148ms step_avg:99.57ms
step:1750/1770 train_time:173252ms step_avg:99.57ms
step:1750/1770 val_loss:3.2819 train_time:173355ms step_avg:99.63ms
step:1751/1770 train_time:173376ms step_avg:99.58ms
step:1752/1770 train_time:173465ms step_avg:99.58ms
step:1753/1770 train_time:173569ms step_avg:99.58ms
step:1754/1770 train_time:173674ms step_avg:99.58ms
step:1755/1770 train_time:173778ms step_avg:99.59ms
step:1756/1770 train_time:173883ms step_avg:99.59ms
step:1757/1770 train_time:173987ms step_avg:99.59ms
step:1758/1770 train_time:174091ms step_avg:99.59ms
step:1759/1770 train_time:174194ms step_avg:99.60ms
step:1760/1770 train_time:174300ms step_avg:99.60ms
step:1761/1770 train_time:174407ms step_avg:99.60ms
step:1762/1770 train_time:174515ms step_avg:99.61ms
step:1763/1770 train_time:174618ms step_avg:99.61ms
step:1764/1770 train_time:174724ms step_avg:99.61ms
step:1765/1770 train_time:174828ms step_avg:99.62ms
step:1766/1770 train_time:174936ms step_avg:99.62ms
step:1767/1770 train_time:175039ms step_avg:99.62ms
step:1768/1770 train_time:175143ms step_avg:99.63ms
step:1769/1770 train_time:175247ms step_avg:99.63ms
step:1770/1770 train_time:175351ms step_avg:99.63ms
step:1770/1770 val_loss:3.2788 train_time:175454ms step_avg:99.69ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
