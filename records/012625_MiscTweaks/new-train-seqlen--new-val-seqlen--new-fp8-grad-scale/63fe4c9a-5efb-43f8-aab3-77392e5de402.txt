import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 08:04:47 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:23284ms step_avg:nanms
step:2/1770 train_time:23697ms step_avg:nanms
step:3/1770 train_time:23792ms step_avg:nanms
step:4/1770 train_time:23885ms step_avg:nanms
step:5/1770 train_time:23978ms step_avg:nanms
step:6/1770 train_time:24072ms step_avg:nanms
step:7/1770 train_time:24166ms step_avg:nanms
step:8/1770 train_time:24259ms step_avg:nanms
step:9/1770 train_time:24352ms step_avg:nanms
step:10/1770 train_time:24446ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:94.00ms
step:14/1770 train_time:376ms step_avg:94.08ms
step:15/1770 train_time:470ms step_avg:94.08ms
step:16/1770 train_time:565ms step_avg:94.10ms
step:17/1770 train_time:658ms step_avg:93.99ms
step:18/1770 train_time:752ms step_avg:93.97ms
step:19/1770 train_time:846ms step_avg:93.95ms
step:20/1770 train_time:939ms step_avg:93.91ms
step:21/1770 train_time:1033ms step_avg:93.90ms
step:22/1770 train_time:1127ms step_avg:93.93ms
step:23/1770 train_time:1221ms step_avg:93.90ms
step:24/1770 train_time:1315ms step_avg:93.91ms
step:25/1770 train_time:1409ms step_avg:93.93ms
step:26/1770 train_time:1503ms step_avg:93.97ms
step:27/1770 train_time:1597ms step_avg:93.94ms
step:28/1770 train_time:1691ms step_avg:93.94ms
step:29/1770 train_time:1785ms step_avg:93.95ms
step:30/1770 train_time:1879ms step_avg:93.93ms
step:31/1770 train_time:1973ms step_avg:93.93ms
step:32/1770 train_time:2067ms step_avg:93.95ms
step:33/1770 train_time:2161ms step_avg:93.94ms
step:34/1770 train_time:2255ms step_avg:93.95ms
step:35/1770 train_time:2350ms step_avg:93.98ms
step:36/1770 train_time:2443ms step_avg:93.98ms
step:37/1770 train_time:2537ms step_avg:93.97ms
step:38/1770 train_time:2631ms step_avg:93.97ms
step:39/1770 train_time:2725ms step_avg:93.97ms
step:40/1770 train_time:2819ms step_avg:93.96ms
step:41/1770 train_time:2913ms step_avg:93.95ms
step:42/1770 train_time:3006ms step_avg:93.95ms
step:43/1770 train_time:3100ms step_avg:93.95ms
step:44/1770 train_time:3194ms step_avg:93.95ms
step:45/1770 train_time:3289ms step_avg:93.96ms
step:46/1770 train_time:3383ms step_avg:93.98ms
step:47/1770 train_time:3478ms step_avg:93.99ms
step:48/1770 train_time:3572ms step_avg:93.99ms
step:49/1770 train_time:3666ms step_avg:94.01ms
step:50/1770 train_time:3760ms step_avg:94.00ms
step:51/1770 train_time:3853ms step_avg:93.99ms
step:52/1770 train_time:3947ms step_avg:93.98ms
step:53/1770 train_time:4042ms step_avg:93.99ms
step:54/1770 train_time:4134ms step_avg:93.97ms
step:55/1770 train_time:4228ms step_avg:93.97ms
step:56/1770 train_time:4323ms step_avg:93.97ms
step:57/1770 train_time:4416ms step_avg:93.96ms
step:58/1770 train_time:4510ms step_avg:93.97ms
step:59/1770 train_time:4605ms step_avg:93.97ms
step:60/1770 train_time:4698ms step_avg:93.97ms
step:61/1770 train_time:4793ms step_avg:93.98ms
step:62/1770 train_time:4887ms step_avg:93.97ms
step:63/1770 train_time:4980ms step_avg:93.96ms
step:64/1770 train_time:5074ms step_avg:93.96ms
step:65/1770 train_time:5168ms step_avg:93.96ms
step:66/1770 train_time:5262ms step_avg:93.97ms
step:67/1770 train_time:5356ms step_avg:93.96ms
step:68/1770 train_time:5450ms step_avg:93.97ms
step:69/1770 train_time:5543ms step_avg:93.96ms
step:70/1770 train_time:5637ms step_avg:93.95ms
step:71/1770 train_time:5731ms step_avg:93.95ms
step:72/1770 train_time:5825ms step_avg:93.95ms
step:73/1770 train_time:5919ms step_avg:93.95ms
step:74/1770 train_time:6013ms step_avg:93.95ms
step:75/1770 train_time:6107ms step_avg:93.95ms
step:76/1770 train_time:6200ms step_avg:93.94ms
step:77/1770 train_time:6294ms step_avg:93.93ms
step:78/1770 train_time:6388ms step_avg:93.93ms
step:79/1770 train_time:6481ms step_avg:93.93ms
step:80/1770 train_time:6577ms step_avg:93.95ms
step:81/1770 train_time:6671ms step_avg:93.95ms
step:82/1770 train_time:6765ms step_avg:93.96ms
step:83/1770 train_time:6859ms step_avg:93.95ms
step:84/1770 train_time:6952ms step_avg:93.94ms
step:85/1770 train_time:7046ms step_avg:93.94ms
step:86/1770 train_time:7139ms step_avg:93.94ms
step:87/1770 train_time:7233ms step_avg:93.93ms
step:88/1770 train_time:7327ms step_avg:93.94ms
step:89/1770 train_time:7421ms step_avg:93.93ms
step:90/1770 train_time:7515ms step_avg:93.93ms
step:91/1770 train_time:7609ms step_avg:93.94ms
step:92/1770 train_time:7703ms step_avg:93.94ms
step:93/1770 train_time:7797ms step_avg:93.94ms
step:94/1770 train_time:7891ms step_avg:93.94ms
step:95/1770 train_time:7985ms step_avg:93.94ms
step:96/1770 train_time:8079ms step_avg:93.94ms
step:97/1770 train_time:8173ms step_avg:93.94ms
step:98/1770 train_time:8267ms step_avg:93.95ms
step:99/1770 train_time:8362ms step_avg:93.95ms
step:100/1770 train_time:8455ms step_avg:93.95ms
step:101/1770 train_time:8549ms step_avg:93.94ms
step:102/1770 train_time:8643ms step_avg:93.94ms
step:103/1770 train_time:8736ms step_avg:93.94ms
step:104/1770 train_time:8830ms step_avg:93.94ms
step:105/1770 train_time:8924ms step_avg:93.94ms
step:106/1770 train_time:9018ms step_avg:93.94ms
step:107/1770 train_time:9112ms step_avg:93.94ms
step:108/1770 train_time:9206ms step_avg:93.94ms
step:109/1770 train_time:9299ms step_avg:93.93ms
step:110/1770 train_time:9393ms step_avg:93.93ms
step:111/1770 train_time:9487ms step_avg:93.93ms
step:112/1770 train_time:9580ms step_avg:93.93ms
step:113/1770 train_time:9674ms step_avg:93.92ms
step:114/1770 train_time:9768ms step_avg:93.93ms
step:115/1770 train_time:9862ms step_avg:93.92ms
step:116/1770 train_time:9956ms step_avg:93.93ms
step:117/1770 train_time:10050ms step_avg:93.93ms
step:118/1770 train_time:10144ms step_avg:93.93ms
step:119/1770 train_time:10237ms step_avg:93.92ms
step:120/1770 train_time:10331ms step_avg:93.92ms
step:121/1770 train_time:10425ms step_avg:93.92ms
step:122/1770 train_time:10519ms step_avg:93.92ms
step:123/1770 train_time:10613ms step_avg:93.92ms
step:124/1770 train_time:10708ms step_avg:93.93ms
step:125/1770 train_time:10802ms step_avg:93.93ms
step:125/1770 val_loss:4.6519 train_time:10894ms step_avg:94.73ms
step:126/1770 train_time:10916ms step_avg:94.10ms
step:127/1770 train_time:10992ms step_avg:93.95ms
step:128/1770 train_time:11091ms step_avg:93.99ms
step:129/1770 train_time:11189ms step_avg:94.03ms
step:130/1770 train_time:11283ms step_avg:94.03ms
step:131/1770 train_time:11377ms step_avg:94.02ms
step:132/1770 train_time:11471ms step_avg:94.02ms
step:133/1770 train_time:11564ms step_avg:94.02ms
step:134/1770 train_time:11658ms step_avg:94.02ms
step:135/1770 train_time:11752ms step_avg:94.02ms
step:136/1770 train_time:11846ms step_avg:94.02ms
step:137/1770 train_time:11940ms step_avg:94.02ms
step:138/1770 train_time:12035ms step_avg:94.02ms
step:139/1770 train_time:12131ms step_avg:94.04ms
step:140/1770 train_time:12226ms step_avg:94.04ms
step:141/1770 train_time:12320ms step_avg:94.05ms
step:142/1770 train_time:12415ms step_avg:94.05ms
step:143/1770 train_time:12509ms step_avg:94.06ms
step:144/1770 train_time:12604ms step_avg:94.06ms
step:145/1770 train_time:12699ms step_avg:94.06ms
step:146/1770 train_time:12793ms step_avg:94.07ms
step:147/1770 train_time:12889ms step_avg:94.08ms
step:148/1770 train_time:12982ms step_avg:94.07ms
step:149/1770 train_time:13077ms step_avg:94.08ms
step:150/1770 train_time:13172ms step_avg:94.09ms
step:151/1770 train_time:13267ms step_avg:94.09ms
step:152/1770 train_time:13361ms step_avg:94.09ms
step:153/1770 train_time:13456ms step_avg:94.10ms
step:154/1770 train_time:13551ms step_avg:94.11ms
step:155/1770 train_time:13646ms step_avg:94.11ms
step:156/1770 train_time:13740ms step_avg:94.11ms
step:157/1770 train_time:13834ms step_avg:94.11ms
step:158/1770 train_time:13929ms step_avg:94.12ms
step:159/1770 train_time:14024ms step_avg:94.12ms
step:160/1770 train_time:14118ms step_avg:94.12ms
step:161/1770 train_time:14213ms step_avg:94.13ms
step:162/1770 train_time:14309ms step_avg:94.14ms
step:163/1770 train_time:14403ms step_avg:94.14ms
step:164/1770 train_time:14497ms step_avg:94.14ms
step:165/1770 train_time:14593ms step_avg:94.15ms
step:166/1770 train_time:14687ms step_avg:94.15ms
step:167/1770 train_time:14781ms step_avg:94.15ms
step:168/1770 train_time:14876ms step_avg:94.15ms
step:169/1770 train_time:14971ms step_avg:94.16ms
step:170/1770 train_time:15066ms step_avg:94.16ms
step:171/1770 train_time:15160ms step_avg:94.16ms
step:172/1770 train_time:15255ms step_avg:94.17ms
step:173/1770 train_time:15350ms step_avg:94.17ms
step:174/1770 train_time:15445ms step_avg:94.18ms
step:175/1770 train_time:15539ms step_avg:94.17ms
step:176/1770 train_time:15634ms step_avg:94.18ms
step:177/1770 train_time:15729ms step_avg:94.18ms
step:178/1770 train_time:15823ms step_avg:94.19ms
step:179/1770 train_time:15918ms step_avg:94.19ms
step:180/1770 train_time:16012ms step_avg:94.19ms
step:181/1770 train_time:16108ms step_avg:94.20ms
step:182/1770 train_time:16202ms step_avg:94.20ms
step:183/1770 train_time:16297ms step_avg:94.20ms
step:184/1770 train_time:16392ms step_avg:94.21ms
step:185/1770 train_time:16486ms step_avg:94.21ms
step:186/1770 train_time:16580ms step_avg:94.20ms
step:187/1770 train_time:16675ms step_avg:94.21ms
step:188/1770 train_time:16770ms step_avg:94.21ms
step:189/1770 train_time:16864ms step_avg:94.21ms
step:190/1770 train_time:16959ms step_avg:94.21ms
step:191/1770 train_time:17053ms step_avg:94.22ms
step:192/1770 train_time:17148ms step_avg:94.22ms
step:193/1770 train_time:17243ms step_avg:94.22ms
step:194/1770 train_time:17337ms step_avg:94.22ms
step:195/1770 train_time:17432ms step_avg:94.22ms
step:196/1770 train_time:17526ms step_avg:94.23ms
step:197/1770 train_time:17620ms step_avg:94.23ms
step:198/1770 train_time:17714ms step_avg:94.23ms
step:199/1770 train_time:17809ms step_avg:94.23ms
step:200/1770 train_time:17904ms step_avg:94.23ms
step:201/1770 train_time:17998ms step_avg:94.23ms
step:202/1770 train_time:18093ms step_avg:94.23ms
step:203/1770 train_time:18188ms step_avg:94.24ms
step:204/1770 train_time:18282ms step_avg:94.24ms
step:205/1770 train_time:18377ms step_avg:94.24ms
step:206/1770 train_time:18471ms step_avg:94.24ms
step:207/1770 train_time:18566ms step_avg:94.24ms
step:208/1770 train_time:18660ms step_avg:94.24ms
step:209/1770 train_time:18754ms step_avg:94.24ms
step:210/1770 train_time:18849ms step_avg:94.24ms
step:211/1770 train_time:18942ms step_avg:94.24ms
step:212/1770 train_time:19037ms step_avg:94.24ms
step:213/1770 train_time:19132ms step_avg:94.25ms
step:214/1770 train_time:19227ms step_avg:94.25ms
step:215/1770 train_time:19321ms step_avg:94.25ms
step:216/1770 train_time:19416ms step_avg:94.25ms
step:217/1770 train_time:19511ms step_avg:94.25ms
step:218/1770 train_time:19605ms step_avg:94.26ms
step:219/1770 train_time:19700ms step_avg:94.26ms
step:220/1770 train_time:19795ms step_avg:94.26ms
step:221/1770 train_time:19890ms step_avg:94.26ms
step:222/1770 train_time:19984ms step_avg:94.27ms
step:223/1770 train_time:20078ms step_avg:94.26ms
step:224/1770 train_time:20173ms step_avg:94.27ms
step:225/1770 train_time:20268ms step_avg:94.27ms
step:226/1770 train_time:20362ms step_avg:94.27ms
step:227/1770 train_time:20457ms step_avg:94.27ms
step:228/1770 train_time:20552ms step_avg:94.28ms
step:229/1770 train_time:20647ms step_avg:94.28ms
step:230/1770 train_time:20742ms step_avg:94.28ms
step:231/1770 train_time:20836ms step_avg:94.28ms
step:232/1770 train_time:20931ms step_avg:94.28ms
step:233/1770 train_time:21025ms step_avg:94.28ms
step:234/1770 train_time:21119ms step_avg:94.28ms
step:235/1770 train_time:21214ms step_avg:94.29ms
step:236/1770 train_time:21310ms step_avg:94.29ms
step:237/1770 train_time:21405ms step_avg:94.29ms
step:238/1770 train_time:21499ms step_avg:94.29ms
step:239/1770 train_time:21594ms step_avg:94.29ms
step:240/1770 train_time:21688ms step_avg:94.30ms
step:241/1770 train_time:21782ms step_avg:94.30ms
step:242/1770 train_time:21877ms step_avg:94.30ms
step:243/1770 train_time:21972ms step_avg:94.30ms
step:244/1770 train_time:22067ms step_avg:94.30ms
step:245/1770 train_time:22161ms step_avg:94.30ms
step:246/1770 train_time:22256ms step_avg:94.30ms
step:247/1770 train_time:22351ms step_avg:94.31ms
step:248/1770 train_time:22446ms step_avg:94.31ms
step:249/1770 train_time:22541ms step_avg:94.31ms
step:250/1770 train_time:22635ms step_avg:94.31ms
step:250/1770 val_loss:4.1157 train_time:22728ms step_avg:94.70ms
step:251/1770 train_time:22750ms step_avg:94.40ms
step:252/1770 train_time:22831ms step_avg:94.34ms
step:253/1770 train_time:22927ms step_avg:94.35ms
step:254/1770 train_time:23021ms step_avg:94.35ms
step:255/1770 train_time:23115ms step_avg:94.35ms
step:256/1770 train_time:23209ms step_avg:94.35ms
step:257/1770 train_time:23303ms step_avg:94.34ms
step:258/1770 train_time:23397ms step_avg:94.34ms
step:259/1770 train_time:23490ms step_avg:94.34ms
step:260/1770 train_time:23584ms step_avg:94.34ms
step:261/1770 train_time:23679ms step_avg:94.34ms
step:262/1770 train_time:23774ms step_avg:94.34ms
step:263/1770 train_time:23869ms step_avg:94.34ms
step:264/1770 train_time:23964ms step_avg:94.35ms
step:265/1770 train_time:24059ms step_avg:94.35ms
step:266/1770 train_time:24153ms step_avg:94.35ms
step:267/1770 train_time:24248ms step_avg:94.35ms
step:268/1770 train_time:24343ms step_avg:94.35ms
step:269/1770 train_time:24438ms step_avg:94.35ms
step:270/1770 train_time:24532ms step_avg:94.35ms
step:271/1770 train_time:24627ms step_avg:94.36ms
step:272/1770 train_time:24722ms step_avg:94.36ms
step:273/1770 train_time:24817ms step_avg:94.36ms
step:274/1770 train_time:24912ms step_avg:94.36ms
step:275/1770 train_time:25008ms step_avg:94.37ms
step:276/1770 train_time:25104ms step_avg:94.38ms
step:277/1770 train_time:25199ms step_avg:94.38ms
step:278/1770 train_time:25294ms step_avg:94.38ms
step:279/1770 train_time:25390ms step_avg:94.38ms
step:280/1770 train_time:25485ms step_avg:94.39ms
step:281/1770 train_time:25579ms step_avg:94.39ms
step:282/1770 train_time:25674ms step_avg:94.39ms
step:283/1770 train_time:25769ms step_avg:94.39ms
step:284/1770 train_time:25865ms step_avg:94.40ms
step:285/1770 train_time:25959ms step_avg:94.40ms
step:286/1770 train_time:26054ms step_avg:94.40ms
step:287/1770 train_time:26149ms step_avg:94.40ms
step:288/1770 train_time:26245ms step_avg:94.41ms
step:289/1770 train_time:26340ms step_avg:94.41ms
step:290/1770 train_time:26434ms step_avg:94.41ms
step:291/1770 train_time:26529ms step_avg:94.41ms
step:292/1770 train_time:26624ms step_avg:94.41ms
step:293/1770 train_time:26719ms step_avg:94.41ms
step:294/1770 train_time:26814ms step_avg:94.42ms
step:295/1770 train_time:26909ms step_avg:94.42ms
step:296/1770 train_time:27005ms step_avg:94.42ms
step:297/1770 train_time:27100ms step_avg:94.42ms
step:298/1770 train_time:27194ms step_avg:94.42ms
step:299/1770 train_time:27289ms step_avg:94.43ms
step:300/1770 train_time:27385ms step_avg:94.43ms
step:301/1770 train_time:27479ms step_avg:94.43ms
step:302/1770 train_time:27574ms step_avg:94.43ms
step:303/1770 train_time:27670ms step_avg:94.44ms
step:304/1770 train_time:27765ms step_avg:94.44ms
step:305/1770 train_time:27860ms step_avg:94.44ms
step:306/1770 train_time:27955ms step_avg:94.44ms
step:307/1770 train_time:28050ms step_avg:94.44ms
step:308/1770 train_time:28146ms step_avg:94.45ms
step:309/1770 train_time:28241ms step_avg:94.45ms
step:310/1770 train_time:28336ms step_avg:94.45ms
step:311/1770 train_time:28430ms step_avg:94.45ms
step:312/1770 train_time:28526ms step_avg:94.46ms
step:313/1770 train_time:28622ms step_avg:94.46ms
step:314/1770 train_time:28717ms step_avg:94.46ms
step:315/1770 train_time:28812ms step_avg:94.46ms
step:316/1770 train_time:28907ms step_avg:94.47ms
step:317/1770 train_time:29002ms step_avg:94.47ms
step:318/1770 train_time:29096ms step_avg:94.47ms
step:319/1770 train_time:29191ms step_avg:94.47ms
step:320/1770 train_time:29287ms step_avg:94.47ms
step:321/1770 train_time:29382ms step_avg:94.48ms
step:322/1770 train_time:29477ms step_avg:94.48ms
step:323/1770 train_time:29571ms step_avg:94.48ms
step:324/1770 train_time:29667ms step_avg:94.48ms
step:325/1770 train_time:29762ms step_avg:94.48ms
step:326/1770 train_time:29857ms step_avg:94.48ms
step:327/1770 train_time:29952ms step_avg:94.49ms
step:328/1770 train_time:30047ms step_avg:94.49ms
step:329/1770 train_time:30142ms step_avg:94.49ms
step:330/1770 train_time:30237ms step_avg:94.49ms
step:331/1770 train_time:30331ms step_avg:94.49ms
step:332/1770 train_time:30426ms step_avg:94.49ms
step:333/1770 train_time:30522ms step_avg:94.49ms
step:334/1770 train_time:30616ms step_avg:94.49ms
step:335/1770 train_time:30711ms step_avg:94.49ms
step:336/1770 train_time:30806ms step_avg:94.50ms
step:337/1770 train_time:30901ms step_avg:94.50ms
step:338/1770 train_time:30995ms step_avg:94.50ms
step:339/1770 train_time:31089ms step_avg:94.50ms
step:340/1770 train_time:31185ms step_avg:94.50ms
step:341/1770 train_time:31280ms step_avg:94.50ms
step:342/1770 train_time:31374ms step_avg:94.50ms
step:343/1770 train_time:31469ms step_avg:94.50ms
step:344/1770 train_time:31564ms step_avg:94.50ms
step:345/1770 train_time:31660ms step_avg:94.51ms
step:346/1770 train_time:31754ms step_avg:94.51ms
step:347/1770 train_time:31850ms step_avg:94.51ms
step:348/1770 train_time:31945ms step_avg:94.51ms
step:349/1770 train_time:32039ms step_avg:94.51ms
step:350/1770 train_time:32134ms step_avg:94.51ms
step:351/1770 train_time:32229ms step_avg:94.51ms
step:352/1770 train_time:32324ms step_avg:94.52ms
step:353/1770 train_time:32419ms step_avg:94.52ms
step:354/1770 train_time:32514ms step_avg:94.52ms
step:355/1770 train_time:32608ms step_avg:94.52ms
step:356/1770 train_time:32704ms step_avg:94.52ms
step:357/1770 train_time:32799ms step_avg:94.52ms
step:358/1770 train_time:32894ms step_avg:94.52ms
step:359/1770 train_time:32989ms step_avg:94.52ms
step:360/1770 train_time:33084ms step_avg:94.53ms
step:361/1770 train_time:33179ms step_avg:94.53ms
step:362/1770 train_time:33274ms step_avg:94.53ms
step:363/1770 train_time:33369ms step_avg:94.53ms
step:364/1770 train_time:33464ms step_avg:94.53ms
step:365/1770 train_time:33559ms step_avg:94.53ms
step:366/1770 train_time:33654ms step_avg:94.53ms
step:367/1770 train_time:33749ms step_avg:94.54ms
step:368/1770 train_time:33844ms step_avg:94.54ms
step:369/1770 train_time:33939ms step_avg:94.54ms
step:370/1770 train_time:34033ms step_avg:94.54ms
step:371/1770 train_time:34129ms step_avg:94.54ms
step:372/1770 train_time:34224ms step_avg:94.54ms
step:373/1770 train_time:34319ms step_avg:94.54ms
step:374/1770 train_time:34413ms step_avg:94.54ms
step:375/1770 train_time:34508ms step_avg:94.54ms
step:375/1770 val_loss:3.9143 train_time:34601ms step_avg:94.80ms
step:376/1770 train_time:34623ms step_avg:94.60ms
step:377/1770 train_time:34705ms step_avg:94.56ms
step:378/1770 train_time:34802ms step_avg:94.57ms
step:379/1770 train_time:34897ms step_avg:94.57ms
step:380/1770 train_time:34991ms step_avg:94.57ms
step:381/1770 train_time:35085ms step_avg:94.57ms
step:382/1770 train_time:35180ms step_avg:94.57ms
step:383/1770 train_time:35274ms step_avg:94.57ms
step:384/1770 train_time:35369ms step_avg:94.57ms
step:385/1770 train_time:35463ms step_avg:94.57ms
step:386/1770 train_time:35559ms step_avg:94.57ms
step:387/1770 train_time:35655ms step_avg:94.57ms
step:388/1770 train_time:35750ms step_avg:94.58ms
step:389/1770 train_time:35846ms step_avg:94.58ms
step:390/1770 train_time:35941ms step_avg:94.58ms
step:391/1770 train_time:36036ms step_avg:94.58ms
step:392/1770 train_time:36130ms step_avg:94.58ms
step:393/1770 train_time:36225ms step_avg:94.58ms
step:394/1770 train_time:36319ms step_avg:94.58ms
step:395/1770 train_time:36414ms step_avg:94.58ms
step:396/1770 train_time:36510ms step_avg:94.59ms
step:397/1770 train_time:36607ms step_avg:94.59ms
step:398/1770 train_time:36704ms step_avg:94.60ms
step:399/1770 train_time:36802ms step_avg:94.61ms
step:400/1770 train_time:36900ms step_avg:94.61ms
step:401/1770 train_time:36997ms step_avg:94.62ms
step:402/1770 train_time:37094ms step_avg:94.63ms
step:403/1770 train_time:37191ms step_avg:94.63ms
step:404/1770 train_time:37287ms step_avg:94.64ms
step:405/1770 train_time:37384ms step_avg:94.64ms
step:406/1770 train_time:37481ms step_avg:94.65ms
step:407/1770 train_time:37578ms step_avg:94.65ms
step:408/1770 train_time:37675ms step_avg:94.66ms
step:409/1770 train_time:37772ms step_avg:94.67ms
step:410/1770 train_time:37869ms step_avg:94.67ms
step:411/1770 train_time:37965ms step_avg:94.68ms
step:412/1770 train_time:38063ms step_avg:94.68ms
step:413/1770 train_time:38160ms step_avg:94.69ms
step:414/1770 train_time:38257ms step_avg:94.70ms
step:415/1770 train_time:38354ms step_avg:94.70ms
step:416/1770 train_time:38451ms step_avg:94.71ms
step:417/1770 train_time:38548ms step_avg:94.71ms
step:418/1770 train_time:38645ms step_avg:94.72ms
step:419/1770 train_time:38742ms step_avg:94.72ms
step:420/1770 train_time:38839ms step_avg:94.73ms
step:421/1770 train_time:38937ms step_avg:94.74ms
step:422/1770 train_time:39034ms step_avg:94.74ms
step:423/1770 train_time:39131ms step_avg:94.75ms
step:424/1770 train_time:39227ms step_avg:94.75ms
step:425/1770 train_time:39324ms step_avg:94.76ms
step:426/1770 train_time:39421ms step_avg:94.76ms
step:427/1770 train_time:39518ms step_avg:94.77ms
step:428/1770 train_time:39616ms step_avg:94.77ms
step:429/1770 train_time:39712ms step_avg:94.78ms
step:430/1770 train_time:39809ms step_avg:94.78ms
step:431/1770 train_time:39906ms step_avg:94.79ms
step:432/1770 train_time:40003ms step_avg:94.79ms
step:433/1770 train_time:40100ms step_avg:94.80ms
step:434/1770 train_time:40198ms step_avg:94.81ms
step:435/1770 train_time:40295ms step_avg:94.81ms
step:436/1770 train_time:40392ms step_avg:94.82ms
step:437/1770 train_time:40488ms step_avg:94.82ms
step:438/1770 train_time:40584ms step_avg:94.82ms
step:439/1770 train_time:40681ms step_avg:94.83ms
step:440/1770 train_time:40779ms step_avg:94.84ms
step:441/1770 train_time:40876ms step_avg:94.84ms
step:442/1770 train_time:40973ms step_avg:94.85ms
step:443/1770 train_time:41071ms step_avg:94.85ms
step:444/1770 train_time:41167ms step_avg:94.85ms
step:445/1770 train_time:41263ms step_avg:94.86ms
step:446/1770 train_time:41361ms step_avg:94.86ms
step:447/1770 train_time:41458ms step_avg:94.87ms
step:448/1770 train_time:41555ms step_avg:94.88ms
step:449/1770 train_time:41652ms step_avg:94.88ms
step:450/1770 train_time:41749ms step_avg:94.88ms
step:451/1770 train_time:41846ms step_avg:94.89ms
step:452/1770 train_time:41943ms step_avg:94.89ms
step:453/1770 train_time:42041ms step_avg:94.90ms
step:454/1770 train_time:42138ms step_avg:94.91ms
step:455/1770 train_time:42236ms step_avg:94.91ms
step:456/1770 train_time:42332ms step_avg:94.92ms
step:457/1770 train_time:42429ms step_avg:94.92ms
step:458/1770 train_time:42525ms step_avg:94.92ms
step:459/1770 train_time:42622ms step_avg:94.93ms
step:460/1770 train_time:42719ms step_avg:94.93ms
step:461/1770 train_time:42817ms step_avg:94.94ms
step:462/1770 train_time:42914ms step_avg:94.94ms
step:463/1770 train_time:43010ms step_avg:94.95ms
step:464/1770 train_time:43107ms step_avg:94.95ms
step:465/1770 train_time:43204ms step_avg:94.95ms
step:466/1770 train_time:43301ms step_avg:94.96ms
step:467/1770 train_time:43398ms step_avg:94.96ms
step:468/1770 train_time:43496ms step_avg:94.97ms
step:469/1770 train_time:43592ms step_avg:94.97ms
step:470/1770 train_time:43688ms step_avg:94.97ms
step:471/1770 train_time:43785ms step_avg:94.98ms
step:472/1770 train_time:43882ms step_avg:94.98ms
step:473/1770 train_time:43979ms step_avg:94.99ms
step:474/1770 train_time:44077ms step_avg:94.99ms
step:475/1770 train_time:44174ms step_avg:95.00ms
step:476/1770 train_time:44270ms step_avg:95.00ms
step:477/1770 train_time:44367ms step_avg:95.00ms
step:478/1770 train_time:44464ms step_avg:95.01ms
step:479/1770 train_time:44561ms step_avg:95.01ms
step:480/1770 train_time:44658ms step_avg:95.02ms
step:481/1770 train_time:44755ms step_avg:95.02ms
step:482/1770 train_time:44851ms step_avg:95.02ms
step:483/1770 train_time:44948ms step_avg:95.03ms
step:484/1770 train_time:45045ms step_avg:95.03ms
step:485/1770 train_time:45142ms step_avg:95.04ms
step:486/1770 train_time:45240ms step_avg:95.04ms
step:487/1770 train_time:45337ms step_avg:95.05ms
step:488/1770 train_time:45434ms step_avg:95.05ms
step:489/1770 train_time:45530ms step_avg:95.05ms
step:490/1770 train_time:45627ms step_avg:95.06ms
step:491/1770 train_time:45724ms step_avg:95.06ms
step:492/1770 train_time:45821ms step_avg:95.06ms
step:493/1770 train_time:45918ms step_avg:95.07ms
step:494/1770 train_time:46015ms step_avg:95.07ms
step:495/1770 train_time:46111ms step_avg:95.08ms
step:496/1770 train_time:46208ms step_avg:95.08ms
step:497/1770 train_time:46305ms step_avg:95.08ms
step:498/1770 train_time:46402ms step_avg:95.09ms
step:499/1770 train_time:46500ms step_avg:95.09ms
step:500/1770 train_time:46597ms step_avg:95.10ms
step:500/1770 val_loss:3.7566 train_time:46692ms step_avg:95.29ms
step:501/1770 train_time:46713ms step_avg:95.14ms
step:502/1770 train_time:46796ms step_avg:95.11ms
step:503/1770 train_time:46895ms step_avg:95.12ms
step:504/1770 train_time:46992ms step_avg:95.12ms
step:505/1770 train_time:47088ms step_avg:95.13ms
step:506/1770 train_time:47185ms step_avg:95.13ms
step:507/1770 train_time:47281ms step_avg:95.13ms
step:508/1770 train_time:47377ms step_avg:95.13ms
step:509/1770 train_time:47474ms step_avg:95.14ms
step:510/1770 train_time:47571ms step_avg:95.14ms
step:511/1770 train_time:47668ms step_avg:95.15ms
step:512/1770 train_time:47765ms step_avg:95.15ms
step:513/1770 train_time:47862ms step_avg:95.15ms
step:514/1770 train_time:47960ms step_avg:95.16ms
step:515/1770 train_time:48057ms step_avg:95.16ms
step:516/1770 train_time:48155ms step_avg:95.17ms
step:517/1770 train_time:48252ms step_avg:95.17ms
step:518/1770 train_time:48349ms step_avg:95.17ms
step:519/1770 train_time:48445ms step_avg:95.18ms
step:520/1770 train_time:48542ms step_avg:95.18ms
step:521/1770 train_time:48638ms step_avg:95.18ms
step:522/1770 train_time:48735ms step_avg:95.19ms
step:523/1770 train_time:48833ms step_avg:95.19ms
step:524/1770 train_time:48930ms step_avg:95.19ms
step:525/1770 train_time:49027ms step_avg:95.20ms
step:526/1770 train_time:49124ms step_avg:95.20ms
step:527/1770 train_time:49221ms step_avg:95.21ms
step:528/1770 train_time:49318ms step_avg:95.21ms
step:529/1770 train_time:49415ms step_avg:95.21ms
step:530/1770 train_time:49513ms step_avg:95.22ms
step:531/1770 train_time:49611ms step_avg:95.22ms
step:532/1770 train_time:49709ms step_avg:95.23ms
step:533/1770 train_time:49806ms step_avg:95.23ms
step:534/1770 train_time:49902ms step_avg:95.23ms
step:535/1770 train_time:49999ms step_avg:95.24ms
step:536/1770 train_time:50097ms step_avg:95.24ms
step:537/1770 train_time:50195ms step_avg:95.25ms
step:538/1770 train_time:50293ms step_avg:95.25ms
step:539/1770 train_time:50391ms step_avg:95.26ms
step:540/1770 train_time:50488ms step_avg:95.26ms
step:541/1770 train_time:50585ms step_avg:95.26ms
step:542/1770 train_time:50682ms step_avg:95.27ms
step:543/1770 train_time:50779ms step_avg:95.27ms
step:544/1770 train_time:50876ms step_avg:95.27ms
step:545/1770 train_time:50974ms step_avg:95.28ms
step:546/1770 train_time:51071ms step_avg:95.28ms
step:547/1770 train_time:51169ms step_avg:95.29ms
step:548/1770 train_time:51266ms step_avg:95.29ms
step:549/1770 train_time:51363ms step_avg:95.29ms
step:550/1770 train_time:51459ms step_avg:95.30ms
step:551/1770 train_time:51557ms step_avg:95.30ms
step:552/1770 train_time:51654ms step_avg:95.30ms
step:553/1770 train_time:51752ms step_avg:95.31ms
step:554/1770 train_time:51850ms step_avg:95.31ms
step:555/1770 train_time:51947ms step_avg:95.32ms
step:556/1770 train_time:52044ms step_avg:95.32ms
step:557/1770 train_time:52141ms step_avg:95.32ms
step:558/1770 train_time:52239ms step_avg:95.33ms
step:559/1770 train_time:52336ms step_avg:95.33ms
step:560/1770 train_time:52434ms step_avg:95.33ms
step:561/1770 train_time:52532ms step_avg:95.34ms
step:562/1770 train_time:52630ms step_avg:95.34ms
step:563/1770 train_time:52728ms step_avg:95.35ms
step:564/1770 train_time:52825ms step_avg:95.35ms
step:565/1770 train_time:52922ms step_avg:95.35ms
step:566/1770 train_time:53019ms step_avg:95.36ms
step:567/1770 train_time:53116ms step_avg:95.36ms
step:568/1770 train_time:53214ms step_avg:95.37ms
step:569/1770 train_time:53312ms step_avg:95.37ms
step:570/1770 train_time:53410ms step_avg:95.37ms
step:571/1770 train_time:53507ms step_avg:95.38ms
step:572/1770 train_time:53604ms step_avg:95.38ms
step:573/1770 train_time:53701ms step_avg:95.38ms
step:574/1770 train_time:53798ms step_avg:95.39ms
step:575/1770 train_time:53896ms step_avg:95.39ms
step:576/1770 train_time:53994ms step_avg:95.40ms
step:577/1770 train_time:54091ms step_avg:95.40ms
step:578/1770 train_time:54189ms step_avg:95.40ms
step:579/1770 train_time:54286ms step_avg:95.41ms
step:580/1770 train_time:54383ms step_avg:95.41ms
step:581/1770 train_time:54480ms step_avg:95.41ms
step:582/1770 train_time:54577ms step_avg:95.41ms
step:583/1770 train_time:54675ms step_avg:95.42ms
step:584/1770 train_time:54772ms step_avg:95.42ms
step:585/1770 train_time:54870ms step_avg:95.43ms
step:586/1770 train_time:54967ms step_avg:95.43ms
step:587/1770 train_time:55064ms step_avg:95.43ms
step:588/1770 train_time:55161ms step_avg:95.43ms
step:589/1770 train_time:55259ms step_avg:95.44ms
step:590/1770 train_time:55357ms step_avg:95.44ms
step:591/1770 train_time:55455ms step_avg:95.45ms
step:592/1770 train_time:55553ms step_avg:95.45ms
step:593/1770 train_time:55650ms step_avg:95.45ms
step:594/1770 train_time:55748ms step_avg:95.46ms
step:595/1770 train_time:55845ms step_avg:95.46ms
step:596/1770 train_time:55941ms step_avg:95.46ms
step:597/1770 train_time:56038ms step_avg:95.47ms
step:598/1770 train_time:56136ms step_avg:95.47ms
step:599/1770 train_time:56233ms step_avg:95.47ms
step:600/1770 train_time:56331ms step_avg:95.48ms
step:601/1770 train_time:56429ms step_avg:95.48ms
step:602/1770 train_time:56526ms step_avg:95.48ms
step:603/1770 train_time:56623ms step_avg:95.49ms
step:604/1770 train_time:56720ms step_avg:95.49ms
step:605/1770 train_time:56817ms step_avg:95.49ms
step:606/1770 train_time:56915ms step_avg:95.49ms
step:607/1770 train_time:57013ms step_avg:95.50ms
step:608/1770 train_time:57110ms step_avg:95.50ms
step:609/1770 train_time:57208ms step_avg:95.51ms
step:610/1770 train_time:57305ms step_avg:95.51ms
step:611/1770 train_time:57402ms step_avg:95.51ms
step:612/1770 train_time:57499ms step_avg:95.51ms
step:613/1770 train_time:57596ms step_avg:95.52ms
step:614/1770 train_time:57694ms step_avg:95.52ms
step:615/1770 train_time:57792ms step_avg:95.52ms
step:616/1770 train_time:57890ms step_avg:95.53ms
step:617/1770 train_time:57987ms step_avg:95.53ms
step:618/1770 train_time:58084ms step_avg:95.53ms
step:619/1770 train_time:58181ms step_avg:95.54ms
step:620/1770 train_time:58279ms step_avg:95.54ms
step:621/1770 train_time:58376ms step_avg:95.54ms
step:622/1770 train_time:58474ms step_avg:95.55ms
step:623/1770 train_time:58571ms step_avg:95.55ms
step:624/1770 train_time:58669ms step_avg:95.55ms
step:625/1770 train_time:58766ms step_avg:95.55ms
step:625/1770 val_loss:3.6677 train_time:58862ms step_avg:95.71ms
step:626/1770 train_time:58883ms step_avg:95.59ms
step:627/1770 train_time:58971ms step_avg:95.58ms
step:628/1770 train_time:59068ms step_avg:95.58ms
step:629/1770 train_time:59165ms step_avg:95.58ms
step:630/1770 train_time:59263ms step_avg:95.58ms
step:631/1770 train_time:59360ms step_avg:95.59ms
step:632/1770 train_time:59457ms step_avg:95.59ms
step:633/1770 train_time:59553ms step_avg:95.59ms
step:634/1770 train_time:59650ms step_avg:95.59ms
step:635/1770 train_time:59747ms step_avg:95.60ms
step:636/1770 train_time:59846ms step_avg:95.60ms
step:637/1770 train_time:59945ms step_avg:95.61ms
step:638/1770 train_time:60044ms step_avg:95.61ms
step:639/1770 train_time:60142ms step_avg:95.61ms
step:640/1770 train_time:60239ms step_avg:95.62ms
step:641/1770 train_time:60336ms step_avg:95.62ms
step:642/1770 train_time:60432ms step_avg:95.62ms
step:643/1770 train_time:60529ms step_avg:95.62ms
step:644/1770 train_time:60626ms step_avg:95.63ms
step:645/1770 train_time:60724ms step_avg:95.63ms
step:646/1770 train_time:60821ms step_avg:95.63ms
step:647/1770 train_time:60919ms step_avg:95.63ms
step:648/1770 train_time:61016ms step_avg:95.64ms
step:649/1770 train_time:61113ms step_avg:95.64ms
step:650/1770 train_time:61210ms step_avg:95.64ms
step:651/1770 train_time:61307ms step_avg:95.64ms
step:652/1770 train_time:61405ms step_avg:95.65ms
step:653/1770 train_time:61503ms step_avg:95.65ms
step:654/1770 train_time:61600ms step_avg:95.65ms
step:655/1770 train_time:61696ms step_avg:95.65ms
step:656/1770 train_time:61794ms step_avg:95.66ms
step:657/1770 train_time:61891ms step_avg:95.66ms
step:658/1770 train_time:61990ms step_avg:95.66ms
step:659/1770 train_time:62090ms step_avg:95.67ms
step:660/1770 train_time:62189ms step_avg:95.67ms
step:661/1770 train_time:62288ms step_avg:95.68ms
step:662/1770 train_time:62387ms step_avg:95.69ms
step:663/1770 train_time:62487ms step_avg:95.69ms
step:664/1770 train_time:62586ms step_avg:95.70ms
step:665/1770 train_time:62685ms step_avg:95.70ms
step:666/1770 train_time:62784ms step_avg:95.71ms
step:667/1770 train_time:62884ms step_avg:95.71ms
step:668/1770 train_time:62984ms step_avg:95.72ms
step:669/1770 train_time:63083ms step_avg:95.73ms
step:670/1770 train_time:63183ms step_avg:95.73ms
step:671/1770 train_time:63283ms step_avg:95.74ms
step:672/1770 train_time:63382ms step_avg:95.74ms
step:673/1770 train_time:63481ms step_avg:95.75ms
step:674/1770 train_time:63580ms step_avg:95.75ms
step:675/1770 train_time:63680ms step_avg:95.76ms
step:676/1770 train_time:63778ms step_avg:95.76ms
step:677/1770 train_time:63877ms step_avg:95.77ms
step:678/1770 train_time:63976ms step_avg:95.77ms
step:679/1770 train_time:64075ms step_avg:95.78ms
step:680/1770 train_time:64173ms step_avg:95.78ms
step:681/1770 train_time:64272ms step_avg:95.78ms
step:682/1770 train_time:64371ms step_avg:95.79ms
step:683/1770 train_time:64470ms step_avg:95.79ms
step:684/1770 train_time:64569ms step_avg:95.80ms
step:685/1770 train_time:64668ms step_avg:95.80ms
step:686/1770 train_time:64767ms step_avg:95.81ms
step:687/1770 train_time:64867ms step_avg:95.82ms
step:688/1770 train_time:64966ms step_avg:95.82ms
step:689/1770 train_time:65067ms step_avg:95.83ms
step:690/1770 train_time:65166ms step_avg:95.83ms
step:691/1770 train_time:65266ms step_avg:95.84ms
step:692/1770 train_time:65365ms step_avg:95.84ms
step:693/1770 train_time:65465ms step_avg:95.85ms
step:694/1770 train_time:65564ms step_avg:95.85ms
step:695/1770 train_time:65663ms step_avg:95.86ms
step:696/1770 train_time:65763ms step_avg:95.86ms
step:697/1770 train_time:65862ms step_avg:95.87ms
step:698/1770 train_time:65961ms step_avg:95.87ms
step:699/1770 train_time:66061ms step_avg:95.88ms
step:700/1770 train_time:66161ms step_avg:95.89ms
step:701/1770 train_time:66260ms step_avg:95.89ms
step:702/1770 train_time:66360ms step_avg:95.90ms
step:703/1770 train_time:66460ms step_avg:95.90ms
step:704/1770 train_time:66559ms step_avg:95.91ms
step:705/1770 train_time:66658ms step_avg:95.91ms
step:706/1770 train_time:66757ms step_avg:95.91ms
step:707/1770 train_time:66856ms step_avg:95.92ms
step:708/1770 train_time:66954ms step_avg:95.92ms
step:709/1770 train_time:67052ms step_avg:95.93ms
step:710/1770 train_time:67151ms step_avg:95.93ms
step:711/1770 train_time:67249ms step_avg:95.93ms
step:712/1770 train_time:67348ms step_avg:95.94ms
step:713/1770 train_time:67447ms step_avg:95.94ms
step:714/1770 train_time:67546ms step_avg:95.95ms
step:715/1770 train_time:67645ms step_avg:95.95ms
step:716/1770 train_time:67745ms step_avg:95.96ms
step:717/1770 train_time:67845ms step_avg:95.96ms
step:718/1770 train_time:67945ms step_avg:95.97ms
step:719/1770 train_time:68045ms step_avg:95.97ms
step:720/1770 train_time:68145ms step_avg:95.98ms
step:721/1770 train_time:68245ms step_avg:95.98ms
step:722/1770 train_time:68344ms step_avg:95.99ms
step:723/1770 train_time:68444ms step_avg:95.99ms
step:724/1770 train_time:68543ms step_avg:96.00ms
step:725/1770 train_time:68642ms step_avg:96.00ms
step:726/1770 train_time:68741ms step_avg:96.01ms
step:727/1770 train_time:68841ms step_avg:96.01ms
step:728/1770 train_time:68940ms step_avg:96.02ms
step:729/1770 train_time:69039ms step_avg:96.02ms
step:730/1770 train_time:69139ms step_avg:96.03ms
step:731/1770 train_time:69238ms step_avg:96.03ms
step:732/1770 train_time:69337ms step_avg:96.03ms
step:733/1770 train_time:69436ms step_avg:96.04ms
step:734/1770 train_time:69534ms step_avg:96.04ms
step:735/1770 train_time:69633ms step_avg:96.05ms
step:736/1770 train_time:69731ms step_avg:96.05ms
step:737/1770 train_time:69830ms step_avg:96.05ms
step:738/1770 train_time:69929ms step_avg:96.06ms
step:739/1770 train_time:70028ms step_avg:96.06ms
step:740/1770 train_time:70128ms step_avg:96.07ms
step:741/1770 train_time:70228ms step_avg:96.07ms
step:742/1770 train_time:70327ms step_avg:96.08ms
step:743/1770 train_time:70427ms step_avg:96.08ms
step:744/1770 train_time:70527ms step_avg:96.09ms
step:745/1770 train_time:70626ms step_avg:96.09ms
step:746/1770 train_time:70727ms step_avg:96.10ms
step:747/1770 train_time:70826ms step_avg:96.10ms
step:748/1770 train_time:70925ms step_avg:96.10ms
step:749/1770 train_time:71025ms step_avg:96.11ms
step:750/1770 train_time:71125ms step_avg:96.11ms
step:750/1770 val_loss:3.6026 train_time:71222ms step_avg:96.25ms
step:751/1770 train_time:71244ms step_avg:96.15ms
step:752/1770 train_time:71330ms step_avg:96.13ms
step:753/1770 train_time:71430ms step_avg:96.14ms
step:754/1770 train_time:71529ms step_avg:96.14ms
step:755/1770 train_time:71627ms step_avg:96.14ms
step:756/1770 train_time:71726ms step_avg:96.15ms
step:757/1770 train_time:71824ms step_avg:96.15ms
step:758/1770 train_time:71923ms step_avg:96.15ms
step:759/1770 train_time:72022ms step_avg:96.16ms
step:760/1770 train_time:72121ms step_avg:96.16ms
step:761/1770 train_time:72220ms step_avg:96.17ms
step:762/1770 train_time:72321ms step_avg:96.17ms
step:763/1770 train_time:72423ms step_avg:96.18ms
step:764/1770 train_time:72522ms step_avg:96.18ms
step:765/1770 train_time:72622ms step_avg:96.19ms
step:766/1770 train_time:72722ms step_avg:96.19ms
step:767/1770 train_time:72822ms step_avg:96.20ms
step:768/1770 train_time:72921ms step_avg:96.20ms
step:769/1770 train_time:73020ms step_avg:96.21ms
step:770/1770 train_time:73119ms step_avg:96.21ms
step:771/1770 train_time:73219ms step_avg:96.21ms
step:772/1770 train_time:73318ms step_avg:96.22ms
step:773/1770 train_time:73419ms step_avg:96.22ms
step:774/1770 train_time:73519ms step_avg:96.23ms
step:775/1770 train_time:73619ms step_avg:96.23ms
step:776/1770 train_time:73719ms step_avg:96.24ms
step:777/1770 train_time:73819ms step_avg:96.24ms
step:778/1770 train_time:73918ms step_avg:96.25ms
step:779/1770 train_time:74017ms step_avg:96.25ms
step:780/1770 train_time:74116ms step_avg:96.25ms
step:781/1770 train_time:74215ms step_avg:96.26ms
step:782/1770 train_time:74314ms step_avg:96.26ms
step:783/1770 train_time:74414ms step_avg:96.27ms
step:784/1770 train_time:74512ms step_avg:96.27ms
step:785/1770 train_time:74612ms step_avg:96.27ms
step:786/1770 train_time:74710ms step_avg:96.28ms
step:787/1770 train_time:74809ms step_avg:96.28ms
step:788/1770 train_time:74907ms step_avg:96.28ms
step:789/1770 train_time:75006ms step_avg:96.29ms
step:790/1770 train_time:75105ms step_avg:96.29ms
step:791/1770 train_time:75205ms step_avg:96.29ms
step:792/1770 train_time:75304ms step_avg:96.30ms
step:793/1770 train_time:75404ms step_avg:96.30ms
step:794/1770 train_time:75504ms step_avg:96.31ms
step:795/1770 train_time:75605ms step_avg:96.31ms
step:796/1770 train_time:75705ms step_avg:96.32ms
step:797/1770 train_time:75805ms step_avg:96.32ms
step:798/1770 train_time:75905ms step_avg:96.33ms
step:799/1770 train_time:76005ms step_avg:96.33ms
step:800/1770 train_time:76105ms step_avg:96.34ms
step:801/1770 train_time:76204ms step_avg:96.34ms
step:802/1770 train_time:76304ms step_avg:96.34ms
step:803/1770 train_time:76403ms step_avg:96.35ms
step:804/1770 train_time:76503ms step_avg:96.35ms
step:805/1770 train_time:76603ms step_avg:96.36ms
step:806/1770 train_time:76703ms step_avg:96.36ms
step:807/1770 train_time:76803ms step_avg:96.37ms
step:808/1770 train_time:76903ms step_avg:96.37ms
step:809/1770 train_time:77003ms step_avg:96.37ms
step:810/1770 train_time:77103ms step_avg:96.38ms
step:811/1770 train_time:77202ms step_avg:96.38ms
step:812/1770 train_time:77303ms step_avg:96.39ms
step:813/1770 train_time:77403ms step_avg:96.39ms
step:814/1770 train_time:77503ms step_avg:96.40ms
step:815/1770 train_time:77603ms step_avg:96.40ms
step:816/1770 train_time:77703ms step_avg:96.41ms
step:817/1770 train_time:77802ms step_avg:96.41ms
step:818/1770 train_time:77902ms step_avg:96.41ms
step:819/1770 train_time:78002ms step_avg:96.42ms
step:820/1770 train_time:78102ms step_avg:96.42ms
step:821/1770 train_time:78203ms step_avg:96.43ms
step:822/1770 train_time:78303ms step_avg:96.43ms
step:823/1770 train_time:78403ms step_avg:96.44ms
step:824/1770 train_time:78503ms step_avg:96.44ms
step:825/1770 train_time:78603ms step_avg:96.45ms
step:826/1770 train_time:78702ms step_avg:96.45ms
step:827/1770 train_time:78802ms step_avg:96.45ms
step:828/1770 train_time:78902ms step_avg:96.46ms
step:829/1770 train_time:79002ms step_avg:96.46ms
step:830/1770 train_time:79102ms step_avg:96.47ms
step:831/1770 train_time:79202ms step_avg:96.47ms
step:832/1770 train_time:79302ms step_avg:96.47ms
step:833/1770 train_time:79402ms step_avg:96.48ms
step:834/1770 train_time:79503ms step_avg:96.48ms
step:835/1770 train_time:79603ms step_avg:96.49ms
step:836/1770 train_time:79703ms step_avg:96.49ms
step:837/1770 train_time:79803ms step_avg:96.50ms
step:838/1770 train_time:79903ms step_avg:96.50ms
step:839/1770 train_time:80003ms step_avg:96.51ms
step:840/1770 train_time:80103ms step_avg:96.51ms
step:841/1770 train_time:80202ms step_avg:96.51ms
step:842/1770 train_time:80302ms step_avg:96.52ms
step:843/1770 train_time:80402ms step_avg:96.52ms
step:844/1770 train_time:80502ms step_avg:96.52ms
step:845/1770 train_time:80602ms step_avg:96.53ms
step:846/1770 train_time:80702ms step_avg:96.53ms
step:847/1770 train_time:80801ms step_avg:96.54ms
step:848/1770 train_time:80902ms step_avg:96.54ms
step:849/1770 train_time:81002ms step_avg:96.55ms
step:850/1770 train_time:81101ms step_avg:96.55ms
step:851/1770 train_time:81201ms step_avg:96.55ms
step:852/1770 train_time:81301ms step_avg:96.56ms
step:853/1770 train_time:81401ms step_avg:96.56ms
step:854/1770 train_time:81501ms step_avg:96.56ms
step:855/1770 train_time:81600ms step_avg:96.57ms
step:856/1770 train_time:81700ms step_avg:96.57ms
step:857/1770 train_time:81799ms step_avg:96.57ms
step:858/1770 train_time:81899ms step_avg:96.58ms
step:859/1770 train_time:82000ms step_avg:96.58ms
step:860/1770 train_time:82100ms step_avg:96.59ms
step:861/1770 train_time:82199ms step_avg:96.59ms
step:862/1770 train_time:82299ms step_avg:96.60ms
step:863/1770 train_time:82399ms step_avg:96.60ms
step:864/1770 train_time:82498ms step_avg:96.60ms
step:865/1770 train_time:82598ms step_avg:96.61ms
step:866/1770 train_time:82698ms step_avg:96.61ms
step:867/1770 train_time:82798ms step_avg:96.61ms
step:868/1770 train_time:82898ms step_avg:96.62ms
step:869/1770 train_time:82997ms step_avg:96.62ms
step:870/1770 train_time:83097ms step_avg:96.62ms
step:871/1770 train_time:83197ms step_avg:96.63ms
step:872/1770 train_time:83296ms step_avg:96.63ms
step:873/1770 train_time:83396ms step_avg:96.64ms
step:874/1770 train_time:83496ms step_avg:96.64ms
step:875/1770 train_time:83595ms step_avg:96.64ms
step:875/1770 val_loss:3.5549 train_time:83692ms step_avg:96.75ms
step:876/1770 train_time:83714ms step_avg:96.67ms
step:877/1770 train_time:83802ms step_avg:96.66ms
step:878/1770 train_time:83901ms step_avg:96.66ms
step:879/1770 train_time:84000ms step_avg:96.66ms
step:880/1770 train_time:84099ms step_avg:96.67ms
step:881/1770 train_time:84198ms step_avg:96.67ms
step:882/1770 train_time:84297ms step_avg:96.67ms
step:883/1770 train_time:84396ms step_avg:96.67ms
step:884/1770 train_time:84495ms step_avg:96.68ms
step:885/1770 train_time:84594ms step_avg:96.68ms
step:886/1770 train_time:84694ms step_avg:96.68ms
step:887/1770 train_time:84796ms step_avg:96.69ms
step:888/1770 train_time:84896ms step_avg:96.69ms
step:889/1770 train_time:84997ms step_avg:96.70ms
step:890/1770 train_time:85096ms step_avg:96.70ms
step:891/1770 train_time:85196ms step_avg:96.70ms
step:892/1770 train_time:85296ms step_avg:96.71ms
step:893/1770 train_time:85395ms step_avg:96.71ms
step:894/1770 train_time:85495ms step_avg:96.71ms
step:895/1770 train_time:85594ms step_avg:96.72ms
step:896/1770 train_time:85695ms step_avg:96.72ms
step:897/1770 train_time:85796ms step_avg:96.73ms
step:898/1770 train_time:85897ms step_avg:96.73ms
step:899/1770 train_time:85997ms step_avg:96.73ms
step:900/1770 train_time:86097ms step_avg:96.74ms
step:901/1770 train_time:86197ms step_avg:96.74ms
step:902/1770 train_time:86297ms step_avg:96.75ms
step:903/1770 train_time:86396ms step_avg:96.75ms
step:904/1770 train_time:86496ms step_avg:96.75ms
step:905/1770 train_time:86595ms step_avg:96.75ms
step:906/1770 train_time:86696ms step_avg:96.76ms
step:907/1770 train_time:86796ms step_avg:96.76ms
step:908/1770 train_time:86897ms step_avg:96.77ms
step:909/1770 train_time:86997ms step_avg:96.77ms
step:910/1770 train_time:87098ms step_avg:96.78ms
step:911/1770 train_time:87197ms step_avg:96.78ms
step:912/1770 train_time:87297ms step_avg:96.78ms
step:913/1770 train_time:87396ms step_avg:96.78ms
step:914/1770 train_time:87496ms step_avg:96.79ms
step:915/1770 train_time:87595ms step_avg:96.79ms
step:916/1770 train_time:87695ms step_avg:96.79ms
step:917/1770 train_time:87796ms step_avg:96.80ms
step:918/1770 train_time:87896ms step_avg:96.80ms
step:919/1770 train_time:87996ms step_avg:96.81ms
step:920/1770 train_time:88098ms step_avg:96.81ms
step:921/1770 train_time:88200ms step_avg:96.82ms
step:922/1770 train_time:88301ms step_avg:96.82ms
step:923/1770 train_time:88401ms step_avg:96.82ms
step:924/1770 train_time:88501ms step_avg:96.83ms
step:925/1770 train_time:88601ms step_avg:96.83ms
step:926/1770 train_time:88702ms step_avg:96.84ms
step:927/1770 train_time:88803ms step_avg:96.84ms
step:928/1770 train_time:88903ms step_avg:96.84ms
step:929/1770 train_time:89004ms step_avg:96.85ms
step:930/1770 train_time:89104ms step_avg:96.85ms
step:931/1770 train_time:89204ms step_avg:96.86ms
step:932/1770 train_time:89304ms step_avg:96.86ms
step:933/1770 train_time:89404ms step_avg:96.86ms
step:934/1770 train_time:89504ms step_avg:96.87ms
step:935/1770 train_time:89604ms step_avg:96.87ms
step:936/1770 train_time:89704ms step_avg:96.87ms
step:937/1770 train_time:89804ms step_avg:96.88ms
step:938/1770 train_time:89904ms step_avg:96.88ms
step:939/1770 train_time:90005ms step_avg:96.88ms
step:940/1770 train_time:90105ms step_avg:96.89ms
step:941/1770 train_time:90205ms step_avg:96.89ms
step:942/1770 train_time:90306ms step_avg:96.89ms
step:943/1770 train_time:90407ms step_avg:96.90ms
step:944/1770 train_time:90506ms step_avg:96.90ms
step:945/1770 train_time:90606ms step_avg:96.90ms
step:946/1770 train_time:90707ms step_avg:96.91ms
step:947/1770 train_time:90808ms step_avg:96.91ms
step:948/1770 train_time:90909ms step_avg:96.92ms
step:949/1770 train_time:91010ms step_avg:96.92ms
step:950/1770 train_time:91111ms step_avg:96.93ms
step:951/1770 train_time:91211ms step_avg:96.93ms
step:952/1770 train_time:91312ms step_avg:96.93ms
step:953/1770 train_time:91414ms step_avg:96.94ms
step:954/1770 train_time:91515ms step_avg:96.94ms
step:955/1770 train_time:91617ms step_avg:96.95ms
step:956/1770 train_time:91718ms step_avg:96.95ms
step:957/1770 train_time:91820ms step_avg:96.96ms
step:958/1770 train_time:91921ms step_avg:96.96ms
step:959/1770 train_time:92022ms step_avg:96.97ms
step:960/1770 train_time:92122ms step_avg:96.97ms
step:961/1770 train_time:92222ms step_avg:96.97ms
step:962/1770 train_time:92324ms step_avg:96.98ms
step:963/1770 train_time:92425ms step_avg:96.98ms
step:964/1770 train_time:92525ms step_avg:96.99ms
step:965/1770 train_time:92626ms step_avg:96.99ms
step:966/1770 train_time:92725ms step_avg:96.99ms
step:967/1770 train_time:92825ms step_avg:97.00ms
step:968/1770 train_time:92925ms step_avg:97.00ms
step:969/1770 train_time:93026ms step_avg:97.00ms
step:970/1770 train_time:93125ms step_avg:97.01ms
step:971/1770 train_time:93226ms step_avg:97.01ms
step:972/1770 train_time:93326ms step_avg:97.01ms
step:973/1770 train_time:93426ms step_avg:97.02ms
step:974/1770 train_time:93526ms step_avg:97.02ms
step:975/1770 train_time:93627ms step_avg:97.02ms
step:976/1770 train_time:93727ms step_avg:97.03ms
step:977/1770 train_time:93827ms step_avg:97.03ms
step:978/1770 train_time:93927ms step_avg:97.03ms
step:979/1770 train_time:94028ms step_avg:97.04ms
step:980/1770 train_time:94128ms step_avg:97.04ms
step:981/1770 train_time:94228ms step_avg:97.04ms
step:982/1770 train_time:94328ms step_avg:97.05ms
step:983/1770 train_time:94429ms step_avg:97.05ms
step:984/1770 train_time:94530ms step_avg:97.05ms
step:985/1770 train_time:94631ms step_avg:97.06ms
step:986/1770 train_time:94731ms step_avg:97.06ms
step:987/1770 train_time:94834ms step_avg:97.07ms
step:988/1770 train_time:94935ms step_avg:97.07ms
step:989/1770 train_time:95039ms step_avg:97.08ms
step:990/1770 train_time:95140ms step_avg:97.08ms
step:991/1770 train_time:95242ms step_avg:97.09ms
step:992/1770 train_time:95342ms step_avg:97.09ms
step:993/1770 train_time:95443ms step_avg:97.09ms
step:994/1770 train_time:95543ms step_avg:97.10ms
step:995/1770 train_time:95644ms step_avg:97.10ms
step:996/1770 train_time:95744ms step_avg:97.10ms
step:997/1770 train_time:95844ms step_avg:97.11ms
step:998/1770 train_time:95944ms step_avg:97.11ms
step:999/1770 train_time:96045ms step_avg:97.11ms
step:1000/1770 train_time:96146ms step_avg:97.12ms
step:1000/1770 val_loss:3.5151 train_time:96245ms step_avg:97.22ms
step:1001/1770 train_time:96266ms step_avg:97.14ms
step:1002/1770 train_time:96355ms step_avg:97.13ms
step:1003/1770 train_time:96456ms step_avg:97.14ms
step:1004/1770 train_time:96557ms step_avg:97.14ms
step:1005/1770 train_time:96656ms step_avg:97.14ms
step:1006/1770 train_time:96756ms step_avg:97.14ms
step:1007/1770 train_time:96856ms step_avg:97.15ms
step:1008/1770 train_time:96956ms step_avg:97.15ms
step:1009/1770 train_time:97056ms step_avg:97.15ms
step:1010/1770 train_time:97156ms step_avg:97.16ms
step:1011/1770 train_time:97258ms step_avg:97.16ms
step:1012/1770 train_time:97361ms step_avg:97.17ms
step:1013/1770 train_time:97463ms step_avg:97.17ms
step:1014/1770 train_time:97564ms step_avg:97.18ms
step:1015/1770 train_time:97665ms step_avg:97.18ms
step:1016/1770 train_time:97766ms step_avg:97.18ms
step:1017/1770 train_time:97866ms step_avg:97.19ms
step:1018/1770 train_time:97967ms step_avg:97.19ms
step:1019/1770 train_time:98068ms step_avg:97.19ms
step:1020/1770 train_time:98170ms step_avg:97.20ms
step:1021/1770 train_time:98271ms step_avg:97.20ms
step:1022/1770 train_time:98372ms step_avg:97.21ms
step:1023/1770 train_time:98472ms step_avg:97.21ms
step:1024/1770 train_time:98572ms step_avg:97.21ms
step:1025/1770 train_time:98672ms step_avg:97.21ms
step:1026/1770 train_time:98772ms step_avg:97.22ms
step:1027/1770 train_time:98873ms step_avg:97.22ms
step:1028/1770 train_time:98972ms step_avg:97.22ms
step:1029/1770 train_time:99072ms step_avg:97.23ms
step:1030/1770 train_time:99173ms step_avg:97.23ms
step:1031/1770 train_time:99273ms step_avg:97.23ms
step:1032/1770 train_time:99373ms step_avg:97.23ms
step:1033/1770 train_time:99473ms step_avg:97.24ms
step:1034/1770 train_time:99574ms step_avg:97.24ms
step:1035/1770 train_time:99674ms step_avg:97.24ms
step:1036/1770 train_time:99774ms step_avg:97.25ms
step:1037/1770 train_time:99874ms step_avg:97.25ms
step:1038/1770 train_time:99974ms step_avg:97.25ms
step:1039/1770 train_time:100074ms step_avg:97.25ms
step:1040/1770 train_time:100174ms step_avg:97.26ms
step:1041/1770 train_time:100274ms step_avg:97.26ms
step:1042/1770 train_time:100375ms step_avg:97.26ms
step:1043/1770 train_time:100475ms step_avg:97.27ms
step:1044/1770 train_time:100576ms step_avg:97.27ms
step:1045/1770 train_time:100676ms step_avg:97.27ms
step:1046/1770 train_time:100776ms step_avg:97.27ms
step:1047/1770 train_time:100877ms step_avg:97.28ms
step:1048/1770 train_time:100977ms step_avg:97.28ms
step:1049/1770 train_time:101077ms step_avg:97.28ms
step:1050/1770 train_time:101179ms step_avg:97.29ms
step:1051/1770 train_time:101281ms step_avg:97.29ms
step:1052/1770 train_time:101383ms step_avg:97.30ms
step:1053/1770 train_time:101484ms step_avg:97.30ms
step:1054/1770 train_time:101586ms step_avg:97.30ms
step:1055/1770 train_time:101687ms step_avg:97.31ms
step:1056/1770 train_time:101788ms step_avg:97.31ms
step:1057/1770 train_time:101890ms step_avg:97.32ms
step:1058/1770 train_time:101991ms step_avg:97.32ms
step:1059/1770 train_time:102091ms step_avg:97.32ms
step:1060/1770 train_time:102191ms step_avg:97.33ms
step:1061/1770 train_time:102292ms step_avg:97.33ms
step:1062/1770 train_time:102393ms step_avg:97.33ms
step:1063/1770 train_time:102495ms step_avg:97.34ms
step:1064/1770 train_time:102595ms step_avg:97.34ms
step:1065/1770 train_time:102696ms step_avg:97.34ms
step:1066/1770 train_time:102796ms step_avg:97.34ms
step:1067/1770 train_time:102897ms step_avg:97.35ms
step:1068/1770 train_time:102999ms step_avg:97.35ms
step:1069/1770 train_time:103100ms step_avg:97.36ms
step:1070/1770 train_time:103202ms step_avg:97.36ms
step:1071/1770 train_time:103303ms step_avg:97.36ms
step:1072/1770 train_time:103404ms step_avg:97.37ms
step:1073/1770 train_time:103506ms step_avg:97.37ms
step:1074/1770 train_time:103608ms step_avg:97.38ms
step:1075/1770 train_time:103709ms step_avg:97.38ms
step:1076/1770 train_time:103811ms step_avg:97.38ms
step:1077/1770 train_time:103912ms step_avg:97.39ms
step:1078/1770 train_time:104013ms step_avg:97.39ms
step:1079/1770 train_time:104113ms step_avg:97.39ms
step:1080/1770 train_time:104213ms step_avg:97.40ms
step:1081/1770 train_time:104313ms step_avg:97.40ms
step:1082/1770 train_time:104413ms step_avg:97.40ms
step:1083/1770 train_time:104513ms step_avg:97.40ms
step:1084/1770 train_time:104614ms step_avg:97.41ms
step:1085/1770 train_time:104714ms step_avg:97.41ms
step:1086/1770 train_time:104814ms step_avg:97.41ms
step:1087/1770 train_time:104914ms step_avg:97.41ms
step:1088/1770 train_time:105015ms step_avg:97.42ms
step:1089/1770 train_time:105115ms step_avg:97.42ms
step:1090/1770 train_time:105217ms step_avg:97.42ms
step:1091/1770 train_time:105317ms step_avg:97.43ms
step:1092/1770 train_time:105417ms step_avg:97.43ms
step:1093/1770 train_time:105518ms step_avg:97.43ms
step:1094/1770 train_time:105620ms step_avg:97.44ms
step:1095/1770 train_time:105721ms step_avg:97.44ms
step:1096/1770 train_time:105823ms step_avg:97.44ms
step:1097/1770 train_time:105924ms step_avg:97.45ms
step:1098/1770 train_time:106025ms step_avg:97.45ms
step:1099/1770 train_time:106126ms step_avg:97.45ms
step:1100/1770 train_time:106228ms step_avg:97.46ms
step:1101/1770 train_time:106329ms step_avg:97.46ms
step:1102/1770 train_time:106430ms step_avg:97.46ms
step:1103/1770 train_time:106531ms step_avg:97.47ms
step:1104/1770 train_time:106633ms step_avg:97.47ms
step:1105/1770 train_time:106734ms step_avg:97.47ms
step:1106/1770 train_time:106834ms step_avg:97.48ms
step:1107/1770 train_time:106934ms step_avg:97.48ms
step:1108/1770 train_time:107034ms step_avg:97.48ms
step:1109/1770 train_time:107134ms step_avg:97.48ms
step:1110/1770 train_time:107235ms step_avg:97.49ms
step:1111/1770 train_time:107336ms step_avg:97.49ms
step:1112/1770 train_time:107436ms step_avg:97.49ms
step:1113/1770 train_time:107537ms step_avg:97.49ms
step:1114/1770 train_time:107638ms step_avg:97.50ms
step:1115/1770 train_time:107739ms step_avg:97.50ms
step:1116/1770 train_time:107841ms step_avg:97.51ms
step:1117/1770 train_time:107942ms step_avg:97.51ms
step:1118/1770 train_time:108044ms step_avg:97.51ms
step:1119/1770 train_time:108146ms step_avg:97.52ms
step:1120/1770 train_time:108247ms step_avg:97.52ms
step:1121/1770 train_time:108347ms step_avg:97.52ms
step:1122/1770 train_time:108448ms step_avg:97.53ms
step:1123/1770 train_time:108549ms step_avg:97.53ms
step:1124/1770 train_time:108651ms step_avg:97.53ms
step:1125/1770 train_time:108752ms step_avg:97.54ms
step:1125/1770 val_loss:3.4751 train_time:108851ms step_avg:97.62ms
step:1126/1770 train_time:108872ms step_avg:97.56ms
step:1127/1770 train_time:108960ms step_avg:97.55ms
step:1128/1770 train_time:109061ms step_avg:97.55ms
step:1129/1770 train_time:109161ms step_avg:97.55ms
step:1130/1770 train_time:109262ms step_avg:97.56ms
step:1131/1770 train_time:109363ms step_avg:97.56ms
step:1132/1770 train_time:109464ms step_avg:97.56ms
step:1133/1770 train_time:109565ms step_avg:97.56ms
step:1134/1770 train_time:109666ms step_avg:97.57ms
step:1135/1770 train_time:109767ms step_avg:97.57ms
step:1136/1770 train_time:109870ms step_avg:97.58ms
step:1137/1770 train_time:109972ms step_avg:97.58ms
step:1138/1770 train_time:110072ms step_avg:97.58ms
step:1139/1770 train_time:110173ms step_avg:97.58ms
step:1140/1770 train_time:110273ms step_avg:97.59ms
step:1141/1770 train_time:110373ms step_avg:97.59ms
step:1142/1770 train_time:110473ms step_avg:97.59ms
step:1143/1770 train_time:110572ms step_avg:97.59ms
step:1144/1770 train_time:110673ms step_avg:97.60ms
step:1145/1770 train_time:110774ms step_avg:97.60ms
step:1146/1770 train_time:110876ms step_avg:97.60ms
step:1147/1770 train_time:110977ms step_avg:97.61ms
step:1148/1770 train_time:111079ms step_avg:97.61ms
step:1149/1770 train_time:111180ms step_avg:97.61ms
step:1150/1770 train_time:111282ms step_avg:97.62ms
step:1151/1770 train_time:111384ms step_avg:97.62ms
step:1152/1770 train_time:111485ms step_avg:97.62ms
step:1153/1770 train_time:111586ms step_avg:97.63ms
step:1154/1770 train_time:111688ms step_avg:97.63ms
step:1155/1770 train_time:111789ms step_avg:97.63ms
step:1156/1770 train_time:111889ms step_avg:97.63ms
step:1157/1770 train_time:111991ms step_avg:97.64ms
step:1158/1770 train_time:112093ms step_avg:97.64ms
step:1159/1770 train_time:112192ms step_avg:97.64ms
step:1160/1770 train_time:112293ms step_avg:97.65ms
step:1161/1770 train_time:112393ms step_avg:97.65ms
step:1162/1770 train_time:112494ms step_avg:97.65ms
step:1163/1770 train_time:112596ms step_avg:97.65ms
step:1164/1770 train_time:112697ms step_avg:97.66ms
step:1165/1770 train_time:112799ms step_avg:97.66ms
step:1166/1770 train_time:112901ms step_avg:97.67ms
step:1167/1770 train_time:113003ms step_avg:97.67ms
step:1168/1770 train_time:113104ms step_avg:97.67ms
step:1169/1770 train_time:113206ms step_avg:97.68ms
step:1170/1770 train_time:113306ms step_avg:97.68ms
step:1171/1770 train_time:113408ms step_avg:97.68ms
step:1172/1770 train_time:113508ms step_avg:97.68ms
step:1173/1770 train_time:113609ms step_avg:97.69ms
step:1174/1770 train_time:113710ms step_avg:97.69ms
step:1175/1770 train_time:113810ms step_avg:97.69ms
step:1176/1770 train_time:113911ms step_avg:97.69ms
step:1177/1770 train_time:114012ms step_avg:97.70ms
step:1178/1770 train_time:114113ms step_avg:97.70ms
step:1179/1770 train_time:114214ms step_avg:97.70ms
step:1180/1770 train_time:114315ms step_avg:97.70ms
step:1181/1770 train_time:114416ms step_avg:97.71ms
step:1182/1770 train_time:114517ms step_avg:97.71ms
step:1183/1770 train_time:114619ms step_avg:97.71ms
step:1184/1770 train_time:114722ms step_avg:97.72ms
step:1185/1770 train_time:114825ms step_avg:97.72ms
step:1186/1770 train_time:114928ms step_avg:97.73ms
step:1187/1770 train_time:115032ms step_avg:97.73ms
step:1188/1770 train_time:115133ms step_avg:97.74ms
step:1189/1770 train_time:115235ms step_avg:97.74ms
step:1190/1770 train_time:115336ms step_avg:97.74ms
step:1191/1770 train_time:115438ms step_avg:97.75ms
step:1192/1770 train_time:115541ms step_avg:97.75ms
step:1193/1770 train_time:115644ms step_avg:97.75ms
step:1194/1770 train_time:115745ms step_avg:97.76ms
step:1195/1770 train_time:115847ms step_avg:97.76ms
step:1196/1770 train_time:115951ms step_avg:97.77ms
step:1197/1770 train_time:116052ms step_avg:97.77ms
step:1198/1770 train_time:116153ms step_avg:97.77ms
step:1199/1770 train_time:116255ms step_avg:97.78ms
step:1200/1770 train_time:116357ms step_avg:97.78ms
step:1201/1770 train_time:116459ms step_avg:97.78ms
step:1202/1770 train_time:116560ms step_avg:97.79ms
step:1203/1770 train_time:116662ms step_avg:97.79ms
step:1204/1770 train_time:116765ms step_avg:97.79ms
step:1205/1770 train_time:116868ms step_avg:97.80ms
step:1206/1770 train_time:116970ms step_avg:97.80ms
step:1207/1770 train_time:117072ms step_avg:97.80ms
step:1208/1770 train_time:117174ms step_avg:97.81ms
step:1209/1770 train_time:117276ms step_avg:97.81ms
step:1210/1770 train_time:117378ms step_avg:97.81ms
step:1211/1770 train_time:117481ms step_avg:97.82ms
step:1212/1770 train_time:117584ms step_avg:97.82ms
step:1213/1770 train_time:117686ms step_avg:97.83ms
step:1214/1770 train_time:117788ms step_avg:97.83ms
step:1215/1770 train_time:117890ms step_avg:97.83ms
step:1216/1770 train_time:117994ms step_avg:97.84ms
step:1217/1770 train_time:118097ms step_avg:97.84ms
step:1218/1770 train_time:118199ms step_avg:97.85ms
step:1219/1770 train_time:118301ms step_avg:97.85ms
step:1220/1770 train_time:118404ms step_avg:97.85ms
step:1221/1770 train_time:118505ms step_avg:97.86ms
step:1222/1770 train_time:118609ms step_avg:97.86ms
step:1223/1770 train_time:118711ms step_avg:97.87ms
step:1224/1770 train_time:118814ms step_avg:97.87ms
step:1225/1770 train_time:118915ms step_avg:97.87ms
step:1226/1770 train_time:119017ms step_avg:97.88ms
step:1227/1770 train_time:119121ms step_avg:97.88ms
step:1228/1770 train_time:119225ms step_avg:97.89ms
step:1229/1770 train_time:119327ms step_avg:97.89ms
step:1230/1770 train_time:119429ms step_avg:97.89ms
step:1231/1770 train_time:119531ms step_avg:97.90ms
step:1232/1770 train_time:119633ms step_avg:97.90ms
step:1233/1770 train_time:119735ms step_avg:97.90ms
step:1234/1770 train_time:119836ms step_avg:97.91ms
step:1235/1770 train_time:119938ms step_avg:97.91ms
step:1236/1770 train_time:120040ms step_avg:97.91ms
step:1237/1770 train_time:120143ms step_avg:97.92ms
step:1238/1770 train_time:120246ms step_avg:97.92ms
step:1239/1770 train_time:120348ms step_avg:97.92ms
step:1240/1770 train_time:120450ms step_avg:97.93ms
step:1241/1770 train_time:120553ms step_avg:97.93ms
step:1242/1770 train_time:120654ms step_avg:97.93ms
step:1243/1770 train_time:120756ms step_avg:97.94ms
step:1244/1770 train_time:120857ms step_avg:97.94ms
step:1245/1770 train_time:120959ms step_avg:97.94ms
step:1246/1770 train_time:121061ms step_avg:97.95ms
step:1247/1770 train_time:121164ms step_avg:97.95ms
step:1248/1770 train_time:121267ms step_avg:97.95ms
step:1249/1770 train_time:121369ms step_avg:97.96ms
step:1250/1770 train_time:121471ms step_avg:97.96ms
step:1250/1770 val_loss:3.4269 train_time:121572ms step_avg:98.04ms
step:1251/1770 train_time:121594ms step_avg:97.98ms
step:1252/1770 train_time:121686ms step_avg:97.98ms
step:1253/1770 train_time:121788ms step_avg:97.98ms
step:1254/1770 train_time:121890ms step_avg:97.98ms
step:1255/1770 train_time:121995ms step_avg:97.99ms
step:1256/1770 train_time:122096ms step_avg:97.99ms
step:1257/1770 train_time:122197ms step_avg:97.99ms
step:1258/1770 train_time:122299ms step_avg:98.00ms
step:1259/1770 train_time:122401ms step_avg:98.00ms
step:1260/1770 train_time:122502ms step_avg:98.00ms
step:1261/1770 train_time:122606ms step_avg:98.01ms
step:1262/1770 train_time:122710ms step_avg:98.01ms
step:1263/1770 train_time:122813ms step_avg:98.01ms
step:1264/1770 train_time:122915ms step_avg:98.02ms
step:1265/1770 train_time:123017ms step_avg:98.02ms
step:1266/1770 train_time:123119ms step_avg:98.02ms
step:1267/1770 train_time:123221ms step_avg:98.03ms
step:1268/1770 train_time:123323ms step_avg:98.03ms
step:1269/1770 train_time:123425ms step_avg:98.03ms
step:1270/1770 train_time:123527ms step_avg:98.04ms
step:1271/1770 train_time:123630ms step_avg:98.04ms
step:1272/1770 train_time:123731ms step_avg:98.04ms
step:1273/1770 train_time:123835ms step_avg:98.05ms
step:1274/1770 train_time:123938ms step_avg:98.05ms
step:1275/1770 train_time:124040ms step_avg:98.06ms
step:1276/1770 train_time:124144ms step_avg:98.06ms
step:1277/1770 train_time:124245ms step_avg:98.06ms
step:1278/1770 train_time:124349ms step_avg:98.07ms
step:1279/1770 train_time:124452ms step_avg:98.07ms
step:1280/1770 train_time:124555ms step_avg:98.08ms
step:1281/1770 train_time:124657ms step_avg:98.08ms
step:1282/1770 train_time:124759ms step_avg:98.08ms
step:1283/1770 train_time:124861ms step_avg:98.08ms
step:1284/1770 train_time:124964ms step_avg:98.09ms
step:1285/1770 train_time:125066ms step_avg:98.09ms
step:1286/1770 train_time:125170ms step_avg:98.10ms
step:1287/1770 train_time:125274ms step_avg:98.10ms
step:1288/1770 train_time:125377ms step_avg:98.10ms
step:1289/1770 train_time:125478ms step_avg:98.11ms
step:1290/1770 train_time:125579ms step_avg:98.11ms
step:1291/1770 train_time:125681ms step_avg:98.11ms
step:1292/1770 train_time:125782ms step_avg:98.11ms
step:1293/1770 train_time:125885ms step_avg:98.12ms
step:1294/1770 train_time:125986ms step_avg:98.12ms
step:1295/1770 train_time:126089ms step_avg:98.12ms
step:1296/1770 train_time:126192ms step_avg:98.13ms
step:1297/1770 train_time:126294ms step_avg:98.13ms
step:1298/1770 train_time:126397ms step_avg:98.13ms
step:1299/1770 train_time:126499ms step_avg:98.14ms
step:1300/1770 train_time:126601ms step_avg:98.14ms
step:1301/1770 train_time:126703ms step_avg:98.14ms
step:1302/1770 train_time:126804ms step_avg:98.15ms
step:1303/1770 train_time:126906ms step_avg:98.15ms
step:1304/1770 train_time:127008ms step_avg:98.15ms
step:1305/1770 train_time:127111ms step_avg:98.16ms
step:1306/1770 train_time:127213ms step_avg:98.16ms
step:1307/1770 train_time:127316ms step_avg:98.16ms
step:1308/1770 train_time:127419ms step_avg:98.17ms
step:1309/1770 train_time:127521ms step_avg:98.17ms
step:1310/1770 train_time:127623ms step_avg:98.17ms
step:1311/1770 train_time:127724ms step_avg:98.17ms
step:1312/1770 train_time:127826ms step_avg:98.18ms
step:1313/1770 train_time:127927ms step_avg:98.18ms
step:1314/1770 train_time:128030ms step_avg:98.18ms
step:1315/1770 train_time:128132ms step_avg:98.19ms
step:1316/1770 train_time:128235ms step_avg:98.19ms
step:1317/1770 train_time:128338ms step_avg:98.19ms
step:1318/1770 train_time:128444ms step_avg:98.20ms
step:1319/1770 train_time:128547ms step_avg:98.20ms
step:1320/1770 train_time:128649ms step_avg:98.21ms
step:1321/1770 train_time:128752ms step_avg:98.21ms
step:1322/1770 train_time:128853ms step_avg:98.21ms
step:1323/1770 train_time:128957ms step_avg:98.22ms
step:1324/1770 train_time:129059ms step_avg:98.22ms
step:1325/1770 train_time:129162ms step_avg:98.22ms
step:1326/1770 train_time:129263ms step_avg:98.22ms
step:1327/1770 train_time:129368ms step_avg:98.23ms
step:1328/1770 train_time:129470ms step_avg:98.23ms
step:1329/1770 train_time:129573ms step_avg:98.24ms
step:1330/1770 train_time:129676ms step_avg:98.24ms
step:1331/1770 train_time:129778ms step_avg:98.24ms
step:1332/1770 train_time:129879ms step_avg:98.24ms
step:1333/1770 train_time:129981ms step_avg:98.25ms
step:1334/1770 train_time:130082ms step_avg:98.25ms
step:1335/1770 train_time:130184ms step_avg:98.25ms
step:1336/1770 train_time:130285ms step_avg:98.25ms
step:1337/1770 train_time:130387ms step_avg:98.26ms
step:1338/1770 train_time:130490ms step_avg:98.26ms
step:1339/1770 train_time:130593ms step_avg:98.26ms
step:1340/1770 train_time:130698ms step_avg:98.27ms
step:1341/1770 train_time:130800ms step_avg:98.27ms
step:1342/1770 train_time:130902ms step_avg:98.28ms
step:1343/1770 train_time:131004ms step_avg:98.28ms
step:1344/1770 train_time:131107ms step_avg:98.28ms
step:1345/1770 train_time:131208ms step_avg:98.28ms
step:1346/1770 train_time:131310ms step_avg:98.29ms
step:1347/1770 train_time:131413ms step_avg:98.29ms
step:1348/1770 train_time:131519ms step_avg:98.29ms
step:1349/1770 train_time:131622ms step_avg:98.30ms
step:1350/1770 train_time:131724ms step_avg:98.30ms
step:1351/1770 train_time:131827ms step_avg:98.31ms
step:1352/1770 train_time:131930ms step_avg:98.31ms
step:1353/1770 train_time:132033ms step_avg:98.31ms
step:1354/1770 train_time:132136ms step_avg:98.32ms
step:1355/1770 train_time:132238ms step_avg:98.32ms
step:1356/1770 train_time:132340ms step_avg:98.32ms
step:1357/1770 train_time:132442ms step_avg:98.32ms
step:1358/1770 train_time:132545ms step_avg:98.33ms
step:1359/1770 train_time:132648ms step_avg:98.33ms
step:1360/1770 train_time:132751ms step_avg:98.33ms
step:1361/1770 train_time:132855ms step_avg:98.34ms
step:1362/1770 train_time:132957ms step_avg:98.34ms
step:1363/1770 train_time:133060ms step_avg:98.34ms
step:1364/1770 train_time:133162ms step_avg:98.35ms
step:1365/1770 train_time:133263ms step_avg:98.35ms
step:1366/1770 train_time:133365ms step_avg:98.35ms
step:1367/1770 train_time:133467ms step_avg:98.35ms
step:1368/1770 train_time:133569ms step_avg:98.36ms
step:1369/1770 train_time:133672ms step_avg:98.36ms
step:1370/1770 train_time:133775ms step_avg:98.36ms
step:1371/1770 train_time:133878ms step_avg:98.37ms
step:1372/1770 train_time:133980ms step_avg:98.37ms
step:1373/1770 train_time:134082ms step_avg:98.37ms
step:1374/1770 train_time:134185ms step_avg:98.38ms
step:1375/1770 train_time:134287ms step_avg:98.38ms
step:1375/1770 val_loss:3.3818 train_time:134388ms step_avg:98.45ms
step:1376/1770 train_time:134410ms step_avg:98.40ms
step:1377/1770 train_time:134496ms step_avg:98.39ms
step:1378/1770 train_time:134598ms step_avg:98.39ms
step:1379/1770 train_time:134700ms step_avg:98.39ms
step:1380/1770 train_time:134802ms step_avg:98.40ms
step:1381/1770 train_time:134905ms step_avg:98.40ms
step:1382/1770 train_time:135006ms step_avg:98.40ms
step:1383/1770 train_time:135108ms step_avg:98.40ms
step:1384/1770 train_time:135210ms step_avg:98.41ms
step:1385/1770 train_time:135311ms step_avg:98.41ms
step:1386/1770 train_time:135414ms step_avg:98.41ms
step:1387/1770 train_time:135517ms step_avg:98.41ms
step:1388/1770 train_time:135619ms step_avg:98.42ms
step:1389/1770 train_time:135722ms step_avg:98.42ms
step:1390/1770 train_time:135824ms step_avg:98.42ms
step:1391/1770 train_time:135926ms step_avg:98.43ms
step:1392/1770 train_time:136029ms step_avg:98.43ms
step:1393/1770 train_time:136130ms step_avg:98.43ms
step:1394/1770 train_time:136231ms step_avg:98.43ms
step:1395/1770 train_time:136334ms step_avg:98.44ms
step:1396/1770 train_time:136438ms step_avg:98.44ms
step:1397/1770 train_time:136540ms step_avg:98.44ms
step:1398/1770 train_time:136642ms step_avg:98.45ms
step:1399/1770 train_time:136745ms step_avg:98.45ms
step:1400/1770 train_time:136847ms step_avg:98.45ms
step:1401/1770 train_time:136949ms step_avg:98.45ms
step:1402/1770 train_time:137050ms step_avg:98.46ms
step:1403/1770 train_time:137152ms step_avg:98.46ms
step:1404/1770 train_time:137254ms step_avg:98.46ms
step:1405/1770 train_time:137356ms step_avg:98.46ms
step:1406/1770 train_time:137458ms step_avg:98.47ms
step:1407/1770 train_time:137562ms step_avg:98.47ms
step:1408/1770 train_time:137664ms step_avg:98.47ms
step:1409/1770 train_time:137767ms step_avg:98.48ms
step:1410/1770 train_time:137869ms step_avg:98.48ms
step:1411/1770 train_time:137971ms step_avg:98.48ms
step:1412/1770 train_time:138073ms step_avg:98.48ms
step:1413/1770 train_time:138175ms step_avg:98.49ms
step:1414/1770 train_time:138278ms step_avg:98.49ms
step:1415/1770 train_time:138381ms step_avg:98.49ms
step:1416/1770 train_time:138484ms step_avg:98.50ms
step:1417/1770 train_time:138587ms step_avg:98.50ms
step:1418/1770 train_time:138689ms step_avg:98.50ms
step:1419/1770 train_time:138791ms step_avg:98.50ms
step:1420/1770 train_time:138893ms step_avg:98.51ms
step:1421/1770 train_time:138995ms step_avg:98.51ms
step:1422/1770 train_time:139098ms step_avg:98.51ms
step:1423/1770 train_time:139200ms step_avg:98.51ms
step:1424/1770 train_time:139303ms step_avg:98.52ms
step:1425/1770 train_time:139406ms step_avg:98.52ms
step:1426/1770 train_time:139508ms step_avg:98.52ms
step:1427/1770 train_time:139609ms step_avg:98.52ms
step:1428/1770 train_time:139712ms step_avg:98.53ms
step:1429/1770 train_time:139816ms step_avg:98.53ms
step:1430/1770 train_time:139917ms step_avg:98.53ms
step:1431/1770 train_time:140021ms step_avg:98.54ms
step:1432/1770 train_time:140122ms step_avg:98.54ms
step:1433/1770 train_time:140224ms step_avg:98.54ms
step:1434/1770 train_time:140326ms step_avg:98.54ms
step:1435/1770 train_time:140428ms step_avg:98.55ms
step:1436/1770 train_time:140532ms step_avg:98.55ms
step:1437/1770 train_time:140634ms step_avg:98.55ms
step:1438/1770 train_time:140735ms step_avg:98.55ms
step:1439/1770 train_time:140837ms step_avg:98.56ms
step:1440/1770 train_time:140939ms step_avg:98.56ms
step:1441/1770 train_time:141045ms step_avg:98.56ms
step:1442/1770 train_time:141146ms step_avg:98.57ms
step:1443/1770 train_time:141248ms step_avg:98.57ms
step:1444/1770 train_time:141351ms step_avg:98.57ms
step:1445/1770 train_time:141455ms step_avg:98.57ms
step:1446/1770 train_time:141558ms step_avg:98.58ms
step:1447/1770 train_time:141662ms step_avg:98.58ms
step:1448/1770 train_time:141765ms step_avg:98.58ms
step:1449/1770 train_time:141869ms step_avg:98.59ms
step:1450/1770 train_time:141971ms step_avg:98.59ms
step:1451/1770 train_time:142074ms step_avg:98.59ms
step:1452/1770 train_time:142178ms step_avg:98.60ms
step:1453/1770 train_time:142281ms step_avg:98.60ms
step:1454/1770 train_time:142385ms step_avg:98.60ms
step:1455/1770 train_time:142488ms step_avg:98.61ms
step:1456/1770 train_time:142592ms step_avg:98.61ms
step:1457/1770 train_time:142695ms step_avg:98.61ms
step:1458/1770 train_time:142800ms step_avg:98.62ms
step:1459/1770 train_time:142905ms step_avg:98.62ms
step:1460/1770 train_time:143008ms step_avg:98.63ms
step:1461/1770 train_time:143110ms step_avg:98.63ms
step:1462/1770 train_time:143214ms step_avg:98.63ms
step:1463/1770 train_time:143317ms step_avg:98.64ms
step:1464/1770 train_time:143423ms step_avg:98.64ms
step:1465/1770 train_time:143526ms step_avg:98.64ms
step:1466/1770 train_time:143630ms step_avg:98.65ms
step:1467/1770 train_time:143734ms step_avg:98.65ms
step:1468/1770 train_time:143837ms step_avg:98.65ms
step:1469/1770 train_time:143942ms step_avg:98.66ms
step:1470/1770 train_time:144046ms step_avg:98.66ms
step:1471/1770 train_time:144150ms step_avg:98.67ms
step:1472/1770 train_time:144252ms step_avg:98.67ms
step:1473/1770 train_time:144356ms step_avg:98.67ms
step:1474/1770 train_time:144461ms step_avg:98.68ms
step:1475/1770 train_time:144564ms step_avg:98.68ms
step:1476/1770 train_time:144666ms step_avg:98.68ms
step:1477/1770 train_time:144772ms step_avg:98.69ms
step:1478/1770 train_time:144876ms step_avg:98.69ms
step:1479/1770 train_time:144978ms step_avg:98.69ms
step:1480/1770 train_time:145081ms step_avg:98.69ms
step:1481/1770 train_time:145190ms step_avg:98.70ms
step:1482/1770 train_time:145293ms step_avg:98.70ms
step:1483/1770 train_time:145396ms step_avg:98.71ms
step:1484/1770 train_time:145498ms step_avg:98.71ms
step:1485/1770 train_time:145602ms step_avg:98.71ms
step:1486/1770 train_time:145707ms step_avg:98.72ms
step:1487/1770 train_time:145810ms step_avg:98.72ms
step:1488/1770 train_time:145913ms step_avg:98.72ms
step:1489/1770 train_time:146017ms step_avg:98.73ms
step:1490/1770 train_time:146122ms step_avg:98.73ms
step:1491/1770 train_time:146225ms step_avg:98.73ms
step:1492/1770 train_time:146330ms step_avg:98.74ms
step:1493/1770 train_time:146435ms step_avg:98.74ms
step:1494/1770 train_time:146543ms step_avg:98.75ms
step:1495/1770 train_time:146645ms step_avg:98.75ms
step:1496/1770 train_time:146748ms step_avg:98.75ms
step:1497/1770 train_time:146851ms step_avg:98.76ms
step:1498/1770 train_time:146953ms step_avg:98.76ms
step:1499/1770 train_time:147057ms step_avg:98.76ms
step:1500/1770 train_time:147160ms step_avg:98.76ms
step:1500/1770 val_loss:3.3444 train_time:147261ms step_avg:98.83ms
step:1501/1770 train_time:147283ms step_avg:98.78ms
step:1502/1770 train_time:147371ms step_avg:98.77ms
step:1503/1770 train_time:147474ms step_avg:98.78ms
step:1504/1770 train_time:147577ms step_avg:98.78ms
step:1505/1770 train_time:147682ms step_avg:98.78ms
step:1506/1770 train_time:147786ms step_avg:98.79ms
step:1507/1770 train_time:147889ms step_avg:98.79ms
step:1508/1770 train_time:147994ms step_avg:98.79ms
step:1509/1770 train_time:148097ms step_avg:98.80ms
step:1510/1770 train_time:148199ms step_avg:98.80ms
step:1511/1770 train_time:148305ms step_avg:98.80ms
step:1512/1770 train_time:148410ms step_avg:98.81ms
step:1513/1770 train_time:148514ms step_avg:98.81ms
step:1514/1770 train_time:148617ms step_avg:98.81ms
step:1515/1770 train_time:148720ms step_avg:98.82ms
step:1516/1770 train_time:148824ms step_avg:98.82ms
step:1517/1770 train_time:148927ms step_avg:98.82ms
step:1518/1770 train_time:149033ms step_avg:98.83ms
step:1519/1770 train_time:149135ms step_avg:98.83ms
step:1520/1770 train_time:149239ms step_avg:98.83ms
step:1521/1770 train_time:149342ms step_avg:98.84ms
step:1522/1770 train_time:149446ms step_avg:98.84ms
step:1523/1770 train_time:149551ms step_avg:98.84ms
step:1524/1770 train_time:149653ms step_avg:98.85ms
step:1525/1770 train_time:149755ms step_avg:98.85ms
step:1526/1770 train_time:149858ms step_avg:98.85ms
step:1527/1770 train_time:149962ms step_avg:98.85ms
step:1528/1770 train_time:150067ms step_avg:98.86ms
step:1529/1770 train_time:150170ms step_avg:98.86ms
step:1530/1770 train_time:150273ms step_avg:98.86ms
step:1531/1770 train_time:150376ms step_avg:98.87ms
step:1532/1770 train_time:150479ms step_avg:98.87ms
step:1533/1770 train_time:150583ms step_avg:98.87ms
step:1534/1770 train_time:150688ms step_avg:98.88ms
step:1535/1770 train_time:150791ms step_avg:98.88ms
step:1536/1770 train_time:150894ms step_avg:98.88ms
step:1537/1770 train_time:150997ms step_avg:98.88ms
step:1538/1770 train_time:151101ms step_avg:98.89ms
step:1539/1770 train_time:151204ms step_avg:98.89ms
step:1540/1770 train_time:151310ms step_avg:98.90ms
step:1541/1770 train_time:151414ms step_avg:98.90ms
step:1542/1770 train_time:151517ms step_avg:98.90ms
step:1543/1770 train_time:151619ms step_avg:98.90ms
step:1544/1770 train_time:151725ms step_avg:98.91ms
step:1545/1770 train_time:151829ms step_avg:98.91ms
step:1546/1770 train_time:151933ms step_avg:98.91ms
step:1547/1770 train_time:152035ms step_avg:98.92ms
step:1548/1770 train_time:152139ms step_avg:98.92ms
step:1549/1770 train_time:152243ms step_avg:98.92ms
step:1550/1770 train_time:152347ms step_avg:98.93ms
step:1551/1770 train_time:152451ms step_avg:98.93ms
step:1552/1770 train_time:152556ms step_avg:98.93ms
step:1553/1770 train_time:152659ms step_avg:98.94ms
step:1554/1770 train_time:152761ms step_avg:98.94ms
step:1555/1770 train_time:152865ms step_avg:98.94ms
step:1556/1770 train_time:152970ms step_avg:98.95ms
step:1557/1770 train_time:153072ms step_avg:98.95ms
step:1558/1770 train_time:153175ms step_avg:98.95ms
step:1559/1770 train_time:153279ms step_avg:98.95ms
step:1560/1770 train_time:153381ms step_avg:98.96ms
step:1561/1770 train_time:153486ms step_avg:98.96ms
step:1562/1770 train_time:153590ms step_avg:98.96ms
step:1563/1770 train_time:153693ms step_avg:98.96ms
step:1564/1770 train_time:153794ms step_avg:98.97ms
step:1565/1770 train_time:153898ms step_avg:98.97ms
step:1566/1770 train_time:154001ms step_avg:98.97ms
step:1567/1770 train_time:154105ms step_avg:98.98ms
step:1568/1770 train_time:154208ms step_avg:98.98ms
step:1569/1770 train_time:154314ms step_avg:98.98ms
step:1570/1770 train_time:154417ms step_avg:98.99ms
step:1571/1770 train_time:154520ms step_avg:98.99ms
step:1572/1770 train_time:154625ms step_avg:98.99ms
step:1573/1770 train_time:154730ms step_avg:99.00ms
step:1574/1770 train_time:154833ms step_avg:99.00ms
step:1575/1770 train_time:154935ms step_avg:99.00ms
step:1576/1770 train_time:155038ms step_avg:99.00ms
step:1577/1770 train_time:155143ms step_avg:99.01ms
step:1578/1770 train_time:155248ms step_avg:99.01ms
step:1579/1770 train_time:155351ms step_avg:99.01ms
step:1580/1770 train_time:155454ms step_avg:99.02ms
step:1581/1770 train_time:155559ms step_avg:99.02ms
step:1582/1770 train_time:155663ms step_avg:99.02ms
step:1583/1770 train_time:155768ms step_avg:99.03ms
step:1584/1770 train_time:155872ms step_avg:99.03ms
step:1585/1770 train_time:155975ms step_avg:99.03ms
step:1586/1770 train_time:156082ms step_avg:99.04ms
step:1587/1770 train_time:156186ms step_avg:99.04ms
step:1588/1770 train_time:156289ms step_avg:99.04ms
step:1589/1770 train_time:156395ms step_avg:99.05ms
step:1590/1770 train_time:156498ms step_avg:99.05ms
step:1591/1770 train_time:156601ms step_avg:99.05ms
step:1592/1770 train_time:156706ms step_avg:99.06ms
step:1593/1770 train_time:156809ms step_avg:99.06ms
step:1594/1770 train_time:156912ms step_avg:99.06ms
step:1595/1770 train_time:157015ms step_avg:99.06ms
step:1596/1770 train_time:157120ms step_avg:99.07ms
step:1597/1770 train_time:157222ms step_avg:99.07ms
step:1598/1770 train_time:157326ms step_avg:99.07ms
step:1599/1770 train_time:157430ms step_avg:99.08ms
step:1600/1770 train_time:157536ms step_avg:99.08ms
step:1601/1770 train_time:157639ms step_avg:99.08ms
step:1602/1770 train_time:157744ms step_avg:99.09ms
step:1603/1770 train_time:157847ms step_avg:99.09ms
step:1604/1770 train_time:157951ms step_avg:99.09ms
step:1605/1770 train_time:158053ms step_avg:99.09ms
step:1606/1770 train_time:158157ms step_avg:99.10ms
step:1607/1770 train_time:158264ms step_avg:99.10ms
step:1608/1770 train_time:158367ms step_avg:99.10ms
step:1609/1770 train_time:158471ms step_avg:99.11ms
step:1610/1770 train_time:158576ms step_avg:99.11ms
step:1611/1770 train_time:158681ms step_avg:99.11ms
step:1612/1770 train_time:158785ms step_avg:99.12ms
step:1613/1770 train_time:158888ms step_avg:99.12ms
step:1614/1770 train_time:158992ms step_avg:99.12ms
step:1615/1770 train_time:159095ms step_avg:99.12ms
step:1616/1770 train_time:159199ms step_avg:99.13ms
step:1617/1770 train_time:159304ms step_avg:99.13ms
step:1618/1770 train_time:159409ms step_avg:99.13ms
step:1619/1770 train_time:159513ms step_avg:99.14ms
step:1620/1770 train_time:159616ms step_avg:99.14ms
step:1621/1770 train_time:159720ms step_avg:99.14ms
step:1622/1770 train_time:159824ms step_avg:99.15ms
step:1623/1770 train_time:159931ms step_avg:99.15ms
step:1624/1770 train_time:160034ms step_avg:99.15ms
step:1625/1770 train_time:160137ms step_avg:99.16ms
step:1625/1770 val_loss:3.3098 train_time:160238ms step_avg:99.22ms
step:1626/1770 train_time:160260ms step_avg:99.17ms
step:1627/1770 train_time:160349ms step_avg:99.16ms
step:1628/1770 train_time:160452ms step_avg:99.17ms
step:1629/1770 train_time:160555ms step_avg:99.17ms
step:1630/1770 train_time:160658ms step_avg:99.17ms
step:1631/1770 train_time:160761ms step_avg:99.17ms
step:1632/1770 train_time:160864ms step_avg:99.18ms
step:1633/1770 train_time:160966ms step_avg:99.18ms
step:1634/1770 train_time:161069ms step_avg:99.18ms
step:1635/1770 train_time:161174ms step_avg:99.18ms
step:1636/1770 train_time:161279ms step_avg:99.19ms
step:1637/1770 train_time:161384ms step_avg:99.19ms
step:1638/1770 train_time:161486ms step_avg:99.19ms
step:1639/1770 train_time:161590ms step_avg:99.20ms
step:1640/1770 train_time:161695ms step_avg:99.20ms
step:1641/1770 train_time:161799ms step_avg:99.20ms
step:1642/1770 train_time:161901ms step_avg:99.20ms
step:1643/1770 train_time:162004ms step_avg:99.21ms
step:1644/1770 train_time:162109ms step_avg:99.21ms
step:1645/1770 train_time:162213ms step_avg:99.21ms
step:1646/1770 train_time:162318ms step_avg:99.22ms
step:1647/1770 train_time:162422ms step_avg:99.22ms
step:1648/1770 train_time:162524ms step_avg:99.22ms
step:1649/1770 train_time:162628ms step_avg:99.22ms
step:1650/1770 train_time:162732ms step_avg:99.23ms
step:1651/1770 train_time:162835ms step_avg:99.23ms
step:1652/1770 train_time:162938ms step_avg:99.23ms
step:1653/1770 train_time:163041ms step_avg:99.23ms
step:1654/1770 train_time:163148ms step_avg:99.24ms
step:1655/1770 train_time:163255ms step_avg:99.24ms
step:1656/1770 train_time:163358ms step_avg:99.25ms
step:1657/1770 train_time:163464ms step_avg:99.25ms
step:1658/1770 train_time:163567ms step_avg:99.25ms
step:1659/1770 train_time:163671ms step_avg:99.25ms
step:1660/1770 train_time:163775ms step_avg:99.26ms
step:1661/1770 train_time:163880ms step_avg:99.26ms
step:1662/1770 train_time:163984ms step_avg:99.26ms
step:1663/1770 train_time:164086ms step_avg:99.27ms
step:1664/1770 train_time:164189ms step_avg:99.27ms
step:1665/1770 train_time:164293ms step_avg:99.27ms
step:1666/1770 train_time:164397ms step_avg:99.27ms
step:1667/1770 train_time:164500ms step_avg:99.28ms
step:1668/1770 train_time:164603ms step_avg:99.28ms
step:1669/1770 train_time:164705ms step_avg:99.28ms
step:1670/1770 train_time:164808ms step_avg:99.28ms
step:1671/1770 train_time:164912ms step_avg:99.28ms
step:1672/1770 train_time:165017ms step_avg:99.29ms
step:1673/1770 train_time:165122ms step_avg:99.29ms
step:1674/1770 train_time:165225ms step_avg:99.29ms
step:1675/1770 train_time:165329ms step_avg:99.30ms
step:1676/1770 train_time:165433ms step_avg:99.30ms
step:1677/1770 train_time:165542ms step_avg:99.31ms
step:1678/1770 train_time:165644ms step_avg:99.31ms
step:1679/1770 train_time:165747ms step_avg:99.31ms
step:1680/1770 train_time:165850ms step_avg:99.31ms
step:1681/1770 train_time:165955ms step_avg:99.31ms
step:1682/1770 train_time:166061ms step_avg:99.32ms
step:1683/1770 train_time:166164ms step_avg:99.32ms
step:1684/1770 train_time:166267ms step_avg:99.32ms
step:1685/1770 train_time:166370ms step_avg:99.33ms
step:1686/1770 train_time:166476ms step_avg:99.33ms
step:1687/1770 train_time:166581ms step_avg:99.33ms
step:1688/1770 train_time:166684ms step_avg:99.33ms
step:1689/1770 train_time:166787ms step_avg:99.34ms
step:1690/1770 train_time:166891ms step_avg:99.34ms
step:1691/1770 train_time:166995ms step_avg:99.34ms
step:1692/1770 train_time:167099ms step_avg:99.35ms
step:1693/1770 train_time:167203ms step_avg:99.35ms
step:1694/1770 train_time:167306ms step_avg:99.35ms
step:1695/1770 train_time:167411ms step_avg:99.35ms
step:1696/1770 train_time:167517ms step_avg:99.36ms
step:1697/1770 train_time:167621ms step_avg:99.36ms
step:1698/1770 train_time:167725ms step_avg:99.36ms
step:1699/1770 train_time:167828ms step_avg:99.37ms
step:1700/1770 train_time:167931ms step_avg:99.37ms
step:1701/1770 train_time:168034ms step_avg:99.37ms
step:1702/1770 train_time:168138ms step_avg:99.37ms
step:1703/1770 train_time:168241ms step_avg:99.37ms
step:1704/1770 train_time:168345ms step_avg:99.38ms
step:1705/1770 train_time:168447ms step_avg:99.38ms
step:1706/1770 train_time:168550ms step_avg:99.38ms
step:1707/1770 train_time:168655ms step_avg:99.38ms
step:1708/1770 train_time:168760ms step_avg:99.39ms
step:1709/1770 train_time:168865ms step_avg:99.39ms
step:1710/1770 train_time:168972ms step_avg:99.40ms
step:1711/1770 train_time:169079ms step_avg:99.40ms
step:1712/1770 train_time:169183ms step_avg:99.40ms
step:1713/1770 train_time:169286ms step_avg:99.40ms
step:1714/1770 train_time:169391ms step_avg:99.41ms
step:1715/1770 train_time:169493ms step_avg:99.41ms
step:1716/1770 train_time:169599ms step_avg:99.41ms
step:1717/1770 train_time:169703ms step_avg:99.42ms
step:1718/1770 train_time:169808ms step_avg:99.42ms
step:1719/1770 train_time:169912ms step_avg:99.42ms
step:1720/1770 train_time:170017ms step_avg:99.43ms
step:1721/1770 train_time:170121ms step_avg:99.43ms
step:1722/1770 train_time:170227ms step_avg:99.43ms
step:1723/1770 train_time:170333ms step_avg:99.44ms
step:1724/1770 train_time:170438ms step_avg:99.44ms
step:1725/1770 train_time:170545ms step_avg:99.44ms
step:1726/1770 train_time:170651ms step_avg:99.45ms
step:1727/1770 train_time:170755ms step_avg:99.45ms
step:1728/1770 train_time:170861ms step_avg:99.45ms
step:1729/1770 train_time:170964ms step_avg:99.46ms
step:1730/1770 train_time:171070ms step_avg:99.46ms
step:1731/1770 train_time:171176ms step_avg:99.46ms
step:1732/1770 train_time:171280ms step_avg:99.47ms
step:1733/1770 train_time:171385ms step_avg:99.47ms
step:1734/1770 train_time:171489ms step_avg:99.47ms
step:1735/1770 train_time:171594ms step_avg:99.47ms
step:1736/1770 train_time:171697ms step_avg:99.48ms
step:1737/1770 train_time:171801ms step_avg:99.48ms
step:1738/1770 train_time:171905ms step_avg:99.48ms
step:1739/1770 train_time:172009ms step_avg:99.48ms
step:1740/1770 train_time:172113ms step_avg:99.49ms
step:1741/1770 train_time:172220ms step_avg:99.49ms
step:1742/1770 train_time:172327ms step_avg:99.50ms
step:1743/1770 train_time:172432ms step_avg:99.50ms
step:1744/1770 train_time:172537ms step_avg:99.50ms
step:1745/1770 train_time:172641ms step_avg:99.50ms
step:1746/1770 train_time:172747ms step_avg:99.51ms
step:1747/1770 train_time:172850ms step_avg:99.51ms
step:1748/1770 train_time:172956ms step_avg:99.51ms
step:1749/1770 train_time:173061ms step_avg:99.52ms
step:1750/1770 train_time:173165ms step_avg:99.52ms
step:1750/1770 val_loss:3.2828 train_time:173268ms step_avg:99.58ms
step:1751/1770 train_time:173289ms step_avg:99.53ms
step:1752/1770 train_time:173379ms step_avg:99.53ms
step:1753/1770 train_time:173482ms step_avg:99.53ms
step:1754/1770 train_time:173587ms step_avg:99.53ms
step:1755/1770 train_time:173691ms step_avg:99.54ms
step:1756/1770 train_time:173796ms step_avg:99.54ms
step:1757/1770 train_time:173900ms step_avg:99.54ms
step:1758/1770 train_time:174003ms step_avg:99.54ms
step:1759/1770 train_time:174108ms step_avg:99.55ms
step:1760/1770 train_time:174214ms step_avg:99.55ms
step:1761/1770 train_time:174320ms step_avg:99.55ms
step:1762/1770 train_time:174430ms step_avg:99.56ms
step:1763/1770 train_time:174533ms step_avg:99.56ms
step:1764/1770 train_time:174638ms step_avg:99.57ms
step:1765/1770 train_time:174741ms step_avg:99.57ms
step:1766/1770 train_time:174850ms step_avg:99.57ms
step:1767/1770 train_time:174953ms step_avg:99.57ms
step:1768/1770 train_time:175057ms step_avg:99.58ms
step:1769/1770 train_time:175160ms step_avg:99.58ms
step:1770/1770 train_time:175263ms step_avg:99.58ms
step:1770/1770 val_loss:3.2796 train_time:175367ms step_avg:99.64ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
