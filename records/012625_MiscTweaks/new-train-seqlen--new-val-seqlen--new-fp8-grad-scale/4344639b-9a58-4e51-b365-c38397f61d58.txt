import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True # turn this off for a faster compile time (but slightly slower run)

# -----------------------------------------------------------------------------
# Custom operators : FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def mm_backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def mm_setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(mm_backward, setup_context=mm_setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(-2) / p_world.size(-1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, layer_idx: int, max_seq_len: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size: int, embedding_dim: int, num_layers: int, num_embeddings: int = 3):
        super().__init__()
        self.num_layers = num_layers
        self.num_embeddings = num_embeddings
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim) for _ in range(num_embeddings)])

    def forward(self, input_seq: Tensor) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (self.num_layers - 2 * self.num_embeddings) + [ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        self.value_embeds = ValueEmbedding(vocab_size, model_dim, num_layers)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx, max_seq_len) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=2.0, w_s=2.0**5, grad_s=2.0**19)
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def create_block_masks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        any_causal_bm = block_idx[:, None] >= block_idx
        all_causal_bm = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        any_document_bm = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        all_document_bm = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        any_bm = any_causal_bm & any_document_bm
        all_bm = all_causal_bm & all_document_bm
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(any_bm & ~all_bm)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(all_bm)
        def build_bm(sw_num_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(sw_num_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, sw_num_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        long_bm, short_bm = self.create_block_masks(input_seq, sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm]
        assert len(block_masks) == self.num_encoder_layers
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_masks[i])
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        block_masks.reverse()
        assert len(block_masks) == self.num_decoder_layers
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_masks[i])
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    num_iterations = 1770 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_batch_size = world_size * args.seq_len
train_loader = distributed_data_generator(args.train_files, train_batch_size, rank, world_size)

model: nn.Module = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768, max_seq_len=max(args.seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(step: int):
    t = 1 - step / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model, dynamic=False)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size , rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB",
    console=True,
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Tue Jan 28 07:51:22 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   25C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   24C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   27C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0             110W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1770 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1770 train_time:30866ms step_avg:nanms
step:2/1770 train_time:31321ms step_avg:nanms
step:3/1770 train_time:31419ms step_avg:nanms
step:4/1770 train_time:31513ms step_avg:nanms
step:5/1770 train_time:31606ms step_avg:nanms
step:6/1770 train_time:31699ms step_avg:nanms
step:7/1770 train_time:31793ms step_avg:nanms
step:8/1770 train_time:31886ms step_avg:nanms
step:9/1770 train_time:31980ms step_avg:nanms
step:10/1770 train_time:32074ms step_avg:nanms
step:11/1770 train_time:94ms step_avg:nanms
step:12/1770 train_time:187ms step_avg:nanms
step:13/1770 train_time:282ms step_avg:93.98ms
step:14/1770 train_time:376ms step_avg:93.94ms
step:15/1770 train_time:469ms step_avg:93.87ms
step:16/1770 train_time:563ms step_avg:93.80ms
step:17/1770 train_time:656ms step_avg:93.76ms
step:18/1770 train_time:750ms step_avg:93.70ms
step:19/1770 train_time:843ms step_avg:93.70ms
step:20/1770 train_time:938ms step_avg:93.77ms
step:21/1770 train_time:1032ms step_avg:93.77ms
step:22/1770 train_time:1125ms step_avg:93.77ms
step:23/1770 train_time:1220ms step_avg:93.83ms
step:24/1770 train_time:1315ms step_avg:93.93ms
step:25/1770 train_time:1409ms step_avg:93.92ms
step:26/1770 train_time:1502ms step_avg:93.90ms
step:27/1770 train_time:1596ms step_avg:93.90ms
step:28/1770 train_time:1690ms step_avg:93.91ms
step:29/1770 train_time:1784ms step_avg:93.90ms
step:30/1770 train_time:1878ms step_avg:93.90ms
step:31/1770 train_time:1972ms step_avg:93.92ms
step:32/1770 train_time:2066ms step_avg:93.91ms
step:33/1770 train_time:2160ms step_avg:93.93ms
step:34/1770 train_time:2254ms step_avg:93.93ms
step:35/1770 train_time:2348ms step_avg:93.92ms
step:36/1770 train_time:2442ms step_avg:93.92ms
step:37/1770 train_time:2536ms step_avg:93.93ms
step:38/1770 train_time:2631ms step_avg:93.98ms
step:39/1770 train_time:2724ms step_avg:93.92ms
step:40/1770 train_time:2818ms step_avg:93.94ms
step:41/1770 train_time:2912ms step_avg:93.93ms
step:42/1770 train_time:3006ms step_avg:93.93ms
step:43/1770 train_time:3100ms step_avg:93.94ms
step:44/1770 train_time:3194ms step_avg:93.95ms
step:45/1770 train_time:3288ms step_avg:93.95ms
step:46/1770 train_time:3382ms step_avg:93.94ms
step:47/1770 train_time:3476ms step_avg:93.94ms
step:48/1770 train_time:3569ms step_avg:93.93ms
step:49/1770 train_time:3663ms step_avg:93.92ms
step:50/1770 train_time:3757ms step_avg:93.93ms
step:51/1770 train_time:3851ms step_avg:93.92ms
step:52/1770 train_time:3944ms step_avg:93.91ms
step:53/1770 train_time:4038ms step_avg:93.92ms
step:54/1770 train_time:4132ms step_avg:93.91ms
step:55/1770 train_time:4226ms step_avg:93.91ms
step:56/1770 train_time:4320ms step_avg:93.91ms
step:57/1770 train_time:4414ms step_avg:93.91ms
step:58/1770 train_time:4507ms step_avg:93.90ms
step:59/1770 train_time:4601ms step_avg:93.90ms
step:60/1770 train_time:4695ms step_avg:93.91ms
step:61/1770 train_time:4789ms step_avg:93.91ms
step:62/1770 train_time:4883ms step_avg:93.90ms
step:63/1770 train_time:4977ms step_avg:93.91ms
step:64/1770 train_time:5071ms step_avg:93.90ms
step:65/1770 train_time:5164ms step_avg:93.89ms
step:66/1770 train_time:5258ms step_avg:93.89ms
step:67/1770 train_time:5352ms step_avg:93.90ms
step:68/1770 train_time:5446ms step_avg:93.90ms
step:69/1770 train_time:5540ms step_avg:93.89ms
step:70/1770 train_time:5634ms step_avg:93.89ms
step:71/1770 train_time:5727ms step_avg:93.89ms
step:72/1770 train_time:5821ms step_avg:93.89ms
step:73/1770 train_time:5915ms step_avg:93.89ms
step:74/1770 train_time:6009ms step_avg:93.89ms
step:75/1770 train_time:6103ms step_avg:93.89ms
step:76/1770 train_time:6197ms step_avg:93.90ms
step:77/1770 train_time:6291ms step_avg:93.89ms
step:78/1770 train_time:6385ms step_avg:93.89ms
step:79/1770 train_time:6478ms step_avg:93.89ms
step:80/1770 train_time:6573ms step_avg:93.90ms
step:81/1770 train_time:6666ms step_avg:93.89ms
step:82/1770 train_time:6760ms step_avg:93.88ms
step:83/1770 train_time:6853ms step_avg:93.88ms
step:84/1770 train_time:6947ms step_avg:93.88ms
step:85/1770 train_time:7041ms step_avg:93.88ms
step:86/1770 train_time:7135ms step_avg:93.88ms
step:87/1770 train_time:7228ms step_avg:93.87ms
step:88/1770 train_time:7322ms step_avg:93.87ms
step:89/1770 train_time:7416ms step_avg:93.87ms
step:90/1770 train_time:7510ms step_avg:93.87ms
step:91/1770 train_time:7604ms step_avg:93.87ms
step:92/1770 train_time:7698ms step_avg:93.88ms
step:93/1770 train_time:7792ms step_avg:93.87ms
step:94/1770 train_time:7885ms step_avg:93.87ms
step:95/1770 train_time:7979ms step_avg:93.87ms
step:96/1770 train_time:8072ms step_avg:93.87ms
step:97/1770 train_time:8166ms step_avg:93.86ms
step:98/1770 train_time:8260ms step_avg:93.86ms
step:99/1770 train_time:8354ms step_avg:93.86ms
step:100/1770 train_time:8447ms step_avg:93.86ms
step:101/1770 train_time:8541ms step_avg:93.86ms
step:102/1770 train_time:8635ms step_avg:93.86ms
step:103/1770 train_time:8729ms step_avg:93.86ms
step:104/1770 train_time:8822ms step_avg:93.85ms
step:105/1770 train_time:8922ms step_avg:93.91ms
step:106/1770 train_time:9010ms step_avg:93.86ms
step:107/1770 train_time:9104ms step_avg:93.86ms
step:108/1770 train_time:9199ms step_avg:93.86ms
step:109/1770 train_time:9293ms step_avg:93.87ms
step:110/1770 train_time:9387ms step_avg:93.87ms
step:111/1770 train_time:9480ms step_avg:93.87ms
step:112/1770 train_time:9575ms step_avg:93.87ms
step:113/1770 train_time:9668ms step_avg:93.87ms
step:114/1770 train_time:9762ms step_avg:93.87ms
step:115/1770 train_time:9856ms step_avg:93.87ms
step:116/1770 train_time:9950ms step_avg:93.87ms
step:117/1770 train_time:10044ms step_avg:93.86ms
step:118/1770 train_time:10138ms step_avg:93.87ms
step:119/1770 train_time:10232ms step_avg:93.87ms
step:120/1770 train_time:10328ms step_avg:93.89ms
step:121/1770 train_time:10420ms step_avg:93.87ms
step:122/1770 train_time:10514ms step_avg:93.87ms
step:123/1770 train_time:10607ms step_avg:93.87ms
step:124/1770 train_time:10701ms step_avg:93.87ms
step:125/1770 train_time:10795ms step_avg:93.87ms
step:125/1770 val_loss:4.6557 train_time:10887ms step_avg:94.67ms
step:126/1770 train_time:10909ms step_avg:94.04ms
step:127/1770 train_time:10986ms step_avg:93.90ms
step:128/1770 train_time:11083ms step_avg:93.92ms
step:129/1770 train_time:11183ms step_avg:93.97ms
step:130/1770 train_time:11278ms step_avg:93.98ms
step:131/1770 train_time:11373ms step_avg:93.99ms
step:132/1770 train_time:11466ms step_avg:93.99ms
step:133/1770 train_time:11560ms step_avg:93.98ms
step:134/1770 train_time:11654ms step_avg:93.99ms
step:135/1770 train_time:11748ms step_avg:93.99ms
step:136/1770 train_time:11842ms step_avg:93.99ms
step:137/1770 train_time:11936ms step_avg:93.99ms
step:138/1770 train_time:12030ms step_avg:93.99ms
step:139/1770 train_time:12125ms step_avg:93.99ms
step:140/1770 train_time:12220ms step_avg:94.00ms
step:141/1770 train_time:12316ms step_avg:94.01ms
step:142/1770 train_time:12410ms step_avg:94.02ms
step:143/1770 train_time:12505ms step_avg:94.02ms
step:144/1770 train_time:12599ms step_avg:94.02ms
step:145/1770 train_time:12693ms step_avg:94.02ms
step:146/1770 train_time:12788ms step_avg:94.03ms
step:147/1770 train_time:12882ms step_avg:94.03ms
step:148/1770 train_time:12976ms step_avg:94.03ms
step:149/1770 train_time:13071ms step_avg:94.03ms
step:150/1770 train_time:13165ms step_avg:94.04ms
step:151/1770 train_time:13261ms step_avg:94.05ms
step:152/1770 train_time:13355ms step_avg:94.05ms
step:153/1770 train_time:13449ms step_avg:94.05ms
step:154/1770 train_time:13544ms step_avg:94.05ms
step:155/1770 train_time:13638ms step_avg:94.06ms
step:156/1770 train_time:13733ms step_avg:94.06ms
step:157/1770 train_time:13828ms step_avg:94.07ms
step:158/1770 train_time:13922ms step_avg:94.07ms
step:159/1770 train_time:14016ms step_avg:94.07ms
step:160/1770 train_time:14111ms step_avg:94.07ms
step:161/1770 train_time:14205ms step_avg:94.07ms
step:162/1770 train_time:14299ms step_avg:94.08ms
step:163/1770 train_time:14394ms step_avg:94.08ms
step:164/1770 train_time:14489ms step_avg:94.08ms
step:165/1770 train_time:14583ms step_avg:94.08ms
step:166/1770 train_time:14677ms step_avg:94.09ms
step:167/1770 train_time:14773ms step_avg:94.09ms
step:168/1770 train_time:14867ms step_avg:94.09ms
step:169/1770 train_time:14961ms step_avg:94.09ms
step:170/1770 train_time:15055ms step_avg:94.09ms
step:171/1770 train_time:15150ms step_avg:94.10ms
step:172/1770 train_time:15244ms step_avg:94.10ms
step:173/1770 train_time:15339ms step_avg:94.10ms
step:174/1770 train_time:15433ms step_avg:94.11ms
step:175/1770 train_time:15528ms step_avg:94.11ms
step:176/1770 train_time:15622ms step_avg:94.11ms
step:177/1770 train_time:15717ms step_avg:94.11ms
step:178/1770 train_time:15812ms step_avg:94.12ms
step:179/1770 train_time:15906ms step_avg:94.12ms
step:180/1770 train_time:16000ms step_avg:94.12ms
step:181/1770 train_time:16094ms step_avg:94.12ms
step:182/1770 train_time:16189ms step_avg:94.12ms
step:183/1770 train_time:16282ms step_avg:94.12ms
step:184/1770 train_time:16377ms step_avg:94.12ms
step:185/1770 train_time:16471ms step_avg:94.12ms
step:186/1770 train_time:16565ms step_avg:94.12ms
step:187/1770 train_time:16660ms step_avg:94.12ms
step:188/1770 train_time:16758ms step_avg:94.14ms
step:189/1770 train_time:16850ms step_avg:94.13ms
step:190/1770 train_time:16944ms step_avg:94.13ms
step:191/1770 train_time:17039ms step_avg:94.14ms
step:192/1770 train_time:17134ms step_avg:94.14ms
step:193/1770 train_time:17230ms step_avg:94.15ms
step:194/1770 train_time:17324ms step_avg:94.15ms
step:195/1770 train_time:17418ms step_avg:94.15ms
step:196/1770 train_time:17512ms step_avg:94.15ms
step:197/1770 train_time:17607ms step_avg:94.15ms
step:198/1770 train_time:17701ms step_avg:94.15ms
step:199/1770 train_time:17796ms step_avg:94.16ms
step:200/1770 train_time:17891ms step_avg:94.16ms
step:201/1770 train_time:17986ms step_avg:94.17ms
step:202/1770 train_time:18080ms step_avg:94.17ms
step:203/1770 train_time:18174ms step_avg:94.17ms
step:204/1770 train_time:18269ms step_avg:94.17ms
step:205/1770 train_time:18363ms step_avg:94.17ms
step:206/1770 train_time:18458ms step_avg:94.18ms
step:207/1770 train_time:18553ms step_avg:94.18ms
step:208/1770 train_time:18648ms step_avg:94.18ms
step:209/1770 train_time:18742ms step_avg:94.18ms
step:210/1770 train_time:18836ms step_avg:94.18ms
step:211/1770 train_time:18931ms step_avg:94.18ms
step:212/1770 train_time:19025ms step_avg:94.18ms
step:213/1770 train_time:19119ms step_avg:94.18ms
step:214/1770 train_time:19214ms step_avg:94.19ms
step:215/1770 train_time:19309ms step_avg:94.19ms
step:216/1770 train_time:19403ms step_avg:94.19ms
step:217/1770 train_time:19498ms step_avg:94.19ms
step:218/1770 train_time:19592ms step_avg:94.19ms
step:219/1770 train_time:19687ms step_avg:94.19ms
step:220/1770 train_time:19781ms step_avg:94.20ms
step:221/1770 train_time:19875ms step_avg:94.20ms
step:222/1770 train_time:19970ms step_avg:94.20ms
step:223/1770 train_time:20064ms step_avg:94.20ms
step:224/1770 train_time:20159ms step_avg:94.20ms
step:225/1770 train_time:20254ms step_avg:94.21ms
step:226/1770 train_time:20349ms step_avg:94.21ms
step:227/1770 train_time:20443ms step_avg:94.21ms
step:228/1770 train_time:20537ms step_avg:94.21ms
step:229/1770 train_time:20632ms step_avg:94.21ms
step:230/1770 train_time:20726ms step_avg:94.21ms
step:231/1770 train_time:20820ms step_avg:94.21ms
step:232/1770 train_time:20915ms step_avg:94.21ms
step:233/1770 train_time:21010ms step_avg:94.21ms
step:234/1770 train_time:21104ms step_avg:94.21ms
step:235/1770 train_time:21199ms step_avg:94.22ms
step:236/1770 train_time:21294ms step_avg:94.22ms
step:237/1770 train_time:21389ms step_avg:94.23ms
step:238/1770 train_time:21483ms step_avg:94.23ms
step:239/1770 train_time:21579ms step_avg:94.23ms
step:240/1770 train_time:21673ms step_avg:94.23ms
step:241/1770 train_time:21767ms step_avg:94.23ms
step:242/1770 train_time:21861ms step_avg:94.23ms
step:243/1770 train_time:21956ms step_avg:94.23ms
step:244/1770 train_time:22051ms step_avg:94.23ms
step:245/1770 train_time:22145ms step_avg:94.23ms
step:246/1770 train_time:22240ms step_avg:94.24ms
step:247/1770 train_time:22334ms step_avg:94.24ms
step:248/1770 train_time:22429ms step_avg:94.24ms
step:249/1770 train_time:22524ms step_avg:94.24ms
step:250/1770 train_time:22618ms step_avg:94.24ms
step:250/1770 val_loss:4.1176 train_time:22711ms step_avg:94.63ms
step:251/1770 train_time:22733ms step_avg:94.33ms
step:252/1770 train_time:22815ms step_avg:94.28ms
step:253/1770 train_time:22912ms step_avg:94.29ms
step:254/1770 train_time:23006ms step_avg:94.29ms
step:255/1770 train_time:23100ms step_avg:94.29ms
step:256/1770 train_time:23195ms step_avg:94.29ms
step:257/1770 train_time:23289ms step_avg:94.29ms
step:258/1770 train_time:23384ms step_avg:94.29ms
step:259/1770 train_time:23478ms step_avg:94.29ms
step:260/1770 train_time:23572ms step_avg:94.29ms
step:261/1770 train_time:23667ms step_avg:94.29ms
step:262/1770 train_time:23761ms step_avg:94.29ms
step:263/1770 train_time:23857ms step_avg:94.30ms
step:264/1770 train_time:23953ms step_avg:94.30ms
step:265/1770 train_time:24048ms step_avg:94.31ms
step:266/1770 train_time:24143ms step_avg:94.31ms
step:267/1770 train_time:24238ms step_avg:94.31ms
step:268/1770 train_time:24333ms step_avg:94.31ms
step:269/1770 train_time:24428ms step_avg:94.32ms
step:270/1770 train_time:24523ms step_avg:94.32ms
step:271/1770 train_time:24618ms step_avg:94.32ms
step:272/1770 train_time:24713ms step_avg:94.32ms
step:273/1770 train_time:24808ms step_avg:94.33ms
step:274/1770 train_time:24902ms step_avg:94.33ms
step:275/1770 train_time:24998ms step_avg:94.33ms
step:276/1770 train_time:25093ms step_avg:94.34ms
step:277/1770 train_time:25189ms step_avg:94.34ms
step:278/1770 train_time:25284ms step_avg:94.34ms
step:279/1770 train_time:25379ms step_avg:94.35ms
step:280/1770 train_time:25474ms step_avg:94.35ms
step:281/1770 train_time:25569ms step_avg:94.35ms
step:282/1770 train_time:25663ms step_avg:94.35ms
step:283/1770 train_time:25758ms step_avg:94.35ms
step:284/1770 train_time:25853ms step_avg:94.35ms
step:285/1770 train_time:25948ms step_avg:94.36ms
step:286/1770 train_time:26044ms step_avg:94.36ms
step:287/1770 train_time:26138ms step_avg:94.36ms
step:288/1770 train_time:26233ms step_avg:94.37ms
step:289/1770 train_time:26329ms step_avg:94.37ms
step:290/1770 train_time:26424ms step_avg:94.37ms
step:291/1770 train_time:26519ms step_avg:94.37ms
step:292/1770 train_time:26614ms step_avg:94.38ms
step:293/1770 train_time:26709ms step_avg:94.38ms
step:294/1770 train_time:26804ms step_avg:94.38ms
step:295/1770 train_time:26898ms step_avg:94.38ms
step:296/1770 train_time:26994ms step_avg:94.39ms
step:297/1770 train_time:27090ms step_avg:94.39ms
step:298/1770 train_time:27185ms step_avg:94.39ms
step:299/1770 train_time:27280ms step_avg:94.39ms
step:300/1770 train_time:27375ms step_avg:94.40ms
step:301/1770 train_time:27470ms step_avg:94.40ms
step:302/1770 train_time:27565ms step_avg:94.40ms
step:303/1770 train_time:27660ms step_avg:94.40ms
step:304/1770 train_time:27755ms step_avg:94.40ms
step:305/1770 train_time:27850ms step_avg:94.41ms
step:306/1770 train_time:27944ms step_avg:94.41ms
step:307/1770 train_time:28039ms step_avg:94.41ms
step:308/1770 train_time:28134ms step_avg:94.41ms
step:309/1770 train_time:28230ms step_avg:94.41ms
step:310/1770 train_time:28325ms step_avg:94.42ms
step:311/1770 train_time:28420ms step_avg:94.42ms
step:312/1770 train_time:28515ms step_avg:94.42ms
step:313/1770 train_time:28611ms step_avg:94.43ms
step:314/1770 train_time:28706ms step_avg:94.43ms
step:315/1770 train_time:28800ms step_avg:94.43ms
step:316/1770 train_time:28895ms step_avg:94.43ms
step:317/1770 train_time:28991ms step_avg:94.43ms
step:318/1770 train_time:29087ms step_avg:94.44ms
step:319/1770 train_time:29180ms step_avg:94.43ms
step:320/1770 train_time:29275ms step_avg:94.44ms
step:321/1770 train_time:29370ms step_avg:94.44ms
step:322/1770 train_time:29465ms step_avg:94.44ms
step:323/1770 train_time:29560ms step_avg:94.44ms
step:324/1770 train_time:29655ms step_avg:94.44ms
step:325/1770 train_time:29751ms step_avg:94.45ms
step:326/1770 train_time:29846ms step_avg:94.45ms
step:327/1770 train_time:29941ms step_avg:94.45ms
step:328/1770 train_time:30036ms step_avg:94.45ms
step:329/1770 train_time:30131ms step_avg:94.45ms
step:330/1770 train_time:30226ms step_avg:94.46ms
step:331/1770 train_time:30321ms step_avg:94.46ms
step:332/1770 train_time:30416ms step_avg:94.46ms
step:333/1770 train_time:30511ms step_avg:94.46ms
step:334/1770 train_time:30607ms step_avg:94.47ms
step:335/1770 train_time:30701ms step_avg:94.47ms
step:336/1770 train_time:30796ms step_avg:94.47ms
step:337/1770 train_time:30891ms step_avg:94.47ms
step:338/1770 train_time:30986ms step_avg:94.47ms
step:339/1770 train_time:31081ms step_avg:94.47ms
step:340/1770 train_time:31177ms step_avg:94.47ms
step:341/1770 train_time:31272ms step_avg:94.48ms
step:342/1770 train_time:31367ms step_avg:94.48ms
step:343/1770 train_time:31462ms step_avg:94.48ms
step:344/1770 train_time:31557ms step_avg:94.48ms
step:345/1770 train_time:31652ms step_avg:94.48ms
step:346/1770 train_time:31748ms step_avg:94.49ms
step:347/1770 train_time:31843ms step_avg:94.49ms
step:348/1770 train_time:31938ms step_avg:94.49ms
step:349/1770 train_time:32033ms step_avg:94.49ms
step:350/1770 train_time:32129ms step_avg:94.50ms
step:351/1770 train_time:32223ms step_avg:94.50ms
step:352/1770 train_time:32318ms step_avg:94.50ms
step:353/1770 train_time:32413ms step_avg:94.50ms
step:354/1770 train_time:32509ms step_avg:94.50ms
step:355/1770 train_time:32604ms step_avg:94.50ms
step:356/1770 train_time:32699ms step_avg:94.50ms
step:357/1770 train_time:32794ms step_avg:94.51ms
step:358/1770 train_time:32889ms step_avg:94.51ms
step:359/1770 train_time:32984ms step_avg:94.51ms
step:360/1770 train_time:33079ms step_avg:94.51ms
step:361/1770 train_time:33174ms step_avg:94.51ms
step:362/1770 train_time:33270ms step_avg:94.52ms
step:363/1770 train_time:33364ms step_avg:94.52ms
step:364/1770 train_time:33459ms step_avg:94.52ms
step:365/1770 train_time:33554ms step_avg:94.52ms
step:366/1770 train_time:33649ms step_avg:94.52ms
step:367/1770 train_time:33744ms step_avg:94.52ms
step:368/1770 train_time:33839ms step_avg:94.52ms
step:369/1770 train_time:33935ms step_avg:94.53ms
step:370/1770 train_time:34030ms step_avg:94.53ms
step:371/1770 train_time:34124ms step_avg:94.53ms
step:372/1770 train_time:34219ms step_avg:94.53ms
step:373/1770 train_time:34315ms step_avg:94.53ms
step:374/1770 train_time:34409ms step_avg:94.53ms
step:375/1770 train_time:34504ms step_avg:94.53ms
step:375/1770 val_loss:3.9139 train_time:34597ms step_avg:94.79ms
step:376/1770 train_time:34619ms step_avg:94.59ms
step:377/1770 train_time:34699ms step_avg:94.55ms
step:378/1770 train_time:34795ms step_avg:94.55ms
step:379/1770 train_time:34893ms step_avg:94.56ms
step:380/1770 train_time:34985ms step_avg:94.55ms
step:381/1770 train_time:35080ms step_avg:94.55ms
step:382/1770 train_time:35174ms step_avg:94.55ms
step:383/1770 train_time:35269ms step_avg:94.55ms
step:384/1770 train_time:35363ms step_avg:94.55ms
step:385/1770 train_time:35458ms step_avg:94.55ms
step:386/1770 train_time:35553ms step_avg:94.56ms
step:387/1770 train_time:35649ms step_avg:94.56ms
step:388/1770 train_time:35746ms step_avg:94.57ms
step:389/1770 train_time:35841ms step_avg:94.57ms
step:390/1770 train_time:35936ms step_avg:94.57ms
step:391/1770 train_time:36031ms step_avg:94.57ms
step:392/1770 train_time:36125ms step_avg:94.57ms
step:393/1770 train_time:36220ms step_avg:94.57ms
step:394/1770 train_time:36315ms step_avg:94.57ms
step:395/1770 train_time:36410ms step_avg:94.57ms
step:396/1770 train_time:36507ms step_avg:94.58ms
step:397/1770 train_time:36604ms step_avg:94.58ms
step:398/1770 train_time:36703ms step_avg:94.59ms
step:399/1770 train_time:36800ms step_avg:94.60ms
step:400/1770 train_time:36897ms step_avg:94.61ms
step:401/1770 train_time:36994ms step_avg:94.61ms
step:402/1770 train_time:37090ms step_avg:94.62ms
step:403/1770 train_time:37187ms step_avg:94.62ms
step:404/1770 train_time:37284ms step_avg:94.63ms
step:405/1770 train_time:37381ms step_avg:94.64ms
step:406/1770 train_time:37478ms step_avg:94.64ms
step:407/1770 train_time:37576ms step_avg:94.65ms
step:408/1770 train_time:37672ms step_avg:94.65ms
step:409/1770 train_time:37769ms step_avg:94.66ms
step:410/1770 train_time:37866ms step_avg:94.66ms
step:411/1770 train_time:37963ms step_avg:94.67ms
step:412/1770 train_time:38061ms step_avg:94.68ms
step:413/1770 train_time:38158ms step_avg:94.69ms
step:414/1770 train_time:38255ms step_avg:94.69ms
step:415/1770 train_time:38352ms step_avg:94.70ms
step:416/1770 train_time:38448ms step_avg:94.70ms
step:417/1770 train_time:38545ms step_avg:94.71ms
step:418/1770 train_time:38642ms step_avg:94.71ms
step:419/1770 train_time:38739ms step_avg:94.72ms
step:420/1770 train_time:38836ms step_avg:94.72ms
step:421/1770 train_time:38933ms step_avg:94.73ms
step:422/1770 train_time:39029ms step_avg:94.73ms
step:423/1770 train_time:39126ms step_avg:94.74ms
step:424/1770 train_time:39224ms step_avg:94.74ms
step:425/1770 train_time:39322ms step_avg:94.75ms
step:426/1770 train_time:39420ms step_avg:94.76ms
step:427/1770 train_time:39517ms step_avg:94.76ms
step:428/1770 train_time:39614ms step_avg:94.77ms
step:429/1770 train_time:39711ms step_avg:94.77ms
step:430/1770 train_time:39807ms step_avg:94.78ms
step:431/1770 train_time:39904ms step_avg:94.78ms
step:432/1770 train_time:40001ms step_avg:94.79ms
step:433/1770 train_time:40098ms step_avg:94.79ms
step:434/1770 train_time:40195ms step_avg:94.80ms
step:435/1770 train_time:40291ms step_avg:94.80ms
step:436/1770 train_time:40388ms step_avg:94.81ms
step:437/1770 train_time:40485ms step_avg:94.81ms
step:438/1770 train_time:40582ms step_avg:94.82ms
step:439/1770 train_time:40680ms step_avg:94.82ms
step:440/1770 train_time:40781ms step_avg:94.84ms
step:441/1770 train_time:40875ms step_avg:94.84ms
step:442/1770 train_time:40971ms step_avg:94.84ms
step:443/1770 train_time:41068ms step_avg:94.85ms
step:444/1770 train_time:41166ms step_avg:94.85ms
step:445/1770 train_time:41263ms step_avg:94.86ms
step:446/1770 train_time:41360ms step_avg:94.86ms
step:447/1770 train_time:41457ms step_avg:94.87ms
step:448/1770 train_time:41554ms step_avg:94.87ms
step:449/1770 train_time:41650ms step_avg:94.87ms
step:450/1770 train_time:41746ms step_avg:94.88ms
step:451/1770 train_time:41844ms step_avg:94.88ms
step:452/1770 train_time:41942ms step_avg:94.89ms
step:453/1770 train_time:42039ms step_avg:94.90ms
step:454/1770 train_time:42137ms step_avg:94.90ms
step:455/1770 train_time:42234ms step_avg:94.91ms
step:456/1770 train_time:42330ms step_avg:94.91ms
step:457/1770 train_time:42427ms step_avg:94.91ms
step:458/1770 train_time:42524ms step_avg:94.92ms
step:459/1770 train_time:42621ms step_avg:94.92ms
step:460/1770 train_time:42718ms step_avg:94.93ms
step:461/1770 train_time:42815ms step_avg:94.93ms
step:462/1770 train_time:42912ms step_avg:94.94ms
step:463/1770 train_time:43009ms step_avg:94.94ms
step:464/1770 train_time:43106ms step_avg:94.95ms
step:465/1770 train_time:43204ms step_avg:94.95ms
step:466/1770 train_time:43301ms step_avg:94.96ms
step:467/1770 train_time:43398ms step_avg:94.96ms
step:468/1770 train_time:43496ms step_avg:94.97ms
step:469/1770 train_time:43592ms step_avg:94.97ms
step:470/1770 train_time:43689ms step_avg:94.98ms
step:471/1770 train_time:43786ms step_avg:94.98ms
step:472/1770 train_time:43884ms step_avg:94.99ms
step:473/1770 train_time:43981ms step_avg:94.99ms
step:474/1770 train_time:44079ms step_avg:95.00ms
step:475/1770 train_time:44176ms step_avg:95.00ms
step:476/1770 train_time:44273ms step_avg:95.01ms
step:477/1770 train_time:44369ms step_avg:95.01ms
step:478/1770 train_time:44467ms step_avg:95.01ms
step:479/1770 train_time:44564ms step_avg:95.02ms
step:480/1770 train_time:44661ms step_avg:95.02ms
step:481/1770 train_time:44759ms step_avg:95.03ms
step:482/1770 train_time:44856ms step_avg:95.03ms
step:483/1770 train_time:44952ms step_avg:95.04ms
step:484/1770 train_time:45049ms step_avg:95.04ms
step:485/1770 train_time:45146ms step_avg:95.05ms
step:486/1770 train_time:45244ms step_avg:95.05ms
step:487/1770 train_time:45341ms step_avg:95.05ms
step:488/1770 train_time:45439ms step_avg:95.06ms
step:489/1770 train_time:45536ms step_avg:95.06ms
step:490/1770 train_time:45633ms step_avg:95.07ms
step:491/1770 train_time:45730ms step_avg:95.07ms
step:492/1770 train_time:45827ms step_avg:95.08ms
step:493/1770 train_time:45924ms step_avg:95.08ms
step:494/1770 train_time:46021ms step_avg:95.09ms
step:495/1770 train_time:46119ms step_avg:95.09ms
step:496/1770 train_time:46216ms step_avg:95.09ms
step:497/1770 train_time:46313ms step_avg:95.10ms
step:498/1770 train_time:46409ms step_avg:95.10ms
step:499/1770 train_time:46506ms step_avg:95.10ms
step:500/1770 train_time:46603ms step_avg:95.11ms
step:500/1770 val_loss:3.7613 train_time:46699ms step_avg:95.30ms
step:501/1770 train_time:46722ms step_avg:95.16ms
step:502/1770 train_time:46804ms step_avg:95.13ms
step:503/1770 train_time:46903ms step_avg:95.14ms
step:504/1770 train_time:46999ms step_avg:95.14ms
step:505/1770 train_time:47096ms step_avg:95.14ms
step:506/1770 train_time:47192ms step_avg:95.15ms
step:507/1770 train_time:47289ms step_avg:95.15ms
step:508/1770 train_time:47385ms step_avg:95.15ms
step:509/1770 train_time:47482ms step_avg:95.15ms
step:510/1770 train_time:47579ms step_avg:95.16ms
step:511/1770 train_time:47676ms step_avg:95.16ms
step:512/1770 train_time:47773ms step_avg:95.17ms
step:513/1770 train_time:47871ms step_avg:95.17ms
step:514/1770 train_time:47968ms step_avg:95.17ms
step:515/1770 train_time:48065ms step_avg:95.18ms
step:516/1770 train_time:48162ms step_avg:95.18ms
step:517/1770 train_time:48258ms step_avg:95.18ms
step:518/1770 train_time:48355ms step_avg:95.19ms
step:519/1770 train_time:48452ms step_avg:95.19ms
step:520/1770 train_time:48549ms step_avg:95.20ms
step:521/1770 train_time:48646ms step_avg:95.20ms
step:522/1770 train_time:48743ms step_avg:95.20ms
step:523/1770 train_time:48839ms step_avg:95.20ms
step:524/1770 train_time:48937ms step_avg:95.21ms
step:525/1770 train_time:49035ms step_avg:95.21ms
step:526/1770 train_time:49132ms step_avg:95.22ms
step:527/1770 train_time:49229ms step_avg:95.22ms
step:528/1770 train_time:49326ms step_avg:95.22ms
step:529/1770 train_time:49423ms step_avg:95.23ms
step:530/1770 train_time:49520ms step_avg:95.23ms
step:531/1770 train_time:49618ms step_avg:95.24ms
step:532/1770 train_time:49715ms step_avg:95.24ms
step:533/1770 train_time:49813ms step_avg:95.24ms
step:534/1770 train_time:49910ms step_avg:95.25ms
step:535/1770 train_time:50007ms step_avg:95.25ms
step:536/1770 train_time:50104ms step_avg:95.25ms
step:537/1770 train_time:50202ms step_avg:95.26ms
step:538/1770 train_time:50299ms step_avg:95.26ms
step:539/1770 train_time:50397ms step_avg:95.27ms
step:540/1770 train_time:50494ms step_avg:95.27ms
step:541/1770 train_time:50592ms step_avg:95.28ms
step:542/1770 train_time:50690ms step_avg:95.28ms
step:543/1770 train_time:50787ms step_avg:95.29ms
step:544/1770 train_time:50884ms step_avg:95.29ms
step:545/1770 train_time:50981ms step_avg:95.29ms
step:546/1770 train_time:51078ms step_avg:95.29ms
step:547/1770 train_time:51176ms step_avg:95.30ms
step:548/1770 train_time:51274ms step_avg:95.31ms
step:549/1770 train_time:51372ms step_avg:95.31ms
step:550/1770 train_time:51470ms step_avg:95.31ms
step:551/1770 train_time:51567ms step_avg:95.32ms
step:552/1770 train_time:52009ms step_avg:95.96ms
step:553/1770 train_time:52104ms step_avg:95.96ms
step:554/1770 train_time:52201ms step_avg:95.96ms
step:555/1770 train_time:52298ms step_avg:95.96ms
step:556/1770 train_time:52394ms step_avg:95.96ms
step:557/1770 train_time:52491ms step_avg:95.96ms
step:558/1770 train_time:52588ms step_avg:95.96ms
step:559/1770 train_time:52685ms step_avg:95.96ms
step:560/1770 train_time:52781ms step_avg:95.97ms
step:561/1770 train_time:52879ms step_avg:95.97ms
step:562/1770 train_time:52979ms step_avg:95.98ms
step:563/1770 train_time:53077ms step_avg:95.98ms
step:564/1770 train_time:53175ms step_avg:95.98ms
step:565/1770 train_time:53272ms step_avg:95.99ms
step:566/1770 train_time:53370ms step_avg:95.99ms
step:567/1770 train_time:53468ms step_avg:95.99ms
step:568/1770 train_time:53565ms step_avg:95.99ms
step:569/1770 train_time:53661ms step_avg:96.00ms
step:570/1770 train_time:53758ms step_avg:96.00ms
step:571/1770 train_time:53855ms step_avg:96.00ms
step:572/1770 train_time:53953ms step_avg:96.00ms
step:573/1770 train_time:54050ms step_avg:96.00ms
step:574/1770 train_time:54148ms step_avg:96.01ms
step:575/1770 train_time:54245ms step_avg:96.01ms
step:576/1770 train_time:54342ms step_avg:96.01ms
step:577/1770 train_time:54440ms step_avg:96.01ms
step:578/1770 train_time:54538ms step_avg:96.02ms
step:579/1770 train_time:54635ms step_avg:96.02ms
step:580/1770 train_time:54733ms step_avg:96.02ms
step:581/1770 train_time:54830ms step_avg:96.02ms
step:582/1770 train_time:54927ms step_avg:96.03ms
step:583/1770 train_time:55024ms step_avg:96.03ms
step:584/1770 train_time:55121ms step_avg:96.03ms
step:585/1770 train_time:55219ms step_avg:96.03ms
step:586/1770 train_time:55316ms step_avg:96.04ms
step:587/1770 train_time:55414ms step_avg:96.04ms
step:588/1770 train_time:55512ms step_avg:96.04ms
step:589/1770 train_time:55610ms step_avg:96.05ms
step:590/1770 train_time:55707ms step_avg:96.05ms
step:591/1770 train_time:55803ms step_avg:96.05ms
step:592/1770 train_time:55900ms step_avg:96.05ms
step:593/1770 train_time:55997ms step_avg:96.05ms
step:594/1770 train_time:56095ms step_avg:96.05ms
step:595/1770 train_time:56193ms step_avg:96.06ms
step:596/1770 train_time:56291ms step_avg:96.06ms
step:597/1770 train_time:56388ms step_avg:96.06ms
step:598/1770 train_time:56486ms step_avg:96.06ms
step:599/1770 train_time:56582ms step_avg:96.07ms
step:600/1770 train_time:56680ms step_avg:96.07ms
step:601/1770 train_time:56778ms step_avg:96.07ms
step:602/1770 train_time:56875ms step_avg:96.07ms
step:603/1770 train_time:56973ms step_avg:96.08ms
step:604/1770 train_time:57071ms step_avg:96.08ms
step:605/1770 train_time:57168ms step_avg:96.08ms
step:606/1770 train_time:57265ms step_avg:96.08ms
step:607/1770 train_time:57362ms step_avg:96.08ms
step:608/1770 train_time:57459ms step_avg:96.09ms
step:609/1770 train_time:57556ms step_avg:96.09ms
step:610/1770 train_time:57654ms step_avg:96.09ms
step:611/1770 train_time:57752ms step_avg:96.09ms
step:612/1770 train_time:57849ms step_avg:96.09ms
step:613/1770 train_time:57946ms step_avg:96.10ms
step:614/1770 train_time:58043ms step_avg:96.10ms
step:615/1770 train_time:58140ms step_avg:96.10ms
step:616/1770 train_time:58238ms step_avg:96.10ms
step:617/1770 train_time:58336ms step_avg:96.10ms
step:618/1770 train_time:58433ms step_avg:96.11ms
step:619/1770 train_time:58532ms step_avg:96.11ms
step:620/1770 train_time:58629ms step_avg:96.11ms
step:621/1770 train_time:58727ms step_avg:96.12ms
step:622/1770 train_time:58824ms step_avg:96.12ms
step:623/1770 train_time:58920ms step_avg:96.12ms
step:624/1770 train_time:59017ms step_avg:96.12ms
step:625/1770 train_time:59115ms step_avg:96.12ms
step:625/1770 val_loss:3.6734 train_time:59211ms step_avg:96.28ms
step:626/1770 train_time:59232ms step_avg:96.16ms
step:627/1770 train_time:59318ms step_avg:96.14ms
step:628/1770 train_time:59416ms step_avg:96.14ms
step:629/1770 train_time:59512ms step_avg:96.14ms
step:630/1770 train_time:59609ms step_avg:96.14ms
step:631/1770 train_time:59706ms step_avg:96.14ms
step:632/1770 train_time:59803ms step_avg:96.15ms
step:633/1770 train_time:59900ms step_avg:96.15ms
step:634/1770 train_time:59996ms step_avg:96.15ms
step:635/1770 train_time:60093ms step_avg:96.15ms
step:636/1770 train_time:60191ms step_avg:96.15ms
step:637/1770 train_time:60289ms step_avg:96.16ms
step:638/1770 train_time:60387ms step_avg:96.16ms
step:639/1770 train_time:60485ms step_avg:96.16ms
step:640/1770 train_time:60584ms step_avg:96.17ms
step:641/1770 train_time:60683ms step_avg:96.17ms
step:642/1770 train_time:60780ms step_avg:96.17ms
step:643/1770 train_time:60877ms step_avg:96.17ms
step:644/1770 train_time:60975ms step_avg:96.17ms
step:645/1770 train_time:61071ms step_avg:96.18ms
step:646/1770 train_time:61169ms step_avg:96.18ms
step:647/1770 train_time:61266ms step_avg:96.18ms
step:648/1770 train_time:61364ms step_avg:96.18ms
step:649/1770 train_time:61461ms step_avg:96.18ms
step:650/1770 train_time:61559ms step_avg:96.19ms
step:651/1770 train_time:61656ms step_avg:96.19ms
step:652/1770 train_time:61753ms step_avg:96.19ms
step:653/1770 train_time:61851ms step_avg:96.19ms
step:654/1770 train_time:61948ms step_avg:96.19ms
step:655/1770 train_time:62046ms step_avg:96.19ms
step:656/1770 train_time:62143ms step_avg:96.20ms
step:657/1770 train_time:62241ms step_avg:96.20ms
step:658/1770 train_time:62339ms step_avg:96.20ms
step:659/1770 train_time:62438ms step_avg:96.21ms
step:660/1770 train_time:62536ms step_avg:96.21ms
step:661/1770 train_time:62635ms step_avg:96.21ms
step:662/1770 train_time:62734ms step_avg:96.22ms
step:663/1770 train_time:62833ms step_avg:96.22ms
step:664/1770 train_time:62932ms step_avg:96.23ms
step:665/1770 train_time:63031ms step_avg:96.23ms
step:666/1770 train_time:63130ms step_avg:96.23ms
step:667/1770 train_time:63229ms step_avg:96.24ms
step:668/1770 train_time:63328ms step_avg:96.24ms
step:669/1770 train_time:63428ms step_avg:96.25ms
step:670/1770 train_time:63528ms step_avg:96.25ms
step:671/1770 train_time:63627ms step_avg:96.26ms
step:672/1770 train_time:63726ms step_avg:96.26ms
step:673/1770 train_time:63825ms step_avg:96.27ms
step:674/1770 train_time:63924ms step_avg:96.27ms
step:675/1770 train_time:64024ms step_avg:96.28ms
step:676/1770 train_time:64124ms step_avg:96.28ms
step:677/1770 train_time:64224ms step_avg:96.29ms
step:678/1770 train_time:64323ms step_avg:96.29ms
step:679/1770 train_time:64423ms step_avg:96.30ms
step:680/1770 train_time:64522ms step_avg:96.30ms
step:681/1770 train_time:64622ms step_avg:96.31ms
step:682/1770 train_time:64722ms step_avg:96.31ms
step:683/1770 train_time:64821ms step_avg:96.32ms
step:684/1770 train_time:64920ms step_avg:96.32ms
step:685/1770 train_time:65019ms step_avg:96.32ms
step:686/1770 train_time:65118ms step_avg:96.33ms
step:687/1770 train_time:65216ms step_avg:96.33ms
step:688/1770 train_time:65316ms step_avg:96.34ms
step:689/1770 train_time:65415ms step_avg:96.34ms
step:690/1770 train_time:65514ms step_avg:96.34ms
step:691/1770 train_time:65612ms step_avg:96.35ms
step:692/1770 train_time:65711ms step_avg:96.35ms
step:693/1770 train_time:65811ms step_avg:96.36ms
step:694/1770 train_time:65910ms step_avg:96.36ms
step:695/1770 train_time:66009ms step_avg:96.36ms
step:696/1770 train_time:66108ms step_avg:96.37ms
step:697/1770 train_time:66208ms step_avg:96.37ms
step:698/1770 train_time:66308ms step_avg:96.38ms
step:699/1770 train_time:66408ms step_avg:96.38ms
step:700/1770 train_time:66508ms step_avg:96.39ms
step:701/1770 train_time:66607ms step_avg:96.39ms
step:702/1770 train_time:66707ms step_avg:96.40ms
step:703/1770 train_time:66806ms step_avg:96.40ms
step:704/1770 train_time:66906ms step_avg:96.41ms
step:705/1770 train_time:67005ms step_avg:96.41ms
step:706/1770 train_time:67105ms step_avg:96.42ms
step:707/1770 train_time:67205ms step_avg:96.42ms
step:708/1770 train_time:67304ms step_avg:96.42ms
step:709/1770 train_time:67403ms step_avg:96.43ms
step:710/1770 train_time:67502ms step_avg:96.43ms
step:711/1770 train_time:67601ms step_avg:96.44ms
step:712/1770 train_time:67701ms step_avg:96.44ms
step:713/1770 train_time:67800ms step_avg:96.44ms
step:714/1770 train_time:67899ms step_avg:96.45ms
step:715/1770 train_time:67997ms step_avg:96.45ms
step:716/1770 train_time:68096ms step_avg:96.45ms
step:717/1770 train_time:68195ms step_avg:96.46ms
step:718/1770 train_time:68294ms step_avg:96.46ms
step:719/1770 train_time:68393ms step_avg:96.46ms
step:720/1770 train_time:68492ms step_avg:96.47ms
step:721/1770 train_time:68591ms step_avg:96.47ms
step:722/1770 train_time:68690ms step_avg:96.47ms
step:723/1770 train_time:68789ms step_avg:96.48ms
step:724/1770 train_time:68889ms step_avg:96.48ms
step:725/1770 train_time:68990ms step_avg:96.49ms
step:726/1770 train_time:69088ms step_avg:96.49ms
step:727/1770 train_time:69187ms step_avg:96.50ms
step:728/1770 train_time:69287ms step_avg:96.50ms
step:729/1770 train_time:69388ms step_avg:96.51ms
step:730/1770 train_time:69487ms step_avg:96.51ms
step:731/1770 train_time:69586ms step_avg:96.51ms
step:732/1770 train_time:69686ms step_avg:96.52ms
step:733/1770 train_time:69786ms step_avg:96.52ms
step:734/1770 train_time:69886ms step_avg:96.53ms
step:735/1770 train_time:69985ms step_avg:96.53ms
step:736/1770 train_time:70084ms step_avg:96.54ms
step:737/1770 train_time:70184ms step_avg:96.54ms
step:738/1770 train_time:70284ms step_avg:96.54ms
step:739/1770 train_time:70384ms step_avg:96.55ms
step:740/1770 train_time:70486ms step_avg:96.56ms
step:741/1770 train_time:70583ms step_avg:96.56ms
step:742/1770 train_time:70682ms step_avg:96.56ms
step:743/1770 train_time:70782ms step_avg:96.56ms
step:744/1770 train_time:70881ms step_avg:96.57ms
step:745/1770 train_time:70980ms step_avg:96.57ms
step:746/1770 train_time:71079ms step_avg:96.57ms
step:747/1770 train_time:71178ms step_avg:96.58ms
step:748/1770 train_time:71277ms step_avg:96.58ms
step:749/1770 train_time:71376ms step_avg:96.58ms
step:750/1770 train_time:71474ms step_avg:96.59ms
step:750/1770 val_loss:3.6080 train_time:71571ms step_avg:96.72ms
step:751/1770 train_time:71592ms step_avg:96.62ms
step:752/1770 train_time:71677ms step_avg:96.60ms
step:753/1770 train_time:71776ms step_avg:96.60ms
step:754/1770 train_time:71875ms step_avg:96.61ms
step:755/1770 train_time:71973ms step_avg:96.61ms
step:756/1770 train_time:72071ms step_avg:96.61ms
step:757/1770 train_time:72170ms step_avg:96.61ms
step:758/1770 train_time:72269ms step_avg:96.62ms
step:759/1770 train_time:72368ms step_avg:96.62ms
step:760/1770 train_time:72466ms step_avg:96.62ms
step:761/1770 train_time:72566ms step_avg:96.63ms
step:762/1770 train_time:72667ms step_avg:96.63ms
step:763/1770 train_time:72768ms step_avg:96.64ms
step:764/1770 train_time:72867ms step_avg:96.64ms
step:765/1770 train_time:72969ms step_avg:96.65ms
step:766/1770 train_time:73067ms step_avg:96.65ms
step:767/1770 train_time:73168ms step_avg:96.65ms
step:768/1770 train_time:73267ms step_avg:96.66ms
step:769/1770 train_time:73366ms step_avg:96.66ms
step:770/1770 train_time:73465ms step_avg:96.66ms
step:771/1770 train_time:73564ms step_avg:96.67ms
step:772/1770 train_time:73664ms step_avg:96.67ms
step:773/1770 train_time:73764ms step_avg:96.68ms
step:774/1770 train_time:73864ms step_avg:96.68ms
step:775/1770 train_time:73964ms step_avg:96.69ms
step:776/1770 train_time:74064ms step_avg:96.69ms
step:777/1770 train_time:74164ms step_avg:96.69ms
step:778/1770 train_time:74264ms step_avg:96.70ms
step:779/1770 train_time:74363ms step_avg:96.70ms
step:780/1770 train_time:74462ms step_avg:96.70ms
step:781/1770 train_time:74562ms step_avg:96.71ms
step:782/1770 train_time:74662ms step_avg:96.71ms
step:783/1770 train_time:74761ms step_avg:96.72ms
step:784/1770 train_time:74864ms step_avg:96.72ms
step:785/1770 train_time:74959ms step_avg:96.72ms
step:786/1770 train_time:75058ms step_avg:96.72ms
step:787/1770 train_time:75157ms step_avg:96.73ms
step:788/1770 train_time:75256ms step_avg:96.73ms
step:789/1770 train_time:75355ms step_avg:96.73ms
step:790/1770 train_time:75454ms step_avg:96.74ms
step:791/1770 train_time:75553ms step_avg:96.74ms
step:792/1770 train_time:75652ms step_avg:96.74ms
step:793/1770 train_time:75752ms step_avg:96.75ms
step:794/1770 train_time:75852ms step_avg:96.75ms
step:795/1770 train_time:75952ms step_avg:96.75ms
step:796/1770 train_time:76053ms step_avg:96.76ms
step:797/1770 train_time:76153ms step_avg:96.76ms
step:798/1770 train_time:76252ms step_avg:96.77ms
step:799/1770 train_time:76352ms step_avg:96.77ms
step:800/1770 train_time:76451ms step_avg:96.77ms
step:801/1770 train_time:76550ms step_avg:96.78ms
step:802/1770 train_time:76650ms step_avg:96.78ms
step:803/1770 train_time:76750ms step_avg:96.78ms
step:804/1770 train_time:76849ms step_avg:96.79ms
step:805/1770 train_time:76949ms step_avg:96.79ms
step:806/1770 train_time:77048ms step_avg:96.79ms
step:807/1770 train_time:77147ms step_avg:96.80ms
step:808/1770 train_time:77247ms step_avg:96.80ms
step:809/1770 train_time:77347ms step_avg:96.80ms
step:810/1770 train_time:77448ms step_avg:96.81ms
step:811/1770 train_time:77546ms step_avg:96.81ms
step:812/1770 train_time:77646ms step_avg:96.82ms
step:813/1770 train_time:77747ms step_avg:96.82ms
step:814/1770 train_time:77847ms step_avg:96.82ms
step:815/1770 train_time:77946ms step_avg:96.83ms
step:816/1770 train_time:78047ms step_avg:96.83ms
step:817/1770 train_time:78146ms step_avg:96.84ms
step:818/1770 train_time:78246ms step_avg:96.84ms
step:819/1770 train_time:78346ms step_avg:96.84ms
step:820/1770 train_time:78446ms step_avg:96.85ms
step:821/1770 train_time:78546ms step_avg:96.85ms
step:822/1770 train_time:78647ms step_avg:96.86ms
step:823/1770 train_time:78746ms step_avg:96.86ms
step:824/1770 train_time:78845ms step_avg:96.86ms
step:825/1770 train_time:78945ms step_avg:96.86ms
step:826/1770 train_time:79044ms step_avg:96.87ms
step:827/1770 train_time:79144ms step_avg:96.87ms
step:828/1770 train_time:79243ms step_avg:96.87ms
step:829/1770 train_time:79343ms step_avg:96.88ms
step:830/1770 train_time:79442ms step_avg:96.88ms
step:831/1770 train_time:79542ms step_avg:96.88ms
step:832/1770 train_time:79641ms step_avg:96.89ms
step:833/1770 train_time:79742ms step_avg:96.89ms
step:834/1770 train_time:79842ms step_avg:96.90ms
step:835/1770 train_time:79941ms step_avg:96.90ms
step:836/1770 train_time:80041ms step_avg:96.90ms
step:837/1770 train_time:80140ms step_avg:96.90ms
step:838/1770 train_time:80239ms step_avg:96.91ms
step:839/1770 train_time:80338ms step_avg:96.91ms
step:840/1770 train_time:80437ms step_avg:96.91ms
step:841/1770 train_time:80536ms step_avg:96.91ms
step:842/1770 train_time:80635ms step_avg:96.92ms
step:843/1770 train_time:80734ms step_avg:96.92ms
step:844/1770 train_time:80833ms step_avg:96.92ms
step:845/1770 train_time:80932ms step_avg:96.92ms
step:846/1770 train_time:81032ms step_avg:96.93ms
step:847/1770 train_time:81131ms step_avg:96.93ms
step:848/1770 train_time:81231ms step_avg:96.93ms
step:849/1770 train_time:81331ms step_avg:96.94ms
step:850/1770 train_time:81431ms step_avg:96.94ms
step:851/1770 train_time:81530ms step_avg:96.94ms
step:852/1770 train_time:81629ms step_avg:96.95ms
step:853/1770 train_time:81729ms step_avg:96.95ms
step:854/1770 train_time:81828ms step_avg:96.95ms
step:855/1770 train_time:81927ms step_avg:96.96ms
step:856/1770 train_time:82026ms step_avg:96.96ms
step:857/1770 train_time:82125ms step_avg:96.96ms
step:858/1770 train_time:82225ms step_avg:96.96ms
step:859/1770 train_time:82325ms step_avg:96.97ms
step:860/1770 train_time:82424ms step_avg:96.97ms
step:861/1770 train_time:82525ms step_avg:96.97ms
step:862/1770 train_time:82626ms step_avg:96.98ms
step:863/1770 train_time:82725ms step_avg:96.98ms
step:864/1770 train_time:82825ms step_avg:96.98ms
step:865/1770 train_time:82924ms step_avg:96.99ms
step:866/1770 train_time:83024ms step_avg:96.99ms
step:867/1770 train_time:83123ms step_avg:96.99ms
step:868/1770 train_time:83223ms step_avg:97.00ms
step:869/1770 train_time:83323ms step_avg:97.00ms
step:870/1770 train_time:83423ms step_avg:97.00ms
step:871/1770 train_time:83522ms step_avg:97.01ms
step:872/1770 train_time:83622ms step_avg:97.01ms
step:873/1770 train_time:83722ms step_avg:97.01ms
step:874/1770 train_time:83823ms step_avg:97.02ms
step:875/1770 train_time:83922ms step_avg:97.02ms
step:875/1770 val_loss:3.5577 train_time:84020ms step_avg:97.13ms
step:876/1770 train_time:84042ms step_avg:97.05ms
step:877/1770 train_time:84129ms step_avg:97.03ms
step:878/1770 train_time:84228ms step_avg:97.04ms
step:879/1770 train_time:84327ms step_avg:97.04ms
step:880/1770 train_time:84425ms step_avg:97.04ms
step:881/1770 train_time:84523ms step_avg:97.04ms
step:882/1770 train_time:84622ms step_avg:97.04ms
step:883/1770 train_time:84721ms step_avg:97.05ms
step:884/1770 train_time:84820ms step_avg:97.05ms
step:885/1770 train_time:84919ms step_avg:97.05ms
step:886/1770 train_time:85019ms step_avg:97.05ms
step:887/1770 train_time:85120ms step_avg:97.06ms
step:888/1770 train_time:85220ms step_avg:97.06ms
step:889/1770 train_time:85321ms step_avg:97.07ms
step:890/1770 train_time:85420ms step_avg:97.07ms
step:891/1770 train_time:85519ms step_avg:97.07ms
step:892/1770 train_time:85619ms step_avg:97.07ms
step:893/1770 train_time:85718ms step_avg:97.08ms
step:894/1770 train_time:85817ms step_avg:97.08ms
step:895/1770 train_time:85917ms step_avg:97.08ms
step:896/1770 train_time:86017ms step_avg:97.08ms
step:897/1770 train_time:86116ms step_avg:97.09ms
step:898/1770 train_time:86216ms step_avg:97.09ms
step:899/1770 train_time:86316ms step_avg:97.09ms
step:900/1770 train_time:86415ms step_avg:97.10ms
step:901/1770 train_time:86515ms step_avg:97.10ms
step:902/1770 train_time:86615ms step_avg:97.10ms
step:903/1770 train_time:86715ms step_avg:97.10ms
step:904/1770 train_time:86815ms step_avg:97.11ms
step:905/1770 train_time:86915ms step_avg:97.11ms
step:906/1770 train_time:87014ms step_avg:97.11ms
step:907/1770 train_time:87114ms step_avg:97.12ms
step:908/1770 train_time:87214ms step_avg:97.12ms
step:909/1770 train_time:87313ms step_avg:97.12ms
step:910/1770 train_time:87414ms step_avg:97.13ms
step:911/1770 train_time:87514ms step_avg:97.13ms
step:912/1770 train_time:87614ms step_avg:97.13ms
step:913/1770 train_time:87715ms step_avg:97.14ms
step:914/1770 train_time:87815ms step_avg:97.14ms
step:915/1770 train_time:87915ms step_avg:97.14ms
step:916/1770 train_time:88015ms step_avg:97.15ms
step:917/1770 train_time:88115ms step_avg:97.15ms
step:918/1770 train_time:88215ms step_avg:97.15ms
step:919/1770 train_time:88314ms step_avg:97.16ms
step:920/1770 train_time:88416ms step_avg:97.16ms
step:921/1770 train_time:88518ms step_avg:97.17ms
step:922/1770 train_time:88619ms step_avg:97.17ms
step:923/1770 train_time:88720ms step_avg:97.17ms
step:924/1770 train_time:88820ms step_avg:97.18ms
step:925/1770 train_time:88922ms step_avg:97.18ms
step:926/1770 train_time:89023ms step_avg:97.19ms
step:927/1770 train_time:89129ms step_avg:97.20ms
step:928/1770 train_time:89225ms step_avg:97.19ms
step:929/1770 train_time:89326ms step_avg:97.20ms
step:930/1770 train_time:89427ms step_avg:97.20ms
step:931/1770 train_time:89528ms step_avg:97.21ms
step:932/1770 train_time:89629ms step_avg:97.21ms
step:933/1770 train_time:89729ms step_avg:97.21ms
step:934/1770 train_time:89829ms step_avg:97.22ms
step:935/1770 train_time:89930ms step_avg:97.22ms
step:936/1770 train_time:90030ms step_avg:97.22ms
step:937/1770 train_time:90131ms step_avg:97.23ms
step:938/1770 train_time:90232ms step_avg:97.23ms
step:939/1770 train_time:90332ms step_avg:97.24ms
step:940/1770 train_time:90433ms step_avg:97.24ms
step:941/1770 train_time:90533ms step_avg:97.24ms
step:942/1770 train_time:90635ms step_avg:97.25ms
step:943/1770 train_time:90737ms step_avg:97.25ms
step:944/1770 train_time:90838ms step_avg:97.26ms
step:945/1770 train_time:90941ms step_avg:97.26ms
step:946/1770 train_time:91041ms step_avg:97.27ms
step:947/1770 train_time:91142ms step_avg:97.27ms
step:948/1770 train_time:91243ms step_avg:97.27ms
step:949/1770 train_time:91345ms step_avg:97.28ms
step:950/1770 train_time:91446ms step_avg:97.28ms
step:951/1770 train_time:91548ms step_avg:97.29ms
step:952/1770 train_time:91648ms step_avg:97.29ms
step:953/1770 train_time:91748ms step_avg:97.29ms
step:954/1770 train_time:91848ms step_avg:97.30ms
step:955/1770 train_time:91949ms step_avg:97.30ms
step:956/1770 train_time:92049ms step_avg:97.30ms
step:957/1770 train_time:92150ms step_avg:97.31ms
step:958/1770 train_time:92249ms step_avg:97.31ms
step:959/1770 train_time:92350ms step_avg:97.31ms
step:960/1770 train_time:92450ms step_avg:97.32ms
step:961/1770 train_time:92550ms step_avg:97.32ms
step:962/1770 train_time:92651ms step_avg:97.32ms
step:963/1770 train_time:92751ms step_avg:97.33ms
step:964/1770 train_time:92851ms step_avg:97.33ms
step:965/1770 train_time:92951ms step_avg:97.33ms
step:966/1770 train_time:93052ms step_avg:97.33ms
step:967/1770 train_time:93153ms step_avg:97.34ms
step:968/1770 train_time:93254ms step_avg:97.34ms
step:969/1770 train_time:93355ms step_avg:97.35ms
step:970/1770 train_time:93456ms step_avg:97.35ms
step:971/1770 train_time:93557ms step_avg:97.35ms
step:972/1770 train_time:93658ms step_avg:97.36ms
step:973/1770 train_time:93760ms step_avg:97.36ms
step:974/1770 train_time:93861ms step_avg:97.37ms
step:975/1770 train_time:93962ms step_avg:97.37ms
step:976/1770 train_time:94063ms step_avg:97.37ms
step:977/1770 train_time:94165ms step_avg:97.38ms
step:978/1770 train_time:94265ms step_avg:97.38ms
step:979/1770 train_time:94366ms step_avg:97.39ms
step:980/1770 train_time:94467ms step_avg:97.39ms
step:981/1770 train_time:94568ms step_avg:97.39ms
step:982/1770 train_time:94668ms step_avg:97.40ms
step:983/1770 train_time:94769ms step_avg:97.40ms
step:984/1770 train_time:94870ms step_avg:97.40ms
step:985/1770 train_time:94972ms step_avg:97.41ms
step:986/1770 train_time:95072ms step_avg:97.41ms
step:987/1770 train_time:95173ms step_avg:97.41ms
step:988/1770 train_time:95273ms step_avg:97.42ms
step:989/1770 train_time:95376ms step_avg:97.42ms
step:990/1770 train_time:95478ms step_avg:97.43ms
step:991/1770 train_time:95578ms step_avg:97.43ms
step:992/1770 train_time:95680ms step_avg:97.43ms
step:993/1770 train_time:95781ms step_avg:97.44ms
step:994/1770 train_time:95883ms step_avg:97.44ms
step:995/1770 train_time:95983ms step_avg:97.44ms
step:996/1770 train_time:96084ms step_avg:97.45ms
step:997/1770 train_time:96185ms step_avg:97.45ms
step:998/1770 train_time:96286ms step_avg:97.46ms
step:999/1770 train_time:96386ms step_avg:97.46ms
step:1000/1770 train_time:96488ms step_avg:97.46ms
step:1000/1770 val_loss:3.5175 train_time:96587ms step_avg:97.56ms
step:1001/1770 train_time:96608ms step_avg:97.49ms
step:1002/1770 train_time:96695ms step_avg:97.48ms
step:1003/1770 train_time:96797ms step_avg:97.48ms
step:1004/1770 train_time:96898ms step_avg:97.48ms
step:1005/1770 train_time:96999ms step_avg:97.49ms
step:1006/1770 train_time:97099ms step_avg:97.49ms
step:1007/1770 train_time:97199ms step_avg:97.49ms
step:1008/1770 train_time:97300ms step_avg:97.50ms
step:1009/1770 train_time:97401ms step_avg:97.50ms
step:1010/1770 train_time:97501ms step_avg:97.50ms
step:1011/1770 train_time:97606ms step_avg:97.51ms
step:1012/1770 train_time:97708ms step_avg:97.51ms
step:1013/1770 train_time:97811ms step_avg:97.52ms
step:1014/1770 train_time:97910ms step_avg:97.52ms
step:1015/1770 train_time:98011ms step_avg:97.52ms
step:1016/1770 train_time:98110ms step_avg:97.53ms
step:1017/1770 train_time:98211ms step_avg:97.53ms
step:1018/1770 train_time:98311ms step_avg:97.53ms
step:1019/1770 train_time:98411ms step_avg:97.53ms
step:1020/1770 train_time:98512ms step_avg:97.54ms
step:1021/1770 train_time:98613ms step_avg:97.54ms
step:1022/1770 train_time:98713ms step_avg:97.54ms
step:1023/1770 train_time:98813ms step_avg:97.54ms
step:1024/1770 train_time:98913ms step_avg:97.55ms
step:1025/1770 train_time:99012ms step_avg:97.55ms
step:1026/1770 train_time:99113ms step_avg:97.55ms
step:1027/1770 train_time:99213ms step_avg:97.55ms
step:1028/1770 train_time:99314ms step_avg:97.56ms
step:1029/1770 train_time:99414ms step_avg:97.56ms
step:1030/1770 train_time:99515ms step_avg:97.56ms
step:1031/1770 train_time:99616ms step_avg:97.57ms
step:1032/1770 train_time:99718ms step_avg:97.57ms
step:1033/1770 train_time:99819ms step_avg:97.58ms
step:1034/1770 train_time:99920ms step_avg:97.58ms
step:1035/1770 train_time:100022ms step_avg:97.58ms
step:1036/1770 train_time:100123ms step_avg:97.59ms
step:1037/1770 train_time:100224ms step_avg:97.59ms
step:1038/1770 train_time:100325ms step_avg:97.59ms
step:1039/1770 train_time:100426ms step_avg:97.60ms
step:1040/1770 train_time:100526ms step_avg:97.60ms
step:1041/1770 train_time:100628ms step_avg:97.60ms
step:1042/1770 train_time:100729ms step_avg:97.61ms
step:1043/1770 train_time:100830ms step_avg:97.61ms
step:1044/1770 train_time:100930ms step_avg:97.61ms
step:1045/1770 train_time:101031ms step_avg:97.61ms
step:1046/1770 train_time:101132ms step_avg:97.62ms
step:1047/1770 train_time:101233ms step_avg:97.62ms
step:1048/1770 train_time:101334ms step_avg:97.62ms
step:1049/1770 train_time:101435ms step_avg:97.63ms
step:1050/1770 train_time:101536ms step_avg:97.63ms
step:1051/1770 train_time:101637ms step_avg:97.63ms
step:1052/1770 train_time:101739ms step_avg:97.64ms
step:1053/1770 train_time:101840ms step_avg:97.64ms
step:1054/1770 train_time:101941ms step_avg:97.65ms
step:1055/1770 train_time:102043ms step_avg:97.65ms
step:1056/1770 train_time:102143ms step_avg:97.65ms
step:1057/1770 train_time:102244ms step_avg:97.65ms
step:1058/1770 train_time:102346ms step_avg:97.66ms
step:1059/1770 train_time:102447ms step_avg:97.66ms
step:1060/1770 train_time:102547ms step_avg:97.66ms
step:1061/1770 train_time:102649ms step_avg:97.67ms
step:1062/1770 train_time:102751ms step_avg:97.67ms
step:1063/1770 train_time:102853ms step_avg:97.68ms
step:1064/1770 train_time:102954ms step_avg:97.68ms
step:1065/1770 train_time:103054ms step_avg:97.68ms
step:1066/1770 train_time:103154ms step_avg:97.68ms
step:1067/1770 train_time:103255ms step_avg:97.69ms
step:1068/1770 train_time:103357ms step_avg:97.69ms
step:1069/1770 train_time:103458ms step_avg:97.69ms
step:1070/1770 train_time:103560ms step_avg:97.70ms
step:1071/1770 train_time:103662ms step_avg:97.70ms
step:1072/1770 train_time:103764ms step_avg:97.71ms
step:1073/1770 train_time:103865ms step_avg:97.71ms
step:1074/1770 train_time:103966ms step_avg:97.71ms
step:1075/1770 train_time:104068ms step_avg:97.72ms
step:1076/1770 train_time:104170ms step_avg:97.72ms
step:1077/1770 train_time:104272ms step_avg:97.72ms
step:1078/1770 train_time:104372ms step_avg:97.73ms
step:1079/1770 train_time:104472ms step_avg:97.73ms
step:1080/1770 train_time:104574ms step_avg:97.73ms
step:1081/1770 train_time:104674ms step_avg:97.73ms
step:1082/1770 train_time:104775ms step_avg:97.74ms
step:1083/1770 train_time:104875ms step_avg:97.74ms
step:1084/1770 train_time:104977ms step_avg:97.74ms
step:1085/1770 train_time:105077ms step_avg:97.75ms
step:1086/1770 train_time:105178ms step_avg:97.75ms
step:1087/1770 train_time:105280ms step_avg:97.75ms
step:1088/1770 train_time:105381ms step_avg:97.76ms
step:1089/1770 train_time:105483ms step_avg:97.76ms
step:1090/1770 train_time:105584ms step_avg:97.76ms
step:1091/1770 train_time:105686ms step_avg:97.77ms
step:1092/1770 train_time:105788ms step_avg:97.77ms
step:1093/1770 train_time:105888ms step_avg:97.77ms
step:1094/1770 train_time:105989ms step_avg:97.78ms
step:1095/1770 train_time:106090ms step_avg:97.78ms
step:1096/1770 train_time:106190ms step_avg:97.78ms
step:1097/1770 train_time:106291ms step_avg:97.78ms
step:1098/1770 train_time:106391ms step_avg:97.79ms
step:1099/1770 train_time:106492ms step_avg:97.79ms
step:1100/1770 train_time:106593ms step_avg:97.79ms
step:1101/1770 train_time:106693ms step_avg:97.79ms
step:1102/1770 train_time:106795ms step_avg:97.80ms
step:1103/1770 train_time:106896ms step_avg:97.80ms
step:1104/1770 train_time:107002ms step_avg:97.81ms
step:1105/1770 train_time:107098ms step_avg:97.81ms
step:1106/1770 train_time:107200ms step_avg:97.81ms
step:1107/1770 train_time:107301ms step_avg:97.81ms
step:1108/1770 train_time:107403ms step_avg:97.82ms
step:1109/1770 train_time:107505ms step_avg:97.82ms
step:1110/1770 train_time:107607ms step_avg:97.82ms
step:1111/1770 train_time:107709ms step_avg:97.83ms
step:1112/1770 train_time:107810ms step_avg:97.83ms
step:1113/1770 train_time:107911ms step_avg:97.83ms
step:1114/1770 train_time:108011ms step_avg:97.84ms
step:1115/1770 train_time:108113ms step_avg:97.84ms
step:1116/1770 train_time:108213ms step_avg:97.84ms
step:1117/1770 train_time:108315ms step_avg:97.85ms
step:1118/1770 train_time:108415ms step_avg:97.85ms
step:1119/1770 train_time:108517ms step_avg:97.85ms
step:1120/1770 train_time:108617ms step_avg:97.85ms
step:1121/1770 train_time:108718ms step_avg:97.86ms
step:1122/1770 train_time:108820ms step_avg:97.86ms
step:1123/1770 train_time:108921ms step_avg:97.86ms
step:1124/1770 train_time:109023ms step_avg:97.87ms
step:1125/1770 train_time:109124ms step_avg:97.87ms
step:1125/1770 val_loss:3.4780 train_time:109224ms step_avg:97.96ms
step:1126/1770 train_time:109245ms step_avg:97.89ms
step:1127/1770 train_time:109332ms step_avg:97.88ms
step:1128/1770 train_time:109433ms step_avg:97.88ms
step:1129/1770 train_time:109534ms step_avg:97.89ms
step:1130/1770 train_time:109634ms step_avg:97.89ms
step:1131/1770 train_time:109735ms step_avg:97.89ms
step:1132/1770 train_time:109835ms step_avg:97.89ms
step:1133/1770 train_time:109935ms step_avg:97.89ms
step:1134/1770 train_time:110036ms step_avg:97.90ms
step:1135/1770 train_time:110136ms step_avg:97.90ms
step:1136/1770 train_time:110238ms step_avg:97.90ms
step:1137/1770 train_time:110339ms step_avg:97.91ms
step:1138/1770 train_time:110440ms step_avg:97.91ms
step:1139/1770 train_time:110542ms step_avg:97.91ms
step:1140/1770 train_time:110644ms step_avg:97.91ms
step:1141/1770 train_time:110745ms step_avg:97.92ms
step:1142/1770 train_time:110846ms step_avg:97.92ms
step:1143/1770 train_time:110947ms step_avg:97.92ms
step:1144/1770 train_time:111049ms step_avg:97.93ms
step:1145/1770 train_time:111150ms step_avg:97.93ms
step:1146/1770 train_time:111252ms step_avg:97.93ms
step:1147/1770 train_time:111353ms step_avg:97.94ms
step:1148/1770 train_time:111454ms step_avg:97.94ms
step:1149/1770 train_time:111554ms step_avg:97.94ms
step:1150/1770 train_time:111655ms step_avg:97.94ms
step:1151/1770 train_time:111757ms step_avg:97.95ms
step:1152/1770 train_time:111858ms step_avg:97.95ms
step:1153/1770 train_time:111959ms step_avg:97.95ms
step:1154/1770 train_time:112059ms step_avg:97.95ms
step:1155/1770 train_time:112160ms step_avg:97.96ms
step:1156/1770 train_time:112266ms step_avg:97.96ms
step:1157/1770 train_time:112364ms step_avg:97.96ms
step:1158/1770 train_time:112465ms step_avg:97.97ms
step:1159/1770 train_time:112566ms step_avg:97.97ms
step:1160/1770 train_time:112668ms step_avg:97.97ms
step:1161/1770 train_time:112769ms step_avg:97.97ms
step:1162/1770 train_time:112870ms step_avg:97.98ms
step:1163/1770 train_time:112972ms step_avg:97.98ms
step:1164/1770 train_time:113073ms step_avg:97.98ms
step:1165/1770 train_time:113173ms step_avg:97.99ms
step:1166/1770 train_time:113274ms step_avg:97.99ms
step:1167/1770 train_time:113374ms step_avg:97.99ms
step:1168/1770 train_time:113476ms step_avg:97.99ms
step:1169/1770 train_time:113576ms step_avg:97.99ms
step:1170/1770 train_time:113677ms step_avg:98.00ms
step:1171/1770 train_time:113778ms step_avg:98.00ms
step:1172/1770 train_time:113881ms step_avg:98.00ms
step:1173/1770 train_time:113980ms step_avg:98.00ms
step:1174/1770 train_time:114081ms step_avg:98.01ms
step:1175/1770 train_time:114183ms step_avg:98.01ms
step:1176/1770 train_time:114285ms step_avg:98.01ms
step:1177/1770 train_time:114386ms step_avg:98.02ms
step:1178/1770 train_time:114489ms step_avg:98.02ms
step:1179/1770 train_time:114589ms step_avg:98.02ms
step:1180/1770 train_time:114691ms step_avg:98.03ms
step:1181/1770 train_time:114792ms step_avg:98.03ms
step:1182/1770 train_time:114893ms step_avg:98.03ms
step:1183/1770 train_time:114995ms step_avg:98.04ms
step:1184/1770 train_time:115098ms step_avg:98.04ms
step:1185/1770 train_time:115199ms step_avg:98.04ms
step:1186/1770 train_time:115302ms step_avg:98.05ms
step:1187/1770 train_time:115408ms step_avg:98.05ms
step:1188/1770 train_time:115510ms step_avg:98.06ms
step:1189/1770 train_time:115611ms step_avg:98.06ms
step:1190/1770 train_time:115713ms step_avg:98.06ms
step:1191/1770 train_time:115815ms step_avg:98.07ms
step:1192/1770 train_time:115917ms step_avg:98.07ms
step:1193/1770 train_time:116020ms step_avg:98.07ms
step:1194/1770 train_time:116121ms step_avg:98.08ms
step:1195/1770 train_time:116224ms step_avg:98.08ms
step:1196/1770 train_time:116327ms step_avg:98.08ms
step:1197/1770 train_time:116429ms step_avg:98.09ms
step:1198/1770 train_time:116531ms step_avg:98.09ms
step:1199/1770 train_time:116633ms step_avg:98.09ms
step:1200/1770 train_time:116736ms step_avg:98.10ms
step:1201/1770 train_time:116838ms step_avg:98.10ms
step:1202/1770 train_time:116939ms step_avg:98.10ms
step:1203/1770 train_time:117040ms step_avg:98.11ms
step:1204/1770 train_time:117143ms step_avg:98.11ms
step:1205/1770 train_time:117245ms step_avg:98.11ms
step:1206/1770 train_time:117348ms step_avg:98.12ms
step:1207/1770 train_time:117450ms step_avg:98.12ms
step:1208/1770 train_time:117552ms step_avg:98.12ms
step:1209/1770 train_time:117654ms step_avg:98.13ms
step:1210/1770 train_time:117756ms step_avg:98.13ms
step:1211/1770 train_time:117858ms step_avg:98.13ms
step:1212/1770 train_time:117963ms step_avg:98.14ms
step:1213/1770 train_time:118065ms step_avg:98.14ms
step:1214/1770 train_time:118166ms step_avg:98.14ms
step:1215/1770 train_time:118269ms step_avg:98.15ms
step:1216/1770 train_time:118374ms step_avg:98.15ms
step:1217/1770 train_time:118475ms step_avg:98.16ms
step:1218/1770 train_time:118577ms step_avg:98.16ms
step:1219/1770 train_time:118680ms step_avg:98.16ms
step:1220/1770 train_time:118783ms step_avg:98.17ms
step:1221/1770 train_time:118885ms step_avg:98.17ms
step:1222/1770 train_time:118990ms step_avg:98.18ms
step:1223/1770 train_time:119091ms step_avg:98.18ms
step:1224/1770 train_time:119194ms step_avg:98.18ms
step:1225/1770 train_time:119296ms step_avg:98.19ms
step:1226/1770 train_time:119398ms step_avg:98.19ms
step:1227/1770 train_time:119502ms step_avg:98.19ms
step:1228/1770 train_time:119606ms step_avg:98.20ms
step:1229/1770 train_time:119708ms step_avg:98.20ms
step:1230/1770 train_time:119810ms step_avg:98.20ms
step:1231/1770 train_time:119912ms step_avg:98.21ms
step:1232/1770 train_time:120014ms step_avg:98.21ms
step:1233/1770 train_time:120116ms step_avg:98.21ms
step:1234/1770 train_time:120218ms step_avg:98.22ms
step:1235/1770 train_time:120320ms step_avg:98.22ms
step:1236/1770 train_time:120422ms step_avg:98.22ms
step:1237/1770 train_time:120524ms step_avg:98.23ms
step:1238/1770 train_time:120628ms step_avg:98.23ms
step:1239/1770 train_time:120730ms step_avg:98.23ms
step:1240/1770 train_time:120832ms step_avg:98.24ms
step:1241/1770 train_time:120935ms step_avg:98.24ms
step:1242/1770 train_time:121036ms step_avg:98.24ms
step:1243/1770 train_time:121138ms step_avg:98.25ms
step:1244/1770 train_time:121239ms step_avg:98.25ms
step:1245/1770 train_time:121340ms step_avg:98.25ms
step:1246/1770 train_time:121443ms step_avg:98.26ms
step:1247/1770 train_time:121545ms step_avg:98.26ms
step:1248/1770 train_time:121648ms step_avg:98.26ms
step:1249/1770 train_time:121751ms step_avg:98.27ms
step:1250/1770 train_time:121853ms step_avg:98.27ms
step:1250/1770 val_loss:3.4284 train_time:121954ms step_avg:98.35ms
step:1251/1770 train_time:121978ms step_avg:98.29ms
step:1252/1770 train_time:122063ms step_avg:98.28ms
step:1253/1770 train_time:122165ms step_avg:98.28ms
step:1254/1770 train_time:122267ms step_avg:98.29ms
step:1255/1770 train_time:122371ms step_avg:98.29ms
step:1256/1770 train_time:122472ms step_avg:98.29ms
step:1257/1770 train_time:122573ms step_avg:98.29ms
step:1258/1770 train_time:122676ms step_avg:98.30ms
step:1259/1770 train_time:122777ms step_avg:98.30ms
step:1260/1770 train_time:122879ms step_avg:98.30ms
step:1261/1770 train_time:122983ms step_avg:98.31ms
step:1262/1770 train_time:123086ms step_avg:98.31ms
step:1263/1770 train_time:123188ms step_avg:98.31ms
step:1264/1770 train_time:123291ms step_avg:98.32ms
step:1265/1770 train_time:123392ms step_avg:98.32ms
step:1266/1770 train_time:123495ms step_avg:98.32ms
step:1267/1770 train_time:123597ms step_avg:98.33ms
step:1268/1770 train_time:123700ms step_avg:98.33ms
step:1269/1770 train_time:123801ms step_avg:98.33ms
step:1270/1770 train_time:123904ms step_avg:98.34ms
step:1271/1770 train_time:124007ms step_avg:98.34ms
step:1272/1770 train_time:124109ms step_avg:98.34ms
step:1273/1770 train_time:124212ms step_avg:98.35ms
step:1274/1770 train_time:124314ms step_avg:98.35ms
step:1275/1770 train_time:124416ms step_avg:98.35ms
step:1276/1770 train_time:124519ms step_avg:98.36ms
step:1277/1770 train_time:124621ms step_avg:98.36ms
step:1278/1770 train_time:124725ms step_avg:98.36ms
step:1279/1770 train_time:124827ms step_avg:98.37ms
step:1280/1770 train_time:124930ms step_avg:98.37ms
step:1281/1770 train_time:125031ms step_avg:98.37ms
step:1282/1770 train_time:125135ms step_avg:98.38ms
step:1283/1770 train_time:125239ms step_avg:98.38ms
step:1284/1770 train_time:125339ms step_avg:98.38ms
step:1285/1770 train_time:125441ms step_avg:98.39ms
step:1286/1770 train_time:125544ms step_avg:98.39ms
step:1287/1770 train_time:125648ms step_avg:98.39ms
step:1288/1770 train_time:125751ms step_avg:98.40ms
step:1289/1770 train_time:125854ms step_avg:98.40ms
step:1290/1770 train_time:125955ms step_avg:98.40ms
step:1291/1770 train_time:126057ms step_avg:98.40ms
step:1292/1770 train_time:126159ms step_avg:98.41ms
step:1293/1770 train_time:126261ms step_avg:98.41ms
step:1294/1770 train_time:126363ms step_avg:98.41ms
step:1295/1770 train_time:126466ms step_avg:98.42ms
step:1296/1770 train_time:126568ms step_avg:98.42ms
step:1297/1770 train_time:126670ms step_avg:98.42ms
step:1298/1770 train_time:126774ms step_avg:98.43ms
step:1299/1770 train_time:126875ms step_avg:98.43ms
step:1300/1770 train_time:126977ms step_avg:98.43ms
step:1301/1770 train_time:127079ms step_avg:98.43ms
step:1302/1770 train_time:127181ms step_avg:98.44ms
step:1303/1770 train_time:127283ms step_avg:98.44ms
step:1304/1770 train_time:127385ms step_avg:98.44ms
step:1305/1770 train_time:127488ms step_avg:98.45ms
step:1306/1770 train_time:127590ms step_avg:98.45ms
step:1307/1770 train_time:127692ms step_avg:98.45ms
step:1308/1770 train_time:127794ms step_avg:98.45ms
step:1309/1770 train_time:127896ms step_avg:98.46ms
step:1310/1770 train_time:127997ms step_avg:98.46ms
step:1311/1770 train_time:128098ms step_avg:98.46ms
step:1312/1770 train_time:128200ms step_avg:98.46ms
step:1313/1770 train_time:128303ms step_avg:98.47ms
step:1314/1770 train_time:128406ms step_avg:98.47ms
step:1315/1770 train_time:128508ms step_avg:98.47ms
step:1316/1770 train_time:128611ms step_avg:98.48ms
step:1317/1770 train_time:128713ms step_avg:98.48ms
step:1318/1770 train_time:128819ms step_avg:98.49ms
step:1319/1770 train_time:128921ms step_avg:98.49ms
step:1320/1770 train_time:129023ms step_avg:98.49ms
step:1321/1770 train_time:129126ms step_avg:98.49ms
step:1322/1770 train_time:129228ms step_avg:98.50ms
step:1323/1770 train_time:129330ms step_avg:98.50ms
step:1324/1770 train_time:129433ms step_avg:98.50ms
step:1325/1770 train_time:129536ms step_avg:98.51ms
step:1326/1770 train_time:129638ms step_avg:98.51ms
step:1327/1770 train_time:129744ms step_avg:98.51ms
step:1328/1770 train_time:129846ms step_avg:98.52ms
step:1329/1770 train_time:129948ms step_avg:98.52ms
step:1330/1770 train_time:130050ms step_avg:98.52ms
step:1331/1770 train_time:130151ms step_avg:98.52ms
step:1332/1770 train_time:130252ms step_avg:98.53ms
step:1333/1770 train_time:130354ms step_avg:98.53ms
step:1334/1770 train_time:130456ms step_avg:98.53ms
step:1335/1770 train_time:130559ms step_avg:98.54ms
step:1336/1770 train_time:130661ms step_avg:98.54ms
step:1337/1770 train_time:130763ms step_avg:98.54ms
step:1338/1770 train_time:130865ms step_avg:98.54ms
step:1339/1770 train_time:130967ms step_avg:98.55ms
step:1340/1770 train_time:131071ms step_avg:98.55ms
step:1341/1770 train_time:131173ms step_avg:98.55ms
step:1342/1770 train_time:131275ms step_avg:98.55ms
step:1343/1770 train_time:131378ms step_avg:98.56ms
step:1344/1770 train_time:131481ms step_avg:98.56ms
step:1345/1770 train_time:131584ms step_avg:98.56ms
step:1346/1770 train_time:131686ms step_avg:98.57ms
step:1347/1770 train_time:131789ms step_avg:98.57ms
step:1348/1770 train_time:131893ms step_avg:98.58ms
step:1349/1770 train_time:131996ms step_avg:98.58ms
step:1350/1770 train_time:132100ms step_avg:98.58ms
step:1351/1770 train_time:132201ms step_avg:98.58ms
step:1352/1770 train_time:132304ms step_avg:98.59ms
step:1353/1770 train_time:132407ms step_avg:98.59ms
step:1354/1770 train_time:132509ms step_avg:98.59ms
step:1355/1770 train_time:132611ms step_avg:98.60ms
step:1356/1770 train_time:132713ms step_avg:98.60ms
step:1357/1770 train_time:132815ms step_avg:98.60ms
step:1358/1770 train_time:132917ms step_avg:98.60ms
step:1359/1770 train_time:133019ms step_avg:98.61ms
step:1360/1770 train_time:133122ms step_avg:98.61ms
step:1361/1770 train_time:133225ms step_avg:98.61ms
step:1362/1770 train_time:133327ms step_avg:98.61ms
step:1363/1770 train_time:133431ms step_avg:98.62ms
step:1364/1770 train_time:133533ms step_avg:98.62ms
step:1365/1770 train_time:133634ms step_avg:98.62ms
step:1366/1770 train_time:133735ms step_avg:98.62ms
step:1367/1770 train_time:133838ms step_avg:98.63ms
step:1368/1770 train_time:133940ms step_avg:98.63ms
step:1369/1770 train_time:134043ms step_avg:98.63ms
step:1370/1770 train_time:134145ms step_avg:98.64ms
step:1371/1770 train_time:134249ms step_avg:98.64ms
step:1372/1770 train_time:134351ms step_avg:98.64ms
step:1373/1770 train_time:134453ms step_avg:98.64ms
step:1374/1770 train_time:134556ms step_avg:98.65ms
step:1375/1770 train_time:134659ms step_avg:98.65ms
step:1375/1770 val_loss:3.3845 train_time:134760ms step_avg:98.73ms
step:1376/1770 train_time:134781ms step_avg:98.67ms
step:1377/1770 train_time:134868ms step_avg:98.66ms
step:1378/1770 train_time:134969ms step_avg:98.66ms
step:1379/1770 train_time:135071ms step_avg:98.66ms
step:1380/1770 train_time:135173ms step_avg:98.67ms
step:1381/1770 train_time:135275ms step_avg:98.67ms
step:1382/1770 train_time:135377ms step_avg:98.67ms
step:1383/1770 train_time:135482ms step_avg:98.68ms
step:1384/1770 train_time:135581ms step_avg:98.68ms
step:1385/1770 train_time:135683ms step_avg:98.68ms
step:1386/1770 train_time:135786ms step_avg:98.68ms
step:1387/1770 train_time:135889ms step_avg:98.68ms
step:1388/1770 train_time:135991ms step_avg:98.69ms
step:1389/1770 train_time:136094ms step_avg:98.69ms
step:1390/1770 train_time:136196ms step_avg:98.69ms
step:1391/1770 train_time:136298ms step_avg:98.70ms
step:1392/1770 train_time:136400ms step_avg:98.70ms
step:1393/1770 train_time:136502ms step_avg:98.70ms
step:1394/1770 train_time:136604ms step_avg:98.70ms
step:1395/1770 train_time:136707ms step_avg:98.71ms
step:1396/1770 train_time:136810ms step_avg:98.71ms
step:1397/1770 train_time:136913ms step_avg:98.71ms
step:1398/1770 train_time:137015ms step_avg:98.71ms
step:1399/1770 train_time:137117ms step_avg:98.72ms
step:1400/1770 train_time:137220ms step_avg:98.72ms
step:1401/1770 train_time:137322ms step_avg:98.72ms
step:1402/1770 train_time:137424ms step_avg:98.72ms
step:1403/1770 train_time:137526ms step_avg:98.73ms
step:1404/1770 train_time:137628ms step_avg:98.73ms
step:1405/1770 train_time:137730ms step_avg:98.73ms
step:1406/1770 train_time:137833ms step_avg:98.73ms
step:1407/1770 train_time:137935ms step_avg:98.74ms
step:1408/1770 train_time:138038ms step_avg:98.74ms
step:1409/1770 train_time:138140ms step_avg:98.74ms
step:1410/1770 train_time:138242ms step_avg:98.74ms
step:1411/1770 train_time:138345ms step_avg:98.75ms
step:1412/1770 train_time:138446ms step_avg:98.75ms
step:1413/1770 train_time:138547ms step_avg:98.75ms
step:1414/1770 train_time:138650ms step_avg:98.75ms
step:1415/1770 train_time:138752ms step_avg:98.76ms
step:1416/1770 train_time:138855ms step_avg:98.76ms
step:1417/1770 train_time:138958ms step_avg:98.76ms
step:1418/1770 train_time:139060ms step_avg:98.76ms
step:1419/1770 train_time:139163ms step_avg:98.77ms
step:1420/1770 train_time:139265ms step_avg:98.77ms
step:1421/1770 train_time:139367ms step_avg:98.77ms
step:1422/1770 train_time:139469ms step_avg:98.77ms
step:1423/1770 train_time:139571ms step_avg:98.78ms
step:1424/1770 train_time:139673ms step_avg:98.78ms
step:1425/1770 train_time:139775ms step_avg:98.78ms
step:1426/1770 train_time:139879ms step_avg:98.78ms
step:1427/1770 train_time:139981ms step_avg:98.79ms
step:1428/1770 train_time:140085ms step_avg:98.79ms
step:1429/1770 train_time:140188ms step_avg:98.79ms
step:1430/1770 train_time:140290ms step_avg:98.80ms
step:1431/1770 train_time:140394ms step_avg:98.80ms
step:1432/1770 train_time:140497ms step_avg:98.80ms
step:1433/1770 train_time:140600ms step_avg:98.81ms
step:1434/1770 train_time:140701ms step_avg:98.81ms
step:1435/1770 train_time:140803ms step_avg:98.81ms
step:1436/1770 train_time:140906ms step_avg:98.81ms
step:1437/1770 train_time:141009ms step_avg:98.81ms
step:1438/1770 train_time:141110ms step_avg:98.82ms
step:1439/1770 train_time:141213ms step_avg:98.82ms
step:1440/1770 train_time:141315ms step_avg:98.82ms
step:1441/1770 train_time:141421ms step_avg:98.83ms
step:1442/1770 train_time:141523ms step_avg:98.83ms
step:1443/1770 train_time:141625ms step_avg:98.83ms
step:1444/1770 train_time:141727ms step_avg:98.83ms
step:1445/1770 train_time:141830ms step_avg:98.84ms
step:1446/1770 train_time:141933ms step_avg:98.84ms
step:1447/1770 train_time:142037ms step_avg:98.84ms
step:1448/1770 train_time:142140ms step_avg:98.85ms
step:1449/1770 train_time:142244ms step_avg:98.85ms
step:1450/1770 train_time:142347ms step_avg:98.85ms
step:1451/1770 train_time:142451ms step_avg:98.86ms
step:1452/1770 train_time:142555ms step_avg:98.86ms
step:1453/1770 train_time:142657ms step_avg:98.86ms
step:1454/1770 train_time:142760ms step_avg:98.86ms
step:1455/1770 train_time:142865ms step_avg:98.87ms
step:1456/1770 train_time:142969ms step_avg:98.87ms
step:1457/1770 train_time:143072ms step_avg:98.87ms
step:1458/1770 train_time:143176ms step_avg:98.88ms
step:1459/1770 train_time:143281ms step_avg:98.88ms
step:1460/1770 train_time:143384ms step_avg:98.89ms
step:1461/1770 train_time:143487ms step_avg:98.89ms
step:1462/1770 train_time:143591ms step_avg:98.89ms
step:1463/1770 train_time:143695ms step_avg:98.90ms
step:1464/1770 train_time:143801ms step_avg:98.90ms
step:1465/1770 train_time:143904ms step_avg:98.90ms
step:1466/1770 train_time:144008ms step_avg:98.91ms
step:1467/1770 train_time:144113ms step_avg:98.91ms
step:1468/1770 train_time:144216ms step_avg:98.91ms
step:1469/1770 train_time:144319ms step_avg:98.92ms
step:1470/1770 train_time:144422ms step_avg:98.92ms
step:1471/1770 train_time:144525ms step_avg:98.92ms
step:1472/1770 train_time:144628ms step_avg:98.92ms
step:1473/1770 train_time:144733ms step_avg:98.93ms
step:1474/1770 train_time:144838ms step_avg:98.93ms
step:1475/1770 train_time:144941ms step_avg:98.94ms
step:1476/1770 train_time:145044ms step_avg:98.94ms
step:1477/1770 train_time:145149ms step_avg:98.94ms
step:1478/1770 train_time:145253ms step_avg:98.95ms
step:1479/1770 train_time:145356ms step_avg:98.95ms
step:1480/1770 train_time:145460ms step_avg:98.95ms
step:1481/1770 train_time:145567ms step_avg:98.96ms
step:1482/1770 train_time:145670ms step_avg:98.96ms
step:1483/1770 train_time:145773ms step_avg:98.96ms
step:1484/1770 train_time:145876ms step_avg:98.97ms
step:1485/1770 train_time:145983ms step_avg:98.97ms
step:1486/1770 train_time:146083ms step_avg:98.97ms
step:1487/1770 train_time:146186ms step_avg:98.98ms
step:1488/1770 train_time:146290ms step_avg:98.98ms
step:1489/1770 train_time:146395ms step_avg:98.98ms
step:1490/1770 train_time:146500ms step_avg:98.99ms
step:1491/1770 train_time:146602ms step_avg:98.99ms
step:1492/1770 train_time:146707ms step_avg:98.99ms
step:1493/1770 train_time:146811ms step_avg:99.00ms
step:1494/1770 train_time:146918ms step_avg:99.00ms
step:1495/1770 train_time:147020ms step_avg:99.00ms
step:1496/1770 train_time:147124ms step_avg:99.01ms
step:1497/1770 train_time:147227ms step_avg:99.01ms
step:1498/1770 train_time:147330ms step_avg:99.01ms
step:1499/1770 train_time:147433ms step_avg:99.01ms
step:1500/1770 train_time:147536ms step_avg:99.02ms
step:1500/1770 val_loss:3.3464 train_time:147639ms step_avg:99.09ms
step:1501/1770 train_time:147660ms step_avg:99.03ms
step:1502/1770 train_time:147750ms step_avg:99.03ms
step:1503/1770 train_time:147853ms step_avg:99.03ms
step:1504/1770 train_time:147956ms step_avg:99.03ms
step:1505/1770 train_time:148061ms step_avg:99.04ms
step:1506/1770 train_time:148164ms step_avg:99.04ms
step:1507/1770 train_time:148268ms step_avg:99.04ms
step:1508/1770 train_time:148373ms step_avg:99.05ms
step:1509/1770 train_time:148476ms step_avg:99.05ms
step:1510/1770 train_time:148578ms step_avg:99.05ms
step:1511/1770 train_time:148684ms step_avg:99.06ms
step:1512/1770 train_time:148789ms step_avg:99.06ms
step:1513/1770 train_time:148892ms step_avg:99.06ms
step:1514/1770 train_time:148995ms step_avg:99.07ms
step:1515/1770 train_time:149099ms step_avg:99.07ms
step:1516/1770 train_time:149203ms step_avg:99.07ms
step:1517/1770 train_time:149306ms step_avg:99.07ms
step:1518/1770 train_time:149411ms step_avg:99.08ms
step:1519/1770 train_time:149513ms step_avg:99.08ms
step:1520/1770 train_time:149617ms step_avg:99.08ms
step:1521/1770 train_time:149721ms step_avg:99.09ms
step:1522/1770 train_time:149825ms step_avg:99.09ms
step:1523/1770 train_time:149929ms step_avg:99.09ms
step:1524/1770 train_time:150031ms step_avg:99.10ms
step:1525/1770 train_time:150133ms step_avg:99.10ms
step:1526/1770 train_time:150236ms step_avg:99.10ms
step:1527/1770 train_time:150340ms step_avg:99.10ms
step:1528/1770 train_time:150446ms step_avg:99.11ms
step:1529/1770 train_time:150549ms step_avg:99.11ms
step:1530/1770 train_time:150651ms step_avg:99.11ms
step:1531/1770 train_time:150754ms step_avg:99.12ms
step:1532/1770 train_time:150858ms step_avg:99.12ms
step:1533/1770 train_time:150962ms step_avg:99.12ms
step:1534/1770 train_time:151066ms step_avg:99.12ms
step:1535/1770 train_time:151169ms step_avg:99.13ms
step:1536/1770 train_time:151272ms step_avg:99.13ms
step:1537/1770 train_time:151375ms step_avg:99.13ms
step:1538/1770 train_time:151480ms step_avg:99.14ms
step:1539/1770 train_time:151584ms step_avg:99.14ms
step:1540/1770 train_time:151690ms step_avg:99.14ms
step:1541/1770 train_time:151794ms step_avg:99.15ms
step:1542/1770 train_time:151898ms step_avg:99.15ms
step:1543/1770 train_time:152000ms step_avg:99.15ms
step:1544/1770 train_time:152106ms step_avg:99.16ms
step:1545/1770 train_time:152210ms step_avg:99.16ms
step:1546/1770 train_time:152313ms step_avg:99.16ms
step:1547/1770 train_time:152416ms step_avg:99.16ms
step:1548/1770 train_time:152520ms step_avg:99.17ms
step:1549/1770 train_time:152625ms step_avg:99.17ms
step:1550/1770 train_time:152729ms step_avg:99.17ms
step:1551/1770 train_time:152831ms step_avg:99.18ms
step:1552/1770 train_time:152936ms step_avg:99.18ms
step:1553/1770 train_time:153040ms step_avg:99.18ms
step:1554/1770 train_time:153143ms step_avg:99.19ms
step:1555/1770 train_time:153248ms step_avg:99.19ms
step:1556/1770 train_time:153350ms step_avg:99.19ms
step:1557/1770 train_time:153453ms step_avg:99.19ms
step:1558/1770 train_time:153558ms step_avg:99.20ms
step:1559/1770 train_time:153662ms step_avg:99.20ms
step:1560/1770 train_time:153765ms step_avg:99.20ms
step:1561/1770 train_time:153871ms step_avg:99.21ms
step:1562/1770 train_time:153974ms step_avg:99.21ms
step:1563/1770 train_time:154078ms step_avg:99.21ms
step:1564/1770 train_time:154181ms step_avg:99.22ms
step:1565/1770 train_time:154284ms step_avg:99.22ms
step:1566/1770 train_time:154387ms step_avg:99.22ms
step:1567/1770 train_time:154490ms step_avg:99.22ms
step:1568/1770 train_time:154593ms step_avg:99.23ms
step:1569/1770 train_time:154700ms step_avg:99.23ms
step:1570/1770 train_time:154803ms step_avg:99.23ms
step:1571/1770 train_time:154906ms step_avg:99.24ms
step:1572/1770 train_time:155010ms step_avg:99.24ms
step:1573/1770 train_time:155116ms step_avg:99.24ms
step:1574/1770 train_time:155219ms step_avg:99.25ms
step:1575/1770 train_time:155322ms step_avg:99.25ms
step:1576/1770 train_time:155425ms step_avg:99.25ms
step:1577/1770 train_time:155530ms step_avg:99.25ms
step:1578/1770 train_time:155635ms step_avg:99.26ms
step:1579/1770 train_time:155738ms step_avg:99.26ms
step:1580/1770 train_time:155841ms step_avg:99.26ms
step:1581/1770 train_time:155948ms step_avg:99.27ms
step:1582/1770 train_time:156052ms step_avg:99.27ms
step:1583/1770 train_time:156156ms step_avg:99.27ms
step:1584/1770 train_time:156260ms step_avg:99.28ms
step:1585/1770 train_time:156365ms step_avg:99.28ms
step:1586/1770 train_time:156471ms step_avg:99.28ms
step:1587/1770 train_time:156575ms step_avg:99.29ms
step:1588/1770 train_time:156678ms step_avg:99.29ms
step:1589/1770 train_time:156783ms step_avg:99.29ms
step:1590/1770 train_time:156886ms step_avg:99.30ms
step:1591/1770 train_time:156989ms step_avg:99.30ms
step:1592/1770 train_time:157093ms step_avg:99.30ms
step:1593/1770 train_time:157196ms step_avg:99.30ms
step:1594/1770 train_time:157301ms step_avg:99.31ms
step:1595/1770 train_time:157404ms step_avg:99.31ms
step:1596/1770 train_time:157509ms step_avg:99.31ms
step:1597/1770 train_time:157612ms step_avg:99.31ms
step:1598/1770 train_time:157715ms step_avg:99.32ms
step:1599/1770 train_time:157820ms step_avg:99.32ms
step:1600/1770 train_time:157926ms step_avg:99.32ms
step:1601/1770 train_time:158031ms step_avg:99.33ms
step:1602/1770 train_time:158135ms step_avg:99.33ms
step:1603/1770 train_time:158241ms step_avg:99.33ms
step:1604/1770 train_time:158341ms step_avg:99.34ms
step:1605/1770 train_time:158444ms step_avg:99.34ms
step:1606/1770 train_time:158548ms step_avg:99.34ms
step:1607/1770 train_time:158654ms step_avg:99.35ms
step:1608/1770 train_time:158758ms step_avg:99.35ms
step:1609/1770 train_time:158860ms step_avg:99.35ms
step:1610/1770 train_time:158966ms step_avg:99.35ms
step:1611/1770 train_time:159071ms step_avg:99.36ms
step:1612/1770 train_time:159176ms step_avg:99.36ms
step:1613/1770 train_time:159279ms step_avg:99.36ms
step:1614/1770 train_time:159383ms step_avg:99.37ms
step:1615/1770 train_time:159487ms step_avg:99.37ms
step:1616/1770 train_time:159590ms step_avg:99.37ms
step:1617/1770 train_time:159695ms step_avg:99.37ms
step:1618/1770 train_time:159800ms step_avg:99.38ms
step:1619/1770 train_time:159904ms step_avg:99.38ms
step:1620/1770 train_time:160008ms step_avg:99.38ms
step:1621/1770 train_time:160111ms step_avg:99.39ms
step:1622/1770 train_time:160215ms step_avg:99.39ms
step:1623/1770 train_time:160322ms step_avg:99.39ms
step:1624/1770 train_time:160424ms step_avg:99.40ms
step:1625/1770 train_time:160527ms step_avg:99.40ms
step:1625/1770 val_loss:3.3118 train_time:160629ms step_avg:99.46ms
step:1626/1770 train_time:160654ms step_avg:99.41ms
step:1627/1770 train_time:160741ms step_avg:99.41ms
step:1628/1770 train_time:160843ms step_avg:99.41ms
step:1629/1770 train_time:160946ms step_avg:99.41ms
step:1630/1770 train_time:161049ms step_avg:99.41ms
step:1631/1770 train_time:161152ms step_avg:99.42ms
step:1632/1770 train_time:161255ms step_avg:99.42ms
step:1633/1770 train_time:161358ms step_avg:99.42ms
step:1634/1770 train_time:161461ms step_avg:99.42ms
step:1635/1770 train_time:161565ms step_avg:99.42ms
step:1636/1770 train_time:161671ms step_avg:99.43ms
step:1637/1770 train_time:161775ms step_avg:99.43ms
step:1638/1770 train_time:161878ms step_avg:99.43ms
step:1639/1770 train_time:161982ms step_avg:99.44ms
step:1640/1770 train_time:162087ms step_avg:99.44ms
step:1641/1770 train_time:162190ms step_avg:99.44ms
step:1642/1770 train_time:162293ms step_avg:99.44ms
step:1643/1770 train_time:162396ms step_avg:99.45ms
step:1644/1770 train_time:162501ms step_avg:99.45ms
step:1645/1770 train_time:162605ms step_avg:99.45ms
step:1646/1770 train_time:162710ms step_avg:99.46ms
step:1647/1770 train_time:162814ms step_avg:99.46ms
step:1648/1770 train_time:162917ms step_avg:99.46ms
step:1649/1770 train_time:163020ms step_avg:99.46ms
step:1650/1770 train_time:163125ms step_avg:99.47ms
step:1651/1770 train_time:163228ms step_avg:99.47ms
step:1652/1770 train_time:163331ms step_avg:99.47ms
step:1653/1770 train_time:163434ms step_avg:99.47ms
step:1654/1770 train_time:163541ms step_avg:99.48ms
step:1655/1770 train_time:163647ms step_avg:99.48ms
step:1656/1770 train_time:163750ms step_avg:99.48ms
step:1657/1770 train_time:163855ms step_avg:99.49ms
step:1658/1770 train_time:163959ms step_avg:99.49ms
step:1659/1770 train_time:164063ms step_avg:99.49ms
step:1660/1770 train_time:164166ms step_avg:99.49ms
step:1661/1770 train_time:164272ms step_avg:99.50ms
step:1662/1770 train_time:164375ms step_avg:99.50ms
step:1663/1770 train_time:164478ms step_avg:99.50ms
step:1664/1770 train_time:164581ms step_avg:99.51ms
step:1665/1770 train_time:164685ms step_avg:99.51ms
step:1666/1770 train_time:164789ms step_avg:99.51ms
step:1667/1770 train_time:164891ms step_avg:99.51ms
step:1668/1770 train_time:164994ms step_avg:99.51ms
step:1669/1770 train_time:165097ms step_avg:99.52ms
step:1670/1770 train_time:165200ms step_avg:99.52ms
step:1671/1770 train_time:165304ms step_avg:99.52ms
step:1672/1770 train_time:165409ms step_avg:99.52ms
step:1673/1770 train_time:165514ms step_avg:99.53ms
step:1674/1770 train_time:165617ms step_avg:99.53ms
step:1675/1770 train_time:165720ms step_avg:99.53ms
step:1676/1770 train_time:165825ms step_avg:99.53ms
step:1677/1770 train_time:165932ms step_avg:99.54ms
step:1678/1770 train_time:166034ms step_avg:99.54ms
step:1679/1770 train_time:166138ms step_avg:99.54ms
step:1680/1770 train_time:166241ms step_avg:99.55ms
step:1681/1770 train_time:166346ms step_avg:99.55ms
step:1682/1770 train_time:166452ms step_avg:99.55ms
step:1683/1770 train_time:166554ms step_avg:99.55ms
step:1684/1770 train_time:166657ms step_avg:99.56ms
step:1685/1770 train_time:166761ms step_avg:99.56ms
step:1686/1770 train_time:166866ms step_avg:99.56ms
step:1687/1770 train_time:166971ms step_avg:99.57ms
step:1688/1770 train_time:167074ms step_avg:99.57ms
step:1689/1770 train_time:167178ms step_avg:99.57ms
step:1690/1770 train_time:167282ms step_avg:99.57ms
step:1691/1770 train_time:167385ms step_avg:99.57ms
step:1692/1770 train_time:167490ms step_avg:99.58ms
step:1693/1770 train_time:167595ms step_avg:99.58ms
step:1694/1770 train_time:167698ms step_avg:99.58ms
step:1695/1770 train_time:167803ms step_avg:99.59ms
step:1696/1770 train_time:167908ms step_avg:99.59ms
step:1697/1770 train_time:168014ms step_avg:99.59ms
step:1698/1770 train_time:168118ms step_avg:99.60ms
step:1699/1770 train_time:168221ms step_avg:99.60ms
step:1700/1770 train_time:168324ms step_avg:99.60ms
step:1701/1770 train_time:168427ms step_avg:99.60ms
step:1702/1770 train_time:168531ms step_avg:99.60ms
step:1703/1770 train_time:168634ms step_avg:99.61ms
step:1704/1770 train_time:168737ms step_avg:99.61ms
step:1705/1770 train_time:168840ms step_avg:99.61ms
step:1706/1770 train_time:168943ms step_avg:99.61ms
step:1707/1770 train_time:169047ms step_avg:99.62ms
step:1708/1770 train_time:169152ms step_avg:99.62ms
step:1709/1770 train_time:169256ms step_avg:99.62ms
step:1710/1770 train_time:169364ms step_avg:99.63ms
step:1711/1770 train_time:169471ms step_avg:99.63ms
step:1712/1770 train_time:169575ms step_avg:99.63ms
step:1713/1770 train_time:169682ms step_avg:99.64ms
step:1714/1770 train_time:169782ms step_avg:99.64ms
step:1715/1770 train_time:169886ms step_avg:99.64ms
step:1716/1770 train_time:169990ms step_avg:99.64ms
step:1717/1770 train_time:170094ms step_avg:99.64ms
step:1718/1770 train_time:170198ms step_avg:99.65ms
step:1719/1770 train_time:170303ms step_avg:99.65ms
step:1720/1770 train_time:170408ms step_avg:99.65ms
step:1721/1770 train_time:170512ms step_avg:99.66ms
step:1722/1770 train_time:170619ms step_avg:99.66ms
step:1723/1770 train_time:170725ms step_avg:99.66ms
step:1724/1770 train_time:170831ms step_avg:99.67ms
step:1725/1770 train_time:170938ms step_avg:99.67ms
step:1726/1770 train_time:171044ms step_avg:99.68ms
step:1727/1770 train_time:171148ms step_avg:99.68ms
step:1728/1770 train_time:171254ms step_avg:99.68ms
step:1729/1770 train_time:171357ms step_avg:99.68ms
step:1730/1770 train_time:171463ms step_avg:99.69ms
step:1731/1770 train_time:171569ms step_avg:99.69ms
step:1732/1770 train_time:171673ms step_avg:99.69ms
step:1733/1770 train_time:171778ms step_avg:99.70ms
step:1734/1770 train_time:171883ms step_avg:99.70ms
step:1735/1770 train_time:171988ms step_avg:99.70ms
step:1736/1770 train_time:172091ms step_avg:99.71ms
step:1737/1770 train_time:172196ms step_avg:99.71ms
step:1738/1770 train_time:172300ms step_avg:99.71ms
step:1739/1770 train_time:172404ms step_avg:99.71ms
step:1740/1770 train_time:172509ms step_avg:99.72ms
step:1741/1770 train_time:172615ms step_avg:99.72ms
step:1742/1770 train_time:172723ms step_avg:99.72ms
step:1743/1770 train_time:172828ms step_avg:99.73ms
step:1744/1770 train_time:172932ms step_avg:99.73ms
step:1745/1770 train_time:173036ms step_avg:99.73ms
step:1746/1770 train_time:173143ms step_avg:99.74ms
step:1747/1770 train_time:173246ms step_avg:99.74ms
step:1748/1770 train_time:173352ms step_avg:99.74ms
step:1749/1770 train_time:173457ms step_avg:99.75ms
step:1750/1770 train_time:173561ms step_avg:99.75ms
step:1750/1770 val_loss:3.2849 train_time:173664ms step_avg:99.81ms
step:1751/1770 train_time:173685ms step_avg:99.76ms
step:1752/1770 train_time:173775ms step_avg:99.76ms
step:1753/1770 train_time:173879ms step_avg:99.76ms
step:1754/1770 train_time:173984ms step_avg:99.76ms
step:1755/1770 train_time:174088ms step_avg:99.76ms
step:1756/1770 train_time:174193ms step_avg:99.77ms
step:1757/1770 train_time:174298ms step_avg:99.77ms
step:1758/1770 train_time:174402ms step_avg:99.77ms
step:1759/1770 train_time:174506ms step_avg:99.77ms
step:1760/1770 train_time:174610ms step_avg:99.78ms
step:1761/1770 train_time:174717ms step_avg:99.78ms
step:1762/1770 train_time:174825ms step_avg:99.79ms
step:1763/1770 train_time:174928ms step_avg:99.79ms
step:1764/1770 train_time:175033ms step_avg:99.79ms
step:1765/1770 train_time:175137ms step_avg:99.79ms
step:1766/1770 train_time:175245ms step_avg:99.80ms
step:1767/1770 train_time:175348ms step_avg:99.80ms
step:1768/1770 train_time:175453ms step_avg:99.80ms
step:1769/1770 train_time:175556ms step_avg:99.80ms
step:1770/1770 train_time:175659ms step_avg:99.81ms
step:1770/1770 val_loss:3.2818 train_time:175763ms step_avg:99.87ms
peak memory allocated: 29898 MiB reserved: 44552 MiB
