import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        self.attn_scale = 0.15

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_mask(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 16:00:00 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   32C    P0             117W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   26C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   32C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   32C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   28C    P0             111W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   31C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0             112W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:38113ms step_avg:nanms
step:2/1395 train_time:38587ms step_avg:nanms
step:3/1395 train_time:38706ms step_avg:nanms
step:4/1395 train_time:38828ms step_avg:nanms
step:5/1395 train_time:38952ms step_avg:nanms
step:6/1395 train_time:39074ms step_avg:nanms
step:7/1395 train_time:39197ms step_avg:nanms
step:8/1395 train_time:39319ms step_avg:nanms
step:9/1395 train_time:39441ms step_avg:nanms
step:10/1395 train_time:39565ms step_avg:nanms
step:11/1395 train_time:126ms step_avg:nanms
step:12/1395 train_time:249ms step_avg:nanms
step:13/1395 train_time:371ms step_avg:123.79ms
step:14/1395 train_time:494ms step_avg:123.47ms
step:15/1395 train_time:616ms step_avg:123.21ms
step:16/1395 train_time:740ms step_avg:123.26ms
step:17/1395 train_time:863ms step_avg:123.30ms
step:18/1395 train_time:987ms step_avg:123.40ms
step:19/1395 train_time:1111ms step_avg:123.39ms
step:20/1395 train_time:1234ms step_avg:123.40ms
step:21/1395 train_time:1357ms step_avg:123.36ms
step:22/1395 train_time:1480ms step_avg:123.34ms
step:23/1395 train_time:1605ms step_avg:123.47ms
step:24/1395 train_time:1729ms step_avg:123.52ms
step:25/1395 train_time:1853ms step_avg:123.53ms
step:26/1395 train_time:1975ms step_avg:123.46ms
step:27/1395 train_time:2098ms step_avg:123.43ms
step:28/1395 train_time:2223ms step_avg:123.49ms
step:29/1395 train_time:2345ms step_avg:123.43ms
step:30/1395 train_time:2469ms step_avg:123.45ms
step:31/1395 train_time:2592ms step_avg:123.43ms
step:32/1395 train_time:2716ms step_avg:123.47ms
step:33/1395 train_time:2840ms step_avg:123.46ms
step:34/1395 train_time:2964ms step_avg:123.49ms
step:35/1395 train_time:3086ms step_avg:123.43ms
step:36/1395 train_time:3210ms step_avg:123.44ms
step:37/1395 train_time:3332ms step_avg:123.41ms
step:38/1395 train_time:3455ms step_avg:123.38ms
step:39/1395 train_time:3577ms step_avg:123.34ms
step:40/1395 train_time:3700ms step_avg:123.34ms
step:41/1395 train_time:3824ms step_avg:123.34ms
step:42/1395 train_time:3946ms step_avg:123.31ms
step:43/1395 train_time:4069ms step_avg:123.30ms
step:44/1395 train_time:4192ms step_avg:123.29ms
step:45/1395 train_time:4315ms step_avg:123.29ms
step:46/1395 train_time:4440ms step_avg:123.32ms
step:47/1395 train_time:4563ms step_avg:123.32ms
step:48/1395 train_time:4686ms step_avg:123.33ms
step:49/1395 train_time:4810ms step_avg:123.34ms
step:50/1395 train_time:4934ms step_avg:123.34ms
step:51/1395 train_time:5056ms step_avg:123.31ms
step:52/1395 train_time:5178ms step_avg:123.29ms
step:53/1395 train_time:5303ms step_avg:123.34ms
step:54/1395 train_time:5426ms step_avg:123.32ms
step:55/1395 train_time:5550ms step_avg:123.34ms
step:56/1395 train_time:5673ms step_avg:123.33ms
step:57/1395 train_time:5797ms step_avg:123.35ms
step:58/1395 train_time:5921ms step_avg:123.36ms
step:59/1395 train_time:6045ms step_avg:123.37ms
step:60/1395 train_time:6169ms step_avg:123.37ms
step:61/1395 train_time:6292ms step_avg:123.37ms
step:62/1395 train_time:6417ms step_avg:123.40ms
step:63/1395 train_time:6541ms step_avg:123.42ms
step:64/1395 train_time:6664ms step_avg:123.41ms
step:65/1395 train_time:6787ms step_avg:123.40ms
step:66/1395 train_time:6911ms step_avg:123.41ms
step:67/1395 train_time:7033ms step_avg:123.39ms
step:68/1395 train_time:7156ms step_avg:123.37ms
step:69/1395 train_time:7278ms step_avg:123.35ms
step:70/1395 train_time:7403ms step_avg:123.38ms
step:71/1395 train_time:7526ms step_avg:123.37ms
step:72/1395 train_time:7649ms step_avg:123.37ms
step:73/1395 train_time:7772ms step_avg:123.36ms
step:74/1395 train_time:7897ms step_avg:123.39ms
step:75/1395 train_time:8019ms step_avg:123.37ms
step:76/1395 train_time:8143ms step_avg:123.37ms
step:77/1395 train_time:8267ms step_avg:123.38ms
step:78/1395 train_time:8390ms step_avg:123.38ms
step:79/1395 train_time:8513ms step_avg:123.38ms
step:80/1395 train_time:8635ms step_avg:123.36ms
step:81/1395 train_time:8758ms step_avg:123.36ms
step:82/1395 train_time:8885ms step_avg:123.40ms
step:83/1395 train_time:9009ms step_avg:123.41ms
step:84/1395 train_time:9132ms step_avg:123.41ms
step:85/1395 train_time:9255ms step_avg:123.40ms
step:86/1395 train_time:9377ms step_avg:123.39ms
step:87/1395 train_time:9500ms step_avg:123.38ms
step:88/1395 train_time:9624ms step_avg:123.39ms
step:89/1395 train_time:9747ms step_avg:123.38ms
step:90/1395 train_time:9870ms step_avg:123.38ms
step:91/1395 train_time:9993ms step_avg:123.37ms
step:92/1395 train_time:10118ms step_avg:123.38ms
step:93/1395 train_time:10242ms step_avg:123.40ms
step:94/1395 train_time:10365ms step_avg:123.39ms
step:95/1395 train_time:10487ms step_avg:123.38ms
step:96/1395 train_time:10611ms step_avg:123.38ms
step:97/1395 train_time:10735ms step_avg:123.39ms
step:98/1395 train_time:10859ms step_avg:123.40ms
step:99/1395 train_time:10981ms step_avg:123.39ms
step:100/1395 train_time:11105ms step_avg:123.39ms
step:101/1395 train_time:11229ms step_avg:123.39ms
step:102/1395 train_time:11352ms step_avg:123.39ms
step:103/1395 train_time:11474ms step_avg:123.38ms
step:104/1395 train_time:11598ms step_avg:123.39ms
step:105/1395 train_time:11724ms step_avg:123.41ms
step:106/1395 train_time:11849ms step_avg:123.42ms
step:107/1395 train_time:11975ms step_avg:123.45ms
step:108/1395 train_time:12102ms step_avg:123.49ms
step:109/1395 train_time:12228ms step_avg:123.52ms
step:110/1395 train_time:12354ms step_avg:123.54ms
step:111/1395 train_time:12480ms step_avg:123.57ms
step:112/1395 train_time:12606ms step_avg:123.59ms
step:113/1395 train_time:12732ms step_avg:123.61ms
step:114/1395 train_time:12858ms step_avg:123.63ms
step:115/1395 train_time:12984ms step_avg:123.65ms
step:116/1395 train_time:13109ms step_avg:123.67ms
step:117/1395 train_time:13237ms step_avg:123.71ms
step:118/1395 train_time:13364ms step_avg:123.74ms
step:119/1395 train_time:13490ms step_avg:123.77ms
step:120/1395 train_time:13616ms step_avg:123.78ms
step:121/1395 train_time:13742ms step_avg:123.80ms
step:122/1395 train_time:13868ms step_avg:123.82ms
step:123/1395 train_time:13994ms step_avg:123.84ms
step:124/1395 train_time:14120ms step_avg:123.86ms
step:125/1395 train_time:14246ms step_avg:123.88ms
step:125/1395 val_loss:4.3877 train_time:14348ms step_avg:124.76ms
step:126/1395 train_time:14377ms step_avg:123.94ms
step:127/1395 train_time:14514ms step_avg:124.05ms
step:128/1395 train_time:14642ms step_avg:124.09ms
step:129/1395 train_time:14769ms step_avg:124.11ms
step:130/1395 train_time:14895ms step_avg:124.12ms
step:131/1395 train_time:15020ms step_avg:124.13ms
step:132/1395 train_time:15147ms step_avg:124.15ms
step:133/1395 train_time:15272ms step_avg:124.16ms
step:134/1395 train_time:15397ms step_avg:124.17ms
step:135/1395 train_time:15524ms step_avg:124.20ms
step:136/1395 train_time:15650ms step_avg:124.21ms
step:137/1395 train_time:15777ms step_avg:124.23ms
step:138/1395 train_time:15903ms step_avg:124.24ms
step:139/1395 train_time:16029ms step_avg:124.26ms
step:140/1395 train_time:16156ms step_avg:124.27ms
step:141/1395 train_time:16282ms step_avg:124.29ms
step:142/1395 train_time:16409ms step_avg:124.31ms
step:143/1395 train_time:16535ms step_avg:124.32ms
step:144/1395 train_time:16662ms step_avg:124.34ms
step:145/1395 train_time:16788ms step_avg:124.35ms
step:146/1395 train_time:16914ms step_avg:124.37ms
step:147/1395 train_time:17040ms step_avg:124.38ms
step:148/1395 train_time:17166ms step_avg:124.39ms
step:149/1395 train_time:17292ms step_avg:124.40ms
step:150/1395 train_time:17417ms step_avg:124.41ms
step:151/1395 train_time:17543ms step_avg:124.42ms
step:152/1395 train_time:17669ms step_avg:124.43ms
step:153/1395 train_time:17795ms step_avg:124.44ms
step:154/1395 train_time:17921ms step_avg:124.45ms
step:155/1395 train_time:18047ms step_avg:124.46ms
step:156/1395 train_time:18173ms step_avg:124.47ms
step:157/1395 train_time:18299ms step_avg:124.48ms
step:158/1395 train_time:18425ms step_avg:124.50ms
step:159/1395 train_time:18552ms step_avg:124.51ms
step:160/1395 train_time:18678ms step_avg:124.52ms
step:161/1395 train_time:18806ms step_avg:124.54ms
step:162/1395 train_time:18932ms step_avg:124.55ms
step:163/1395 train_time:19059ms step_avg:124.57ms
step:164/1395 train_time:19185ms step_avg:124.58ms
step:165/1395 train_time:19311ms step_avg:124.59ms
step:166/1395 train_time:19436ms step_avg:124.59ms
step:167/1395 train_time:19562ms step_avg:124.60ms
step:168/1395 train_time:19688ms step_avg:124.61ms
step:169/1395 train_time:19815ms step_avg:124.62ms
step:170/1395 train_time:19941ms step_avg:124.63ms
step:171/1395 train_time:20067ms step_avg:124.64ms
step:172/1395 train_time:20194ms step_avg:124.65ms
step:173/1395 train_time:20320ms step_avg:124.66ms
step:174/1395 train_time:20446ms step_avg:124.67ms
step:175/1395 train_time:20573ms step_avg:124.68ms
step:176/1395 train_time:20698ms step_avg:124.69ms
step:177/1395 train_time:20825ms step_avg:124.70ms
step:178/1395 train_time:20951ms step_avg:124.71ms
step:179/1395 train_time:21077ms step_avg:124.72ms
step:180/1395 train_time:21203ms step_avg:124.73ms
step:181/1395 train_time:21330ms step_avg:124.73ms
step:182/1395 train_time:21456ms step_avg:124.74ms
step:183/1395 train_time:21582ms step_avg:124.75ms
step:184/1395 train_time:21709ms step_avg:124.76ms
step:185/1395 train_time:21835ms step_avg:124.77ms
step:186/1395 train_time:21962ms step_avg:124.79ms
step:187/1395 train_time:22089ms step_avg:124.80ms
step:188/1395 train_time:22215ms step_avg:124.81ms
step:189/1395 train_time:22341ms step_avg:124.81ms
step:190/1395 train_time:22468ms step_avg:124.82ms
step:191/1395 train_time:22594ms step_avg:124.83ms
step:192/1395 train_time:22720ms step_avg:124.83ms
step:193/1395 train_time:22846ms step_avg:124.84ms
step:194/1395 train_time:22972ms step_avg:124.85ms
step:195/1395 train_time:23098ms step_avg:124.85ms
step:196/1395 train_time:23226ms step_avg:124.87ms
step:197/1395 train_time:23352ms step_avg:124.88ms
step:198/1395 train_time:23479ms step_avg:124.89ms
step:199/1395 train_time:23606ms step_avg:124.90ms
step:200/1395 train_time:23731ms step_avg:124.90ms
step:201/1395 train_time:23857ms step_avg:124.90ms
step:202/1395 train_time:23983ms step_avg:124.91ms
step:203/1395 train_time:24109ms step_avg:124.91ms
step:204/1395 train_time:24235ms step_avg:124.92ms
step:205/1395 train_time:24361ms step_avg:124.93ms
step:206/1395 train_time:24488ms step_avg:124.94ms
step:207/1395 train_time:24615ms step_avg:124.95ms
step:208/1395 train_time:24740ms step_avg:124.95ms
step:209/1395 train_time:24869ms step_avg:124.97ms
step:210/1395 train_time:24998ms step_avg:124.99ms
step:211/1395 train_time:25127ms step_avg:125.01ms
step:212/1395 train_time:25256ms step_avg:125.03ms
step:213/1395 train_time:25384ms step_avg:125.04ms
step:214/1395 train_time:25512ms step_avg:125.06ms
step:215/1395 train_time:25640ms step_avg:125.07ms
step:216/1395 train_time:25769ms step_avg:125.09ms
step:217/1395 train_time:25897ms step_avg:125.11ms
step:218/1395 train_time:26026ms step_avg:125.12ms
step:219/1395 train_time:26155ms step_avg:125.14ms
step:220/1395 train_time:26282ms step_avg:125.15ms
step:221/1395 train_time:26411ms step_avg:125.17ms
step:222/1395 train_time:26539ms step_avg:125.18ms
step:223/1395 train_time:26668ms step_avg:125.20ms
step:224/1395 train_time:26796ms step_avg:125.22ms
step:225/1395 train_time:26925ms step_avg:125.23ms
step:226/1395 train_time:27054ms step_avg:125.25ms
step:227/1395 train_time:27181ms step_avg:125.26ms
step:228/1395 train_time:27310ms step_avg:125.28ms
step:229/1395 train_time:27439ms step_avg:125.29ms
step:230/1395 train_time:27569ms step_avg:125.31ms
step:231/1395 train_time:27698ms step_avg:125.33ms
step:232/1395 train_time:27827ms step_avg:125.35ms
step:233/1395 train_time:27955ms step_avg:125.36ms
step:234/1395 train_time:28084ms step_avg:125.38ms
step:235/1395 train_time:28213ms step_avg:125.39ms
step:236/1395 train_time:28340ms step_avg:125.40ms
step:237/1395 train_time:28469ms step_avg:125.42ms
step:238/1395 train_time:28597ms step_avg:125.43ms
step:239/1395 train_time:28726ms step_avg:125.44ms
step:240/1395 train_time:28854ms step_avg:125.45ms
step:241/1395 train_time:28982ms step_avg:125.46ms
step:242/1395 train_time:29111ms step_avg:125.48ms
step:243/1395 train_time:29240ms step_avg:125.49ms
step:244/1395 train_time:29369ms step_avg:125.51ms
step:245/1395 train_time:29498ms step_avg:125.52ms
step:246/1395 train_time:29626ms step_avg:125.54ms
step:247/1395 train_time:29755ms step_avg:125.55ms
step:248/1395 train_time:29883ms step_avg:125.56ms
step:249/1395 train_time:30012ms step_avg:125.57ms
step:250/1395 train_time:30140ms step_avg:125.58ms
step:250/1395 val_loss:3.9534 train_time:30244ms step_avg:126.02ms
step:251/1395 train_time:30274ms step_avg:125.62ms
step:252/1395 train_time:30414ms step_avg:125.68ms
step:253/1395 train_time:30543ms step_avg:125.69ms
step:254/1395 train_time:30672ms step_avg:125.70ms
step:255/1395 train_time:30799ms step_avg:125.71ms
step:256/1395 train_time:30928ms step_avg:125.72ms
step:257/1395 train_time:31056ms step_avg:125.73ms
step:258/1395 train_time:31184ms step_avg:125.74ms
step:259/1395 train_time:31312ms step_avg:125.75ms
step:260/1395 train_time:31442ms step_avg:125.77ms
step:261/1395 train_time:31571ms step_avg:125.78ms
step:262/1395 train_time:31699ms step_avg:125.79ms
step:263/1395 train_time:31828ms step_avg:125.80ms
step:264/1395 train_time:31956ms step_avg:125.81ms
step:265/1395 train_time:32084ms step_avg:125.82ms
step:266/1395 train_time:32213ms step_avg:125.83ms
step:267/1395 train_time:32342ms step_avg:125.84ms
step:268/1395 train_time:32470ms step_avg:125.85ms
step:269/1395 train_time:32598ms step_avg:125.86ms
step:270/1395 train_time:32726ms step_avg:125.87ms
step:271/1395 train_time:32855ms step_avg:125.88ms
step:272/1395 train_time:32984ms step_avg:125.89ms
step:273/1395 train_time:33114ms step_avg:125.91ms
step:274/1395 train_time:33241ms step_avg:125.91ms
step:275/1395 train_time:33370ms step_avg:125.92ms
step:276/1395 train_time:33499ms step_avg:125.94ms
step:277/1395 train_time:33627ms step_avg:125.94ms
step:278/1395 train_time:33756ms step_avg:125.95ms
step:279/1395 train_time:33885ms step_avg:125.97ms
step:280/1395 train_time:34014ms step_avg:125.98ms
step:281/1395 train_time:34143ms step_avg:125.99ms
step:282/1395 train_time:34272ms step_avg:126.00ms
step:283/1395 train_time:34400ms step_avg:126.01ms
step:284/1395 train_time:34529ms step_avg:126.02ms
step:285/1395 train_time:34659ms step_avg:126.03ms
step:286/1395 train_time:34787ms step_avg:126.04ms
step:287/1395 train_time:34916ms step_avg:126.05ms
step:288/1395 train_time:35044ms step_avg:126.06ms
step:289/1395 train_time:35173ms step_avg:126.07ms
step:290/1395 train_time:35302ms step_avg:126.08ms
step:291/1395 train_time:35431ms step_avg:126.09ms
step:292/1395 train_time:35560ms step_avg:126.10ms
step:293/1395 train_time:35688ms step_avg:126.11ms
step:294/1395 train_time:35817ms step_avg:126.12ms
step:295/1395 train_time:35946ms step_avg:126.13ms
step:296/1395 train_time:36075ms step_avg:126.14ms
step:297/1395 train_time:36204ms step_avg:126.15ms
step:298/1395 train_time:36333ms step_avg:126.16ms
step:299/1395 train_time:36461ms step_avg:126.16ms
step:300/1395 train_time:36590ms step_avg:126.17ms
step:301/1395 train_time:36719ms step_avg:126.18ms
step:302/1395 train_time:36848ms step_avg:126.19ms
step:303/1395 train_time:36976ms step_avg:126.20ms
step:304/1395 train_time:37105ms step_avg:126.21ms
step:305/1395 train_time:37233ms step_avg:126.21ms
step:306/1395 train_time:37362ms step_avg:126.22ms
step:307/1395 train_time:37490ms step_avg:126.23ms
step:308/1395 train_time:37618ms step_avg:126.23ms
step:309/1395 train_time:37746ms step_avg:126.24ms
step:310/1395 train_time:37875ms step_avg:126.25ms
step:311/1395 train_time:38003ms step_avg:126.26ms
step:312/1395 train_time:38132ms step_avg:126.27ms
step:313/1395 train_time:38262ms step_avg:126.28ms
step:314/1395 train_time:38393ms step_avg:126.29ms
step:315/1395 train_time:38523ms step_avg:126.31ms
step:316/1395 train_time:38654ms step_avg:126.32ms
step:317/1395 train_time:38784ms step_avg:126.33ms
step:318/1395 train_time:38915ms step_avg:126.35ms
step:319/1395 train_time:39046ms step_avg:126.36ms
step:320/1395 train_time:39176ms step_avg:126.38ms
step:321/1395 train_time:39307ms step_avg:126.39ms
step:322/1395 train_time:39437ms step_avg:126.40ms
step:323/1395 train_time:39567ms step_avg:126.41ms
step:324/1395 train_time:39698ms step_avg:126.43ms
step:325/1395 train_time:39828ms step_avg:126.44ms
step:326/1395 train_time:39960ms step_avg:126.45ms
step:327/1395 train_time:40090ms step_avg:126.47ms
step:328/1395 train_time:40221ms step_avg:126.48ms
step:329/1395 train_time:40351ms step_avg:126.49ms
step:330/1395 train_time:40481ms step_avg:126.50ms
step:331/1395 train_time:40612ms step_avg:126.52ms
step:332/1395 train_time:40743ms step_avg:126.53ms
step:333/1395 train_time:40873ms step_avg:126.54ms
step:334/1395 train_time:41003ms step_avg:126.55ms
step:335/1395 train_time:41133ms step_avg:126.56ms
step:336/1395 train_time:41263ms step_avg:126.57ms
step:337/1395 train_time:41393ms step_avg:126.58ms
step:338/1395 train_time:41524ms step_avg:126.60ms
step:339/1395 train_time:41655ms step_avg:126.61ms
step:340/1395 train_time:41786ms step_avg:126.62ms
step:341/1395 train_time:41916ms step_avg:126.64ms
step:342/1395 train_time:42047ms step_avg:126.65ms
step:343/1395 train_time:42178ms step_avg:126.66ms
step:344/1395 train_time:42308ms step_avg:126.67ms
step:345/1395 train_time:42439ms step_avg:126.68ms
step:346/1395 train_time:42569ms step_avg:126.69ms
step:347/1395 train_time:42700ms step_avg:126.71ms
step:348/1395 train_time:42831ms step_avg:126.72ms
step:349/1395 train_time:42962ms step_avg:126.73ms
step:350/1395 train_time:43091ms step_avg:126.74ms
step:351/1395 train_time:43222ms step_avg:126.75ms
step:352/1395 train_time:43351ms step_avg:126.76ms
step:353/1395 train_time:43482ms step_avg:126.77ms
step:354/1395 train_time:43612ms step_avg:126.78ms
step:355/1395 train_time:43742ms step_avg:126.79ms
step:356/1395 train_time:43873ms step_avg:126.80ms
step:357/1395 train_time:44004ms step_avg:126.81ms
step:358/1395 train_time:44134ms step_avg:126.82ms
step:359/1395 train_time:44265ms step_avg:126.83ms
step:360/1395 train_time:44396ms step_avg:126.84ms
step:361/1395 train_time:44525ms step_avg:126.85ms
step:362/1395 train_time:44655ms step_avg:126.86ms
step:363/1395 train_time:44785ms step_avg:126.87ms
step:364/1395 train_time:44916ms step_avg:126.88ms
step:365/1395 train_time:45046ms step_avg:126.89ms
step:366/1395 train_time:45177ms step_avg:126.90ms
step:367/1395 train_time:45307ms step_avg:126.91ms
step:368/1395 train_time:45437ms step_avg:126.92ms
step:369/1395 train_time:45568ms step_avg:126.93ms
step:370/1395 train_time:45698ms step_avg:126.94ms
step:371/1395 train_time:45828ms step_avg:126.95ms
step:372/1395 train_time:45958ms step_avg:126.96ms
step:373/1395 train_time:46089ms step_avg:126.97ms
step:374/1395 train_time:46219ms step_avg:126.98ms
step:375/1395 train_time:46349ms step_avg:126.98ms
step:375/1395 val_loss:3.7765 train_time:46454ms step_avg:127.27ms
step:376/1395 train_time:46485ms step_avg:127.01ms
step:377/1395 train_time:46624ms step_avg:127.04ms
step:378/1395 train_time:46755ms step_avg:127.05ms
step:379/1395 train_time:46885ms step_avg:127.06ms
step:380/1395 train_time:47014ms step_avg:127.07ms
step:381/1395 train_time:47143ms step_avg:127.07ms
step:382/1395 train_time:47275ms step_avg:127.08ms
step:383/1395 train_time:47405ms step_avg:127.09ms
step:384/1395 train_time:47536ms step_avg:127.10ms
step:385/1395 train_time:47667ms step_avg:127.11ms
step:386/1395 train_time:47797ms step_avg:127.12ms
step:387/1395 train_time:47928ms step_avg:127.13ms
step:388/1395 train_time:48058ms step_avg:127.14ms
step:389/1395 train_time:48187ms step_avg:127.14ms
step:390/1395 train_time:48317ms step_avg:127.15ms
step:391/1395 train_time:48448ms step_avg:127.16ms
step:392/1395 train_time:48578ms step_avg:127.17ms
step:393/1395 train_time:48709ms step_avg:127.18ms
step:394/1395 train_time:48839ms step_avg:127.19ms
step:395/1395 train_time:48969ms step_avg:127.19ms
step:396/1395 train_time:49100ms step_avg:127.20ms
step:397/1395 train_time:49230ms step_avg:127.21ms
step:398/1395 train_time:49361ms step_avg:127.22ms
step:399/1395 train_time:49491ms step_avg:127.23ms
step:400/1395 train_time:49621ms step_avg:127.23ms
step:401/1395 train_time:49752ms step_avg:127.24ms
step:402/1395 train_time:49882ms step_avg:127.25ms
step:403/1395 train_time:50013ms step_avg:127.26ms
step:404/1395 train_time:50143ms step_avg:127.27ms
step:405/1395 train_time:50274ms step_avg:127.28ms
step:406/1395 train_time:50404ms step_avg:127.28ms
step:407/1395 train_time:50535ms step_avg:127.29ms
step:408/1395 train_time:50666ms step_avg:127.30ms
step:409/1395 train_time:50795ms step_avg:127.31ms
step:410/1395 train_time:50925ms step_avg:127.31ms
step:411/1395 train_time:51055ms step_avg:127.32ms
step:412/1395 train_time:51185ms step_avg:127.33ms
step:413/1395 train_time:51315ms step_avg:127.33ms
step:414/1395 train_time:51445ms step_avg:127.34ms
step:415/1395 train_time:51576ms step_avg:127.35ms
step:416/1395 train_time:51708ms step_avg:127.36ms
step:417/1395 train_time:51841ms step_avg:127.37ms
step:418/1395 train_time:51972ms step_avg:127.38ms
step:419/1395 train_time:52105ms step_avg:127.40ms
step:420/1395 train_time:52237ms step_avg:127.41ms
step:421/1395 train_time:52368ms step_avg:127.42ms
step:422/1395 train_time:52500ms step_avg:127.43ms
step:423/1395 train_time:52634ms step_avg:127.44ms
step:424/1395 train_time:52766ms step_avg:127.45ms
step:425/1395 train_time:52898ms step_avg:127.46ms
step:426/1395 train_time:53029ms step_avg:127.47ms
step:427/1395 train_time:53160ms step_avg:127.48ms
step:428/1395 train_time:53293ms step_avg:127.49ms
step:429/1395 train_time:53425ms step_avg:127.51ms
step:430/1395 train_time:53557ms step_avg:127.52ms
step:431/1395 train_time:53689ms step_avg:127.53ms
step:432/1395 train_time:53822ms step_avg:127.54ms
step:433/1395 train_time:53955ms step_avg:127.55ms
step:434/1395 train_time:54087ms step_avg:127.56ms
step:435/1395 train_time:54219ms step_avg:127.57ms
step:436/1395 train_time:54352ms step_avg:127.59ms
step:437/1395 train_time:54484ms step_avg:127.60ms
step:438/1395 train_time:54616ms step_avg:127.61ms
step:439/1395 train_time:54748ms step_avg:127.62ms
step:440/1395 train_time:54880ms step_avg:127.63ms
step:441/1395 train_time:55013ms step_avg:127.64ms
step:442/1395 train_time:55145ms step_avg:127.65ms
step:443/1395 train_time:55278ms step_avg:127.66ms
step:444/1395 train_time:55411ms step_avg:127.67ms
step:445/1395 train_time:55543ms step_avg:127.69ms
step:446/1395 train_time:55675ms step_avg:127.69ms
step:447/1395 train_time:55807ms step_avg:127.70ms
step:448/1395 train_time:55938ms step_avg:127.71ms
step:449/1395 train_time:56071ms step_avg:127.72ms
step:450/1395 train_time:56203ms step_avg:127.73ms
step:451/1395 train_time:56335ms step_avg:127.74ms
step:452/1395 train_time:56467ms step_avg:127.75ms
step:453/1395 train_time:56600ms step_avg:127.76ms
step:454/1395 train_time:56732ms step_avg:127.78ms
step:455/1395 train_time:56865ms step_avg:127.79ms
step:456/1395 train_time:56997ms step_avg:127.80ms
step:457/1395 train_time:57129ms step_avg:127.80ms
step:458/1395 train_time:57260ms step_avg:127.81ms
step:459/1395 train_time:57392ms step_avg:127.82ms
step:460/1395 train_time:57524ms step_avg:127.83ms
step:461/1395 train_time:57657ms step_avg:127.84ms
step:462/1395 train_time:57789ms step_avg:127.85ms
step:463/1395 train_time:57922ms step_avg:127.86ms
step:464/1395 train_time:58053ms step_avg:127.87ms
step:465/1395 train_time:58185ms step_avg:127.88ms
step:466/1395 train_time:58317ms step_avg:127.89ms
step:467/1395 train_time:58450ms step_avg:127.90ms
step:468/1395 train_time:58582ms step_avg:127.91ms
step:469/1395 train_time:58715ms step_avg:127.92ms
step:470/1395 train_time:58848ms step_avg:127.93ms
step:471/1395 train_time:58979ms step_avg:127.94ms
step:472/1395 train_time:59112ms step_avg:127.95ms
step:473/1395 train_time:59244ms step_avg:127.96ms
step:474/1395 train_time:59376ms step_avg:127.97ms
step:475/1395 train_time:59510ms step_avg:127.98ms
step:476/1395 train_time:59642ms step_avg:127.99ms
step:477/1395 train_time:59775ms step_avg:128.00ms
step:478/1395 train_time:59907ms step_avg:128.01ms
step:479/1395 train_time:60039ms step_avg:128.02ms
step:480/1395 train_time:60174ms step_avg:128.03ms
step:481/1395 train_time:60305ms step_avg:128.04ms
step:482/1395 train_time:60437ms step_avg:128.04ms
step:483/1395 train_time:60570ms step_avg:128.05ms
step:484/1395 train_time:60701ms step_avg:128.06ms
step:485/1395 train_time:60834ms step_avg:128.07ms
step:486/1395 train_time:60966ms step_avg:128.08ms
step:487/1395 train_time:61098ms step_avg:128.09ms
step:488/1395 train_time:61230ms step_avg:128.10ms
step:489/1395 train_time:61362ms step_avg:128.10ms
step:490/1395 train_time:61495ms step_avg:128.11ms
step:491/1395 train_time:61627ms step_avg:128.12ms
step:492/1395 train_time:61760ms step_avg:128.13ms
step:493/1395 train_time:61892ms step_avg:128.14ms
step:494/1395 train_time:62024ms step_avg:128.15ms
step:495/1395 train_time:62157ms step_avg:128.16ms
step:496/1395 train_time:62288ms step_avg:128.16ms
step:497/1395 train_time:62420ms step_avg:128.17ms
step:498/1395 train_time:62554ms step_avg:128.18ms
step:499/1395 train_time:62686ms step_avg:128.19ms
step:500/1395 train_time:62817ms step_avg:128.20ms
step:500/1395 val_loss:3.6572 train_time:62923ms step_avg:128.41ms
step:501/1395 train_time:62954ms step_avg:128.22ms
step:502/1395 train_time:63093ms step_avg:128.24ms
step:503/1395 train_time:63225ms step_avg:128.25ms
step:504/1395 train_time:63357ms step_avg:128.25ms
step:505/1395 train_time:63488ms step_avg:128.26ms
step:506/1395 train_time:63620ms step_avg:128.27ms
step:507/1395 train_time:63752ms step_avg:128.27ms
step:508/1395 train_time:63885ms step_avg:128.28ms
step:509/1395 train_time:64018ms step_avg:128.29ms
step:510/1395 train_time:64150ms step_avg:128.30ms
step:511/1395 train_time:64282ms step_avg:128.31ms
step:512/1395 train_time:64414ms step_avg:128.32ms
step:513/1395 train_time:64547ms step_avg:128.32ms
step:514/1395 train_time:64679ms step_avg:128.33ms
step:515/1395 train_time:64811ms step_avg:128.34ms
step:516/1395 train_time:64943ms step_avg:128.35ms
step:517/1395 train_time:65076ms step_avg:128.35ms
step:518/1395 train_time:65208ms step_avg:128.36ms
step:519/1395 train_time:65343ms step_avg:128.38ms
step:520/1395 train_time:65476ms step_avg:128.38ms
step:521/1395 train_time:65610ms step_avg:128.39ms
step:522/1395 train_time:65743ms step_avg:128.40ms
step:523/1395 train_time:65877ms step_avg:128.42ms
step:524/1395 train_time:66011ms step_avg:128.43ms
step:525/1395 train_time:66143ms step_avg:128.43ms
step:526/1395 train_time:66278ms step_avg:128.45ms
step:527/1395 train_time:66412ms step_avg:128.46ms
step:528/1395 train_time:66546ms step_avg:128.47ms
step:529/1395 train_time:66680ms step_avg:128.48ms
step:530/1395 train_time:66814ms step_avg:128.49ms
step:531/1395 train_time:66947ms step_avg:128.50ms
step:532/1395 train_time:67080ms step_avg:128.51ms
step:533/1395 train_time:67216ms step_avg:128.52ms
step:534/1395 train_time:67353ms step_avg:128.54ms
step:535/1395 train_time:67485ms step_avg:128.54ms
step:536/1395 train_time:67621ms step_avg:128.56ms
step:537/1395 train_time:67755ms step_avg:128.57ms
step:538/1395 train_time:67888ms step_avg:128.58ms
step:539/1395 train_time:68022ms step_avg:128.59ms
step:540/1395 train_time:68156ms step_avg:128.60ms
step:541/1395 train_time:68290ms step_avg:128.61ms
step:542/1395 train_time:68422ms step_avg:128.61ms
step:543/1395 train_time:68556ms step_avg:128.62ms
step:544/1395 train_time:68689ms step_avg:128.63ms
step:545/1395 train_time:68822ms step_avg:128.64ms
step:546/1395 train_time:68957ms step_avg:128.65ms
step:547/1395 train_time:69090ms step_avg:128.66ms
step:548/1395 train_time:69225ms step_avg:128.67ms
step:549/1395 train_time:69359ms step_avg:128.68ms
step:550/1395 train_time:69495ms step_avg:128.69ms
step:551/1395 train_time:69628ms step_avg:128.70ms
step:552/1395 train_time:69762ms step_avg:128.71ms
step:553/1395 train_time:69896ms step_avg:128.72ms
step:554/1395 train_time:70030ms step_avg:128.73ms
step:555/1395 train_time:70163ms step_avg:128.74ms
step:556/1395 train_time:70296ms step_avg:128.75ms
step:557/1395 train_time:70429ms step_avg:128.76ms
step:558/1395 train_time:70562ms step_avg:128.76ms
step:559/1395 train_time:70696ms step_avg:128.77ms
step:560/1395 train_time:70830ms step_avg:128.78ms
step:561/1395 train_time:70964ms step_avg:128.79ms
step:562/1395 train_time:71098ms step_avg:128.80ms
step:563/1395 train_time:71232ms step_avg:128.81ms
step:564/1395 train_time:71365ms step_avg:128.82ms
step:565/1395 train_time:71500ms step_avg:128.83ms
step:566/1395 train_time:71634ms step_avg:128.84ms
step:567/1395 train_time:71767ms step_avg:128.85ms
step:568/1395 train_time:71902ms step_avg:128.86ms
step:569/1395 train_time:72035ms step_avg:128.86ms
step:570/1395 train_time:72169ms step_avg:128.87ms
step:571/1395 train_time:72302ms step_avg:128.88ms
step:572/1395 train_time:72436ms step_avg:128.89ms
step:573/1395 train_time:72570ms step_avg:128.90ms
step:574/1395 train_time:72706ms step_avg:128.91ms
step:575/1395 train_time:72840ms step_avg:128.92ms
step:576/1395 train_time:72973ms step_avg:128.93ms
step:577/1395 train_time:73107ms step_avg:128.94ms
step:578/1395 train_time:73240ms step_avg:128.94ms
step:579/1395 train_time:73373ms step_avg:128.95ms
step:580/1395 train_time:73506ms step_avg:128.96ms
step:581/1395 train_time:73641ms step_avg:128.97ms
step:582/1395 train_time:73775ms step_avg:128.98ms
step:583/1395 train_time:73909ms step_avg:128.99ms
step:584/1395 train_time:74043ms step_avg:128.99ms
step:585/1395 train_time:74175ms step_avg:129.00ms
step:586/1395 train_time:74309ms step_avg:129.01ms
step:587/1395 train_time:74442ms step_avg:129.02ms
step:588/1395 train_time:74576ms step_avg:129.02ms
step:589/1395 train_time:74709ms step_avg:129.03ms
step:590/1395 train_time:74843ms step_avg:129.04ms
step:591/1395 train_time:74976ms step_avg:129.05ms
step:592/1395 train_time:75110ms step_avg:129.05ms
step:593/1395 train_time:75243ms step_avg:129.06ms
step:594/1395 train_time:75377ms step_avg:129.07ms
step:595/1395 train_time:75511ms step_avg:129.08ms
step:596/1395 train_time:75646ms step_avg:129.09ms
step:597/1395 train_time:75780ms step_avg:129.10ms
step:598/1395 train_time:75913ms step_avg:129.10ms
step:599/1395 train_time:76046ms step_avg:129.11ms
step:600/1395 train_time:76180ms step_avg:129.12ms
step:601/1395 train_time:76314ms step_avg:129.13ms
step:602/1395 train_time:76447ms step_avg:129.13ms
step:603/1395 train_time:76583ms step_avg:129.15ms
step:604/1395 train_time:76717ms step_avg:129.15ms
step:605/1395 train_time:76852ms step_avg:129.16ms
step:606/1395 train_time:76986ms step_avg:129.17ms
step:607/1395 train_time:77120ms step_avg:129.18ms
step:608/1395 train_time:77254ms step_avg:129.19ms
step:609/1395 train_time:77388ms step_avg:129.20ms
step:610/1395 train_time:77522ms step_avg:129.20ms
step:611/1395 train_time:77655ms step_avg:129.21ms
step:612/1395 train_time:77788ms step_avg:129.22ms
step:613/1395 train_time:77922ms step_avg:129.22ms
step:614/1395 train_time:78056ms step_avg:129.23ms
step:615/1395 train_time:78191ms step_avg:129.24ms
step:616/1395 train_time:78324ms step_avg:129.25ms
step:617/1395 train_time:78458ms step_avg:129.25ms
step:618/1395 train_time:78592ms step_avg:129.26ms
step:619/1395 train_time:78726ms step_avg:129.27ms
step:620/1395 train_time:78861ms step_avg:129.28ms
step:621/1395 train_time:78994ms step_avg:129.29ms
step:622/1395 train_time:79128ms step_avg:129.29ms
step:623/1395 train_time:79263ms step_avg:129.30ms
step:624/1395 train_time:79399ms step_avg:129.31ms
step:625/1395 train_time:79534ms step_avg:129.32ms
step:625/1395 val_loss:3.5759 train_time:79644ms step_avg:129.50ms
step:626/1395 train_time:79675ms step_avg:129.34ms
step:627/1395 train_time:79816ms step_avg:129.36ms
step:628/1395 train_time:79951ms step_avg:129.37ms
step:629/1395 train_time:80086ms step_avg:129.38ms
step:630/1395 train_time:80220ms step_avg:129.39ms
step:631/1395 train_time:80356ms step_avg:129.40ms
step:632/1395 train_time:80490ms step_avg:129.41ms
step:633/1395 train_time:80624ms step_avg:129.41ms
step:634/1395 train_time:80759ms step_avg:129.42ms
step:635/1395 train_time:80896ms step_avg:129.43ms
step:636/1395 train_time:81031ms step_avg:129.44ms
step:637/1395 train_time:81166ms step_avg:129.45ms
step:638/1395 train_time:81301ms step_avg:129.46ms
step:639/1395 train_time:81436ms step_avg:129.47ms
step:640/1395 train_time:81571ms step_avg:129.48ms
step:641/1395 train_time:81705ms step_avg:129.49ms
step:642/1395 train_time:81840ms step_avg:129.49ms
step:643/1395 train_time:81977ms step_avg:129.51ms
step:644/1395 train_time:82112ms step_avg:129.51ms
step:645/1395 train_time:82247ms step_avg:129.52ms
step:646/1395 train_time:82381ms step_avg:129.53ms
step:647/1395 train_time:82515ms step_avg:129.54ms
step:648/1395 train_time:82654ms step_avg:129.55ms
step:649/1395 train_time:82788ms step_avg:129.56ms
step:650/1395 train_time:82925ms step_avg:129.57ms
step:651/1395 train_time:83061ms step_avg:129.58ms
step:652/1395 train_time:83196ms step_avg:129.59ms
step:653/1395 train_time:83331ms step_avg:129.60ms
step:654/1395 train_time:83467ms step_avg:129.61ms
step:655/1395 train_time:83601ms step_avg:129.61ms
step:656/1395 train_time:83735ms step_avg:129.62ms
step:657/1395 train_time:83871ms step_avg:129.63ms
step:658/1395 train_time:84006ms step_avg:129.64ms
step:659/1395 train_time:84141ms step_avg:129.65ms
step:660/1395 train_time:84276ms step_avg:129.66ms
step:661/1395 train_time:84411ms step_avg:129.66ms
step:662/1395 train_time:84546ms step_avg:129.67ms
step:663/1395 train_time:84680ms step_avg:129.68ms
step:664/1395 train_time:84817ms step_avg:129.69ms
step:665/1395 train_time:84951ms step_avg:129.70ms
step:666/1395 train_time:85085ms step_avg:129.70ms
step:667/1395 train_time:85221ms step_avg:129.71ms
step:668/1395 train_time:85355ms step_avg:129.72ms
step:669/1395 train_time:85492ms step_avg:129.73ms
step:670/1395 train_time:85626ms step_avg:129.74ms
step:671/1395 train_time:85761ms step_avg:129.74ms
step:672/1395 train_time:85897ms step_avg:129.75ms
step:673/1395 train_time:86033ms step_avg:129.76ms
step:674/1395 train_time:86169ms step_avg:129.77ms
step:675/1395 train_time:86306ms step_avg:129.78ms
step:676/1395 train_time:86441ms step_avg:129.79ms
step:677/1395 train_time:86575ms step_avg:129.80ms
step:678/1395 train_time:86710ms step_avg:129.81ms
step:679/1395 train_time:86846ms step_avg:129.82ms
step:680/1395 train_time:86981ms step_avg:129.82ms
step:681/1395 train_time:87118ms step_avg:129.83ms
step:682/1395 train_time:87254ms step_avg:129.84ms
step:683/1395 train_time:87388ms step_avg:129.85ms
step:684/1395 train_time:87524ms step_avg:129.86ms
step:685/1395 train_time:87659ms step_avg:129.86ms
step:686/1395 train_time:87794ms step_avg:129.87ms
step:687/1395 train_time:87928ms step_avg:129.88ms
step:688/1395 train_time:88064ms step_avg:129.89ms
step:689/1395 train_time:88201ms step_avg:129.90ms
step:690/1395 train_time:88339ms step_avg:129.91ms
step:691/1395 train_time:88473ms step_avg:129.92ms
step:692/1395 train_time:88608ms step_avg:129.92ms
step:693/1395 train_time:88743ms step_avg:129.93ms
step:694/1395 train_time:88878ms step_avg:129.94ms
step:695/1395 train_time:89011ms step_avg:129.94ms
step:696/1395 train_time:89146ms step_avg:129.95ms
step:697/1395 train_time:89282ms step_avg:129.96ms
step:698/1395 train_time:89417ms step_avg:129.97ms
step:699/1395 train_time:89554ms step_avg:129.98ms
step:700/1395 train_time:89691ms step_avg:129.99ms
step:701/1395 train_time:89825ms step_avg:129.99ms
step:702/1395 train_time:89960ms step_avg:130.00ms
step:703/1395 train_time:90095ms step_avg:130.01ms
step:704/1395 train_time:90230ms step_avg:130.01ms
step:705/1395 train_time:90365ms step_avg:130.02ms
step:706/1395 train_time:90502ms step_avg:130.03ms
step:707/1395 train_time:90639ms step_avg:130.04ms
step:708/1395 train_time:90775ms step_avg:130.05ms
step:709/1395 train_time:90911ms step_avg:130.06ms
step:710/1395 train_time:91046ms step_avg:130.07ms
step:711/1395 train_time:91182ms step_avg:130.07ms
step:712/1395 train_time:91318ms step_avg:130.08ms
step:713/1395 train_time:91453ms step_avg:130.09ms
step:714/1395 train_time:91588ms step_avg:130.10ms
step:715/1395 train_time:91723ms step_avg:130.10ms
step:716/1395 train_time:91859ms step_avg:130.11ms
step:717/1395 train_time:91995ms step_avg:130.12ms
step:718/1395 train_time:92129ms step_avg:130.13ms
step:719/1395 train_time:92264ms step_avg:130.13ms
step:720/1395 train_time:92399ms step_avg:130.14ms
step:721/1395 train_time:92534ms step_avg:130.15ms
step:722/1395 train_time:92670ms step_avg:130.15ms
step:723/1395 train_time:92805ms step_avg:130.16ms
step:724/1395 train_time:92941ms step_avg:130.17ms
step:725/1395 train_time:93077ms step_avg:130.18ms
step:726/1395 train_time:93213ms step_avg:130.19ms
step:727/1395 train_time:93351ms step_avg:130.20ms
step:728/1395 train_time:93487ms step_avg:130.20ms
step:729/1395 train_time:93622ms step_avg:130.21ms
step:730/1395 train_time:93760ms step_avg:130.22ms
step:731/1395 train_time:93897ms step_avg:130.23ms
step:732/1395 train_time:94033ms step_avg:130.24ms
step:733/1395 train_time:94169ms step_avg:130.25ms
step:734/1395 train_time:94306ms step_avg:130.26ms
step:735/1395 train_time:94444ms step_avg:130.27ms
step:736/1395 train_time:94581ms step_avg:130.28ms
step:737/1395 train_time:94717ms step_avg:130.28ms
step:738/1395 train_time:94853ms step_avg:130.29ms
step:739/1395 train_time:94989ms step_avg:130.30ms
step:740/1395 train_time:95126ms step_avg:130.31ms
step:741/1395 train_time:95265ms step_avg:130.32ms
step:742/1395 train_time:95401ms step_avg:130.33ms
step:743/1395 train_time:95538ms step_avg:130.34ms
step:744/1395 train_time:95675ms step_avg:130.35ms
step:745/1395 train_time:95813ms step_avg:130.36ms
step:746/1395 train_time:95949ms step_avg:130.37ms
step:747/1395 train_time:96085ms step_avg:130.37ms
step:748/1395 train_time:96221ms step_avg:130.38ms
step:749/1395 train_time:96358ms step_avg:130.39ms
step:750/1395 train_time:96496ms step_avg:130.40ms
step:750/1395 val_loss:3.5218 train_time:96608ms step_avg:130.55ms
step:751/1395 train_time:96639ms step_avg:130.42ms
step:752/1395 train_time:96779ms step_avg:130.43ms
step:753/1395 train_time:96916ms step_avg:130.44ms
step:754/1395 train_time:97051ms step_avg:130.45ms
step:755/1395 train_time:97187ms step_avg:130.45ms
step:756/1395 train_time:97322ms step_avg:130.46ms
step:757/1395 train_time:97462ms step_avg:130.47ms
step:758/1395 train_time:97600ms step_avg:130.48ms
step:759/1395 train_time:97737ms step_avg:130.49ms
step:760/1395 train_time:97873ms step_avg:130.50ms
step:761/1395 train_time:98009ms step_avg:130.50ms
step:762/1395 train_time:98146ms step_avg:130.51ms
step:763/1395 train_time:98282ms step_avg:130.52ms
step:764/1395 train_time:98419ms step_avg:130.53ms
step:765/1395 train_time:98555ms step_avg:130.54ms
step:766/1395 train_time:98694ms step_avg:130.55ms
step:767/1395 train_time:98831ms step_avg:130.56ms
step:768/1395 train_time:98969ms step_avg:130.57ms
step:769/1395 train_time:99105ms step_avg:130.57ms
step:770/1395 train_time:99241ms step_avg:130.58ms
step:771/1395 train_time:99377ms step_avg:130.59ms
step:772/1395 train_time:99513ms step_avg:130.59ms
step:773/1395 train_time:99650ms step_avg:130.60ms
step:774/1395 train_time:99786ms step_avg:130.61ms
step:775/1395 train_time:99923ms step_avg:130.62ms
step:776/1395 train_time:100059ms step_avg:130.63ms
step:777/1395 train_time:100196ms step_avg:130.63ms
step:778/1395 train_time:100334ms step_avg:130.64ms
step:779/1395 train_time:100469ms step_avg:130.65ms
step:780/1395 train_time:100606ms step_avg:130.66ms
step:781/1395 train_time:100742ms step_avg:130.66ms
step:782/1395 train_time:100879ms step_avg:130.67ms
step:783/1395 train_time:101016ms step_avg:130.68ms
step:784/1395 train_time:101153ms step_avg:130.69ms
step:785/1395 train_time:101288ms step_avg:130.69ms
step:786/1395 train_time:101424ms step_avg:130.70ms
step:787/1395 train_time:101561ms step_avg:130.71ms
step:788/1395 train_time:101697ms step_avg:130.72ms
step:789/1395 train_time:101832ms step_avg:130.72ms
step:790/1395 train_time:101969ms step_avg:130.73ms
step:791/1395 train_time:102105ms step_avg:130.74ms
step:792/1395 train_time:102243ms step_avg:130.75ms
step:793/1395 train_time:102379ms step_avg:130.75ms
step:794/1395 train_time:102516ms step_avg:130.76ms
step:795/1395 train_time:102656ms step_avg:130.77ms
step:796/1395 train_time:102792ms step_avg:130.78ms
step:797/1395 train_time:102930ms step_avg:130.79ms
step:798/1395 train_time:103067ms step_avg:130.80ms
step:799/1395 train_time:103206ms step_avg:130.81ms
step:800/1395 train_time:103342ms step_avg:130.81ms
step:801/1395 train_time:103479ms step_avg:130.82ms
step:802/1395 train_time:103617ms step_avg:130.83ms
step:803/1395 train_time:103752ms step_avg:130.84ms
step:804/1395 train_time:103887ms step_avg:130.84ms
step:805/1395 train_time:104026ms step_avg:130.85ms
step:806/1395 train_time:104162ms step_avg:130.86ms
step:807/1395 train_time:104297ms step_avg:130.86ms
step:808/1395 train_time:104435ms step_avg:130.87ms
step:809/1395 train_time:104571ms step_avg:130.88ms
step:810/1395 train_time:104706ms step_avg:130.88ms
step:811/1395 train_time:104842ms step_avg:130.89ms
step:812/1395 train_time:104980ms step_avg:130.90ms
step:813/1395 train_time:105115ms step_avg:130.90ms
step:814/1395 train_time:105251ms step_avg:130.91ms
step:815/1395 train_time:105385ms step_avg:130.91ms
step:816/1395 train_time:105524ms step_avg:130.92ms
step:817/1395 train_time:105660ms step_avg:130.93ms
step:818/1395 train_time:105796ms step_avg:130.94ms
step:819/1395 train_time:105933ms step_avg:130.94ms
step:820/1395 train_time:106070ms step_avg:130.95ms
step:821/1395 train_time:106206ms step_avg:130.96ms
step:822/1395 train_time:106343ms step_avg:130.96ms
step:823/1395 train_time:106479ms step_avg:130.97ms
step:824/1395 train_time:106616ms step_avg:130.98ms
step:825/1395 train_time:106753ms step_avg:130.98ms
step:826/1395 train_time:106890ms step_avg:130.99ms
step:827/1395 train_time:107027ms step_avg:131.00ms
step:828/1395 train_time:107165ms step_avg:131.01ms
step:829/1395 train_time:107303ms step_avg:131.02ms
step:830/1395 train_time:107441ms step_avg:131.03ms
step:831/1395 train_time:107578ms step_avg:131.03ms
step:832/1395 train_time:107717ms step_avg:131.04ms
step:833/1395 train_time:107853ms step_avg:131.05ms
step:834/1395 train_time:107992ms step_avg:131.06ms
step:835/1395 train_time:108129ms step_avg:131.07ms
step:836/1395 train_time:108268ms step_avg:131.08ms
step:837/1395 train_time:108405ms step_avg:131.08ms
step:838/1395 train_time:108541ms step_avg:131.09ms
step:839/1395 train_time:108679ms step_avg:131.10ms
step:840/1395 train_time:108815ms step_avg:131.10ms
step:841/1395 train_time:108954ms step_avg:131.11ms
step:842/1395 train_time:109091ms step_avg:131.12ms
step:843/1395 train_time:109228ms step_avg:131.13ms
step:844/1395 train_time:109365ms step_avg:131.13ms
step:845/1395 train_time:109502ms step_avg:131.14ms
step:846/1395 train_time:109642ms step_avg:131.15ms
step:847/1395 train_time:109781ms step_avg:131.16ms
step:848/1395 train_time:109918ms step_avg:131.17ms
step:849/1395 train_time:110056ms step_avg:131.18ms
step:850/1395 train_time:110194ms step_avg:131.18ms
step:851/1395 train_time:110334ms step_avg:131.19ms
step:852/1395 train_time:110473ms step_avg:131.20ms
step:853/1395 train_time:110609ms step_avg:131.21ms
step:854/1395 train_time:110745ms step_avg:131.21ms
step:855/1395 train_time:110882ms step_avg:131.22ms
step:856/1395 train_time:111019ms step_avg:131.23ms
step:857/1395 train_time:111156ms step_avg:131.23ms
step:858/1395 train_time:111295ms step_avg:131.24ms
step:859/1395 train_time:111434ms step_avg:131.25ms
step:860/1395 train_time:111571ms step_avg:131.26ms
step:861/1395 train_time:111709ms step_avg:131.27ms
step:862/1395 train_time:111848ms step_avg:131.28ms
step:863/1395 train_time:111988ms step_avg:131.29ms
step:864/1395 train_time:112127ms step_avg:131.30ms
step:865/1395 train_time:112263ms step_avg:131.30ms
step:866/1395 train_time:112409ms step_avg:131.32ms
step:867/1395 train_time:112547ms step_avg:131.33ms
step:868/1395 train_time:112683ms step_avg:131.33ms
step:869/1395 train_time:112820ms step_avg:131.34ms
step:870/1395 train_time:112960ms step_avg:131.35ms
step:871/1395 train_time:113099ms step_avg:131.36ms
step:872/1395 train_time:113236ms step_avg:131.36ms
step:873/1395 train_time:113372ms step_avg:131.37ms
step:874/1395 train_time:113510ms step_avg:131.38ms
step:875/1395 train_time:113649ms step_avg:131.39ms
step:875/1395 val_loss:3.4746 train_time:113759ms step_avg:131.51ms
step:876/1395 train_time:113790ms step_avg:131.40ms
step:877/1395 train_time:113931ms step_avg:131.41ms
step:878/1395 train_time:114070ms step_avg:131.42ms
step:879/1395 train_time:114208ms step_avg:131.42ms
step:880/1395 train_time:114345ms step_avg:131.43ms
step:881/1395 train_time:114480ms step_avg:131.44ms
step:882/1395 train_time:114619ms step_avg:131.44ms
step:883/1395 train_time:114755ms step_avg:131.45ms
step:884/1395 train_time:114893ms step_avg:131.46ms
step:885/1395 train_time:115031ms step_avg:131.46ms
step:886/1395 train_time:115171ms step_avg:131.47ms
step:887/1395 train_time:115308ms step_avg:131.48ms
step:888/1395 train_time:115448ms step_avg:131.49ms
step:889/1395 train_time:115589ms step_avg:131.50ms
step:890/1395 train_time:115725ms step_avg:131.51ms
step:891/1395 train_time:115863ms step_avg:131.51ms
step:892/1395 train_time:116001ms step_avg:131.52ms
step:893/1395 train_time:116138ms step_avg:131.53ms
step:894/1395 train_time:116275ms step_avg:131.53ms
step:895/1395 train_time:116415ms step_avg:131.54ms
step:896/1395 train_time:116552ms step_avg:131.55ms
step:897/1395 train_time:116690ms step_avg:131.56ms
step:898/1395 train_time:116829ms step_avg:131.56ms
step:899/1395 train_time:116968ms step_avg:131.57ms
step:900/1395 train_time:117105ms step_avg:131.58ms
step:901/1395 train_time:117243ms step_avg:131.59ms
step:902/1395 train_time:117379ms step_avg:131.59ms
step:903/1395 train_time:117520ms step_avg:131.60ms
step:904/1395 train_time:117658ms step_avg:131.61ms
step:905/1395 train_time:117796ms step_avg:131.62ms
step:906/1395 train_time:117933ms step_avg:131.62ms
step:907/1395 train_time:118074ms step_avg:131.63ms
step:908/1395 train_time:118210ms step_avg:131.64ms
step:909/1395 train_time:118347ms step_avg:131.64ms
step:910/1395 train_time:118489ms step_avg:131.65ms
step:911/1395 train_time:118625ms step_avg:131.66ms
step:912/1395 train_time:118762ms step_avg:131.67ms
step:913/1395 train_time:118901ms step_avg:131.67ms
step:914/1395 train_time:119039ms step_avg:131.68ms
step:915/1395 train_time:119179ms step_avg:131.69ms
step:916/1395 train_time:119317ms step_avg:131.70ms
step:917/1395 train_time:119455ms step_avg:131.70ms
step:918/1395 train_time:119593ms step_avg:131.71ms
step:919/1395 train_time:119737ms step_avg:131.72ms
step:920/1395 train_time:119874ms step_avg:131.73ms
step:921/1395 train_time:120011ms step_avg:131.74ms
step:922/1395 train_time:120153ms step_avg:131.75ms
step:923/1395 train_time:120288ms step_avg:131.75ms
step:924/1395 train_time:120425ms step_avg:131.76ms
step:925/1395 train_time:120564ms step_avg:131.76ms
step:926/1395 train_time:120701ms step_avg:131.77ms
step:927/1395 train_time:120837ms step_avg:131.77ms
step:928/1395 train_time:120974ms step_avg:131.78ms
step:929/1395 train_time:121112ms step_avg:131.79ms
step:930/1395 train_time:121251ms step_avg:131.79ms
step:931/1395 train_time:121388ms step_avg:131.80ms
step:932/1395 train_time:121527ms step_avg:131.81ms
step:933/1395 train_time:121666ms step_avg:131.82ms
step:934/1395 train_time:121804ms step_avg:131.82ms
step:935/1395 train_time:121945ms step_avg:131.83ms
step:936/1395 train_time:122084ms step_avg:131.84ms
step:937/1395 train_time:122228ms step_avg:131.85ms
step:938/1395 train_time:122369ms step_avg:131.86ms
step:939/1395 train_time:122508ms step_avg:131.87ms
step:940/1395 train_time:122649ms step_avg:131.88ms
step:941/1395 train_time:122787ms step_avg:131.89ms
step:942/1395 train_time:122924ms step_avg:131.89ms
step:943/1395 train_time:123067ms step_avg:131.90ms
step:944/1395 train_time:123210ms step_avg:131.92ms
step:945/1395 train_time:123349ms step_avg:131.92ms
step:946/1395 train_time:123490ms step_avg:131.93ms
step:947/1395 train_time:123631ms step_avg:131.94ms
step:948/1395 train_time:123770ms step_avg:131.95ms
step:949/1395 train_time:123910ms step_avg:131.96ms
step:950/1395 train_time:124047ms step_avg:131.96ms
step:951/1395 train_time:124188ms step_avg:131.97ms
step:952/1395 train_time:124326ms step_avg:131.98ms
step:953/1395 train_time:124466ms step_avg:131.99ms
step:954/1395 train_time:124604ms step_avg:132.00ms
step:955/1395 train_time:124741ms step_avg:132.00ms
step:956/1395 train_time:124883ms step_avg:132.01ms
step:957/1395 train_time:125021ms step_avg:132.02ms
step:958/1395 train_time:125163ms step_avg:132.03ms
step:959/1395 train_time:125306ms step_avg:132.04ms
step:960/1395 train_time:125445ms step_avg:132.05ms
step:961/1395 train_time:125584ms step_avg:132.05ms
step:962/1395 train_time:125723ms step_avg:132.06ms
step:963/1395 train_time:125868ms step_avg:132.08ms
step:964/1395 train_time:126007ms step_avg:132.08ms
step:965/1395 train_time:126146ms step_avg:132.09ms
step:966/1395 train_time:126284ms step_avg:132.10ms
step:967/1395 train_time:126424ms step_avg:132.10ms
step:968/1395 train_time:126559ms step_avg:132.11ms
step:969/1395 train_time:126699ms step_avg:132.12ms
step:970/1395 train_time:126837ms step_avg:132.12ms
step:971/1395 train_time:126976ms step_avg:132.13ms
step:972/1395 train_time:127114ms step_avg:132.14ms
step:973/1395 train_time:127253ms step_avg:132.14ms
step:974/1395 train_time:127393ms step_avg:132.15ms
step:975/1395 train_time:127531ms step_avg:132.16ms
step:976/1395 train_time:127668ms step_avg:132.16ms
step:977/1395 train_time:127806ms step_avg:132.17ms
step:978/1395 train_time:127944ms step_avg:132.17ms
step:979/1395 train_time:128083ms step_avg:132.18ms
step:980/1395 train_time:128222ms step_avg:132.19ms
step:981/1395 train_time:128358ms step_avg:132.19ms
step:982/1395 train_time:128496ms step_avg:132.20ms
step:983/1395 train_time:128633ms step_avg:132.20ms
step:984/1395 train_time:128771ms step_avg:132.21ms
step:985/1395 train_time:128912ms step_avg:132.22ms
step:986/1395 train_time:129057ms step_avg:132.23ms
step:987/1395 train_time:129194ms step_avg:132.24ms
step:988/1395 train_time:129333ms step_avg:132.24ms
step:989/1395 train_time:129473ms step_avg:132.25ms
step:990/1395 train_time:129614ms step_avg:132.26ms
step:991/1395 train_time:129752ms step_avg:132.27ms
step:992/1395 train_time:129895ms step_avg:132.28ms
step:993/1395 train_time:130043ms step_avg:132.29ms
step:994/1395 train_time:130181ms step_avg:132.30ms
step:995/1395 train_time:130318ms step_avg:132.30ms
step:996/1395 train_time:130455ms step_avg:132.31ms
step:997/1395 train_time:130592ms step_avg:132.31ms
step:998/1395 train_time:130730ms step_avg:132.32ms
step:999/1395 train_time:130868ms step_avg:132.32ms
step:1000/1395 train_time:131007ms step_avg:132.33ms
step:1000/1395 val_loss:3.4112 train_time:131118ms step_avg:132.44ms
step:1001/1395 train_time:131149ms step_avg:132.34ms
step:1002/1395 train_time:131290ms step_avg:132.35ms
step:1003/1395 train_time:131430ms step_avg:132.36ms
step:1004/1395 train_time:131571ms step_avg:132.37ms
step:1005/1395 train_time:131711ms step_avg:132.37ms
step:1006/1395 train_time:131847ms step_avg:132.38ms
step:1007/1395 train_time:131986ms step_avg:132.38ms
step:1008/1395 train_time:132126ms step_avg:132.39ms
step:1009/1395 train_time:132271ms step_avg:132.40ms
step:1010/1395 train_time:132410ms step_avg:132.41ms
step:1011/1395 train_time:132550ms step_avg:132.42ms
step:1012/1395 train_time:132688ms step_avg:132.42ms
step:1013/1395 train_time:132828ms step_avg:132.43ms
step:1014/1395 train_time:132965ms step_avg:132.44ms
step:1015/1395 train_time:133104ms step_avg:132.44ms
step:1016/1395 train_time:133243ms step_avg:132.45ms
step:1017/1395 train_time:133382ms step_avg:132.45ms
step:1018/1395 train_time:133520ms step_avg:132.46ms
step:1019/1395 train_time:133661ms step_avg:132.47ms
step:1020/1395 train_time:133803ms step_avg:132.48ms
step:1021/1395 train_time:133940ms step_avg:132.48ms
step:1022/1395 train_time:134078ms step_avg:132.49ms
step:1023/1395 train_time:134218ms step_avg:132.50ms
step:1024/1395 train_time:134358ms step_avg:132.50ms
step:1025/1395 train_time:134498ms step_avg:132.51ms
step:1026/1395 train_time:134638ms step_avg:132.52ms
step:1027/1395 train_time:134778ms step_avg:132.52ms
step:1028/1395 train_time:134918ms step_avg:132.53ms
step:1029/1395 train_time:135059ms step_avg:132.54ms
step:1030/1395 train_time:135198ms step_avg:132.55ms
step:1031/1395 train_time:135334ms step_avg:132.55ms
step:1032/1395 train_time:135471ms step_avg:132.55ms
step:1033/1395 train_time:135609ms step_avg:132.56ms
step:1034/1395 train_time:135747ms step_avg:132.57ms
step:1035/1395 train_time:135888ms step_avg:132.57ms
step:1036/1395 train_time:136028ms step_avg:132.58ms
step:1037/1395 train_time:136173ms step_avg:132.59ms
step:1038/1395 train_time:136313ms step_avg:132.60ms
step:1039/1395 train_time:136451ms step_avg:132.61ms
step:1040/1395 train_time:136589ms step_avg:132.61ms
step:1041/1395 train_time:136729ms step_avg:132.62ms
step:1042/1395 train_time:136867ms step_avg:132.62ms
step:1043/1395 train_time:137007ms step_avg:132.63ms
step:1044/1395 train_time:137150ms step_avg:132.64ms
step:1045/1395 train_time:137290ms step_avg:132.65ms
step:1046/1395 train_time:137431ms step_avg:132.66ms
step:1047/1395 train_time:137569ms step_avg:132.66ms
step:1048/1395 train_time:137709ms step_avg:132.67ms
step:1049/1395 train_time:137849ms step_avg:132.67ms
step:1050/1395 train_time:137991ms step_avg:132.68ms
step:1051/1395 train_time:138134ms step_avg:132.69ms
step:1052/1395 train_time:138273ms step_avg:132.70ms
step:1053/1395 train_time:138412ms step_avg:132.71ms
step:1054/1395 train_time:138553ms step_avg:132.71ms
step:1055/1395 train_time:138692ms step_avg:132.72ms
step:1056/1395 train_time:138832ms step_avg:132.73ms
step:1057/1395 train_time:138972ms step_avg:132.73ms
step:1058/1395 train_time:139114ms step_avg:132.74ms
step:1059/1395 train_time:139256ms step_avg:132.75ms
step:1060/1395 train_time:139399ms step_avg:132.76ms
step:1061/1395 train_time:139536ms step_avg:132.76ms
step:1062/1395 train_time:139676ms step_avg:132.77ms
step:1063/1395 train_time:139815ms step_avg:132.78ms
step:1064/1395 train_time:139953ms step_avg:132.78ms
step:1065/1395 train_time:140092ms step_avg:132.79ms
step:1066/1395 train_time:140235ms step_avg:132.80ms
step:1067/1395 train_time:140376ms step_avg:132.81ms
step:1068/1395 train_time:140516ms step_avg:132.81ms
step:1069/1395 train_time:140660ms step_avg:132.82ms
step:1070/1395 train_time:140797ms step_avg:132.83ms
step:1071/1395 train_time:140943ms step_avg:132.84ms
step:1072/1395 train_time:141081ms step_avg:132.84ms
step:1073/1395 train_time:141218ms step_avg:132.85ms
step:1074/1395 train_time:141356ms step_avg:132.85ms
step:1075/1395 train_time:141496ms step_avg:132.86ms
step:1076/1395 train_time:141635ms step_avg:132.87ms
step:1077/1395 train_time:141774ms step_avg:132.87ms
step:1078/1395 train_time:141916ms step_avg:132.88ms
step:1079/1395 train_time:142063ms step_avg:132.89ms
step:1080/1395 train_time:142204ms step_avg:132.90ms
step:1081/1395 train_time:142342ms step_avg:132.91ms
step:1082/1395 train_time:142481ms step_avg:132.91ms
step:1083/1395 train_time:142619ms step_avg:132.92ms
step:1084/1395 train_time:142763ms step_avg:132.93ms
step:1085/1395 train_time:142902ms step_avg:132.93ms
step:1086/1395 train_time:143042ms step_avg:132.94ms
step:1087/1395 train_time:143182ms step_avg:132.94ms
step:1088/1395 train_time:143321ms step_avg:132.95ms
step:1089/1395 train_time:143465ms step_avg:132.96ms
step:1090/1395 train_time:143610ms step_avg:132.97ms
step:1091/1395 train_time:143750ms step_avg:132.98ms
step:1092/1395 train_time:143888ms step_avg:132.98ms
step:1093/1395 train_time:144029ms step_avg:132.99ms
step:1094/1395 train_time:144168ms step_avg:133.00ms
step:1095/1395 train_time:144306ms step_avg:133.00ms
step:1096/1395 train_time:144449ms step_avg:133.01ms
step:1097/1395 train_time:144592ms step_avg:133.02ms
step:1098/1395 train_time:144734ms step_avg:133.03ms
step:1099/1395 train_time:144874ms step_avg:133.03ms
step:1100/1395 train_time:145012ms step_avg:133.04ms
step:1101/1395 train_time:145152ms step_avg:133.04ms
step:1102/1395 train_time:145293ms step_avg:133.05ms
step:1103/1395 train_time:145434ms step_avg:133.06ms
step:1104/1395 train_time:145575ms step_avg:133.07ms
step:1105/1395 train_time:145719ms step_avg:133.08ms
step:1106/1395 train_time:145859ms step_avg:133.08ms
step:1107/1395 train_time:145998ms step_avg:133.09ms
step:1108/1395 train_time:146144ms step_avg:133.10ms
step:1109/1395 train_time:146283ms step_avg:133.11ms
step:1110/1395 train_time:146424ms step_avg:133.11ms
step:1111/1395 train_time:146564ms step_avg:133.12ms
step:1112/1395 train_time:146702ms step_avg:133.12ms
step:1113/1395 train_time:146840ms step_avg:133.13ms
step:1114/1395 train_time:146983ms step_avg:133.14ms
step:1115/1395 train_time:147124ms step_avg:133.14ms
step:1116/1395 train_time:147262ms step_avg:133.15ms
step:1117/1395 train_time:147403ms step_avg:133.16ms
step:1118/1395 train_time:147550ms step_avg:133.17ms
step:1119/1395 train_time:147690ms step_avg:133.17ms
step:1120/1395 train_time:147829ms step_avg:133.18ms
step:1121/1395 train_time:147969ms step_avg:133.19ms
step:1122/1395 train_time:148107ms step_avg:133.19ms
step:1123/1395 train_time:148246ms step_avg:133.20ms
step:1124/1395 train_time:148388ms step_avg:133.20ms
step:1125/1395 train_time:148527ms step_avg:133.21ms
step:1125/1395 val_loss:3.3615 train_time:148642ms step_avg:133.31ms
step:1126/1395 train_time:148673ms step_avg:133.22ms
step:1127/1395 train_time:148817ms step_avg:133.23ms
step:1128/1395 train_time:148956ms step_avg:133.23ms
step:1129/1395 train_time:149098ms step_avg:133.24ms
step:1130/1395 train_time:149238ms step_avg:133.25ms
step:1131/1395 train_time:149378ms step_avg:133.25ms
step:1132/1395 train_time:149518ms step_avg:133.26ms
step:1133/1395 train_time:149657ms step_avg:133.27ms
step:1134/1395 train_time:149800ms step_avg:133.27ms
step:1135/1395 train_time:149940ms step_avg:133.28ms
step:1136/1395 train_time:150086ms step_avg:133.29ms
step:1137/1395 train_time:150223ms step_avg:133.29ms
step:1138/1395 train_time:150367ms step_avg:133.30ms
step:1139/1395 train_time:150508ms step_avg:133.31ms
step:1140/1395 train_time:150650ms step_avg:133.32ms
step:1141/1395 train_time:150791ms step_avg:133.33ms
step:1142/1395 train_time:150932ms step_avg:133.33ms
step:1143/1395 train_time:151077ms step_avg:133.34ms
step:1144/1395 train_time:151218ms step_avg:133.35ms
step:1145/1395 train_time:151358ms step_avg:133.36ms
step:1146/1395 train_time:151501ms step_avg:133.36ms
step:1147/1395 train_time:151643ms step_avg:133.37ms
step:1148/1395 train_time:151785ms step_avg:133.38ms
step:1149/1395 train_time:151926ms step_avg:133.39ms
step:1150/1395 train_time:152064ms step_avg:133.39ms
step:1151/1395 train_time:152210ms step_avg:133.40ms
step:1152/1395 train_time:152351ms step_avg:133.41ms
step:1153/1395 train_time:152495ms step_avg:133.42ms
step:1154/1395 train_time:152635ms step_avg:133.42ms
step:1155/1395 train_time:152775ms step_avg:133.43ms
step:1156/1395 train_time:152923ms step_avg:133.44ms
step:1157/1395 train_time:153066ms step_avg:133.45ms
step:1158/1395 train_time:153205ms step_avg:133.45ms
step:1159/1395 train_time:153346ms step_avg:133.46ms
step:1160/1395 train_time:153485ms step_avg:133.47ms
step:1161/1395 train_time:153627ms step_avg:133.47ms
step:1162/1395 train_time:153767ms step_avg:133.48ms
step:1163/1395 train_time:153909ms step_avg:133.49ms
step:1164/1395 train_time:154049ms step_avg:133.49ms
step:1165/1395 train_time:154189ms step_avg:133.50ms
step:1166/1395 train_time:154330ms step_avg:133.50ms
step:1167/1395 train_time:154470ms step_avg:133.51ms
step:1168/1395 train_time:154611ms step_avg:133.52ms
step:1169/1395 train_time:154753ms step_avg:133.52ms
step:1170/1395 train_time:154893ms step_avg:133.53ms
step:1171/1395 train_time:155035ms step_avg:133.54ms
step:1172/1395 train_time:155175ms step_avg:133.54ms
step:1173/1395 train_time:155316ms step_avg:133.55ms
step:1174/1395 train_time:155468ms step_avg:133.56ms
step:1175/1395 train_time:155609ms step_avg:133.57ms
step:1176/1395 train_time:155752ms step_avg:133.58ms
step:1177/1395 train_time:155901ms step_avg:133.59ms
step:1178/1395 train_time:156041ms step_avg:133.60ms
step:1179/1395 train_time:156180ms step_avg:133.60ms
step:1180/1395 train_time:156327ms step_avg:133.61ms
step:1181/1395 train_time:156469ms step_avg:133.62ms
step:1182/1395 train_time:156609ms step_avg:133.63ms
step:1183/1395 train_time:156752ms step_avg:133.63ms
step:1184/1395 train_time:156892ms step_avg:133.64ms
step:1185/1395 train_time:157037ms step_avg:133.65ms
step:1186/1395 train_time:157177ms step_avg:133.65ms
step:1187/1395 train_time:157329ms step_avg:133.67ms
step:1188/1395 train_time:157468ms step_avg:133.67ms
step:1189/1395 train_time:157611ms step_avg:133.68ms
step:1190/1395 train_time:157753ms step_avg:133.69ms
step:1191/1395 train_time:157896ms step_avg:133.70ms
step:1192/1395 train_time:158035ms step_avg:133.70ms
step:1193/1395 train_time:158175ms step_avg:133.71ms
step:1194/1395 train_time:158316ms step_avg:133.71ms
step:1195/1395 train_time:158458ms step_avg:133.72ms
step:1196/1395 train_time:158598ms step_avg:133.73ms
step:1197/1395 train_time:158740ms step_avg:133.73ms
step:1198/1395 train_time:158888ms step_avg:133.74ms
step:1199/1395 train_time:159029ms step_avg:133.75ms
step:1200/1395 train_time:159170ms step_avg:133.76ms
step:1201/1395 train_time:159310ms step_avg:133.76ms
step:1202/1395 train_time:159465ms step_avg:133.78ms
step:1203/1395 train_time:159612ms step_avg:133.79ms
step:1204/1395 train_time:159755ms step_avg:133.80ms
step:1205/1395 train_time:159897ms step_avg:133.81ms
step:1206/1395 train_time:160039ms step_avg:133.81ms
step:1207/1395 train_time:160180ms step_avg:133.82ms
step:1208/1395 train_time:160324ms step_avg:133.83ms
step:1209/1395 train_time:160465ms step_avg:133.83ms
step:1210/1395 train_time:160610ms step_avg:133.84ms
step:1211/1395 train_time:160752ms step_avg:133.85ms
step:1212/1395 train_time:160894ms step_avg:133.86ms
step:1213/1395 train_time:161035ms step_avg:133.86ms
step:1214/1395 train_time:161177ms step_avg:133.87ms
step:1215/1395 train_time:161322ms step_avg:133.88ms
step:1216/1395 train_time:161459ms step_avg:133.88ms
step:1217/1395 train_time:161601ms step_avg:133.89ms
step:1218/1395 train_time:161739ms step_avg:133.89ms
step:1219/1395 train_time:161877ms step_avg:133.89ms
step:1220/1395 train_time:162016ms step_avg:133.90ms
step:1221/1395 train_time:162156ms step_avg:133.90ms
step:1222/1395 train_time:162296ms step_avg:133.91ms
step:1223/1395 train_time:162437ms step_avg:133.91ms
step:1224/1395 train_time:162580ms step_avg:133.92ms
step:1225/1395 train_time:162724ms step_avg:133.93ms
step:1226/1395 train_time:162865ms step_avg:133.94ms
step:1227/1395 train_time:163007ms step_avg:133.94ms
step:1228/1395 train_time:163148ms step_avg:133.95ms
step:1229/1395 train_time:163287ms step_avg:133.95ms
step:1230/1395 train_time:163433ms step_avg:133.96ms
step:1231/1395 train_time:163577ms step_avg:133.97ms
step:1232/1395 train_time:163720ms step_avg:133.98ms
step:1233/1395 train_time:163861ms step_avg:133.98ms
step:1234/1395 train_time:164002ms step_avg:133.99ms
step:1235/1395 train_time:164143ms step_avg:133.99ms
step:1236/1395 train_time:164285ms step_avg:134.00ms
step:1237/1395 train_time:164424ms step_avg:134.00ms
step:1238/1395 train_time:164576ms step_avg:134.02ms
step:1239/1395 train_time:164716ms step_avg:134.02ms
step:1240/1395 train_time:164859ms step_avg:134.03ms
step:1241/1395 train_time:165004ms step_avg:134.04ms
step:1242/1395 train_time:165144ms step_avg:134.05ms
step:1243/1395 train_time:165288ms step_avg:134.05ms
step:1244/1395 train_time:165429ms step_avg:134.06ms
step:1245/1395 train_time:165570ms step_avg:134.07ms
step:1246/1395 train_time:165710ms step_avg:134.07ms
step:1247/1395 train_time:165853ms step_avg:134.08ms
step:1248/1395 train_time:165993ms step_avg:134.08ms
step:1249/1395 train_time:166132ms step_avg:134.09ms
step:1250/1395 train_time:166272ms step_avg:134.09ms
step:1250/1395 val_loss:3.3151 train_time:166388ms step_avg:134.18ms
step:1251/1395 train_time:166420ms step_avg:134.10ms
step:1252/1395 train_time:166566ms step_avg:134.11ms
step:1253/1395 train_time:166706ms step_avg:134.12ms
step:1254/1395 train_time:166846ms step_avg:134.12ms
step:1255/1395 train_time:166997ms step_avg:134.13ms
step:1256/1395 train_time:167140ms step_avg:134.14ms
step:1257/1395 train_time:167280ms step_avg:134.15ms
step:1258/1395 train_time:167424ms step_avg:134.15ms
step:1259/1395 train_time:167567ms step_avg:134.16ms
step:1260/1395 train_time:167706ms step_avg:134.16ms
step:1261/1395 train_time:167847ms step_avg:134.17ms
step:1262/1395 train_time:167992ms step_avg:134.18ms
step:1263/1395 train_time:168134ms step_avg:134.19ms
step:1264/1395 train_time:168275ms step_avg:134.19ms
step:1265/1395 train_time:168414ms step_avg:134.19ms
step:1266/1395 train_time:168557ms step_avg:134.20ms
step:1267/1395 train_time:168699ms step_avg:134.21ms
step:1268/1395 train_time:168842ms step_avg:134.21ms
step:1269/1395 train_time:168990ms step_avg:134.23ms
step:1270/1395 train_time:169132ms step_avg:134.23ms
step:1271/1395 train_time:169274ms step_avg:134.24ms
step:1272/1395 train_time:169414ms step_avg:134.24ms
step:1273/1395 train_time:169554ms step_avg:134.25ms
step:1274/1395 train_time:169695ms step_avg:134.25ms
step:1275/1395 train_time:169840ms step_avg:134.26ms
step:1276/1395 train_time:169979ms step_avg:134.26ms
step:1277/1395 train_time:170120ms step_avg:134.27ms
step:1278/1395 train_time:170259ms step_avg:134.27ms
step:1279/1395 train_time:170403ms step_avg:134.28ms
step:1280/1395 train_time:170550ms step_avg:134.29ms
step:1281/1395 train_time:170691ms step_avg:134.30ms
step:1282/1395 train_time:170829ms step_avg:134.30ms
step:1283/1395 train_time:170971ms step_avg:134.31ms
step:1284/1395 train_time:171115ms step_avg:134.31ms
step:1285/1395 train_time:171256ms step_avg:134.32ms
step:1286/1395 train_time:171398ms step_avg:134.32ms
step:1287/1395 train_time:171540ms step_avg:134.33ms
step:1288/1395 train_time:171682ms step_avg:134.34ms
step:1289/1395 train_time:171831ms step_avg:134.35ms
step:1290/1395 train_time:171979ms step_avg:134.36ms
step:1291/1395 train_time:172125ms step_avg:134.37ms
step:1292/1395 train_time:172268ms step_avg:134.37ms
step:1293/1395 train_time:172414ms step_avg:134.38ms
step:1294/1395 train_time:172555ms step_avg:134.39ms
step:1295/1395 train_time:172697ms step_avg:134.39ms
step:1296/1395 train_time:172841ms step_avg:134.40ms
step:1297/1395 train_time:172985ms step_avg:134.41ms
step:1298/1395 train_time:173125ms step_avg:134.41ms
step:1299/1395 train_time:173267ms step_avg:134.42ms
step:1300/1395 train_time:173407ms step_avg:134.42ms
step:1301/1395 train_time:173547ms step_avg:134.43ms
step:1302/1395 train_time:173689ms step_avg:134.43ms
step:1303/1395 train_time:173833ms step_avg:134.44ms
step:1304/1395 train_time:173977ms step_avg:134.45ms
step:1305/1395 train_time:174117ms step_avg:134.45ms
step:1306/1395 train_time:174261ms step_avg:134.46ms
step:1307/1395 train_time:174402ms step_avg:134.47ms
step:1308/1395 train_time:174547ms step_avg:134.47ms
step:1309/1395 train_time:174691ms step_avg:134.48ms
step:1310/1395 train_time:174835ms step_avg:134.49ms
step:1311/1395 train_time:174974ms step_avg:134.49ms
step:1312/1395 train_time:175114ms step_avg:134.50ms
step:1313/1395 train_time:175256ms step_avg:134.50ms
step:1314/1395 train_time:175397ms step_avg:134.51ms
step:1315/1395 train_time:175540ms step_avg:134.51ms
step:1316/1395 train_time:175681ms step_avg:134.52ms
step:1317/1395 train_time:175822ms step_avg:134.52ms
step:1318/1395 train_time:175969ms step_avg:134.53ms
step:1319/1395 train_time:176115ms step_avg:134.54ms
step:1320/1395 train_time:176257ms step_avg:134.55ms
step:1321/1395 train_time:176398ms step_avg:134.55ms
step:1322/1395 train_time:176548ms step_avg:134.56ms
step:1323/1395 train_time:176689ms step_avg:134.57ms
step:1324/1395 train_time:176831ms step_avg:134.57ms
step:1325/1395 train_time:176975ms step_avg:134.58ms
step:1326/1395 train_time:177121ms step_avg:134.59ms
step:1327/1395 train_time:177261ms step_avg:134.59ms
step:1328/1395 train_time:177400ms step_avg:134.60ms
step:1329/1395 train_time:177557ms step_avg:134.61ms
step:1330/1395 train_time:177701ms step_avg:134.62ms
step:1331/1395 train_time:177847ms step_avg:134.63ms
step:1332/1395 train_time:177998ms step_avg:134.64ms
step:1333/1395 train_time:178140ms step_avg:134.65ms
step:1334/1395 train_time:178281ms step_avg:134.65ms
step:1335/1395 train_time:178420ms step_avg:134.66ms
step:1336/1395 train_time:178568ms step_avg:134.67ms
step:1337/1395 train_time:178713ms step_avg:134.67ms
step:1338/1395 train_time:178853ms step_avg:134.68ms
step:1339/1395 train_time:178996ms step_avg:134.68ms
step:1340/1395 train_time:179144ms step_avg:134.69ms
step:1341/1395 train_time:179285ms step_avg:134.70ms
step:1342/1395 train_time:179428ms step_avg:134.71ms
step:1343/1395 train_time:179568ms step_avg:134.71ms
step:1344/1395 train_time:179708ms step_avg:134.71ms
step:1345/1395 train_time:179849ms step_avg:134.72ms
step:1346/1395 train_time:179990ms step_avg:134.72ms
step:1347/1395 train_time:180133ms step_avg:134.73ms
step:1348/1395 train_time:180273ms step_avg:134.73ms
step:1349/1395 train_time:180415ms step_avg:134.74ms
step:1350/1395 train_time:180554ms step_avg:134.74ms
step:1351/1395 train_time:180694ms step_avg:134.75ms
step:1352/1395 train_time:180845ms step_avg:134.76ms
step:1353/1395 train_time:180992ms step_avg:134.77ms
step:1354/1395 train_time:181134ms step_avg:134.77ms
step:1355/1395 train_time:181274ms step_avg:134.78ms
step:1356/1395 train_time:181414ms step_avg:134.78ms
step:1357/1395 train_time:181558ms step_avg:134.79ms
step:1358/1395 train_time:181703ms step_avg:134.79ms
step:1359/1395 train_time:181845ms step_avg:134.80ms
step:1360/1395 train_time:181991ms step_avg:134.81ms
step:1361/1395 train_time:182135ms step_avg:134.82ms
step:1362/1395 train_time:182280ms step_avg:134.82ms
step:1363/1395 train_time:182428ms step_avg:134.83ms
step:1364/1395 train_time:182569ms step_avg:134.84ms
step:1365/1395 train_time:182707ms step_avg:134.84ms
step:1366/1395 train_time:182849ms step_avg:134.84ms
step:1367/1395 train_time:182992ms step_avg:134.85ms
step:1368/1395 train_time:183136ms step_avg:134.86ms
step:1369/1395 train_time:183287ms step_avg:134.87ms
step:1370/1395 train_time:183435ms step_avg:134.88ms
step:1371/1395 train_time:183579ms step_avg:134.89ms
step:1372/1395 train_time:183729ms step_avg:134.90ms
step:1373/1395 train_time:183871ms step_avg:134.90ms
step:1374/1395 train_time:184018ms step_avg:134.91ms
step:1375/1395 train_time:184159ms step_avg:134.92ms
step:1375/1395 val_loss:3.2810 train_time:184271ms step_avg:135.00ms
step:1376/1395 train_time:184302ms step_avg:134.92ms
step:1377/1395 train_time:184445ms step_avg:134.93ms
step:1378/1395 train_time:184586ms step_avg:134.93ms
step:1379/1395 train_time:184730ms step_avg:134.94ms
step:1380/1395 train_time:184872ms step_avg:134.94ms
step:1381/1395 train_time:185019ms step_avg:134.95ms
step:1382/1395 train_time:185162ms step_avg:134.96ms
step:1383/1395 train_time:185305ms step_avg:134.96ms
step:1384/1395 train_time:185452ms step_avg:134.97ms
step:1385/1395 train_time:185593ms step_avg:134.98ms
step:1386/1395 train_time:185735ms step_avg:134.98ms
step:1387/1395 train_time:185880ms step_avg:134.99ms
step:1388/1395 train_time:186020ms step_avg:134.99ms
step:1389/1395 train_time:186163ms step_avg:135.00ms
step:1390/1395 train_time:186305ms step_avg:135.00ms
step:1391/1395 train_time:186447ms step_avg:135.01ms
step:1392/1395 train_time:186592ms step_avg:135.02ms
step:1393/1395 train_time:186733ms step_avg:135.02ms
step:1394/1395 train_time:186875ms step_avg:135.02ms
step:1395/1395 train_time:187016ms step_avg:135.03ms
step:1395/1395 val_loss:3.2767 train_time:187132ms step_avg:135.11ms
peak memory allocated: 37620 MiB reserved: 39114 MiB
