import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        self.attn_scale = 0.15

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_mask(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 16:09:18 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   28C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:26520ms step_avg:nanms
step:2/1395 train_time:26995ms step_avg:nanms
step:3/1395 train_time:27116ms step_avg:nanms
step:4/1395 train_time:27238ms step_avg:nanms
step:5/1395 train_time:27361ms step_avg:nanms
step:6/1395 train_time:27484ms step_avg:nanms
step:7/1395 train_time:27607ms step_avg:nanms
step:8/1395 train_time:27730ms step_avg:nanms
step:9/1395 train_time:27853ms step_avg:nanms
step:10/1395 train_time:27977ms step_avg:nanms
step:11/1395 train_time:123ms step_avg:nanms
step:12/1395 train_time:247ms step_avg:nanms
step:13/1395 train_time:371ms step_avg:123.70ms
step:14/1395 train_time:496ms step_avg:123.96ms
step:15/1395 train_time:619ms step_avg:123.70ms
step:16/1395 train_time:742ms step_avg:123.59ms
step:17/1395 train_time:865ms step_avg:123.54ms
step:18/1395 train_time:988ms step_avg:123.44ms
step:19/1395 train_time:1110ms step_avg:123.37ms
step:20/1395 train_time:1234ms step_avg:123.35ms
step:21/1395 train_time:1357ms step_avg:123.34ms
step:22/1395 train_time:1480ms step_avg:123.31ms
step:23/1395 train_time:1606ms step_avg:123.51ms
step:24/1395 train_time:1729ms step_avg:123.51ms
step:25/1395 train_time:1853ms step_avg:123.53ms
step:26/1395 train_time:1976ms step_avg:123.52ms
step:27/1395 train_time:2099ms step_avg:123.49ms
step:28/1395 train_time:2223ms step_avg:123.53ms
step:29/1395 train_time:2347ms step_avg:123.53ms
step:30/1395 train_time:2470ms step_avg:123.52ms
step:31/1395 train_time:2593ms step_avg:123.49ms
step:32/1395 train_time:2716ms step_avg:123.46ms
step:33/1395 train_time:2839ms step_avg:123.44ms
step:34/1395 train_time:2964ms step_avg:123.48ms
step:35/1395 train_time:3088ms step_avg:123.52ms
step:36/1395 train_time:3211ms step_avg:123.51ms
step:37/1395 train_time:3334ms step_avg:123.49ms
step:38/1395 train_time:3460ms step_avg:123.58ms
step:39/1395 train_time:3586ms step_avg:123.65ms
step:40/1395 train_time:3710ms step_avg:123.67ms
step:41/1395 train_time:3834ms step_avg:123.67ms
step:42/1395 train_time:3957ms step_avg:123.65ms
step:43/1395 train_time:4081ms step_avg:123.67ms
step:44/1395 train_time:4206ms step_avg:123.71ms
step:45/1395 train_time:4330ms step_avg:123.72ms
step:46/1395 train_time:4454ms step_avg:123.71ms
step:47/1395 train_time:4577ms step_avg:123.70ms
step:48/1395 train_time:4701ms step_avg:123.70ms
step:49/1395 train_time:4825ms step_avg:123.71ms
step:50/1395 train_time:4948ms step_avg:123.70ms
step:51/1395 train_time:5071ms step_avg:123.69ms
step:52/1395 train_time:5195ms step_avg:123.68ms
step:53/1395 train_time:5319ms step_avg:123.70ms
step:54/1395 train_time:5444ms step_avg:123.72ms
step:55/1395 train_time:5567ms step_avg:123.72ms
step:56/1395 train_time:5691ms step_avg:123.73ms
step:57/1395 train_time:5817ms step_avg:123.76ms
step:58/1395 train_time:5941ms step_avg:123.78ms
step:59/1395 train_time:6067ms step_avg:123.82ms
step:60/1395 train_time:6191ms step_avg:123.83ms
step:61/1395 train_time:6316ms step_avg:123.83ms
step:62/1395 train_time:6440ms step_avg:123.86ms
step:63/1395 train_time:6564ms step_avg:123.84ms
step:64/1395 train_time:6687ms step_avg:123.83ms
step:65/1395 train_time:6810ms step_avg:123.81ms
step:66/1395 train_time:6934ms step_avg:123.81ms
step:67/1395 train_time:7057ms step_avg:123.80ms
step:68/1395 train_time:7180ms step_avg:123.79ms
step:69/1395 train_time:7304ms step_avg:123.79ms
step:70/1395 train_time:7428ms step_avg:123.81ms
step:71/1395 train_time:7552ms step_avg:123.80ms
step:72/1395 train_time:7676ms step_avg:123.81ms
step:73/1395 train_time:7798ms step_avg:123.78ms
step:74/1395 train_time:7922ms step_avg:123.77ms
step:75/1395 train_time:8046ms step_avg:123.78ms
step:76/1395 train_time:8169ms step_avg:123.78ms
step:77/1395 train_time:8293ms step_avg:123.78ms
step:78/1395 train_time:8417ms step_avg:123.79ms
step:79/1395 train_time:8541ms step_avg:123.79ms
step:80/1395 train_time:8667ms step_avg:123.82ms
step:81/1395 train_time:8790ms step_avg:123.81ms
step:82/1395 train_time:8913ms step_avg:123.80ms
step:83/1395 train_time:9036ms step_avg:123.79ms
step:84/1395 train_time:9159ms step_avg:123.77ms
step:85/1395 train_time:9283ms step_avg:123.77ms
step:86/1395 train_time:9406ms step_avg:123.77ms
step:87/1395 train_time:9530ms step_avg:123.77ms
step:88/1395 train_time:9653ms step_avg:123.76ms
step:89/1395 train_time:9776ms step_avg:123.75ms
step:90/1395 train_time:9900ms step_avg:123.75ms
step:91/1395 train_time:10023ms step_avg:123.74ms
step:92/1395 train_time:10147ms step_avg:123.74ms
step:93/1395 train_time:10269ms step_avg:123.73ms
step:94/1395 train_time:10393ms step_avg:123.73ms
step:95/1395 train_time:10516ms step_avg:123.72ms
step:96/1395 train_time:10640ms step_avg:123.72ms
step:97/1395 train_time:10763ms step_avg:123.71ms
step:98/1395 train_time:10887ms step_avg:123.72ms
step:99/1395 train_time:11011ms step_avg:123.72ms
step:100/1395 train_time:11134ms step_avg:123.71ms
step:101/1395 train_time:11256ms step_avg:123.70ms
step:102/1395 train_time:11379ms step_avg:123.69ms
step:103/1395 train_time:11502ms step_avg:123.68ms
step:104/1395 train_time:11626ms step_avg:123.68ms
step:105/1395 train_time:11750ms step_avg:123.68ms
step:106/1395 train_time:11876ms step_avg:123.71ms
step:107/1395 train_time:12002ms step_avg:123.73ms
step:108/1395 train_time:12129ms step_avg:123.77ms
step:109/1395 train_time:12255ms step_avg:123.79ms
step:110/1395 train_time:12382ms step_avg:123.82ms
step:111/1395 train_time:12509ms step_avg:123.85ms
step:112/1395 train_time:12634ms step_avg:123.87ms
step:113/1395 train_time:12760ms step_avg:123.89ms
step:114/1395 train_time:12887ms step_avg:123.91ms
step:115/1395 train_time:13012ms step_avg:123.93ms
step:116/1395 train_time:13139ms step_avg:123.95ms
step:117/1395 train_time:13265ms step_avg:123.97ms
step:118/1395 train_time:13391ms step_avg:123.99ms
step:119/1395 train_time:13518ms step_avg:124.02ms
step:120/1395 train_time:13644ms step_avg:124.04ms
step:121/1395 train_time:13770ms step_avg:124.06ms
step:122/1395 train_time:13897ms step_avg:124.08ms
step:123/1395 train_time:14023ms step_avg:124.10ms
step:124/1395 train_time:14149ms step_avg:124.12ms
step:125/1395 train_time:14275ms step_avg:124.13ms
step:125/1395 val_loss:4.3694 train_time:14377ms step_avg:125.01ms
step:126/1395 train_time:14403ms step_avg:124.17ms
step:127/1395 train_time:14543ms step_avg:124.30ms
step:128/1395 train_time:14672ms step_avg:124.34ms
step:129/1395 train_time:14799ms step_avg:124.36ms
step:130/1395 train_time:14925ms step_avg:124.37ms
step:131/1395 train_time:15050ms step_avg:124.38ms
step:132/1395 train_time:15176ms step_avg:124.40ms
step:133/1395 train_time:15303ms step_avg:124.42ms
step:134/1395 train_time:15428ms step_avg:124.42ms
step:135/1395 train_time:15555ms step_avg:124.44ms
step:136/1395 train_time:15681ms step_avg:124.45ms
step:137/1395 train_time:15807ms step_avg:124.47ms
step:138/1395 train_time:15934ms step_avg:124.48ms
step:139/1395 train_time:16060ms step_avg:124.49ms
step:140/1395 train_time:16185ms step_avg:124.50ms
step:141/1395 train_time:16313ms step_avg:124.52ms
step:142/1395 train_time:16439ms step_avg:124.54ms
step:143/1395 train_time:16565ms step_avg:124.55ms
step:144/1395 train_time:16691ms step_avg:124.56ms
step:145/1395 train_time:16817ms step_avg:124.57ms
step:146/1395 train_time:16944ms step_avg:124.59ms
step:147/1395 train_time:17070ms step_avg:124.60ms
step:148/1395 train_time:17196ms step_avg:124.61ms
step:149/1395 train_time:17322ms step_avg:124.62ms
step:150/1395 train_time:17449ms step_avg:124.64ms
step:151/1395 train_time:17575ms step_avg:124.65ms
step:152/1395 train_time:17702ms step_avg:124.66ms
step:153/1395 train_time:17828ms step_avg:124.67ms
step:154/1395 train_time:17955ms step_avg:124.69ms
step:155/1395 train_time:18081ms step_avg:124.70ms
step:156/1395 train_time:18208ms step_avg:124.71ms
step:157/1395 train_time:18336ms step_avg:124.73ms
step:158/1395 train_time:18463ms step_avg:124.75ms
step:159/1395 train_time:18588ms step_avg:124.75ms
step:160/1395 train_time:18714ms step_avg:124.76ms
step:161/1395 train_time:18840ms step_avg:124.77ms
step:162/1395 train_time:18966ms step_avg:124.78ms
step:163/1395 train_time:19092ms step_avg:124.78ms
step:164/1395 train_time:19219ms step_avg:124.80ms
step:165/1395 train_time:19345ms step_avg:124.81ms
step:166/1395 train_time:19470ms step_avg:124.81ms
step:167/1395 train_time:19597ms step_avg:124.82ms
step:168/1395 train_time:19724ms step_avg:124.83ms
step:169/1395 train_time:19850ms step_avg:124.84ms
step:170/1395 train_time:19975ms step_avg:124.85ms
step:171/1395 train_time:20102ms step_avg:124.86ms
step:172/1395 train_time:20227ms step_avg:124.86ms
step:173/1395 train_time:20354ms step_avg:124.87ms
step:174/1395 train_time:20480ms step_avg:124.88ms
step:175/1395 train_time:20606ms step_avg:124.88ms
step:176/1395 train_time:20732ms step_avg:124.89ms
step:177/1395 train_time:20858ms step_avg:124.90ms
step:178/1395 train_time:20984ms step_avg:124.90ms
step:179/1395 train_time:21110ms step_avg:124.91ms
step:180/1395 train_time:21236ms step_avg:124.92ms
step:181/1395 train_time:21363ms step_avg:124.93ms
step:182/1395 train_time:21489ms step_avg:124.94ms
step:183/1395 train_time:21615ms step_avg:124.94ms
step:184/1395 train_time:21742ms step_avg:124.96ms
step:185/1395 train_time:21868ms step_avg:124.96ms
step:186/1395 train_time:21995ms step_avg:124.97ms
step:187/1395 train_time:22120ms step_avg:124.97ms
step:188/1395 train_time:22246ms step_avg:124.98ms
step:189/1395 train_time:22372ms step_avg:124.99ms
step:190/1395 train_time:22498ms step_avg:124.99ms
step:191/1395 train_time:22625ms step_avg:125.00ms
step:192/1395 train_time:22750ms step_avg:125.00ms
step:193/1395 train_time:22876ms step_avg:125.00ms
step:194/1395 train_time:23002ms step_avg:125.01ms
step:195/1395 train_time:23128ms step_avg:125.02ms
step:196/1395 train_time:23254ms step_avg:125.02ms
step:197/1395 train_time:23381ms step_avg:125.03ms
step:198/1395 train_time:23506ms step_avg:125.03ms
step:199/1395 train_time:23632ms step_avg:125.04ms
step:200/1395 train_time:23759ms step_avg:125.05ms
step:201/1395 train_time:23885ms step_avg:125.05ms
step:202/1395 train_time:24011ms step_avg:125.06ms
step:203/1395 train_time:24138ms step_avg:125.07ms
step:204/1395 train_time:24266ms step_avg:125.08ms
step:205/1395 train_time:24392ms step_avg:125.09ms
step:206/1395 train_time:24518ms step_avg:125.09ms
step:207/1395 train_time:24644ms step_avg:125.10ms
step:208/1395 train_time:24771ms step_avg:125.10ms
step:209/1395 train_time:24899ms step_avg:125.12ms
step:210/1395 train_time:25027ms step_avg:125.14ms
step:211/1395 train_time:25156ms step_avg:125.15ms
step:212/1395 train_time:25285ms step_avg:125.17ms
step:213/1395 train_time:25413ms step_avg:125.18ms
step:214/1395 train_time:25542ms step_avg:125.20ms
step:215/1395 train_time:25670ms step_avg:125.22ms
step:216/1395 train_time:25799ms step_avg:125.24ms
step:217/1395 train_time:25927ms step_avg:125.25ms
step:218/1395 train_time:26055ms step_avg:125.26ms
step:219/1395 train_time:26184ms step_avg:125.28ms
step:220/1395 train_time:26311ms step_avg:125.29ms
step:221/1395 train_time:26440ms step_avg:125.31ms
step:222/1395 train_time:26568ms step_avg:125.32ms
step:223/1395 train_time:26697ms step_avg:125.34ms
step:224/1395 train_time:26826ms step_avg:125.36ms
step:225/1395 train_time:26954ms step_avg:125.37ms
step:226/1395 train_time:27082ms step_avg:125.38ms
step:227/1395 train_time:27210ms step_avg:125.39ms
step:228/1395 train_time:27338ms step_avg:125.40ms
step:229/1395 train_time:27467ms step_avg:125.42ms
step:230/1395 train_time:27595ms step_avg:125.43ms
step:231/1395 train_time:27723ms step_avg:125.44ms
step:232/1395 train_time:27852ms step_avg:125.46ms
step:233/1395 train_time:27980ms step_avg:125.47ms
step:234/1395 train_time:28109ms step_avg:125.49ms
step:235/1395 train_time:28237ms step_avg:125.50ms
step:236/1395 train_time:28366ms step_avg:125.51ms
step:237/1395 train_time:28495ms step_avg:125.53ms
step:238/1395 train_time:28624ms step_avg:125.54ms
step:239/1395 train_time:28752ms step_avg:125.55ms
step:240/1395 train_time:28880ms step_avg:125.56ms
step:241/1395 train_time:29009ms step_avg:125.58ms
step:242/1395 train_time:29137ms step_avg:125.59ms
step:243/1395 train_time:29266ms step_avg:125.61ms
step:244/1395 train_time:29395ms step_avg:125.62ms
step:245/1395 train_time:29524ms step_avg:125.63ms
step:246/1395 train_time:29651ms step_avg:125.64ms
step:247/1395 train_time:29779ms step_avg:125.65ms
step:248/1395 train_time:29908ms step_avg:125.66ms
step:249/1395 train_time:30037ms step_avg:125.68ms
step:250/1395 train_time:30166ms step_avg:125.69ms
step:250/1395 val_loss:3.9513 train_time:30268ms step_avg:126.12ms
step:251/1395 train_time:30299ms step_avg:125.72ms
step:252/1395 train_time:30437ms step_avg:125.77ms
step:253/1395 train_time:30567ms step_avg:125.79ms
step:254/1395 train_time:30695ms step_avg:125.80ms
step:255/1395 train_time:30824ms step_avg:125.81ms
step:256/1395 train_time:30951ms step_avg:125.82ms
step:257/1395 train_time:31080ms step_avg:125.83ms
step:258/1395 train_time:31207ms step_avg:125.83ms
step:259/1395 train_time:31335ms step_avg:125.85ms
step:260/1395 train_time:31466ms step_avg:125.86ms
step:261/1395 train_time:31595ms step_avg:125.88ms
step:262/1395 train_time:31724ms step_avg:125.89ms
step:263/1395 train_time:31852ms step_avg:125.90ms
step:264/1395 train_time:31981ms step_avg:125.91ms
step:265/1395 train_time:32109ms step_avg:125.92ms
step:266/1395 train_time:32237ms step_avg:125.92ms
step:267/1395 train_time:32365ms step_avg:125.94ms
step:268/1395 train_time:32494ms step_avg:125.95ms
step:269/1395 train_time:32623ms step_avg:125.96ms
step:270/1395 train_time:32752ms step_avg:125.97ms
step:271/1395 train_time:32881ms step_avg:125.98ms
step:272/1395 train_time:33009ms step_avg:125.99ms
step:273/1395 train_time:33137ms step_avg:125.99ms
step:274/1395 train_time:33265ms step_avg:126.00ms
step:275/1395 train_time:33393ms step_avg:126.01ms
step:276/1395 train_time:33523ms step_avg:126.02ms
step:277/1395 train_time:33652ms step_avg:126.04ms
step:278/1395 train_time:33781ms step_avg:126.05ms
step:279/1395 train_time:33909ms step_avg:126.06ms
step:280/1395 train_time:34038ms step_avg:126.07ms
step:281/1395 train_time:34166ms step_avg:126.08ms
step:282/1395 train_time:34294ms step_avg:126.08ms
step:283/1395 train_time:34424ms step_avg:126.09ms
step:284/1395 train_time:34552ms step_avg:126.10ms
step:285/1395 train_time:34680ms step_avg:126.11ms
step:286/1395 train_time:34808ms step_avg:126.12ms
step:287/1395 train_time:34938ms step_avg:126.13ms
step:288/1395 train_time:35066ms step_avg:126.14ms
step:289/1395 train_time:35195ms step_avg:126.15ms
step:290/1395 train_time:35325ms step_avg:126.16ms
step:291/1395 train_time:35454ms step_avg:126.17ms
step:292/1395 train_time:35583ms step_avg:126.18ms
step:293/1395 train_time:35712ms step_avg:126.19ms
step:294/1395 train_time:35840ms step_avg:126.20ms
step:295/1395 train_time:35968ms step_avg:126.20ms
step:296/1395 train_time:36097ms step_avg:126.21ms
step:297/1395 train_time:36226ms step_avg:126.22ms
step:298/1395 train_time:36354ms step_avg:126.23ms
step:299/1395 train_time:36483ms step_avg:126.24ms
step:300/1395 train_time:36612ms step_avg:126.25ms
step:301/1395 train_time:36741ms step_avg:126.26ms
step:302/1395 train_time:36869ms step_avg:126.26ms
step:303/1395 train_time:36997ms step_avg:126.27ms
step:304/1395 train_time:37126ms step_avg:126.28ms
step:305/1395 train_time:37255ms step_avg:126.29ms
step:306/1395 train_time:37385ms step_avg:126.30ms
step:307/1395 train_time:37513ms step_avg:126.31ms
step:308/1395 train_time:37643ms step_avg:126.32ms
step:309/1395 train_time:37770ms step_avg:126.32ms
step:310/1395 train_time:37899ms step_avg:126.33ms
step:311/1395 train_time:38027ms step_avg:126.34ms
step:312/1395 train_time:38156ms step_avg:126.35ms
step:313/1395 train_time:38287ms step_avg:126.36ms
step:314/1395 train_time:38418ms step_avg:126.38ms
step:315/1395 train_time:38549ms step_avg:126.39ms
step:316/1395 train_time:38679ms step_avg:126.40ms
step:317/1395 train_time:38809ms step_avg:126.41ms
step:318/1395 train_time:38940ms step_avg:126.43ms
step:319/1395 train_time:39070ms step_avg:126.44ms
step:320/1395 train_time:39200ms step_avg:126.45ms
step:321/1395 train_time:39330ms step_avg:126.46ms
step:322/1395 train_time:39460ms step_avg:126.48ms
step:323/1395 train_time:39590ms step_avg:126.49ms
step:324/1395 train_time:39721ms step_avg:126.50ms
step:325/1395 train_time:39851ms step_avg:126.51ms
step:326/1395 train_time:39983ms step_avg:126.53ms
step:327/1395 train_time:40113ms step_avg:126.54ms
step:328/1395 train_time:40244ms step_avg:126.55ms
step:329/1395 train_time:40375ms step_avg:126.57ms
step:330/1395 train_time:40507ms step_avg:126.58ms
step:331/1395 train_time:40637ms step_avg:126.59ms
step:332/1395 train_time:40767ms step_avg:126.61ms
step:333/1395 train_time:40898ms step_avg:126.62ms
step:334/1395 train_time:41028ms step_avg:126.63ms
step:335/1395 train_time:41159ms step_avg:126.64ms
step:336/1395 train_time:41289ms step_avg:126.65ms
step:337/1395 train_time:41420ms step_avg:126.67ms
step:338/1395 train_time:41550ms step_avg:126.68ms
step:339/1395 train_time:41682ms step_avg:126.69ms
step:340/1395 train_time:41812ms step_avg:126.70ms
step:341/1395 train_time:41943ms step_avg:126.72ms
step:342/1395 train_time:42074ms step_avg:126.73ms
step:343/1395 train_time:42205ms step_avg:126.74ms
step:344/1395 train_time:42335ms step_avg:126.75ms
step:345/1395 train_time:42466ms step_avg:126.77ms
step:346/1395 train_time:42598ms step_avg:126.78ms
step:347/1395 train_time:42729ms step_avg:126.79ms
step:348/1395 train_time:42860ms step_avg:126.80ms
step:349/1395 train_time:42989ms step_avg:126.81ms
step:350/1395 train_time:43121ms step_avg:126.83ms
step:351/1395 train_time:43251ms step_avg:126.84ms
step:352/1395 train_time:43382ms step_avg:126.85ms
step:353/1395 train_time:43512ms step_avg:126.86ms
step:354/1395 train_time:43642ms step_avg:126.87ms
step:355/1395 train_time:43773ms step_avg:126.88ms
step:356/1395 train_time:43905ms step_avg:126.89ms
step:357/1395 train_time:44035ms step_avg:126.90ms
step:358/1395 train_time:44166ms step_avg:126.91ms
step:359/1395 train_time:44297ms step_avg:126.93ms
step:360/1395 train_time:44427ms step_avg:126.93ms
step:361/1395 train_time:44557ms step_avg:126.94ms
step:362/1395 train_time:44688ms step_avg:126.96ms
step:363/1395 train_time:44818ms step_avg:126.96ms
step:364/1395 train_time:44949ms step_avg:126.97ms
step:365/1395 train_time:45079ms step_avg:126.98ms
step:366/1395 train_time:45209ms step_avg:126.99ms
step:367/1395 train_time:45339ms step_avg:127.00ms
step:368/1395 train_time:45470ms step_avg:127.01ms
step:369/1395 train_time:45600ms step_avg:127.02ms
step:370/1395 train_time:45730ms step_avg:127.03ms
step:371/1395 train_time:45861ms step_avg:127.04ms
step:372/1395 train_time:45991ms step_avg:127.05ms
step:373/1395 train_time:46122ms step_avg:127.06ms
step:374/1395 train_time:46252ms step_avg:127.07ms
step:375/1395 train_time:46382ms step_avg:127.07ms
step:375/1395 val_loss:3.7702 train_time:46487ms step_avg:127.36ms
step:376/1395 train_time:46515ms step_avg:127.09ms
step:377/1395 train_time:46657ms step_avg:127.13ms
step:378/1395 train_time:46787ms step_avg:127.14ms
step:379/1395 train_time:46918ms step_avg:127.15ms
step:380/1395 train_time:47049ms step_avg:127.16ms
step:381/1395 train_time:47179ms step_avg:127.17ms
step:382/1395 train_time:47309ms step_avg:127.18ms
step:383/1395 train_time:47440ms step_avg:127.18ms
step:384/1395 train_time:47570ms step_avg:127.19ms
step:385/1395 train_time:47701ms step_avg:127.20ms
step:386/1395 train_time:47831ms step_avg:127.21ms
step:387/1395 train_time:47962ms step_avg:127.22ms
step:388/1395 train_time:48092ms step_avg:127.23ms
step:389/1395 train_time:48223ms step_avg:127.24ms
step:390/1395 train_time:48353ms step_avg:127.24ms
step:391/1395 train_time:48484ms step_avg:127.25ms
step:392/1395 train_time:48614ms step_avg:127.26ms
step:393/1395 train_time:48745ms step_avg:127.27ms
step:394/1395 train_time:48874ms step_avg:127.28ms
step:395/1395 train_time:49004ms step_avg:127.28ms
step:396/1395 train_time:49134ms step_avg:127.29ms
step:397/1395 train_time:49264ms step_avg:127.30ms
step:398/1395 train_time:49394ms step_avg:127.31ms
step:399/1395 train_time:49525ms step_avg:127.31ms
step:400/1395 train_time:49655ms step_avg:127.32ms
step:401/1395 train_time:49786ms step_avg:127.33ms
step:402/1395 train_time:49916ms step_avg:127.34ms
step:403/1395 train_time:50046ms step_avg:127.34ms
step:404/1395 train_time:50176ms step_avg:127.35ms
step:405/1395 train_time:50306ms step_avg:127.36ms
step:406/1395 train_time:50436ms step_avg:127.36ms
step:407/1395 train_time:50566ms step_avg:127.37ms
step:408/1395 train_time:50696ms step_avg:127.38ms
step:409/1395 train_time:50826ms step_avg:127.38ms
step:410/1395 train_time:50956ms step_avg:127.39ms
step:411/1395 train_time:51086ms step_avg:127.40ms
step:412/1395 train_time:51217ms step_avg:127.41ms
step:413/1395 train_time:51347ms step_avg:127.41ms
step:414/1395 train_time:51477ms step_avg:127.42ms
step:415/1395 train_time:51609ms step_avg:127.43ms
step:416/1395 train_time:51741ms step_avg:127.44ms
step:417/1395 train_time:51874ms step_avg:127.45ms
step:418/1395 train_time:52006ms step_avg:127.46ms
step:419/1395 train_time:52137ms step_avg:127.48ms
step:420/1395 train_time:52271ms step_avg:127.49ms
step:421/1395 train_time:52403ms step_avg:127.50ms
step:422/1395 train_time:52534ms step_avg:127.51ms
step:423/1395 train_time:52666ms step_avg:127.52ms
step:424/1395 train_time:52799ms step_avg:127.53ms
step:425/1395 train_time:52931ms step_avg:127.55ms
step:426/1395 train_time:53064ms step_avg:127.56ms
step:427/1395 train_time:53196ms step_avg:127.57ms
step:428/1395 train_time:53328ms step_avg:127.58ms
step:429/1395 train_time:53461ms step_avg:127.59ms
step:430/1395 train_time:53594ms step_avg:127.61ms
step:431/1395 train_time:53726ms step_avg:127.62ms
step:432/1395 train_time:53858ms step_avg:127.63ms
step:433/1395 train_time:53991ms step_avg:127.64ms
step:434/1395 train_time:54124ms step_avg:127.65ms
step:435/1395 train_time:54255ms step_avg:127.66ms
step:436/1395 train_time:54389ms step_avg:127.67ms
step:437/1395 train_time:54522ms step_avg:127.69ms
step:438/1395 train_time:54655ms step_avg:127.70ms
step:439/1395 train_time:54789ms step_avg:127.71ms
step:440/1395 train_time:54920ms step_avg:127.72ms
step:441/1395 train_time:55053ms step_avg:127.73ms
step:442/1395 train_time:55185ms step_avg:127.74ms
step:443/1395 train_time:55317ms step_avg:127.75ms
step:444/1395 train_time:55450ms step_avg:127.76ms
step:445/1395 train_time:55582ms step_avg:127.78ms
step:446/1395 train_time:55714ms step_avg:127.78ms
step:447/1395 train_time:55848ms step_avg:127.80ms
step:448/1395 train_time:55981ms step_avg:127.81ms
step:449/1395 train_time:56114ms step_avg:127.82ms
step:450/1395 train_time:56247ms step_avg:127.83ms
step:451/1395 train_time:56379ms step_avg:127.84ms
step:452/1395 train_time:56511ms step_avg:127.85ms
step:453/1395 train_time:56643ms step_avg:127.86ms
step:454/1395 train_time:56775ms step_avg:127.87ms
step:455/1395 train_time:56908ms step_avg:127.88ms
step:456/1395 train_time:57040ms step_avg:127.89ms
step:457/1395 train_time:57173ms step_avg:127.90ms
step:458/1395 train_time:57305ms step_avg:127.91ms
step:459/1395 train_time:57437ms step_avg:127.92ms
step:460/1395 train_time:57569ms step_avg:127.93ms
step:461/1395 train_time:57701ms step_avg:127.94ms
step:462/1395 train_time:57833ms step_avg:127.95ms
step:463/1395 train_time:57966ms step_avg:127.96ms
step:464/1395 train_time:58098ms step_avg:127.97ms
step:465/1395 train_time:58230ms step_avg:127.98ms
step:466/1395 train_time:58363ms step_avg:127.99ms
step:467/1395 train_time:58495ms step_avg:128.00ms
step:468/1395 train_time:58626ms step_avg:128.00ms
step:469/1395 train_time:58758ms step_avg:128.01ms
step:470/1395 train_time:58892ms step_avg:128.03ms
step:471/1395 train_time:59024ms step_avg:128.03ms
step:472/1395 train_time:59157ms step_avg:128.05ms
step:473/1395 train_time:59290ms step_avg:128.06ms
step:474/1395 train_time:59424ms step_avg:128.07ms
step:475/1395 train_time:59556ms step_avg:128.08ms
step:476/1395 train_time:59687ms step_avg:128.08ms
step:477/1395 train_time:59819ms step_avg:128.09ms
step:478/1395 train_time:59951ms step_avg:128.10ms
step:479/1395 train_time:60084ms step_avg:128.11ms
step:480/1395 train_time:60216ms step_avg:128.12ms
step:481/1395 train_time:60349ms step_avg:128.13ms
step:482/1395 train_time:60482ms step_avg:128.14ms
step:483/1395 train_time:60615ms step_avg:128.15ms
step:484/1395 train_time:60749ms step_avg:128.16ms
step:485/1395 train_time:60881ms step_avg:128.17ms
step:486/1395 train_time:61013ms step_avg:128.18ms
step:487/1395 train_time:61146ms step_avg:128.19ms
step:488/1395 train_time:61278ms step_avg:128.20ms
step:489/1395 train_time:61411ms step_avg:128.21ms
step:490/1395 train_time:61544ms step_avg:128.22ms
step:491/1395 train_time:61677ms step_avg:128.23ms
step:492/1395 train_time:61810ms step_avg:128.24ms
step:493/1395 train_time:61941ms step_avg:128.24ms
step:494/1395 train_time:62074ms step_avg:128.25ms
step:495/1395 train_time:62207ms step_avg:128.26ms
step:496/1395 train_time:62339ms step_avg:128.27ms
step:497/1395 train_time:62471ms step_avg:128.28ms
step:498/1395 train_time:62603ms step_avg:128.28ms
step:499/1395 train_time:62735ms step_avg:128.29ms
step:500/1395 train_time:62866ms step_avg:128.30ms
step:500/1395 val_loss:3.6556 train_time:62972ms step_avg:128.51ms
step:501/1395 train_time:63003ms step_avg:128.32ms
step:502/1395 train_time:63143ms step_avg:128.34ms
step:503/1395 train_time:63275ms step_avg:128.35ms
step:504/1395 train_time:63408ms step_avg:128.36ms
step:505/1395 train_time:63540ms step_avg:128.36ms
step:506/1395 train_time:63671ms step_avg:128.37ms
step:507/1395 train_time:63803ms step_avg:128.38ms
step:508/1395 train_time:63936ms step_avg:128.38ms
step:509/1395 train_time:64070ms step_avg:128.40ms
step:510/1395 train_time:64203ms step_avg:128.41ms
step:511/1395 train_time:64335ms step_avg:128.41ms
step:512/1395 train_time:64468ms step_avg:128.42ms
step:513/1395 train_time:64602ms step_avg:128.43ms
step:514/1395 train_time:64734ms step_avg:128.44ms
step:515/1395 train_time:64866ms step_avg:128.45ms
step:516/1395 train_time:65000ms step_avg:128.46ms
step:517/1395 train_time:65132ms step_avg:128.47ms
step:518/1395 train_time:65265ms step_avg:128.47ms
step:519/1395 train_time:65399ms step_avg:128.49ms
step:520/1395 train_time:65533ms step_avg:128.50ms
step:521/1395 train_time:65666ms step_avg:128.51ms
step:522/1395 train_time:65800ms step_avg:128.52ms
step:523/1395 train_time:65934ms step_avg:128.53ms
step:524/1395 train_time:66069ms step_avg:128.54ms
step:525/1395 train_time:66203ms step_avg:128.55ms
step:526/1395 train_time:66336ms step_avg:128.56ms
step:527/1395 train_time:66471ms step_avg:128.57ms
step:528/1395 train_time:66604ms step_avg:128.58ms
step:529/1395 train_time:66738ms step_avg:128.59ms
step:530/1395 train_time:66872ms step_avg:128.60ms
step:531/1395 train_time:67005ms step_avg:128.61ms
step:532/1395 train_time:67138ms step_avg:128.62ms
step:533/1395 train_time:67273ms step_avg:128.63ms
step:534/1395 train_time:67408ms step_avg:128.64ms
step:535/1395 train_time:67542ms step_avg:128.65ms
step:536/1395 train_time:67674ms step_avg:128.66ms
step:537/1395 train_time:67808ms step_avg:128.67ms
step:538/1395 train_time:67943ms step_avg:128.68ms
step:539/1395 train_time:68076ms step_avg:128.69ms
step:540/1395 train_time:68211ms step_avg:128.70ms
step:541/1395 train_time:68347ms step_avg:128.71ms
step:542/1395 train_time:68480ms step_avg:128.72ms
step:543/1395 train_time:68614ms step_avg:128.73ms
step:544/1395 train_time:68748ms step_avg:128.74ms
step:545/1395 train_time:68882ms step_avg:128.75ms
step:546/1395 train_time:69016ms step_avg:128.76ms
step:547/1395 train_time:69149ms step_avg:128.77ms
step:548/1395 train_time:69283ms step_avg:128.78ms
step:549/1395 train_time:69417ms step_avg:128.79ms
step:550/1395 train_time:69552ms step_avg:128.80ms
step:551/1395 train_time:69685ms step_avg:128.81ms
step:552/1395 train_time:69819ms step_avg:128.82ms
step:553/1395 train_time:69954ms step_avg:128.83ms
step:554/1395 train_time:70086ms step_avg:128.83ms
step:555/1395 train_time:70220ms step_avg:128.84ms
step:556/1395 train_time:70354ms step_avg:128.85ms
step:557/1395 train_time:70488ms step_avg:128.86ms
step:558/1395 train_time:70623ms step_avg:128.87ms
step:559/1395 train_time:70756ms step_avg:128.88ms
step:560/1395 train_time:70890ms step_avg:128.89ms
step:561/1395 train_time:71024ms step_avg:128.90ms
step:562/1395 train_time:71157ms step_avg:128.91ms
step:563/1395 train_time:71291ms step_avg:128.92ms
step:564/1395 train_time:71425ms step_avg:128.93ms
step:565/1395 train_time:71558ms step_avg:128.93ms
step:566/1395 train_time:71693ms step_avg:128.94ms
step:567/1395 train_time:71826ms step_avg:128.95ms
step:568/1395 train_time:71959ms step_avg:128.96ms
step:569/1395 train_time:72093ms step_avg:128.97ms
step:570/1395 train_time:72226ms step_avg:128.98ms
step:571/1395 train_time:72360ms step_avg:128.98ms
step:572/1395 train_time:72493ms step_avg:128.99ms
step:573/1395 train_time:72627ms step_avg:129.00ms
step:574/1395 train_time:72765ms step_avg:129.02ms
step:575/1395 train_time:72898ms step_avg:129.02ms
step:576/1395 train_time:73031ms step_avg:129.03ms
step:577/1395 train_time:73164ms step_avg:129.04ms
step:578/1395 train_time:73297ms step_avg:129.04ms
step:579/1395 train_time:73432ms step_avg:129.05ms
step:580/1395 train_time:73566ms step_avg:129.06ms
step:581/1395 train_time:73701ms step_avg:129.07ms
step:582/1395 train_time:73834ms step_avg:129.08ms
step:583/1395 train_time:73969ms step_avg:129.09ms
step:584/1395 train_time:74103ms step_avg:129.10ms
step:585/1395 train_time:74236ms step_avg:129.11ms
step:586/1395 train_time:74372ms step_avg:129.12ms
step:587/1395 train_time:74507ms step_avg:129.13ms
step:588/1395 train_time:74640ms step_avg:129.14ms
step:589/1395 train_time:74775ms step_avg:129.14ms
step:590/1395 train_time:74910ms step_avg:129.16ms
step:591/1395 train_time:75045ms step_avg:129.16ms
step:592/1395 train_time:75179ms step_avg:129.17ms
step:593/1395 train_time:75313ms step_avg:129.18ms
step:594/1395 train_time:75447ms step_avg:129.19ms
step:595/1395 train_time:75581ms step_avg:129.20ms
step:596/1395 train_time:75715ms step_avg:129.21ms
step:597/1395 train_time:75849ms step_avg:129.21ms
step:598/1395 train_time:75982ms step_avg:129.22ms
step:599/1395 train_time:76116ms step_avg:129.23ms
step:600/1395 train_time:76251ms step_avg:129.24ms
step:601/1395 train_time:76384ms step_avg:129.25ms
step:602/1395 train_time:76517ms step_avg:129.25ms
step:603/1395 train_time:76651ms step_avg:129.26ms
step:604/1395 train_time:76784ms step_avg:129.27ms
step:605/1395 train_time:76920ms step_avg:129.28ms
step:606/1395 train_time:77053ms step_avg:129.28ms
step:607/1395 train_time:77188ms step_avg:129.29ms
step:608/1395 train_time:77322ms step_avg:129.30ms
step:609/1395 train_time:77457ms step_avg:129.31ms
step:610/1395 train_time:77590ms step_avg:129.32ms
step:611/1395 train_time:77724ms step_avg:129.32ms
step:612/1395 train_time:77857ms step_avg:129.33ms
step:613/1395 train_time:77990ms step_avg:129.34ms
step:614/1395 train_time:78124ms step_avg:129.34ms
step:615/1395 train_time:78258ms step_avg:129.35ms
step:616/1395 train_time:78391ms step_avg:129.36ms
step:617/1395 train_time:78524ms step_avg:129.36ms
step:618/1395 train_time:78658ms step_avg:129.37ms
step:619/1395 train_time:78792ms step_avg:129.38ms
step:620/1395 train_time:78926ms step_avg:129.39ms
step:621/1395 train_time:79060ms step_avg:129.40ms
step:622/1395 train_time:79196ms step_avg:129.41ms
step:623/1395 train_time:79332ms step_avg:129.42ms
step:624/1395 train_time:79467ms step_avg:129.43ms
step:625/1395 train_time:79602ms step_avg:129.43ms
step:625/1395 val_loss:3.5737 train_time:79711ms step_avg:129.61ms
step:626/1395 train_time:79741ms step_avg:129.45ms
step:627/1395 train_time:79883ms step_avg:129.47ms
step:628/1395 train_time:80020ms step_avg:129.48ms
step:629/1395 train_time:80155ms step_avg:129.49ms
step:630/1395 train_time:80289ms step_avg:129.50ms
step:631/1395 train_time:80423ms step_avg:129.51ms
step:632/1395 train_time:80558ms step_avg:129.51ms
step:633/1395 train_time:80693ms step_avg:129.52ms
step:634/1395 train_time:80827ms step_avg:129.53ms
step:635/1395 train_time:80963ms step_avg:129.54ms
step:636/1395 train_time:81099ms step_avg:129.55ms
step:637/1395 train_time:81234ms step_avg:129.56ms
step:638/1395 train_time:81368ms step_avg:129.57ms
step:639/1395 train_time:81502ms step_avg:129.57ms
step:640/1395 train_time:81637ms step_avg:129.58ms
step:641/1395 train_time:81772ms step_avg:129.59ms
step:642/1395 train_time:81907ms step_avg:129.60ms
step:643/1395 train_time:82042ms step_avg:129.61ms
step:644/1395 train_time:82177ms step_avg:129.62ms
step:645/1395 train_time:82313ms step_avg:129.63ms
step:646/1395 train_time:82448ms step_avg:129.64ms
step:647/1395 train_time:82581ms step_avg:129.64ms
step:648/1395 train_time:82720ms step_avg:129.65ms
step:649/1395 train_time:82855ms step_avg:129.66ms
step:650/1395 train_time:82991ms step_avg:129.67ms
step:651/1395 train_time:83126ms step_avg:129.68ms
step:652/1395 train_time:83262ms step_avg:129.69ms
step:653/1395 train_time:83397ms step_avg:129.70ms
step:654/1395 train_time:83533ms step_avg:129.71ms
step:655/1395 train_time:83667ms step_avg:129.72ms
step:656/1395 train_time:83801ms step_avg:129.72ms
step:657/1395 train_time:83937ms step_avg:129.73ms
step:658/1395 train_time:84072ms step_avg:129.74ms
step:659/1395 train_time:84207ms step_avg:129.75ms
step:660/1395 train_time:84342ms step_avg:129.76ms
step:661/1395 train_time:84477ms step_avg:129.77ms
step:662/1395 train_time:84614ms step_avg:129.78ms
step:663/1395 train_time:84748ms step_avg:129.78ms
step:664/1395 train_time:84884ms step_avg:129.79ms
step:665/1395 train_time:85019ms step_avg:129.80ms
step:666/1395 train_time:85156ms step_avg:129.81ms
step:667/1395 train_time:85290ms step_avg:129.82ms
step:668/1395 train_time:85427ms step_avg:129.83ms
step:669/1395 train_time:85562ms step_avg:129.84ms
step:670/1395 train_time:85697ms step_avg:129.84ms
step:671/1395 train_time:85833ms step_avg:129.85ms
step:672/1395 train_time:85969ms step_avg:129.86ms
step:673/1395 train_time:86103ms step_avg:129.87ms
step:674/1395 train_time:86238ms step_avg:129.88ms
step:675/1395 train_time:86374ms step_avg:129.89ms
step:676/1395 train_time:86511ms step_avg:129.90ms
step:677/1395 train_time:86646ms step_avg:129.90ms
step:678/1395 train_time:86780ms step_avg:129.91ms
step:679/1395 train_time:86916ms step_avg:129.92ms
step:680/1395 train_time:87052ms step_avg:129.93ms
step:681/1395 train_time:87188ms step_avg:129.94ms
step:682/1395 train_time:87323ms step_avg:129.94ms
step:683/1395 train_time:87458ms step_avg:129.95ms
step:684/1395 train_time:87593ms step_avg:129.96ms
step:685/1395 train_time:87729ms step_avg:129.97ms
step:686/1395 train_time:87863ms step_avg:129.98ms
step:687/1395 train_time:87998ms step_avg:129.98ms
step:688/1395 train_time:88134ms step_avg:129.99ms
step:689/1395 train_time:88270ms step_avg:130.00ms
step:690/1395 train_time:88408ms step_avg:130.01ms
step:691/1395 train_time:88542ms step_avg:130.02ms
step:692/1395 train_time:88677ms step_avg:130.03ms
step:693/1395 train_time:88813ms step_avg:130.03ms
step:694/1395 train_time:88948ms step_avg:130.04ms
step:695/1395 train_time:89081ms step_avg:130.05ms
step:696/1395 train_time:89216ms step_avg:130.05ms
step:697/1395 train_time:89351ms step_avg:130.06ms
step:698/1395 train_time:89487ms step_avg:130.07ms
step:699/1395 train_time:89623ms step_avg:130.08ms
step:700/1395 train_time:89758ms step_avg:130.08ms
step:701/1395 train_time:89894ms step_avg:130.09ms
step:702/1395 train_time:90029ms step_avg:130.10ms
step:703/1395 train_time:90164ms step_avg:130.11ms
step:704/1395 train_time:90298ms step_avg:130.11ms
step:705/1395 train_time:90435ms step_avg:130.12ms
step:706/1395 train_time:90573ms step_avg:130.13ms
step:707/1395 train_time:90708ms step_avg:130.14ms
step:708/1395 train_time:90844ms step_avg:130.15ms
step:709/1395 train_time:90981ms step_avg:130.16ms
step:710/1395 train_time:91116ms step_avg:130.17ms
step:711/1395 train_time:91253ms step_avg:130.17ms
step:712/1395 train_time:91389ms step_avg:130.18ms
step:713/1395 train_time:91525ms step_avg:130.19ms
step:714/1395 train_time:91660ms step_avg:130.20ms
step:715/1395 train_time:91795ms step_avg:130.21ms
step:716/1395 train_time:91931ms step_avg:130.21ms
step:717/1395 train_time:92066ms step_avg:130.22ms
step:718/1395 train_time:92201ms step_avg:130.23ms
step:719/1395 train_time:92336ms step_avg:130.23ms
step:720/1395 train_time:92472ms step_avg:130.24ms
step:721/1395 train_time:92607ms step_avg:130.25ms
step:722/1395 train_time:92744ms step_avg:130.26ms
step:723/1395 train_time:92878ms step_avg:130.26ms
step:724/1395 train_time:93014ms step_avg:130.27ms
step:725/1395 train_time:93150ms step_avg:130.28ms
step:726/1395 train_time:93287ms step_avg:130.29ms
step:727/1395 train_time:93426ms step_avg:130.30ms
step:728/1395 train_time:93562ms step_avg:130.31ms
step:729/1395 train_time:93698ms step_avg:130.32ms
step:730/1395 train_time:93835ms step_avg:130.33ms
step:731/1395 train_time:93971ms step_avg:130.33ms
step:732/1395 train_time:94107ms step_avg:130.34ms
step:733/1395 train_time:94245ms step_avg:130.35ms
step:734/1395 train_time:94382ms step_avg:130.36ms
step:735/1395 train_time:94520ms step_avg:130.37ms
step:736/1395 train_time:94655ms step_avg:130.38ms
step:737/1395 train_time:94792ms step_avg:130.39ms
step:738/1395 train_time:94928ms step_avg:130.40ms
step:739/1395 train_time:95064ms step_avg:130.40ms
step:740/1395 train_time:95201ms step_avg:130.41ms
step:741/1395 train_time:95341ms step_avg:130.43ms
step:742/1395 train_time:95478ms step_avg:130.44ms
step:743/1395 train_time:95614ms step_avg:130.44ms
step:744/1395 train_time:95750ms step_avg:130.45ms
step:745/1395 train_time:95890ms step_avg:130.46ms
step:746/1395 train_time:96026ms step_avg:130.47ms
step:747/1395 train_time:96162ms step_avg:130.48ms
step:748/1395 train_time:96298ms step_avg:130.49ms
step:749/1395 train_time:96436ms step_avg:130.50ms
step:750/1395 train_time:96573ms step_avg:130.50ms
step:750/1395 val_loss:3.5203 train_time:96686ms step_avg:130.66ms
step:751/1395 train_time:96718ms step_avg:130.52ms
step:752/1395 train_time:96859ms step_avg:130.54ms
step:753/1395 train_time:96995ms step_avg:130.54ms
step:754/1395 train_time:97131ms step_avg:130.55ms
step:755/1395 train_time:97267ms step_avg:130.56ms
step:756/1395 train_time:97403ms step_avg:130.57ms
step:757/1395 train_time:97541ms step_avg:130.58ms
step:758/1395 train_time:97678ms step_avg:130.59ms
step:759/1395 train_time:97815ms step_avg:130.59ms
step:760/1395 train_time:97950ms step_avg:130.60ms
step:761/1395 train_time:98087ms step_avg:130.61ms
step:762/1395 train_time:98224ms step_avg:130.62ms
step:763/1395 train_time:98359ms step_avg:130.62ms
step:764/1395 train_time:98498ms step_avg:130.63ms
step:765/1395 train_time:98634ms step_avg:130.64ms
step:766/1395 train_time:98772ms step_avg:130.65ms
step:767/1395 train_time:98908ms step_avg:130.66ms
step:768/1395 train_time:99044ms step_avg:130.67ms
step:769/1395 train_time:99182ms step_avg:130.67ms
step:770/1395 train_time:99318ms step_avg:130.68ms
step:771/1395 train_time:99455ms step_avg:130.69ms
step:772/1395 train_time:99592ms step_avg:130.70ms
step:773/1395 train_time:99729ms step_avg:130.71ms
step:774/1395 train_time:99865ms step_avg:130.71ms
step:775/1395 train_time:100002ms step_avg:130.72ms
step:776/1395 train_time:100140ms step_avg:130.73ms
step:777/1395 train_time:100277ms step_avg:130.74ms
step:778/1395 train_time:100414ms step_avg:130.75ms
step:779/1395 train_time:100548ms step_avg:130.75ms
step:780/1395 train_time:100687ms step_avg:130.76ms
step:781/1395 train_time:100824ms step_avg:130.77ms
step:782/1395 train_time:100960ms step_avg:130.78ms
step:783/1395 train_time:101095ms step_avg:130.78ms
step:784/1395 train_time:101232ms step_avg:130.79ms
step:785/1395 train_time:101369ms step_avg:130.80ms
step:786/1395 train_time:101506ms step_avg:130.81ms
step:787/1395 train_time:101642ms step_avg:130.81ms
step:788/1395 train_time:101778ms step_avg:130.82ms
step:789/1395 train_time:101914ms step_avg:130.83ms
step:790/1395 train_time:102051ms step_avg:130.83ms
step:791/1395 train_time:102187ms step_avg:130.84ms
step:792/1395 train_time:102325ms step_avg:130.85ms
step:793/1395 train_time:102460ms step_avg:130.86ms
step:794/1395 train_time:102597ms step_avg:130.86ms
step:795/1395 train_time:102738ms step_avg:130.88ms
step:796/1395 train_time:102876ms step_avg:130.89ms
step:797/1395 train_time:103013ms step_avg:130.89ms
step:798/1395 train_time:103149ms step_avg:130.90ms
step:799/1395 train_time:103289ms step_avg:130.91ms
step:800/1395 train_time:103426ms step_avg:130.92ms
step:801/1395 train_time:103562ms step_avg:130.92ms
step:802/1395 train_time:103699ms step_avg:130.93ms
step:803/1395 train_time:103835ms step_avg:130.94ms
step:804/1395 train_time:103970ms step_avg:130.94ms
step:805/1395 train_time:104108ms step_avg:130.95ms
step:806/1395 train_time:104245ms step_avg:130.96ms
step:807/1395 train_time:104380ms step_avg:130.97ms
step:808/1395 train_time:104517ms step_avg:130.97ms
step:809/1395 train_time:104654ms step_avg:130.98ms
step:810/1395 train_time:104790ms step_avg:130.99ms
step:811/1395 train_time:104927ms step_avg:130.99ms
step:812/1395 train_time:105063ms step_avg:131.00ms
step:813/1395 train_time:105198ms step_avg:131.01ms
step:814/1395 train_time:105336ms step_avg:131.01ms
step:815/1395 train_time:105472ms step_avg:131.02ms
step:816/1395 train_time:105610ms step_avg:131.03ms
step:817/1395 train_time:105746ms step_avg:131.04ms
step:818/1395 train_time:105882ms step_avg:131.04ms
step:819/1395 train_time:106019ms step_avg:131.05ms
step:820/1395 train_time:106156ms step_avg:131.06ms
step:821/1395 train_time:106292ms step_avg:131.06ms
step:822/1395 train_time:106428ms step_avg:131.07ms
step:823/1395 train_time:106565ms step_avg:131.08ms
step:824/1395 train_time:106701ms step_avg:131.08ms
step:825/1395 train_time:106839ms step_avg:131.09ms
step:826/1395 train_time:106978ms step_avg:131.10ms
step:827/1395 train_time:107114ms step_avg:131.11ms
step:828/1395 train_time:107252ms step_avg:131.11ms
step:829/1395 train_time:107390ms step_avg:131.12ms
step:830/1395 train_time:107528ms step_avg:131.13ms
step:831/1395 train_time:107665ms step_avg:131.14ms
step:832/1395 train_time:107804ms step_avg:131.15ms
step:833/1395 train_time:107940ms step_avg:131.15ms
step:834/1395 train_time:108079ms step_avg:131.16ms
step:835/1395 train_time:108217ms step_avg:131.17ms
step:836/1395 train_time:108357ms step_avg:131.18ms
step:837/1395 train_time:108496ms step_avg:131.19ms
step:838/1395 train_time:108634ms step_avg:131.20ms
step:839/1395 train_time:108770ms step_avg:131.21ms
step:840/1395 train_time:108907ms step_avg:131.21ms
step:841/1395 train_time:109045ms step_avg:131.22ms
step:842/1395 train_time:109183ms step_avg:131.23ms
step:843/1395 train_time:109319ms step_avg:131.24ms
step:844/1395 train_time:109457ms step_avg:131.24ms
step:845/1395 train_time:109594ms step_avg:131.25ms
step:846/1395 train_time:109734ms step_avg:131.26ms
step:847/1395 train_time:109871ms step_avg:131.27ms
step:848/1395 train_time:110008ms step_avg:131.27ms
step:849/1395 train_time:110146ms step_avg:131.28ms
step:850/1395 train_time:110286ms step_avg:131.29ms
step:851/1395 train_time:110425ms step_avg:131.30ms
step:852/1395 train_time:110562ms step_avg:131.31ms
step:853/1395 train_time:110698ms step_avg:131.31ms
step:854/1395 train_time:110835ms step_avg:131.32ms
step:855/1395 train_time:110972ms step_avg:131.33ms
step:856/1395 train_time:111108ms step_avg:131.33ms
step:857/1395 train_time:111247ms step_avg:131.34ms
step:858/1395 train_time:111387ms step_avg:131.35ms
step:859/1395 train_time:111525ms step_avg:131.36ms
step:860/1395 train_time:111662ms step_avg:131.37ms
step:861/1395 train_time:111800ms step_avg:131.37ms
step:862/1395 train_time:111938ms step_avg:131.38ms
step:863/1395 train_time:112078ms step_avg:131.39ms
step:864/1395 train_time:112216ms step_avg:131.40ms
step:865/1395 train_time:112352ms step_avg:131.41ms
step:866/1395 train_time:112497ms step_avg:131.42ms
step:867/1395 train_time:112635ms step_avg:131.43ms
step:868/1395 train_time:112771ms step_avg:131.44ms
step:869/1395 train_time:112909ms step_avg:131.44ms
step:870/1395 train_time:113048ms step_avg:131.45ms
step:871/1395 train_time:113185ms step_avg:131.46ms
step:872/1395 train_time:113322ms step_avg:131.46ms
step:873/1395 train_time:113458ms step_avg:131.47ms
step:874/1395 train_time:113597ms step_avg:131.48ms
step:875/1395 train_time:113736ms step_avg:131.49ms
step:875/1395 val_loss:3.4719 train_time:113848ms step_avg:131.62ms
step:876/1395 train_time:113879ms step_avg:131.50ms
step:877/1395 train_time:114021ms step_avg:131.51ms
step:878/1395 train_time:114159ms step_avg:131.52ms
step:879/1395 train_time:114297ms step_avg:131.53ms
step:880/1395 train_time:114435ms step_avg:131.53ms
step:881/1395 train_time:114572ms step_avg:131.54ms
step:882/1395 train_time:114711ms step_avg:131.55ms
step:883/1395 train_time:114847ms step_avg:131.55ms
step:884/1395 train_time:114985ms step_avg:131.56ms
step:885/1395 train_time:115122ms step_avg:131.57ms
step:886/1395 train_time:115262ms step_avg:131.58ms
step:887/1395 train_time:115399ms step_avg:131.58ms
step:888/1395 train_time:115538ms step_avg:131.59ms
step:889/1395 train_time:115679ms step_avg:131.60ms
step:890/1395 train_time:115815ms step_avg:131.61ms
step:891/1395 train_time:115952ms step_avg:131.61ms
step:892/1395 train_time:116090ms step_avg:131.62ms
step:893/1395 train_time:116226ms step_avg:131.63ms
step:894/1395 train_time:116364ms step_avg:131.63ms
step:895/1395 train_time:116504ms step_avg:131.64ms
step:896/1395 train_time:116640ms step_avg:131.65ms
step:897/1395 train_time:116778ms step_avg:131.66ms
step:898/1395 train_time:116917ms step_avg:131.66ms
step:899/1395 train_time:117057ms step_avg:131.67ms
step:900/1395 train_time:117194ms step_avg:131.68ms
step:901/1395 train_time:117332ms step_avg:131.69ms
step:902/1395 train_time:117467ms step_avg:131.69ms
step:903/1395 train_time:117607ms step_avg:131.70ms
step:904/1395 train_time:117745ms step_avg:131.71ms
step:905/1395 train_time:117882ms step_avg:131.71ms
step:906/1395 train_time:118019ms step_avg:131.72ms
step:907/1395 train_time:118160ms step_avg:131.73ms
step:908/1395 train_time:118296ms step_avg:131.73ms
step:909/1395 train_time:118434ms step_avg:131.74ms
step:910/1395 train_time:118575ms step_avg:131.75ms
step:911/1395 train_time:118713ms step_avg:131.76ms
step:912/1395 train_time:118849ms step_avg:131.76ms
step:913/1395 train_time:118988ms step_avg:131.77ms
step:914/1395 train_time:119126ms step_avg:131.78ms
step:915/1395 train_time:119265ms step_avg:131.78ms
step:916/1395 train_time:119403ms step_avg:131.79ms
step:917/1395 train_time:119540ms step_avg:131.80ms
step:918/1395 train_time:119678ms step_avg:131.80ms
step:919/1395 train_time:119821ms step_avg:131.82ms
step:920/1395 train_time:119958ms step_avg:131.82ms
step:921/1395 train_time:120096ms step_avg:131.83ms
step:922/1395 train_time:120237ms step_avg:131.84ms
step:923/1395 train_time:120374ms step_avg:131.84ms
step:924/1395 train_time:120512ms step_avg:131.85ms
step:925/1395 train_time:120650ms step_avg:131.86ms
step:926/1395 train_time:120788ms step_avg:131.86ms
step:927/1395 train_time:120926ms step_avg:131.87ms
step:928/1395 train_time:121062ms step_avg:131.88ms
step:929/1395 train_time:121202ms step_avg:131.88ms
step:930/1395 train_time:121339ms step_avg:131.89ms
step:931/1395 train_time:121475ms step_avg:131.89ms
step:932/1395 train_time:121614ms step_avg:131.90ms
step:933/1395 train_time:121754ms step_avg:131.91ms
step:934/1395 train_time:121893ms step_avg:131.92ms
step:935/1395 train_time:122034ms step_avg:131.93ms
step:936/1395 train_time:122172ms step_avg:131.94ms
step:937/1395 train_time:122317ms step_avg:131.95ms
step:938/1395 train_time:122458ms step_avg:131.96ms
step:939/1395 train_time:122599ms step_avg:131.97ms
step:940/1395 train_time:122739ms step_avg:131.98ms
step:941/1395 train_time:122877ms step_avg:131.98ms
step:942/1395 train_time:123014ms step_avg:131.99ms
step:943/1395 train_time:123156ms step_avg:132.00ms
step:944/1395 train_time:123299ms step_avg:132.01ms
step:945/1395 train_time:123440ms step_avg:132.02ms
step:946/1395 train_time:123581ms step_avg:132.03ms
step:947/1395 train_time:123722ms step_avg:132.04ms
step:948/1395 train_time:123860ms step_avg:132.05ms
step:949/1395 train_time:124001ms step_avg:132.06ms
step:950/1395 train_time:124138ms step_avg:132.06ms
step:951/1395 train_time:124280ms step_avg:132.07ms
step:952/1395 train_time:124418ms step_avg:132.08ms
step:953/1395 train_time:124557ms step_avg:132.09ms
step:954/1395 train_time:124696ms step_avg:132.09ms
step:955/1395 train_time:124834ms step_avg:132.10ms
step:956/1395 train_time:124977ms step_avg:132.11ms
step:957/1395 train_time:125116ms step_avg:132.12ms
step:958/1395 train_time:125257ms step_avg:132.13ms
step:959/1395 train_time:125400ms step_avg:132.14ms
step:960/1395 train_time:125539ms step_avg:132.15ms
step:961/1395 train_time:125677ms step_avg:132.15ms
step:962/1395 train_time:125817ms step_avg:132.16ms
step:963/1395 train_time:125962ms step_avg:132.17ms
step:964/1395 train_time:126101ms step_avg:132.18ms
step:965/1395 train_time:126240ms step_avg:132.19ms
step:966/1395 train_time:126380ms step_avg:132.20ms
step:967/1395 train_time:126519ms step_avg:132.20ms
step:968/1395 train_time:126656ms step_avg:132.21ms
step:969/1395 train_time:126797ms step_avg:132.22ms
step:970/1395 train_time:126934ms step_avg:132.22ms
step:971/1395 train_time:127074ms step_avg:132.23ms
step:972/1395 train_time:127213ms step_avg:132.24ms
step:973/1395 train_time:127352ms step_avg:132.24ms
step:974/1395 train_time:127490ms step_avg:132.25ms
step:975/1395 train_time:127629ms step_avg:132.26ms
step:976/1395 train_time:127769ms step_avg:132.27ms
step:977/1395 train_time:127907ms step_avg:132.27ms
step:978/1395 train_time:128045ms step_avg:132.28ms
step:979/1395 train_time:128184ms step_avg:132.28ms
step:980/1395 train_time:128323ms step_avg:132.29ms
step:981/1395 train_time:128460ms step_avg:132.30ms
step:982/1395 train_time:128598ms step_avg:132.30ms
step:983/1395 train_time:128737ms step_avg:132.31ms
step:984/1395 train_time:128875ms step_avg:132.31ms
step:985/1395 train_time:129015ms step_avg:132.32ms
step:986/1395 train_time:129161ms step_avg:132.34ms
step:987/1395 train_time:129297ms step_avg:132.34ms
step:988/1395 train_time:129437ms step_avg:132.35ms
step:989/1395 train_time:129576ms step_avg:132.36ms
step:990/1395 train_time:129717ms step_avg:132.36ms
step:991/1395 train_time:129855ms step_avg:132.37ms
step:992/1395 train_time:129998ms step_avg:132.38ms
step:993/1395 train_time:130146ms step_avg:132.40ms
step:994/1395 train_time:130283ms step_avg:132.40ms
step:995/1395 train_time:130422ms step_avg:132.41ms
step:996/1395 train_time:130558ms step_avg:132.41ms
step:997/1395 train_time:130697ms step_avg:132.42ms
step:998/1395 train_time:130834ms step_avg:132.42ms
step:999/1395 train_time:130973ms step_avg:132.43ms
step:1000/1395 train_time:131112ms step_avg:132.44ms
step:1000/1395 val_loss:3.4087 train_time:131223ms step_avg:132.55ms
step:1001/1395 train_time:131255ms step_avg:132.45ms
step:1002/1395 train_time:131397ms step_avg:132.46ms
step:1003/1395 train_time:131539ms step_avg:132.47ms
step:1004/1395 train_time:131679ms step_avg:132.47ms
step:1005/1395 train_time:131819ms step_avg:132.48ms
step:1006/1395 train_time:131956ms step_avg:132.49ms
step:1007/1395 train_time:132095ms step_avg:132.49ms
step:1008/1395 train_time:132235ms step_avg:132.50ms
step:1009/1395 train_time:132380ms step_avg:132.51ms
step:1010/1395 train_time:132519ms step_avg:132.52ms
step:1011/1395 train_time:132658ms step_avg:132.53ms
step:1012/1395 train_time:132797ms step_avg:132.53ms
step:1013/1395 train_time:132938ms step_avg:132.54ms
step:1014/1395 train_time:133076ms step_avg:132.55ms
step:1015/1395 train_time:133214ms step_avg:132.55ms
step:1016/1395 train_time:133353ms step_avg:132.56ms
step:1017/1395 train_time:133493ms step_avg:132.57ms
step:1018/1395 train_time:133631ms step_avg:132.57ms
step:1019/1395 train_time:133771ms step_avg:132.58ms
step:1020/1395 train_time:133913ms step_avg:132.59ms
step:1021/1395 train_time:134050ms step_avg:132.59ms
step:1022/1395 train_time:134188ms step_avg:132.60ms
step:1023/1395 train_time:134328ms step_avg:132.60ms
step:1024/1395 train_time:134469ms step_avg:132.61ms
step:1025/1395 train_time:134609ms step_avg:132.62ms
step:1026/1395 train_time:134748ms step_avg:132.63ms
step:1027/1395 train_time:134887ms step_avg:132.63ms
step:1028/1395 train_time:135027ms step_avg:132.64ms
step:1029/1395 train_time:135169ms step_avg:132.65ms
step:1030/1395 train_time:135309ms step_avg:132.66ms
step:1031/1395 train_time:135445ms step_avg:132.66ms
step:1032/1395 train_time:135582ms step_avg:132.66ms
step:1033/1395 train_time:135719ms step_avg:132.67ms
step:1034/1395 train_time:135858ms step_avg:132.67ms
step:1035/1395 train_time:135998ms step_avg:132.68ms
step:1036/1395 train_time:136138ms step_avg:132.69ms
step:1037/1395 train_time:136283ms step_avg:132.70ms
step:1038/1395 train_time:136424ms step_avg:132.71ms
step:1039/1395 train_time:136562ms step_avg:132.71ms
step:1040/1395 train_time:136701ms step_avg:132.72ms
step:1041/1395 train_time:136842ms step_avg:132.73ms
step:1042/1395 train_time:136981ms step_avg:132.73ms
step:1043/1395 train_time:137121ms step_avg:132.74ms
step:1044/1395 train_time:137264ms step_avg:132.75ms
step:1045/1395 train_time:137405ms step_avg:132.76ms
step:1046/1395 train_time:137545ms step_avg:132.77ms
step:1047/1395 train_time:137683ms step_avg:132.77ms
step:1048/1395 train_time:137823ms step_avg:132.78ms
step:1049/1395 train_time:137964ms step_avg:132.79ms
step:1050/1395 train_time:138105ms step_avg:132.79ms
step:1051/1395 train_time:138248ms step_avg:132.80ms
step:1052/1395 train_time:138386ms step_avg:132.81ms
step:1053/1395 train_time:138525ms step_avg:132.81ms
step:1054/1395 train_time:138666ms step_avg:132.82ms
step:1055/1395 train_time:138806ms step_avg:132.83ms
step:1056/1395 train_time:138946ms step_avg:132.84ms
step:1057/1395 train_time:139086ms step_avg:132.84ms
step:1058/1395 train_time:139228ms step_avg:132.85ms
step:1059/1395 train_time:139370ms step_avg:132.86ms
step:1060/1395 train_time:139511ms step_avg:132.87ms
step:1061/1395 train_time:139648ms step_avg:132.87ms
step:1062/1395 train_time:139789ms step_avg:132.88ms
step:1063/1395 train_time:139928ms step_avg:132.89ms
step:1064/1395 train_time:140068ms step_avg:132.89ms
step:1065/1395 train_time:140210ms step_avg:132.90ms
step:1066/1395 train_time:140353ms step_avg:132.91ms
step:1067/1395 train_time:140494ms step_avg:132.92ms
step:1068/1395 train_time:140631ms step_avg:132.92ms
step:1069/1395 train_time:140775ms step_avg:132.93ms
step:1070/1395 train_time:140913ms step_avg:132.94ms
step:1071/1395 train_time:141059ms step_avg:132.95ms
step:1072/1395 train_time:141198ms step_avg:132.95ms
step:1073/1395 train_time:141336ms step_avg:132.96ms
step:1074/1395 train_time:141475ms step_avg:132.96ms
step:1075/1395 train_time:141617ms step_avg:132.97ms
step:1076/1395 train_time:141756ms step_avg:132.98ms
step:1077/1395 train_time:141894ms step_avg:132.98ms
step:1078/1395 train_time:142037ms step_avg:132.99ms
step:1079/1395 train_time:142182ms step_avg:133.01ms
step:1080/1395 train_time:142324ms step_avg:133.01ms
step:1081/1395 train_time:142463ms step_avg:133.02ms
step:1082/1395 train_time:142602ms step_avg:133.02ms
step:1083/1395 train_time:142741ms step_avg:133.03ms
step:1084/1395 train_time:142886ms step_avg:133.04ms
step:1085/1395 train_time:143027ms step_avg:133.05ms
step:1086/1395 train_time:143167ms step_avg:133.06ms
step:1087/1395 train_time:143308ms step_avg:133.06ms
step:1088/1395 train_time:143447ms step_avg:133.07ms
step:1089/1395 train_time:143591ms step_avg:133.08ms
step:1090/1395 train_time:143736ms step_avg:133.09ms
step:1091/1395 train_time:143875ms step_avg:133.09ms
step:1092/1395 train_time:144013ms step_avg:133.10ms
step:1093/1395 train_time:144153ms step_avg:133.11ms
step:1094/1395 train_time:144291ms step_avg:133.11ms
step:1095/1395 train_time:144430ms step_avg:133.11ms
step:1096/1395 train_time:144572ms step_avg:133.12ms
step:1097/1395 train_time:144715ms step_avg:133.13ms
step:1098/1395 train_time:144857ms step_avg:133.14ms
step:1099/1395 train_time:144996ms step_avg:133.15ms
step:1100/1395 train_time:145135ms step_avg:133.15ms
step:1101/1395 train_time:145275ms step_avg:133.16ms
step:1102/1395 train_time:145418ms step_avg:133.17ms
step:1103/1395 train_time:145559ms step_avg:133.17ms
step:1104/1395 train_time:145698ms step_avg:133.18ms
step:1105/1395 train_time:145841ms step_avg:133.19ms
step:1106/1395 train_time:145982ms step_avg:133.20ms
step:1107/1395 train_time:146122ms step_avg:133.20ms
step:1108/1395 train_time:146267ms step_avg:133.21ms
step:1109/1395 train_time:146408ms step_avg:133.22ms
step:1110/1395 train_time:146549ms step_avg:133.23ms
step:1111/1395 train_time:146689ms step_avg:133.23ms
step:1112/1395 train_time:146827ms step_avg:133.24ms
step:1113/1395 train_time:146965ms step_avg:133.24ms
step:1114/1395 train_time:147107ms step_avg:133.25ms
step:1115/1395 train_time:147249ms step_avg:133.26ms
step:1116/1395 train_time:147387ms step_avg:133.26ms
step:1117/1395 train_time:147529ms step_avg:133.27ms
step:1118/1395 train_time:147675ms step_avg:133.28ms
step:1119/1395 train_time:147814ms step_avg:133.29ms
step:1120/1395 train_time:147954ms step_avg:133.29ms
step:1121/1395 train_time:148093ms step_avg:133.30ms
step:1122/1395 train_time:148232ms step_avg:133.30ms
step:1123/1395 train_time:148371ms step_avg:133.31ms
step:1124/1395 train_time:148513ms step_avg:133.32ms
step:1125/1395 train_time:148653ms step_avg:133.32ms
step:1125/1395 val_loss:3.3594 train_time:148767ms step_avg:133.42ms
step:1126/1395 train_time:148796ms step_avg:133.33ms
step:1127/1395 train_time:148938ms step_avg:133.34ms
step:1128/1395 train_time:149079ms step_avg:133.34ms
step:1129/1395 train_time:149221ms step_avg:133.35ms
step:1130/1395 train_time:149359ms step_avg:133.36ms
step:1131/1395 train_time:149502ms step_avg:133.36ms
step:1132/1395 train_time:149642ms step_avg:133.37ms
step:1133/1395 train_time:149782ms step_avg:133.38ms
step:1134/1395 train_time:149924ms step_avg:133.38ms
step:1135/1395 train_time:150063ms step_avg:133.39ms
step:1136/1395 train_time:150210ms step_avg:133.40ms
step:1137/1395 train_time:150349ms step_avg:133.41ms
step:1138/1395 train_time:150491ms step_avg:133.41ms
step:1139/1395 train_time:150632ms step_avg:133.42ms
step:1140/1395 train_time:150772ms step_avg:133.43ms
step:1141/1395 train_time:150913ms step_avg:133.43ms
step:1142/1395 train_time:151054ms step_avg:133.44ms
step:1143/1395 train_time:151200ms step_avg:133.45ms
step:1144/1395 train_time:151341ms step_avg:133.46ms
step:1145/1395 train_time:151482ms step_avg:133.46ms
step:1146/1395 train_time:151623ms step_avg:133.47ms
step:1147/1395 train_time:151766ms step_avg:133.48ms
step:1148/1395 train_time:151907ms step_avg:133.49ms
step:1149/1395 train_time:152048ms step_avg:133.49ms
step:1150/1395 train_time:152189ms step_avg:133.50ms
step:1151/1395 train_time:152333ms step_avg:133.51ms
step:1152/1395 train_time:152474ms step_avg:133.51ms
step:1153/1395 train_time:152618ms step_avg:133.52ms
step:1154/1395 train_time:152758ms step_avg:133.53ms
step:1155/1395 train_time:152900ms step_avg:133.54ms
step:1156/1395 train_time:153047ms step_avg:133.55ms
step:1157/1395 train_time:153190ms step_avg:133.56ms
step:1158/1395 train_time:153330ms step_avg:133.56ms
step:1159/1395 train_time:153470ms step_avg:133.57ms
step:1160/1395 train_time:153609ms step_avg:133.57ms
step:1161/1395 train_time:153750ms step_avg:133.58ms
step:1162/1395 train_time:153891ms step_avg:133.59ms
step:1163/1395 train_time:154033ms step_avg:133.59ms
step:1164/1395 train_time:154175ms step_avg:133.60ms
step:1165/1395 train_time:154313ms step_avg:133.60ms
step:1166/1395 train_time:154453ms step_avg:133.61ms
step:1167/1395 train_time:154592ms step_avg:133.61ms
step:1168/1395 train_time:154733ms step_avg:133.62ms
step:1169/1395 train_time:154874ms step_avg:133.63ms
step:1170/1395 train_time:155014ms step_avg:133.63ms
step:1171/1395 train_time:155155ms step_avg:133.64ms
step:1172/1395 train_time:155296ms step_avg:133.65ms
step:1173/1395 train_time:155437ms step_avg:133.65ms
step:1174/1395 train_time:155590ms step_avg:133.67ms
step:1175/1395 train_time:155732ms step_avg:133.68ms
step:1176/1395 train_time:155876ms step_avg:133.68ms
step:1177/1395 train_time:156024ms step_avg:133.70ms
step:1178/1395 train_time:156164ms step_avg:133.70ms
step:1179/1395 train_time:156303ms step_avg:133.71ms
step:1180/1395 train_time:156453ms step_avg:133.72ms
step:1181/1395 train_time:156597ms step_avg:133.73ms
step:1182/1395 train_time:156736ms step_avg:133.73ms
step:1183/1395 train_time:156879ms step_avg:133.74ms
step:1184/1395 train_time:157020ms step_avg:133.75ms
step:1185/1395 train_time:157164ms step_avg:133.76ms
step:1186/1395 train_time:157304ms step_avg:133.76ms
step:1187/1395 train_time:157455ms step_avg:133.78ms
step:1188/1395 train_time:157594ms step_avg:133.78ms
step:1189/1395 train_time:157738ms step_avg:133.79ms
step:1190/1395 train_time:157879ms step_avg:133.80ms
step:1191/1395 train_time:158022ms step_avg:133.80ms
step:1192/1395 train_time:158162ms step_avg:133.81ms
step:1193/1395 train_time:158302ms step_avg:133.81ms
step:1194/1395 train_time:158443ms step_avg:133.82ms
step:1195/1395 train_time:158584ms step_avg:133.83ms
step:1196/1395 train_time:158726ms step_avg:133.83ms
step:1197/1395 train_time:158869ms step_avg:133.84ms
step:1198/1395 train_time:159017ms step_avg:133.85ms
step:1199/1395 train_time:159158ms step_avg:133.86ms
step:1200/1395 train_time:159297ms step_avg:133.86ms
step:1201/1395 train_time:159436ms step_avg:133.87ms
step:1202/1395 train_time:159590ms step_avg:133.88ms
step:1203/1395 train_time:159738ms step_avg:133.90ms
step:1204/1395 train_time:159881ms step_avg:133.90ms
step:1205/1395 train_time:160025ms step_avg:133.91ms
step:1206/1395 train_time:160167ms step_avg:133.92ms
step:1207/1395 train_time:160308ms step_avg:133.92ms
step:1208/1395 train_time:160453ms step_avg:133.93ms
step:1209/1395 train_time:160594ms step_avg:133.94ms
step:1210/1395 train_time:160738ms step_avg:133.95ms
step:1211/1395 train_time:160881ms step_avg:133.96ms
step:1212/1395 train_time:161022ms step_avg:133.96ms
step:1213/1395 train_time:161162ms step_avg:133.97ms
step:1214/1395 train_time:161305ms step_avg:133.97ms
step:1215/1395 train_time:161450ms step_avg:133.98ms
step:1216/1395 train_time:161587ms step_avg:133.99ms
step:1217/1395 train_time:161731ms step_avg:133.99ms
step:1218/1395 train_time:161869ms step_avg:134.00ms
step:1219/1395 train_time:162007ms step_avg:134.00ms
step:1220/1395 train_time:162149ms step_avg:134.01ms
step:1221/1395 train_time:162288ms step_avg:134.01ms
step:1222/1395 train_time:162428ms step_avg:134.02ms
step:1223/1395 train_time:162569ms step_avg:134.02ms
step:1224/1395 train_time:162712ms step_avg:134.03ms
step:1225/1395 train_time:162856ms step_avg:134.04ms
step:1226/1395 train_time:162996ms step_avg:134.04ms
step:1227/1395 train_time:163138ms step_avg:134.05ms
step:1228/1395 train_time:163278ms step_avg:134.05ms
step:1229/1395 train_time:163418ms step_avg:134.06ms
step:1230/1395 train_time:163564ms step_avg:134.07ms
step:1231/1395 train_time:163707ms step_avg:134.08ms
step:1232/1395 train_time:163853ms step_avg:134.09ms
step:1233/1395 train_time:163994ms step_avg:134.09ms
step:1234/1395 train_time:164134ms step_avg:134.10ms
step:1235/1395 train_time:164274ms step_avg:134.10ms
step:1236/1395 train_time:164416ms step_avg:134.11ms
step:1237/1395 train_time:164555ms step_avg:134.11ms
step:1238/1395 train_time:164705ms step_avg:134.12ms
step:1239/1395 train_time:164844ms step_avg:134.13ms
step:1240/1395 train_time:164987ms step_avg:134.14ms
step:1241/1395 train_time:165132ms step_avg:134.14ms
step:1242/1395 train_time:165272ms step_avg:134.15ms
step:1243/1395 train_time:165415ms step_avg:134.16ms
step:1244/1395 train_time:165556ms step_avg:134.16ms
step:1245/1395 train_time:165697ms step_avg:134.17ms
step:1246/1395 train_time:165837ms step_avg:134.17ms
step:1247/1395 train_time:165979ms step_avg:134.18ms
step:1248/1395 train_time:166119ms step_avg:134.18ms
step:1249/1395 train_time:166259ms step_avg:134.19ms
step:1250/1395 train_time:166399ms step_avg:134.19ms
step:1250/1395 val_loss:3.3134 train_time:166514ms step_avg:134.29ms
step:1251/1395 train_time:166546ms step_avg:134.20ms
step:1252/1395 train_time:166694ms step_avg:134.21ms
step:1253/1395 train_time:166833ms step_avg:134.22ms
step:1254/1395 train_time:166971ms step_avg:134.22ms
step:1255/1395 train_time:167125ms step_avg:134.24ms
step:1256/1395 train_time:167266ms step_avg:134.24ms
step:1257/1395 train_time:167406ms step_avg:134.25ms
step:1258/1395 train_time:167549ms step_avg:134.25ms
step:1259/1395 train_time:167694ms step_avg:134.26ms
step:1260/1395 train_time:167833ms step_avg:134.27ms
step:1261/1395 train_time:167974ms step_avg:134.27ms
step:1262/1395 train_time:168119ms step_avg:134.28ms
step:1263/1395 train_time:168263ms step_avg:134.29ms
step:1264/1395 train_time:168403ms step_avg:134.29ms
step:1265/1395 train_time:168543ms step_avg:134.30ms
step:1266/1395 train_time:168685ms step_avg:134.30ms
step:1267/1395 train_time:168827ms step_avg:134.31ms
step:1268/1395 train_time:168968ms step_avg:134.32ms
step:1269/1395 train_time:169115ms step_avg:134.32ms
step:1270/1395 train_time:169256ms step_avg:134.33ms
step:1271/1395 train_time:169399ms step_avg:134.34ms
step:1272/1395 train_time:169539ms step_avg:134.34ms
step:1273/1395 train_time:169679ms step_avg:134.35ms
step:1274/1395 train_time:169820ms step_avg:134.35ms
step:1275/1395 train_time:169963ms step_avg:134.36ms
step:1276/1395 train_time:170102ms step_avg:134.36ms
step:1277/1395 train_time:170243ms step_avg:134.37ms
step:1278/1395 train_time:170384ms step_avg:134.37ms
step:1279/1395 train_time:170525ms step_avg:134.38ms
step:1280/1395 train_time:170673ms step_avg:134.39ms
step:1281/1395 train_time:170815ms step_avg:134.39ms
step:1282/1395 train_time:170954ms step_avg:134.40ms
step:1283/1395 train_time:171096ms step_avg:134.40ms
step:1284/1395 train_time:171240ms step_avg:134.41ms
step:1285/1395 train_time:171380ms step_avg:134.42ms
step:1286/1395 train_time:171523ms step_avg:134.42ms
step:1287/1395 train_time:171666ms step_avg:134.43ms
step:1288/1395 train_time:171807ms step_avg:134.43ms
step:1289/1395 train_time:171955ms step_avg:134.44ms
step:1290/1395 train_time:172102ms step_avg:134.45ms
step:1291/1395 train_time:172249ms step_avg:134.46ms
step:1292/1395 train_time:172392ms step_avg:134.47ms
step:1293/1395 train_time:172540ms step_avg:134.48ms
step:1294/1395 train_time:172681ms step_avg:134.49ms
step:1295/1395 train_time:172822ms step_avg:134.49ms
step:1296/1395 train_time:172967ms step_avg:134.50ms
step:1297/1395 train_time:173111ms step_avg:134.51ms
step:1298/1395 train_time:173252ms step_avg:134.51ms
step:1299/1395 train_time:173392ms step_avg:134.52ms
step:1300/1395 train_time:173531ms step_avg:134.52ms
step:1301/1395 train_time:173672ms step_avg:134.53ms
step:1302/1395 train_time:173814ms step_avg:134.53ms
step:1303/1395 train_time:173960ms step_avg:134.54ms
step:1304/1395 train_time:174105ms step_avg:134.55ms
step:1305/1395 train_time:174247ms step_avg:134.55ms
step:1306/1395 train_time:174391ms step_avg:134.56ms
step:1307/1395 train_time:174531ms step_avg:134.57ms
step:1308/1395 train_time:174676ms step_avg:134.57ms
step:1309/1395 train_time:174821ms step_avg:134.58ms
step:1310/1395 train_time:174963ms step_avg:134.59ms
step:1311/1395 train_time:175102ms step_avg:134.59ms
step:1312/1395 train_time:175242ms step_avg:134.59ms
step:1313/1395 train_time:175382ms step_avg:134.60ms
step:1314/1395 train_time:175523ms step_avg:134.60ms
step:1315/1395 train_time:175668ms step_avg:134.61ms
step:1316/1395 train_time:175809ms step_avg:134.62ms
step:1317/1395 train_time:175949ms step_avg:134.62ms
step:1318/1395 train_time:176097ms step_avg:134.63ms
step:1319/1395 train_time:176240ms step_avg:134.64ms
step:1320/1395 train_time:176381ms step_avg:134.64ms
step:1321/1395 train_time:176523ms step_avg:134.65ms
step:1322/1395 train_time:176671ms step_avg:134.66ms
step:1323/1395 train_time:176813ms step_avg:134.66ms
step:1324/1395 train_time:176954ms step_avg:134.67ms
step:1325/1395 train_time:177098ms step_avg:134.68ms
step:1326/1395 train_time:177244ms step_avg:134.68ms
step:1327/1395 train_time:177385ms step_avg:134.69ms
step:1328/1395 train_time:177524ms step_avg:134.69ms
step:1329/1395 train_time:177681ms step_avg:134.71ms
step:1330/1395 train_time:177828ms step_avg:134.72ms
step:1331/1395 train_time:177975ms step_avg:134.73ms
step:1332/1395 train_time:178124ms step_avg:134.74ms
step:1333/1395 train_time:178268ms step_avg:134.75ms
step:1334/1395 train_time:178409ms step_avg:134.75ms
step:1335/1395 train_time:178547ms step_avg:134.75ms
step:1336/1395 train_time:178697ms step_avg:134.76ms
step:1337/1395 train_time:178842ms step_avg:134.77ms
step:1338/1395 train_time:178983ms step_avg:134.78ms
step:1339/1395 train_time:179127ms step_avg:134.78ms
step:1340/1395 train_time:179273ms step_avg:134.79ms
step:1341/1395 train_time:179415ms step_avg:134.80ms
step:1342/1395 train_time:179558ms step_avg:134.80ms
step:1343/1395 train_time:179698ms step_avg:134.81ms
step:1344/1395 train_time:179837ms step_avg:134.81ms
step:1345/1395 train_time:179980ms step_avg:134.82ms
step:1346/1395 train_time:180122ms step_avg:134.82ms
step:1347/1395 train_time:180266ms step_avg:134.83ms
step:1348/1395 train_time:180406ms step_avg:134.83ms
step:1349/1395 train_time:180550ms step_avg:134.84ms
step:1350/1395 train_time:180689ms step_avg:134.84ms
step:1351/1395 train_time:180830ms step_avg:134.85ms
step:1352/1395 train_time:180981ms step_avg:134.86ms
step:1353/1395 train_time:181128ms step_avg:134.87ms
step:1354/1395 train_time:181271ms step_avg:134.87ms
step:1355/1395 train_time:181413ms step_avg:134.88ms
step:1356/1395 train_time:181553ms step_avg:134.88ms
step:1357/1395 train_time:181698ms step_avg:134.89ms
step:1358/1395 train_time:181843ms step_avg:134.90ms
step:1359/1395 train_time:181986ms step_avg:134.90ms
step:1360/1395 train_time:182131ms step_avg:134.91ms
step:1361/1395 train_time:182276ms step_avg:134.92ms
step:1362/1395 train_time:182422ms step_avg:134.93ms
step:1363/1395 train_time:182571ms step_avg:134.94ms
step:1364/1395 train_time:182715ms step_avg:134.94ms
step:1365/1395 train_time:182854ms step_avg:134.95ms
step:1366/1395 train_time:182997ms step_avg:134.95ms
step:1367/1395 train_time:183139ms step_avg:134.96ms
step:1368/1395 train_time:183283ms step_avg:134.97ms
step:1369/1395 train_time:183432ms step_avg:134.98ms
step:1370/1395 train_time:183579ms step_avg:134.98ms
step:1371/1395 train_time:183723ms step_avg:134.99ms
step:1372/1395 train_time:183871ms step_avg:135.00ms
step:1373/1395 train_time:184013ms step_avg:135.01ms
step:1374/1395 train_time:184162ms step_avg:135.02ms
step:1375/1395 train_time:184303ms step_avg:135.02ms
step:1375/1395 val_loss:3.2796 train_time:184415ms step_avg:135.10ms
step:1376/1395 train_time:184447ms step_avg:135.03ms
step:1377/1395 train_time:184590ms step_avg:135.03ms
step:1378/1395 train_time:184733ms step_avg:135.04ms
step:1379/1395 train_time:184877ms step_avg:135.05ms
step:1380/1395 train_time:185021ms step_avg:135.05ms
step:1381/1395 train_time:185167ms step_avg:135.06ms
step:1382/1395 train_time:185310ms step_avg:135.07ms
step:1383/1395 train_time:185453ms step_avg:135.07ms
step:1384/1395 train_time:185601ms step_avg:135.08ms
step:1385/1395 train_time:185740ms step_avg:135.08ms
step:1386/1395 train_time:185882ms step_avg:135.09ms
step:1387/1395 train_time:186025ms step_avg:135.09ms
step:1388/1395 train_time:186165ms step_avg:135.10ms
step:1389/1395 train_time:186308ms step_avg:135.10ms
step:1390/1395 train_time:186450ms step_avg:135.11ms
step:1391/1395 train_time:186592ms step_avg:135.11ms
step:1392/1395 train_time:186737ms step_avg:135.12ms
step:1393/1395 train_time:186877ms step_avg:135.12ms
step:1394/1395 train_time:187020ms step_avg:135.13ms
step:1395/1395 train_time:187160ms step_avg:135.13ms
step:1395/1395 val_loss:3.2753 train_time:187277ms step_avg:135.22ms
peak memory allocated: 37620 MiB reserved: 39114 MiB
