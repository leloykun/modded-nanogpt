import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
torch._inductor.config.coordinate_descent_tuning = True

# -----------------------------------------------------------------------------
# Custom operators

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.mul(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.mul(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.t(),
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(1 / x_s, dtype=torch.float32),
            scale_b=x.new_tensor(1 / w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.t(), x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(1 / x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(1 / w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(1 / grad_s, dtype=torch.float32)
        grad_f8 = grad.mul(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.t().contiguous().t(),
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.t().contiguous(),
            grad_f8.t().contiguous().t(),
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).t()
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

def lm_head_fp8(x: Tensor, w: Tensor) -> Tensor:
    _x = x.flatten(0, -2)
    out: Tensor = torch.ops.nanogpt.mm(_x, w, x_s=2.0, w_s=32.0, grad_s=2.0**29)[0]
    return out.reshape(*x.shape[:-1], -1)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven"t tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        assert all(isinstance(p, Tensor) for p in params)
        sizes = {p.numel() for p in params}
        def create_update_buffer(size: int):
            b = torch.empty(self.world_size, size, dtype=torch.bfloat16, device="cuda")
            return dict(update_buffer=b, update_buffer_views=[b[i] for i in range(self.world_size)])
        param_groups = [
            dict(params=[p for p in params if p.numel() == size], **create_update_buffer(size)) for size in sizes]
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            lr = group["lr"]
            momentum = group["momentum"]
            nesterov = group["nesterov"]
            ns_steps = group["ns_steps"]
            update_buffer = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffer_views[self.rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int):
        super().__init__(in_features, out_features, bias=False)

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        self.attn_scale = 0.15

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, model_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(model_dim, num_heads) if layer_idx != 7 else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, num_embeddings: int, embedding_dim: int):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(num_embeddings, embedding_dim) for _ in range(3)])

    def forward(self, input_seq) -> list[Tensor | None]:
        ve = [emb(input_seq) for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_idx) for layer_idx in range(num_layers)])
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128))
        self.lm_head.weight.detach().zero_() # @Grad62304977

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        assert input_seq.ndim == 1
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        docs = (input_seq == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask: Tensor):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        def create_doc_swc_block_mask(sliding_window_num_blocks: Tensor):
            kv_idx = block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977
        ve = self.value_embeds(input_seq)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]
        assert len(ve_enc) == self.num_encoder_layers and len(ve_dec) == self.num_decoder_layers

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)
        x = norm(x)
        logits = lm_head_fp8(x, self.lm_head.weight) if self.training else self.lm_head(x)
        # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(f"{file}", False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use cycle(files) if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn"t helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1395 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    seq_len = 64*1024 # FlexAttention sequence length
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
             if console:
                 print(s)
             print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

# load data
train_loader = distributed_data_generator(args.train_files, args.batch_size, rank, world_size)

model = GPT(vocab_size=50257, num_layers=12, num_heads=6, model_dim=768).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.008), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it: int):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t >= 0
    w = min(t / args.cooldown_frac, 1.0) # 1 -> 0
    return w * 1.0 + (1 - w) * 0.1
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]
@lru_cache(1)
def sw_num_blks(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)

model: nn.Module = torch.compile(model)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float("nan") if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    # Linearly increase the block-wise sliding window size over training 128 -> 1792:
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * step / train_steps, n=128)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_bs = world_size * args.seq_len
        assert args.val_tokens % val_bs == 0
        val_steps = args.val_tokens // val_bs
        val_loader = distributed_data_generator(args.val_files, val_bs, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                x, y = next(val_loader)
                val_loss += model(x, y, sw_num_blks(window_size))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION BEGIN -----------------
    inputs, targets = next(train_loader)
    for input_seq, target_seq in zip(inputs.split(args.seq_len), targets.split(args.seq_len)):
        model(input_seq, target_seq, sw_num_blks(window_size)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # momentum warmup for Muon
    frac = min(step / 300, 1)
    for group in optimizer2.param_groups:
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms", console=True)

print0(
    f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
    f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB"
)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Thu Jan 16 16:18:23 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   39C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   38C    P0             117W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
step:0/1395 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1395 train_time:26564ms step_avg:nanms
step:2/1395 train_time:27003ms step_avg:nanms
step:3/1395 train_time:27124ms step_avg:nanms
step:4/1395 train_time:27246ms step_avg:nanms
step:5/1395 train_time:27369ms step_avg:nanms
step:6/1395 train_time:27492ms step_avg:nanms
step:7/1395 train_time:27615ms step_avg:nanms
step:8/1395 train_time:27738ms step_avg:nanms
step:9/1395 train_time:27861ms step_avg:nanms
step:10/1395 train_time:27985ms step_avg:nanms
step:11/1395 train_time:126ms step_avg:nanms
step:12/1395 train_time:248ms step_avg:nanms
step:13/1395 train_time:372ms step_avg:123.85ms
step:14/1395 train_time:495ms step_avg:123.76ms
step:15/1395 train_time:618ms step_avg:123.55ms
step:16/1395 train_time:741ms step_avg:123.45ms
step:17/1395 train_time:865ms step_avg:123.55ms
step:18/1395 train_time:988ms step_avg:123.54ms
step:19/1395 train_time:1112ms step_avg:123.61ms
step:20/1395 train_time:1236ms step_avg:123.63ms
step:21/1395 train_time:1360ms step_avg:123.65ms
step:22/1395 train_time:1486ms step_avg:123.83ms
step:23/1395 train_time:1609ms step_avg:123.76ms
step:24/1395 train_time:1732ms step_avg:123.71ms
step:25/1395 train_time:1856ms step_avg:123.72ms
step:26/1395 train_time:1979ms step_avg:123.69ms
step:27/1395 train_time:2103ms step_avg:123.69ms
step:28/1395 train_time:2226ms step_avg:123.69ms
step:29/1395 train_time:2350ms step_avg:123.68ms
step:30/1395 train_time:2473ms step_avg:123.66ms
step:31/1395 train_time:2596ms step_avg:123.64ms
step:32/1395 train_time:2719ms step_avg:123.59ms
step:33/1395 train_time:2843ms step_avg:123.59ms
step:34/1395 train_time:2966ms step_avg:123.59ms
step:35/1395 train_time:3089ms step_avg:123.57ms
step:36/1395 train_time:3212ms step_avg:123.54ms
step:37/1395 train_time:3336ms step_avg:123.54ms
step:38/1395 train_time:3462ms step_avg:123.64ms
step:39/1395 train_time:3586ms step_avg:123.66ms
step:40/1395 train_time:3711ms step_avg:123.69ms
step:41/1395 train_time:3834ms step_avg:123.69ms
step:42/1395 train_time:3958ms step_avg:123.67ms
step:43/1395 train_time:4082ms step_avg:123.69ms
step:44/1395 train_time:4205ms step_avg:123.67ms
step:45/1395 train_time:4329ms step_avg:123.68ms
step:46/1395 train_time:4452ms step_avg:123.67ms
step:47/1395 train_time:4575ms step_avg:123.66ms
step:48/1395 train_time:4699ms step_avg:123.67ms
step:49/1395 train_time:4823ms step_avg:123.68ms
step:50/1395 train_time:4948ms step_avg:123.70ms
step:51/1395 train_time:5071ms step_avg:123.69ms
step:52/1395 train_time:5195ms step_avg:123.69ms
step:53/1395 train_time:5318ms step_avg:123.67ms
step:54/1395 train_time:5442ms step_avg:123.67ms
step:55/1395 train_time:5565ms step_avg:123.66ms
step:56/1395 train_time:5688ms step_avg:123.65ms
step:57/1395 train_time:5811ms step_avg:123.64ms
step:58/1395 train_time:5934ms step_avg:123.63ms
step:59/1395 train_time:6059ms step_avg:123.65ms
step:60/1395 train_time:6183ms step_avg:123.65ms
step:61/1395 train_time:6306ms step_avg:123.66ms
step:62/1395 train_time:6429ms step_avg:123.64ms
step:63/1395 train_time:6552ms step_avg:123.63ms
step:64/1395 train_time:6676ms step_avg:123.63ms
step:65/1395 train_time:6799ms step_avg:123.62ms
step:66/1395 train_time:6923ms step_avg:123.63ms
step:67/1395 train_time:7048ms step_avg:123.66ms
step:68/1395 train_time:7173ms step_avg:123.67ms
step:69/1395 train_time:7298ms step_avg:123.69ms
step:70/1395 train_time:7422ms step_avg:123.71ms
step:71/1395 train_time:7546ms step_avg:123.70ms
step:72/1395 train_time:7670ms step_avg:123.71ms
step:73/1395 train_time:7794ms step_avg:123.71ms
step:74/1395 train_time:7917ms step_avg:123.70ms
step:75/1395 train_time:8042ms step_avg:123.72ms
step:76/1395 train_time:8168ms step_avg:123.75ms
step:77/1395 train_time:8290ms step_avg:123.74ms
step:78/1395 train_time:8414ms step_avg:123.73ms
step:79/1395 train_time:8537ms step_avg:123.72ms
step:80/1395 train_time:8660ms step_avg:123.72ms
step:81/1395 train_time:8785ms step_avg:123.74ms
step:82/1395 train_time:8909ms step_avg:123.73ms
step:83/1395 train_time:9032ms step_avg:123.72ms
step:84/1395 train_time:9157ms step_avg:123.74ms
step:85/1395 train_time:9280ms step_avg:123.73ms
step:86/1395 train_time:9404ms step_avg:123.74ms
step:87/1395 train_time:9528ms step_avg:123.74ms
step:88/1395 train_time:9651ms step_avg:123.73ms
step:89/1395 train_time:9775ms step_avg:123.73ms
step:90/1395 train_time:9897ms step_avg:123.72ms
step:91/1395 train_time:10022ms step_avg:123.73ms
step:92/1395 train_time:10146ms step_avg:123.73ms
step:93/1395 train_time:10270ms step_avg:123.73ms
step:94/1395 train_time:10393ms step_avg:123.73ms
step:95/1395 train_time:10516ms step_avg:123.72ms
step:96/1395 train_time:10639ms step_avg:123.71ms
step:97/1395 train_time:10764ms step_avg:123.73ms
step:98/1395 train_time:10887ms step_avg:123.72ms
step:99/1395 train_time:11011ms step_avg:123.71ms
step:100/1395 train_time:11134ms step_avg:123.72ms
step:101/1395 train_time:11258ms step_avg:123.71ms
step:102/1395 train_time:11380ms step_avg:123.70ms
step:103/1395 train_time:11504ms step_avg:123.70ms
step:104/1395 train_time:11627ms step_avg:123.70ms
step:105/1395 train_time:11751ms step_avg:123.69ms
step:106/1395 train_time:11878ms step_avg:123.73ms
step:107/1395 train_time:12006ms step_avg:123.77ms
step:108/1395 train_time:12132ms step_avg:123.79ms
step:109/1395 train_time:12258ms step_avg:123.82ms
step:110/1395 train_time:12384ms step_avg:123.84ms
step:111/1395 train_time:12510ms step_avg:123.87ms
step:112/1395 train_time:12636ms step_avg:123.89ms
step:113/1395 train_time:12763ms step_avg:123.91ms
step:114/1395 train_time:12889ms step_avg:123.94ms
step:115/1395 train_time:13015ms step_avg:123.95ms
step:116/1395 train_time:13141ms step_avg:123.97ms
step:117/1395 train_time:13267ms step_avg:123.99ms
step:118/1395 train_time:13393ms step_avg:124.01ms
step:119/1395 train_time:13519ms step_avg:124.03ms
step:120/1395 train_time:13646ms step_avg:124.05ms
step:121/1395 train_time:13772ms step_avg:124.08ms
step:122/1395 train_time:13899ms step_avg:124.10ms
step:123/1395 train_time:14026ms step_avg:124.12ms
step:124/1395 train_time:14151ms step_avg:124.13ms
step:125/1395 train_time:14278ms step_avg:124.16ms
step:125/1395 val_loss:4.3746 train_time:14381ms step_avg:125.05ms
step:126/1395 train_time:14409ms step_avg:124.22ms
step:127/1395 train_time:14551ms step_avg:124.36ms
step:128/1395 train_time:14679ms step_avg:124.40ms
step:129/1395 train_time:14805ms step_avg:124.41ms
step:130/1395 train_time:14931ms step_avg:124.42ms
step:131/1395 train_time:15056ms step_avg:124.43ms
step:132/1395 train_time:15183ms step_avg:124.45ms
step:133/1395 train_time:15309ms step_avg:124.46ms
step:134/1395 train_time:15436ms step_avg:124.48ms
step:135/1395 train_time:15563ms step_avg:124.51ms
step:136/1395 train_time:15689ms step_avg:124.52ms
step:137/1395 train_time:15816ms step_avg:124.53ms
step:138/1395 train_time:15943ms step_avg:124.55ms
step:139/1395 train_time:16069ms step_avg:124.56ms
step:140/1395 train_time:16195ms step_avg:124.58ms
step:141/1395 train_time:16322ms step_avg:124.60ms
step:142/1395 train_time:16448ms step_avg:124.61ms
step:143/1395 train_time:16574ms step_avg:124.62ms
step:144/1395 train_time:16700ms step_avg:124.63ms
step:145/1395 train_time:16826ms step_avg:124.64ms
step:146/1395 train_time:16952ms step_avg:124.65ms
step:147/1395 train_time:17079ms step_avg:124.66ms
step:148/1395 train_time:17205ms step_avg:124.67ms
step:149/1395 train_time:17331ms step_avg:124.68ms
step:150/1395 train_time:17457ms step_avg:124.69ms
step:151/1395 train_time:17584ms step_avg:124.71ms
step:152/1395 train_time:17711ms step_avg:124.72ms
step:153/1395 train_time:17837ms step_avg:124.74ms
step:154/1395 train_time:17965ms step_avg:124.76ms
step:155/1395 train_time:18091ms step_avg:124.77ms
step:156/1395 train_time:18217ms step_avg:124.77ms
step:157/1395 train_time:18344ms step_avg:124.79ms
step:158/1395 train_time:18469ms step_avg:124.79ms
step:159/1395 train_time:18596ms step_avg:124.80ms
step:160/1395 train_time:18722ms step_avg:124.82ms
step:161/1395 train_time:18849ms step_avg:124.83ms
step:162/1395 train_time:18975ms step_avg:124.84ms
step:163/1395 train_time:19101ms step_avg:124.84ms
step:164/1395 train_time:19227ms step_avg:124.85ms
step:165/1395 train_time:19354ms step_avg:124.86ms
step:166/1395 train_time:19480ms step_avg:124.87ms
step:167/1395 train_time:19606ms step_avg:124.88ms
step:168/1395 train_time:19732ms step_avg:124.89ms
step:169/1395 train_time:19859ms step_avg:124.90ms
step:170/1395 train_time:19985ms step_avg:124.90ms
step:171/1395 train_time:20110ms step_avg:124.91ms
step:172/1395 train_time:20236ms step_avg:124.92ms
step:173/1395 train_time:20363ms step_avg:124.93ms
step:174/1395 train_time:20490ms step_avg:124.94ms
step:175/1395 train_time:20617ms step_avg:124.95ms
step:176/1395 train_time:20744ms step_avg:124.96ms
step:177/1395 train_time:20869ms step_avg:124.97ms
step:178/1395 train_time:20996ms step_avg:124.97ms
step:179/1395 train_time:21122ms step_avg:124.98ms
step:180/1395 train_time:21247ms step_avg:124.99ms
step:181/1395 train_time:21373ms step_avg:124.99ms
step:182/1395 train_time:21499ms step_avg:124.99ms
step:183/1395 train_time:21626ms step_avg:125.00ms
step:184/1395 train_time:21752ms step_avg:125.01ms
step:185/1395 train_time:21879ms step_avg:125.02ms
step:186/1395 train_time:22005ms step_avg:125.03ms
step:187/1395 train_time:22131ms step_avg:125.03ms
step:188/1395 train_time:22257ms step_avg:125.04ms
step:189/1395 train_time:22383ms step_avg:125.05ms
step:190/1395 train_time:22509ms step_avg:125.05ms
step:191/1395 train_time:22636ms step_avg:125.06ms
step:192/1395 train_time:22763ms step_avg:125.07ms
step:193/1395 train_time:22889ms step_avg:125.08ms
step:194/1395 train_time:23017ms step_avg:125.09ms
step:195/1395 train_time:23143ms step_avg:125.10ms
step:196/1395 train_time:23269ms step_avg:125.10ms
step:197/1395 train_time:23395ms step_avg:125.11ms
step:198/1395 train_time:23522ms step_avg:125.12ms
step:199/1395 train_time:23647ms step_avg:125.12ms
step:200/1395 train_time:23773ms step_avg:125.12ms
step:201/1395 train_time:23899ms step_avg:125.13ms
step:202/1395 train_time:24025ms step_avg:125.13ms
step:203/1395 train_time:24151ms step_avg:125.14ms
step:204/1395 train_time:24277ms step_avg:125.14ms
step:205/1395 train_time:24404ms step_avg:125.15ms
step:206/1395 train_time:24530ms step_avg:125.15ms
step:207/1395 train_time:24655ms step_avg:125.15ms
step:208/1395 train_time:24782ms step_avg:125.16ms
step:209/1395 train_time:24911ms step_avg:125.18ms
step:210/1395 train_time:25039ms step_avg:125.19ms
step:211/1395 train_time:25167ms step_avg:125.21ms
step:212/1395 train_time:25296ms step_avg:125.23ms
step:213/1395 train_time:25426ms step_avg:125.25ms
step:214/1395 train_time:25555ms step_avg:125.27ms
step:215/1395 train_time:25684ms step_avg:125.29ms
step:216/1395 train_time:25812ms step_avg:125.30ms
step:217/1395 train_time:25941ms step_avg:125.32ms
step:218/1395 train_time:26069ms step_avg:125.33ms
step:219/1395 train_time:26197ms step_avg:125.35ms
step:220/1395 train_time:26326ms step_avg:125.36ms
step:221/1395 train_time:26454ms step_avg:125.38ms
step:222/1395 train_time:26584ms step_avg:125.39ms
step:223/1395 train_time:26713ms step_avg:125.41ms
step:224/1395 train_time:26842ms step_avg:125.43ms
step:225/1395 train_time:26970ms step_avg:125.44ms
step:226/1395 train_time:27099ms step_avg:125.46ms
step:227/1395 train_time:27228ms step_avg:125.48ms
step:228/1395 train_time:27356ms step_avg:125.49ms
step:229/1395 train_time:27485ms step_avg:125.50ms
step:230/1395 train_time:27614ms step_avg:125.52ms
step:231/1395 train_time:27743ms step_avg:125.53ms
step:232/1395 train_time:27871ms step_avg:125.55ms
step:233/1395 train_time:27999ms step_avg:125.56ms
step:234/1395 train_time:28129ms step_avg:125.57ms
step:235/1395 train_time:28258ms step_avg:125.59ms
step:236/1395 train_time:28387ms step_avg:125.61ms
step:237/1395 train_time:28515ms step_avg:125.62ms
step:238/1395 train_time:28643ms step_avg:125.63ms
step:239/1395 train_time:28772ms step_avg:125.64ms
step:240/1395 train_time:28900ms step_avg:125.65ms
step:241/1395 train_time:29029ms step_avg:125.66ms
step:242/1395 train_time:29157ms step_avg:125.68ms
step:243/1395 train_time:29286ms step_avg:125.69ms
step:244/1395 train_time:29414ms step_avg:125.70ms
step:245/1395 train_time:29543ms step_avg:125.71ms
step:246/1395 train_time:29671ms step_avg:125.72ms
step:247/1395 train_time:29800ms step_avg:125.74ms
step:248/1395 train_time:29929ms step_avg:125.75ms
step:249/1395 train_time:30058ms step_avg:125.77ms
step:250/1395 train_time:30188ms step_avg:125.78ms
step:250/1395 val_loss:3.9536 train_time:30290ms step_avg:126.21ms
step:251/1395 train_time:30319ms step_avg:125.80ms
step:252/1395 train_time:30455ms step_avg:125.85ms
step:253/1395 train_time:30585ms step_avg:125.86ms
step:254/1395 train_time:30713ms step_avg:125.87ms
step:255/1395 train_time:30841ms step_avg:125.88ms
step:256/1395 train_time:30969ms step_avg:125.89ms
step:257/1395 train_time:31098ms step_avg:125.90ms
step:258/1395 train_time:31225ms step_avg:125.91ms
step:259/1395 train_time:31354ms step_avg:125.92ms
step:260/1395 train_time:31485ms step_avg:125.94ms
step:261/1395 train_time:31614ms step_avg:125.95ms
step:262/1395 train_time:31743ms step_avg:125.96ms
step:263/1395 train_time:31872ms step_avg:125.98ms
step:264/1395 train_time:32000ms step_avg:125.98ms
step:265/1395 train_time:32127ms step_avg:125.99ms
step:266/1395 train_time:32255ms step_avg:126.00ms
step:267/1395 train_time:32384ms step_avg:126.01ms
step:268/1395 train_time:32513ms step_avg:126.02ms
step:269/1395 train_time:32642ms step_avg:126.03ms
step:270/1395 train_time:32771ms step_avg:126.04ms
step:271/1395 train_time:32899ms step_avg:126.05ms
step:272/1395 train_time:33028ms step_avg:126.06ms
step:273/1395 train_time:33157ms step_avg:126.07ms
step:274/1395 train_time:33285ms step_avg:126.08ms
step:275/1395 train_time:33413ms step_avg:126.09ms
step:276/1395 train_time:33542ms step_avg:126.10ms
step:277/1395 train_time:33671ms step_avg:126.11ms
step:278/1395 train_time:33800ms step_avg:126.12ms
step:279/1395 train_time:33928ms step_avg:126.13ms
step:280/1395 train_time:34057ms step_avg:126.14ms
step:281/1395 train_time:34186ms step_avg:126.15ms
step:282/1395 train_time:34314ms step_avg:126.16ms
step:283/1395 train_time:34443ms step_avg:126.16ms
step:284/1395 train_time:34572ms step_avg:126.18ms
step:285/1395 train_time:34700ms step_avg:126.18ms
step:286/1395 train_time:34828ms step_avg:126.19ms
step:287/1395 train_time:34956ms step_avg:126.20ms
step:288/1395 train_time:35085ms step_avg:126.21ms
step:289/1395 train_time:35214ms step_avg:126.21ms
step:290/1395 train_time:35342ms step_avg:126.22ms
step:291/1395 train_time:35470ms step_avg:126.23ms
step:292/1395 train_time:35598ms step_avg:126.23ms
step:293/1395 train_time:35727ms step_avg:126.24ms
step:294/1395 train_time:35856ms step_avg:126.25ms
step:295/1395 train_time:35984ms step_avg:126.26ms
step:296/1395 train_time:36114ms step_avg:126.27ms
step:297/1395 train_time:36242ms step_avg:126.28ms
step:298/1395 train_time:36370ms step_avg:126.29ms
step:299/1395 train_time:36499ms step_avg:126.29ms
step:300/1395 train_time:36627ms step_avg:126.30ms
step:301/1395 train_time:36755ms step_avg:126.31ms
step:302/1395 train_time:36884ms step_avg:126.32ms
step:303/1395 train_time:37013ms step_avg:126.32ms
step:304/1395 train_time:37141ms step_avg:126.33ms
step:305/1395 train_time:37270ms step_avg:126.34ms
step:306/1395 train_time:37398ms step_avg:126.35ms
step:307/1395 train_time:37528ms step_avg:126.36ms
step:308/1395 train_time:37655ms step_avg:126.36ms
step:309/1395 train_time:37783ms step_avg:126.37ms
step:310/1395 train_time:37914ms step_avg:126.38ms
step:311/1395 train_time:38042ms step_avg:126.38ms
step:312/1395 train_time:38171ms step_avg:126.39ms
step:313/1395 train_time:38302ms step_avg:126.41ms
step:314/1395 train_time:38433ms step_avg:126.42ms
step:315/1395 train_time:38562ms step_avg:126.43ms
step:316/1395 train_time:38694ms step_avg:126.45ms
step:317/1395 train_time:38825ms step_avg:126.46ms
step:318/1395 train_time:38955ms step_avg:126.48ms
step:319/1395 train_time:39085ms step_avg:126.49ms
step:320/1395 train_time:39216ms step_avg:126.50ms
step:321/1395 train_time:39347ms step_avg:126.52ms
step:322/1395 train_time:39477ms step_avg:126.53ms
step:323/1395 train_time:39607ms step_avg:126.54ms
step:324/1395 train_time:39737ms step_avg:126.55ms
step:325/1395 train_time:39867ms step_avg:126.56ms
step:326/1395 train_time:39998ms step_avg:126.57ms
step:327/1395 train_time:40129ms step_avg:126.59ms
step:328/1395 train_time:40259ms step_avg:126.60ms
step:329/1395 train_time:40389ms step_avg:126.61ms
step:330/1395 train_time:40519ms step_avg:126.62ms
step:331/1395 train_time:40650ms step_avg:126.64ms
step:332/1395 train_time:40780ms step_avg:126.64ms
step:333/1395 train_time:40912ms step_avg:126.66ms
step:334/1395 train_time:41042ms step_avg:126.67ms
step:335/1395 train_time:41174ms step_avg:126.69ms
step:336/1395 train_time:41305ms step_avg:126.70ms
step:337/1395 train_time:41435ms step_avg:126.71ms
step:338/1395 train_time:41566ms step_avg:126.72ms
step:339/1395 train_time:41696ms step_avg:126.74ms
step:340/1395 train_time:41827ms step_avg:126.75ms
step:341/1395 train_time:41957ms step_avg:126.76ms
step:342/1395 train_time:42087ms step_avg:126.77ms
step:343/1395 train_time:42218ms step_avg:126.78ms
step:344/1395 train_time:42349ms step_avg:126.79ms
step:345/1395 train_time:42479ms step_avg:126.80ms
step:346/1395 train_time:42611ms step_avg:126.82ms
step:347/1395 train_time:42741ms step_avg:126.83ms
step:348/1395 train_time:42872ms step_avg:126.84ms
step:349/1395 train_time:43003ms step_avg:126.85ms
step:350/1395 train_time:43134ms step_avg:126.86ms
step:351/1395 train_time:43264ms step_avg:126.88ms
step:352/1395 train_time:43395ms step_avg:126.88ms
step:353/1395 train_time:43525ms step_avg:126.90ms
step:354/1395 train_time:43656ms step_avg:126.91ms
step:355/1395 train_time:43786ms step_avg:126.92ms
step:356/1395 train_time:43916ms step_avg:126.93ms
step:357/1395 train_time:44047ms step_avg:126.94ms
step:358/1395 train_time:44178ms step_avg:126.95ms
step:359/1395 train_time:44309ms step_avg:126.96ms
step:360/1395 train_time:44438ms step_avg:126.97ms
step:361/1395 train_time:44570ms step_avg:126.98ms
step:362/1395 train_time:44700ms step_avg:126.99ms
step:363/1395 train_time:44832ms step_avg:127.00ms
step:364/1395 train_time:44962ms step_avg:127.01ms
step:365/1395 train_time:45093ms step_avg:127.02ms
step:366/1395 train_time:45223ms step_avg:127.03ms
step:367/1395 train_time:45353ms step_avg:127.04ms
step:368/1395 train_time:45484ms step_avg:127.05ms
step:369/1395 train_time:45614ms step_avg:127.06ms
step:370/1395 train_time:45744ms step_avg:127.07ms
step:371/1395 train_time:45875ms step_avg:127.08ms
step:372/1395 train_time:46005ms step_avg:127.09ms
step:373/1395 train_time:46136ms step_avg:127.10ms
step:374/1395 train_time:46267ms step_avg:127.11ms
step:375/1395 train_time:46397ms step_avg:127.12ms
step:375/1395 val_loss:3.7721 train_time:46502ms step_avg:127.40ms
step:376/1395 train_time:46532ms step_avg:127.14ms
step:377/1395 train_time:46671ms step_avg:127.17ms
step:378/1395 train_time:46801ms step_avg:127.18ms
step:379/1395 train_time:46931ms step_avg:127.18ms
step:380/1395 train_time:47061ms step_avg:127.19ms
step:381/1395 train_time:47191ms step_avg:127.20ms
step:382/1395 train_time:47322ms step_avg:127.21ms
step:383/1395 train_time:47452ms step_avg:127.22ms
step:384/1395 train_time:47584ms step_avg:127.23ms
step:385/1395 train_time:47716ms step_avg:127.24ms
step:386/1395 train_time:47848ms step_avg:127.25ms
step:387/1395 train_time:47978ms step_avg:127.26ms
step:388/1395 train_time:48109ms step_avg:127.27ms
step:389/1395 train_time:48238ms step_avg:127.28ms
step:390/1395 train_time:48368ms step_avg:127.28ms
step:391/1395 train_time:48498ms step_avg:127.29ms
step:392/1395 train_time:48629ms step_avg:127.30ms
step:393/1395 train_time:48759ms step_avg:127.31ms
step:394/1395 train_time:48890ms step_avg:127.32ms
step:395/1395 train_time:49020ms step_avg:127.32ms
step:396/1395 train_time:49151ms step_avg:127.33ms
step:397/1395 train_time:49280ms step_avg:127.34ms
step:398/1395 train_time:49411ms step_avg:127.35ms
step:399/1395 train_time:49540ms step_avg:127.35ms
step:400/1395 train_time:49671ms step_avg:127.36ms
step:401/1395 train_time:49801ms step_avg:127.37ms
step:402/1395 train_time:49931ms step_avg:127.38ms
step:403/1395 train_time:50063ms step_avg:127.39ms
step:404/1395 train_time:50195ms step_avg:127.40ms
step:405/1395 train_time:50326ms step_avg:127.41ms
step:406/1395 train_time:50457ms step_avg:127.42ms
step:407/1395 train_time:50587ms step_avg:127.42ms
step:408/1395 train_time:50718ms step_avg:127.43ms
step:409/1395 train_time:50848ms step_avg:127.44ms
step:410/1395 train_time:50978ms step_avg:127.44ms
step:411/1395 train_time:51109ms step_avg:127.45ms
step:412/1395 train_time:51240ms step_avg:127.46ms
step:413/1395 train_time:51369ms step_avg:127.47ms
step:414/1395 train_time:51500ms step_avg:127.47ms
step:415/1395 train_time:51630ms step_avg:127.48ms
step:416/1395 train_time:51763ms step_avg:127.49ms
step:417/1395 train_time:51895ms step_avg:127.51ms
step:418/1395 train_time:52028ms step_avg:127.52ms
step:419/1395 train_time:52159ms step_avg:127.53ms
step:420/1395 train_time:52292ms step_avg:127.54ms
step:421/1395 train_time:52424ms step_avg:127.55ms
step:422/1395 train_time:52556ms step_avg:127.56ms
step:423/1395 train_time:52688ms step_avg:127.57ms
step:424/1395 train_time:52821ms step_avg:127.59ms
step:425/1395 train_time:52954ms step_avg:127.60ms
step:426/1395 train_time:53086ms step_avg:127.61ms
step:427/1395 train_time:53220ms step_avg:127.62ms
step:428/1395 train_time:53351ms step_avg:127.64ms
step:429/1395 train_time:53485ms step_avg:127.65ms
step:430/1395 train_time:53618ms step_avg:127.66ms
step:431/1395 train_time:53750ms step_avg:127.67ms
step:432/1395 train_time:53883ms step_avg:127.69ms
step:433/1395 train_time:54015ms step_avg:127.70ms
step:434/1395 train_time:54148ms step_avg:127.71ms
step:435/1395 train_time:54280ms step_avg:127.72ms
step:436/1395 train_time:54412ms step_avg:127.73ms
step:437/1395 train_time:54544ms step_avg:127.74ms
step:438/1395 train_time:54676ms step_avg:127.75ms
step:439/1395 train_time:54808ms step_avg:127.76ms
step:440/1395 train_time:54941ms step_avg:127.77ms
step:441/1395 train_time:55073ms step_avg:127.78ms
step:442/1395 train_time:55207ms step_avg:127.79ms
step:443/1395 train_time:55340ms step_avg:127.80ms
step:444/1395 train_time:55471ms step_avg:127.81ms
step:445/1395 train_time:55603ms step_avg:127.82ms
step:446/1395 train_time:55735ms step_avg:127.83ms
step:447/1395 train_time:55867ms step_avg:127.84ms
step:448/1395 train_time:56000ms step_avg:127.85ms
step:449/1395 train_time:56132ms step_avg:127.86ms
step:450/1395 train_time:56264ms step_avg:127.87ms
step:451/1395 train_time:56397ms step_avg:127.88ms
step:452/1395 train_time:56529ms step_avg:127.89ms
step:453/1395 train_time:56661ms step_avg:127.90ms
step:454/1395 train_time:56793ms step_avg:127.91ms
step:455/1395 train_time:56926ms step_avg:127.92ms
step:456/1395 train_time:57059ms step_avg:127.94ms
step:457/1395 train_time:57191ms step_avg:127.94ms
step:458/1395 train_time:57322ms step_avg:127.95ms
step:459/1395 train_time:57455ms step_avg:127.96ms
step:460/1395 train_time:57587ms step_avg:127.97ms
step:461/1395 train_time:57720ms step_avg:127.98ms
step:462/1395 train_time:57852ms step_avg:127.99ms
step:463/1395 train_time:57986ms step_avg:128.00ms
step:464/1395 train_time:58119ms step_avg:128.01ms
step:465/1395 train_time:58249ms step_avg:128.02ms
step:466/1395 train_time:58382ms step_avg:128.03ms
step:467/1395 train_time:58515ms step_avg:128.04ms
step:468/1395 train_time:58647ms step_avg:128.05ms
step:469/1395 train_time:58780ms step_avg:128.06ms
step:470/1395 train_time:58912ms step_avg:128.07ms
step:471/1395 train_time:59044ms step_avg:128.08ms
step:472/1395 train_time:59177ms step_avg:128.09ms
step:473/1395 train_time:59309ms step_avg:128.10ms
step:474/1395 train_time:59441ms step_avg:128.11ms
step:475/1395 train_time:59574ms step_avg:128.12ms
step:476/1395 train_time:59708ms step_avg:128.13ms
step:477/1395 train_time:59839ms step_avg:128.14ms
step:478/1395 train_time:59971ms step_avg:128.14ms
step:479/1395 train_time:60103ms step_avg:128.15ms
step:480/1395 train_time:60235ms step_avg:128.16ms
step:481/1395 train_time:60368ms step_avg:128.17ms
step:482/1395 train_time:60500ms step_avg:128.18ms
step:483/1395 train_time:60633ms step_avg:128.19ms
step:484/1395 train_time:60765ms step_avg:128.20ms
step:485/1395 train_time:60897ms step_avg:128.20ms
step:486/1395 train_time:61029ms step_avg:128.21ms
step:487/1395 train_time:61161ms step_avg:128.22ms
step:488/1395 train_time:61292ms step_avg:128.23ms
step:489/1395 train_time:61425ms step_avg:128.23ms
step:490/1395 train_time:61557ms step_avg:128.24ms
step:491/1395 train_time:61690ms step_avg:128.25ms
step:492/1395 train_time:61823ms step_avg:128.26ms
step:493/1395 train_time:61955ms step_avg:128.27ms
step:494/1395 train_time:62089ms step_avg:128.28ms
step:495/1395 train_time:62222ms step_avg:128.29ms
step:496/1395 train_time:62354ms step_avg:128.30ms
step:497/1395 train_time:62486ms step_avg:128.31ms
step:498/1395 train_time:62618ms step_avg:128.32ms
step:499/1395 train_time:62750ms step_avg:128.32ms
step:500/1395 train_time:62882ms step_avg:128.33ms
step:500/1395 val_loss:3.6587 train_time:62988ms step_avg:128.55ms
step:501/1395 train_time:63019ms step_avg:128.35ms
step:502/1395 train_time:63157ms step_avg:128.37ms
step:503/1395 train_time:63290ms step_avg:128.38ms
step:504/1395 train_time:63422ms step_avg:128.38ms
step:505/1395 train_time:63553ms step_avg:128.39ms
step:506/1395 train_time:63685ms step_avg:128.40ms
step:507/1395 train_time:63817ms step_avg:128.40ms
step:508/1395 train_time:63949ms step_avg:128.41ms
step:509/1395 train_time:64083ms step_avg:128.42ms
step:510/1395 train_time:64216ms step_avg:128.43ms
step:511/1395 train_time:64349ms step_avg:128.44ms
step:512/1395 train_time:64481ms step_avg:128.45ms
step:513/1395 train_time:64614ms step_avg:128.46ms
step:514/1395 train_time:64747ms step_avg:128.47ms
step:515/1395 train_time:64879ms step_avg:128.47ms
step:516/1395 train_time:65011ms step_avg:128.48ms
step:517/1395 train_time:65144ms step_avg:128.49ms
step:518/1395 train_time:65277ms step_avg:128.50ms
step:519/1395 train_time:65411ms step_avg:128.51ms
step:520/1395 train_time:65545ms step_avg:128.52ms
step:521/1395 train_time:65679ms step_avg:128.53ms
step:522/1395 train_time:65812ms step_avg:128.54ms
step:523/1395 train_time:65947ms step_avg:128.55ms
step:524/1395 train_time:66081ms step_avg:128.56ms
step:525/1395 train_time:66214ms step_avg:128.57ms
step:526/1395 train_time:66349ms step_avg:128.58ms
step:527/1395 train_time:66484ms step_avg:128.60ms
step:528/1395 train_time:66617ms step_avg:128.60ms
step:529/1395 train_time:66752ms step_avg:128.62ms
step:530/1395 train_time:66886ms step_avg:128.63ms
step:531/1395 train_time:67020ms step_avg:128.64ms
step:532/1395 train_time:67153ms step_avg:128.65ms
step:533/1395 train_time:67289ms step_avg:128.66ms
step:534/1395 train_time:67423ms step_avg:128.67ms
step:535/1395 train_time:67556ms step_avg:128.68ms
step:536/1395 train_time:67691ms step_avg:128.69ms
step:537/1395 train_time:67824ms step_avg:128.70ms
step:538/1395 train_time:67957ms step_avg:128.71ms
step:539/1395 train_time:68092ms step_avg:128.72ms
step:540/1395 train_time:68226ms step_avg:128.73ms
step:541/1395 train_time:68360ms step_avg:128.74ms
step:542/1395 train_time:68494ms step_avg:128.75ms
step:543/1395 train_time:68628ms step_avg:128.76ms
step:544/1395 train_time:68761ms step_avg:128.77ms
step:545/1395 train_time:68895ms step_avg:128.78ms
step:546/1395 train_time:69029ms step_avg:128.79ms
step:547/1395 train_time:69163ms step_avg:128.79ms
step:548/1395 train_time:69298ms step_avg:128.81ms
step:549/1395 train_time:69432ms step_avg:128.82ms
step:550/1395 train_time:69568ms step_avg:128.83ms
step:551/1395 train_time:69701ms step_avg:128.84ms
step:552/1395 train_time:69835ms step_avg:128.85ms
step:553/1395 train_time:69970ms step_avg:128.86ms
step:554/1395 train_time:70104ms step_avg:128.87ms
step:555/1395 train_time:70237ms step_avg:128.88ms
step:556/1395 train_time:70371ms step_avg:128.89ms
step:557/1395 train_time:70504ms step_avg:128.89ms
step:558/1395 train_time:70637ms step_avg:128.90ms
step:559/1395 train_time:70771ms step_avg:128.91ms
step:560/1395 train_time:70905ms step_avg:128.92ms
step:561/1395 train_time:71038ms step_avg:128.93ms
step:562/1395 train_time:71172ms step_avg:128.94ms
step:563/1395 train_time:71305ms step_avg:128.94ms
step:564/1395 train_time:71439ms step_avg:128.95ms
step:565/1395 train_time:71572ms step_avg:128.96ms
step:566/1395 train_time:71707ms step_avg:128.97ms
step:567/1395 train_time:71841ms step_avg:128.98ms
step:568/1395 train_time:71974ms step_avg:128.99ms
step:569/1395 train_time:72109ms step_avg:129.00ms
step:570/1395 train_time:72242ms step_avg:129.00ms
step:571/1395 train_time:72376ms step_avg:129.01ms
step:572/1395 train_time:72511ms step_avg:129.02ms
step:573/1395 train_time:72644ms step_avg:129.03ms
step:574/1395 train_time:72780ms step_avg:129.04ms
step:575/1395 train_time:72914ms step_avg:129.05ms
step:576/1395 train_time:73047ms step_avg:129.06ms
step:577/1395 train_time:73180ms step_avg:129.07ms
step:578/1395 train_time:73314ms step_avg:129.07ms
step:579/1395 train_time:73449ms step_avg:129.08ms
step:580/1395 train_time:73582ms step_avg:129.09ms
step:581/1395 train_time:73716ms step_avg:129.10ms
step:582/1395 train_time:73851ms step_avg:129.11ms
step:583/1395 train_time:73986ms step_avg:129.12ms
step:584/1395 train_time:74120ms step_avg:129.13ms
step:585/1395 train_time:74253ms step_avg:129.14ms
step:586/1395 train_time:74389ms step_avg:129.15ms
step:587/1395 train_time:74522ms step_avg:129.16ms
step:588/1395 train_time:74656ms step_avg:129.16ms
step:589/1395 train_time:74790ms step_avg:129.17ms
step:590/1395 train_time:74924ms step_avg:129.18ms
step:591/1395 train_time:75058ms step_avg:129.19ms
step:592/1395 train_time:75193ms step_avg:129.20ms
step:593/1395 train_time:75326ms step_avg:129.20ms
step:594/1395 train_time:75460ms step_avg:129.21ms
step:595/1395 train_time:75595ms step_avg:129.22ms
step:596/1395 train_time:75729ms step_avg:129.23ms
step:597/1395 train_time:75863ms step_avg:129.24ms
step:598/1395 train_time:75995ms step_avg:129.24ms
step:599/1395 train_time:76129ms step_avg:129.25ms
step:600/1395 train_time:76263ms step_avg:129.26ms
step:601/1395 train_time:76396ms step_avg:129.27ms
step:602/1395 train_time:76529ms step_avg:129.27ms
step:603/1395 train_time:76665ms step_avg:129.28ms
step:604/1395 train_time:76797ms step_avg:129.29ms
step:605/1395 train_time:76933ms step_avg:129.30ms
step:606/1395 train_time:77066ms step_avg:129.31ms
step:607/1395 train_time:77201ms step_avg:129.32ms
step:608/1395 train_time:77335ms step_avg:129.32ms
step:609/1395 train_time:77469ms step_avg:129.33ms
step:610/1395 train_time:77602ms step_avg:129.34ms
step:611/1395 train_time:77735ms step_avg:129.34ms
step:612/1395 train_time:77868ms step_avg:129.35ms
step:613/1395 train_time:78002ms step_avg:129.36ms
step:614/1395 train_time:78135ms step_avg:129.36ms
step:615/1395 train_time:78268ms step_avg:129.37ms
step:616/1395 train_time:78403ms step_avg:129.38ms
step:617/1395 train_time:78537ms step_avg:129.38ms
step:618/1395 train_time:78670ms step_avg:129.39ms
step:619/1395 train_time:78804ms step_avg:129.40ms
step:620/1395 train_time:78938ms step_avg:129.41ms
step:621/1395 train_time:79072ms step_avg:129.41ms
step:622/1395 train_time:79205ms step_avg:129.42ms
step:623/1395 train_time:79340ms step_avg:129.43ms
step:624/1395 train_time:79475ms step_avg:129.44ms
step:625/1395 train_time:79610ms step_avg:129.45ms
step:625/1395 val_loss:3.5764 train_time:79719ms step_avg:129.63ms
step:626/1395 train_time:79749ms step_avg:129.46ms
step:627/1395 train_time:79890ms step_avg:129.48ms
step:628/1395 train_time:80025ms step_avg:129.49ms
step:629/1395 train_time:80160ms step_avg:129.50ms
step:630/1395 train_time:80294ms step_avg:129.51ms
step:631/1395 train_time:80429ms step_avg:129.52ms
step:632/1395 train_time:80563ms step_avg:129.52ms
step:633/1395 train_time:80697ms step_avg:129.53ms
step:634/1395 train_time:80833ms step_avg:129.54ms
step:635/1395 train_time:80969ms step_avg:129.55ms
step:636/1395 train_time:81106ms step_avg:129.56ms
step:637/1395 train_time:81241ms step_avg:129.57ms
step:638/1395 train_time:81375ms step_avg:129.58ms
step:639/1395 train_time:81510ms step_avg:129.59ms
step:640/1395 train_time:81645ms step_avg:129.60ms
step:641/1395 train_time:81779ms step_avg:129.60ms
step:642/1395 train_time:81913ms step_avg:129.61ms
step:643/1395 train_time:82049ms step_avg:129.62ms
step:644/1395 train_time:82184ms step_avg:129.63ms
step:645/1395 train_time:82320ms step_avg:129.64ms
step:646/1395 train_time:82455ms step_avg:129.65ms
step:647/1395 train_time:82589ms step_avg:129.65ms
step:648/1395 train_time:82727ms step_avg:129.67ms
step:649/1395 train_time:82863ms step_avg:129.68ms
step:650/1395 train_time:82999ms step_avg:129.69ms
step:651/1395 train_time:83134ms step_avg:129.69ms
step:652/1395 train_time:83269ms step_avg:129.70ms
step:653/1395 train_time:83405ms step_avg:129.71ms
step:654/1395 train_time:83539ms step_avg:129.72ms
step:655/1395 train_time:83674ms step_avg:129.73ms
step:656/1395 train_time:83810ms step_avg:129.74ms
step:657/1395 train_time:83945ms step_avg:129.75ms
step:658/1395 train_time:84080ms step_avg:129.75ms
step:659/1395 train_time:84215ms step_avg:129.76ms
step:660/1395 train_time:84350ms step_avg:129.77ms
step:661/1395 train_time:84485ms step_avg:129.78ms
step:662/1395 train_time:84621ms step_avg:129.79ms
step:663/1395 train_time:84754ms step_avg:129.79ms
step:664/1395 train_time:84891ms step_avg:129.80ms
step:665/1395 train_time:85025ms step_avg:129.81ms
step:666/1395 train_time:85162ms step_avg:129.82ms
step:667/1395 train_time:85297ms step_avg:129.83ms
step:668/1395 train_time:85433ms step_avg:129.84ms
step:669/1395 train_time:85569ms step_avg:129.85ms
step:670/1395 train_time:85704ms step_avg:129.86ms
step:671/1395 train_time:85840ms step_avg:129.86ms
step:672/1395 train_time:85976ms step_avg:129.87ms
step:673/1395 train_time:86112ms step_avg:129.88ms
step:674/1395 train_time:86246ms step_avg:129.89ms
step:675/1395 train_time:86383ms step_avg:129.90ms
step:676/1395 train_time:86520ms step_avg:129.91ms
step:677/1395 train_time:86655ms step_avg:129.92ms
step:678/1395 train_time:86790ms step_avg:129.93ms
step:679/1395 train_time:86925ms step_avg:129.93ms
step:680/1395 train_time:87060ms step_avg:129.94ms
step:681/1395 train_time:87196ms step_avg:129.95ms
step:682/1395 train_time:87331ms step_avg:129.96ms
step:683/1395 train_time:87467ms step_avg:129.97ms
step:684/1395 train_time:87602ms step_avg:129.97ms
step:685/1395 train_time:87737ms step_avg:129.98ms
step:686/1395 train_time:87872ms step_avg:129.99ms
step:687/1395 train_time:88006ms step_avg:129.99ms
step:688/1395 train_time:88142ms step_avg:130.00ms
step:689/1395 train_time:88279ms step_avg:130.01ms
step:690/1395 train_time:88416ms step_avg:130.02ms
step:691/1395 train_time:88550ms step_avg:130.03ms
step:692/1395 train_time:88685ms step_avg:130.04ms
step:693/1395 train_time:88820ms step_avg:130.04ms
step:694/1395 train_time:88953ms step_avg:130.05ms
step:695/1395 train_time:89087ms step_avg:130.05ms
step:696/1395 train_time:89223ms step_avg:130.06ms
step:697/1395 train_time:89357ms step_avg:130.07ms
step:698/1395 train_time:89493ms step_avg:130.08ms
step:699/1395 train_time:89629ms step_avg:130.09ms
step:700/1395 train_time:89765ms step_avg:130.09ms
step:701/1395 train_time:89899ms step_avg:130.10ms
step:702/1395 train_time:90034ms step_avg:130.11ms
step:703/1395 train_time:90168ms step_avg:130.11ms
step:704/1395 train_time:90303ms step_avg:130.12ms
step:705/1395 train_time:90438ms step_avg:130.13ms
step:706/1395 train_time:90576ms step_avg:130.14ms
step:707/1395 train_time:90711ms step_avg:130.14ms
step:708/1395 train_time:90848ms step_avg:130.15ms
step:709/1395 train_time:90983ms step_avg:130.16ms
step:710/1395 train_time:91119ms step_avg:130.17ms
step:711/1395 train_time:91254ms step_avg:130.18ms
step:712/1395 train_time:91391ms step_avg:130.19ms
step:713/1395 train_time:91526ms step_avg:130.19ms
step:714/1395 train_time:91663ms step_avg:130.20ms
step:715/1395 train_time:91798ms step_avg:130.21ms
step:716/1395 train_time:91934ms step_avg:130.22ms
step:717/1395 train_time:92069ms step_avg:130.22ms
step:718/1395 train_time:92203ms step_avg:130.23ms
step:719/1395 train_time:92337ms step_avg:130.24ms
step:720/1395 train_time:92473ms step_avg:130.24ms
step:721/1395 train_time:92609ms step_avg:130.25ms
step:722/1395 train_time:92745ms step_avg:130.26ms
step:723/1395 train_time:92880ms step_avg:130.27ms
step:724/1395 train_time:93016ms step_avg:130.27ms
step:725/1395 train_time:93152ms step_avg:130.28ms
step:726/1395 train_time:93289ms step_avg:130.29ms
step:727/1395 train_time:93426ms step_avg:130.30ms
step:728/1395 train_time:93562ms step_avg:130.31ms
step:729/1395 train_time:93698ms step_avg:130.32ms
step:730/1395 train_time:93836ms step_avg:130.33ms
step:731/1395 train_time:93972ms step_avg:130.34ms
step:732/1395 train_time:94108ms step_avg:130.34ms
step:733/1395 train_time:94244ms step_avg:130.35ms
step:734/1395 train_time:94381ms step_avg:130.36ms
step:735/1395 train_time:94518ms step_avg:130.37ms
step:736/1395 train_time:94654ms step_avg:130.38ms
step:737/1395 train_time:94792ms step_avg:130.39ms
step:738/1395 train_time:94928ms step_avg:130.40ms
step:739/1395 train_time:95065ms step_avg:130.40ms
step:740/1395 train_time:95202ms step_avg:130.41ms
step:741/1395 train_time:95342ms step_avg:130.43ms
step:742/1395 train_time:95479ms step_avg:130.44ms
step:743/1395 train_time:95615ms step_avg:130.44ms
step:744/1395 train_time:95753ms step_avg:130.45ms
step:745/1395 train_time:95891ms step_avg:130.46ms
step:746/1395 train_time:96027ms step_avg:130.47ms
step:747/1395 train_time:96163ms step_avg:130.48ms
step:748/1395 train_time:96299ms step_avg:130.49ms
step:749/1395 train_time:96437ms step_avg:130.50ms
step:750/1395 train_time:96575ms step_avg:130.51ms
step:750/1395 val_loss:3.5241 train_time:96688ms step_avg:130.66ms
step:751/1395 train_time:96718ms step_avg:130.52ms
step:752/1395 train_time:96858ms step_avg:130.54ms
step:753/1395 train_time:96993ms step_avg:130.54ms
step:754/1395 train_time:97128ms step_avg:130.55ms
step:755/1395 train_time:97265ms step_avg:130.56ms
step:756/1395 train_time:97400ms step_avg:130.56ms
step:757/1395 train_time:97540ms step_avg:130.57ms
step:758/1395 train_time:97677ms step_avg:130.58ms
step:759/1395 train_time:97812ms step_avg:130.59ms
step:760/1395 train_time:97948ms step_avg:130.60ms
step:761/1395 train_time:98085ms step_avg:130.61ms
step:762/1395 train_time:98221ms step_avg:130.61ms
step:763/1395 train_time:98357ms step_avg:130.62ms
step:764/1395 train_time:98494ms step_avg:130.63ms
step:765/1395 train_time:98630ms step_avg:130.64ms
step:766/1395 train_time:98768ms step_avg:130.65ms
step:767/1395 train_time:98904ms step_avg:130.65ms
step:768/1395 train_time:99041ms step_avg:130.66ms
step:769/1395 train_time:99178ms step_avg:130.67ms
step:770/1395 train_time:99315ms step_avg:130.68ms
step:771/1395 train_time:99452ms step_avg:130.69ms
step:772/1395 train_time:99587ms step_avg:130.69ms
step:773/1395 train_time:99724ms step_avg:130.70ms
step:774/1395 train_time:99860ms step_avg:130.71ms
step:775/1395 train_time:99997ms step_avg:130.72ms
step:776/1395 train_time:100133ms step_avg:130.72ms
step:777/1395 train_time:100271ms step_avg:130.73ms
step:778/1395 train_time:100407ms step_avg:130.74ms
step:779/1395 train_time:100543ms step_avg:130.75ms
step:780/1395 train_time:100681ms step_avg:130.75ms
step:781/1395 train_time:100817ms step_avg:130.76ms
step:782/1395 train_time:100954ms step_avg:130.77ms
step:783/1395 train_time:101089ms step_avg:130.78ms
step:784/1395 train_time:101227ms step_avg:130.78ms
step:785/1395 train_time:101364ms step_avg:130.79ms
step:786/1395 train_time:101501ms step_avg:130.80ms
step:787/1395 train_time:101638ms step_avg:130.81ms
step:788/1395 train_time:101774ms step_avg:130.81ms
step:789/1395 train_time:101909ms step_avg:130.82ms
step:790/1395 train_time:102045ms step_avg:130.83ms
step:791/1395 train_time:102182ms step_avg:130.83ms
step:792/1395 train_time:102319ms step_avg:130.84ms
step:793/1395 train_time:102454ms step_avg:130.85ms
step:794/1395 train_time:102590ms step_avg:130.85ms
step:795/1395 train_time:102731ms step_avg:130.87ms
step:796/1395 train_time:102868ms step_avg:130.88ms
step:797/1395 train_time:103005ms step_avg:130.88ms
step:798/1395 train_time:103141ms step_avg:130.89ms
step:799/1395 train_time:103282ms step_avg:130.90ms
step:800/1395 train_time:103417ms step_avg:130.91ms
step:801/1395 train_time:103554ms step_avg:130.92ms
step:802/1395 train_time:103692ms step_avg:130.92ms
step:803/1395 train_time:103827ms step_avg:130.93ms
step:804/1395 train_time:103961ms step_avg:130.93ms
step:805/1395 train_time:104101ms step_avg:130.94ms
step:806/1395 train_time:104237ms step_avg:130.95ms
step:807/1395 train_time:104372ms step_avg:130.96ms
step:808/1395 train_time:104509ms step_avg:130.96ms
step:809/1395 train_time:104645ms step_avg:130.97ms
step:810/1395 train_time:104781ms step_avg:130.98ms
step:811/1395 train_time:104918ms step_avg:130.98ms
step:812/1395 train_time:105055ms step_avg:130.99ms
step:813/1395 train_time:105191ms step_avg:131.00ms
step:814/1395 train_time:105327ms step_avg:131.00ms
step:815/1395 train_time:105463ms step_avg:131.01ms
step:816/1395 train_time:105601ms step_avg:131.02ms
step:817/1395 train_time:105738ms step_avg:131.03ms
step:818/1395 train_time:105873ms step_avg:131.03ms
step:819/1395 train_time:106011ms step_avg:131.04ms
step:820/1395 train_time:106148ms step_avg:131.05ms
step:821/1395 train_time:106283ms step_avg:131.05ms
step:822/1395 train_time:106420ms step_avg:131.06ms
step:823/1395 train_time:106557ms step_avg:131.07ms
step:824/1395 train_time:106693ms step_avg:131.07ms
step:825/1395 train_time:106831ms step_avg:131.08ms
step:826/1395 train_time:106970ms step_avg:131.09ms
step:827/1395 train_time:107108ms step_avg:131.10ms
step:828/1395 train_time:107244ms step_avg:131.10ms
step:829/1395 train_time:107382ms step_avg:131.11ms
step:830/1395 train_time:107521ms step_avg:131.12ms
step:831/1395 train_time:107658ms step_avg:131.13ms
step:832/1395 train_time:107796ms step_avg:131.14ms
step:833/1395 train_time:107932ms step_avg:131.14ms
step:834/1395 train_time:108071ms step_avg:131.15ms
step:835/1395 train_time:108209ms step_avg:131.16ms
step:836/1395 train_time:108350ms step_avg:131.17ms
step:837/1395 train_time:108488ms step_avg:131.18ms
step:838/1395 train_time:108625ms step_avg:131.19ms
step:839/1395 train_time:108762ms step_avg:131.20ms
step:840/1395 train_time:108899ms step_avg:131.20ms
step:841/1395 train_time:109037ms step_avg:131.21ms
step:842/1395 train_time:109174ms step_avg:131.22ms
step:843/1395 train_time:109310ms step_avg:131.22ms
step:844/1395 train_time:109448ms step_avg:131.23ms
step:845/1395 train_time:109585ms step_avg:131.24ms
step:846/1395 train_time:109724ms step_avg:131.25ms
step:847/1395 train_time:109863ms step_avg:131.26ms
step:848/1395 train_time:110000ms step_avg:131.27ms
step:849/1395 train_time:110138ms step_avg:131.27ms
step:850/1395 train_time:110277ms step_avg:131.28ms
step:851/1395 train_time:110417ms step_avg:131.29ms
step:852/1395 train_time:110554ms step_avg:131.30ms
step:853/1395 train_time:110691ms step_avg:131.31ms
step:854/1395 train_time:110826ms step_avg:131.31ms
step:855/1395 train_time:110964ms step_avg:131.32ms
step:856/1395 train_time:111100ms step_avg:131.32ms
step:857/1395 train_time:111238ms step_avg:131.33ms
step:858/1395 train_time:111379ms step_avg:131.34ms
step:859/1395 train_time:111517ms step_avg:131.35ms
step:860/1395 train_time:111653ms step_avg:131.36ms
step:861/1395 train_time:111792ms step_avg:131.37ms
step:862/1395 train_time:111931ms step_avg:131.37ms
step:863/1395 train_time:112071ms step_avg:131.38ms
step:864/1395 train_time:112209ms step_avg:131.39ms
step:865/1395 train_time:112346ms step_avg:131.40ms
step:866/1395 train_time:112490ms step_avg:131.41ms
step:867/1395 train_time:112628ms step_avg:131.42ms
step:868/1395 train_time:112763ms step_avg:131.43ms
step:869/1395 train_time:112900ms step_avg:131.43ms
step:870/1395 train_time:113039ms step_avg:131.44ms
step:871/1395 train_time:113177ms step_avg:131.45ms
step:872/1395 train_time:113313ms step_avg:131.45ms
step:873/1395 train_time:113450ms step_avg:131.46ms
step:874/1395 train_time:113589ms step_avg:131.47ms
step:875/1395 train_time:113727ms step_avg:131.48ms
step:875/1395 val_loss:3.4750 train_time:113839ms step_avg:131.61ms
step:876/1395 train_time:113869ms step_avg:131.49ms
step:877/1395 train_time:114009ms step_avg:131.50ms
step:878/1395 train_time:114149ms step_avg:131.51ms
step:879/1395 train_time:114287ms step_avg:131.52ms
step:880/1395 train_time:114425ms step_avg:131.52ms
step:881/1395 train_time:114561ms step_avg:131.53ms
step:882/1395 train_time:114700ms step_avg:131.54ms
step:883/1395 train_time:114837ms step_avg:131.54ms
step:884/1395 train_time:114975ms step_avg:131.55ms
step:885/1395 train_time:115113ms step_avg:131.56ms
step:886/1395 train_time:115252ms step_avg:131.57ms
step:887/1395 train_time:115389ms step_avg:131.57ms
step:888/1395 train_time:115529ms step_avg:131.58ms
step:889/1395 train_time:115671ms step_avg:131.59ms
step:890/1395 train_time:115807ms step_avg:131.60ms
step:891/1395 train_time:115943ms step_avg:131.60ms
step:892/1395 train_time:116081ms step_avg:131.61ms
step:893/1395 train_time:116217ms step_avg:131.62ms
step:894/1395 train_time:116354ms step_avg:131.62ms
step:895/1395 train_time:116492ms step_avg:131.63ms
step:896/1395 train_time:116629ms step_avg:131.64ms
step:897/1395 train_time:116767ms step_avg:131.64ms
step:898/1395 train_time:116906ms step_avg:131.65ms
step:899/1395 train_time:117045ms step_avg:131.66ms
step:900/1395 train_time:117182ms step_avg:131.67ms
step:901/1395 train_time:117321ms step_avg:131.67ms
step:902/1395 train_time:117456ms step_avg:131.68ms
step:903/1395 train_time:117597ms step_avg:131.69ms
step:904/1395 train_time:117735ms step_avg:131.69ms
step:905/1395 train_time:117871ms step_avg:131.70ms
step:906/1395 train_time:118009ms step_avg:131.71ms
step:907/1395 train_time:118150ms step_avg:131.72ms
step:908/1395 train_time:118286ms step_avg:131.72ms
step:909/1395 train_time:118423ms step_avg:131.73ms
step:910/1395 train_time:118567ms step_avg:131.74ms
step:911/1395 train_time:118704ms step_avg:131.75ms
step:912/1395 train_time:118841ms step_avg:131.75ms
step:913/1395 train_time:118978ms step_avg:131.76ms
step:914/1395 train_time:119117ms step_avg:131.77ms
step:915/1395 train_time:119256ms step_avg:131.77ms
step:916/1395 train_time:119395ms step_avg:131.78ms
step:917/1395 train_time:119535ms step_avg:131.79ms
step:918/1395 train_time:119672ms step_avg:131.80ms
step:919/1395 train_time:119814ms step_avg:131.81ms
step:920/1395 train_time:119953ms step_avg:131.82ms
step:921/1395 train_time:120091ms step_avg:131.82ms
step:922/1395 train_time:120231ms step_avg:131.83ms
step:923/1395 train_time:120367ms step_avg:131.84ms
step:924/1395 train_time:120504ms step_avg:131.84ms
step:925/1395 train_time:120642ms step_avg:131.85ms
step:926/1395 train_time:120778ms step_avg:131.85ms
step:927/1395 train_time:120916ms step_avg:131.86ms
step:928/1395 train_time:121054ms step_avg:131.87ms
step:929/1395 train_time:121192ms step_avg:131.87ms
step:930/1395 train_time:121329ms step_avg:131.88ms
step:931/1395 train_time:121466ms step_avg:131.88ms
step:932/1395 train_time:121603ms step_avg:131.89ms
step:933/1395 train_time:121744ms step_avg:131.90ms
step:934/1395 train_time:121882ms step_avg:131.91ms
step:935/1395 train_time:122024ms step_avg:131.92ms
step:936/1395 train_time:122162ms step_avg:131.92ms
step:937/1395 train_time:122306ms step_avg:131.94ms
step:938/1395 train_time:122446ms step_avg:131.95ms
step:939/1395 train_time:122586ms step_avg:131.96ms
step:940/1395 train_time:122728ms step_avg:131.97ms
step:941/1395 train_time:122865ms step_avg:131.97ms
step:942/1395 train_time:123003ms step_avg:131.98ms
step:943/1395 train_time:123144ms step_avg:131.99ms
step:944/1395 train_time:123289ms step_avg:132.00ms
step:945/1395 train_time:123429ms step_avg:132.01ms
step:946/1395 train_time:123570ms step_avg:132.02ms
step:947/1395 train_time:123713ms step_avg:132.03ms
step:948/1395 train_time:123852ms step_avg:132.04ms
step:949/1395 train_time:123992ms step_avg:132.05ms
step:950/1395 train_time:124129ms step_avg:132.05ms
step:951/1395 train_time:124271ms step_avg:132.06ms
step:952/1395 train_time:124410ms step_avg:132.07ms
step:953/1395 train_time:124549ms step_avg:132.08ms
step:954/1395 train_time:124688ms step_avg:132.08ms
step:955/1395 train_time:124827ms step_avg:132.09ms
step:956/1395 train_time:124969ms step_avg:132.10ms
step:957/1395 train_time:125110ms step_avg:132.11ms
step:958/1395 train_time:125251ms step_avg:132.12ms
step:959/1395 train_time:125395ms step_avg:132.13ms
step:960/1395 train_time:125534ms step_avg:132.14ms
step:961/1395 train_time:125672ms step_avg:132.15ms
step:962/1395 train_time:125811ms step_avg:132.15ms
step:963/1395 train_time:125957ms step_avg:132.17ms
step:964/1395 train_time:126095ms step_avg:132.17ms
step:965/1395 train_time:126233ms step_avg:132.18ms
step:966/1395 train_time:126372ms step_avg:132.19ms
step:967/1395 train_time:126511ms step_avg:132.20ms
step:968/1395 train_time:126649ms step_avg:132.20ms
step:969/1395 train_time:126788ms step_avg:132.21ms
step:970/1395 train_time:126927ms step_avg:132.22ms
step:971/1395 train_time:127067ms step_avg:132.22ms
step:972/1395 train_time:127204ms step_avg:132.23ms
step:973/1395 train_time:127341ms step_avg:132.23ms
step:974/1395 train_time:127482ms step_avg:132.24ms
step:975/1395 train_time:127621ms step_avg:132.25ms
step:976/1395 train_time:127759ms step_avg:132.26ms
step:977/1395 train_time:127897ms step_avg:132.26ms
step:978/1395 train_time:128035ms step_avg:132.27ms
step:979/1395 train_time:128173ms step_avg:132.27ms
step:980/1395 train_time:128311ms step_avg:132.28ms
step:981/1395 train_time:128449ms step_avg:132.29ms
step:982/1395 train_time:128588ms step_avg:132.29ms
step:983/1395 train_time:128726ms step_avg:132.30ms
step:984/1395 train_time:128864ms step_avg:132.30ms
step:985/1395 train_time:129004ms step_avg:132.31ms
step:986/1395 train_time:129148ms step_avg:132.32ms
step:987/1395 train_time:129286ms step_avg:132.33ms
step:988/1395 train_time:129425ms step_avg:132.34ms
step:989/1395 train_time:129563ms step_avg:132.34ms
step:990/1395 train_time:129703ms step_avg:132.35ms
step:991/1395 train_time:129842ms step_avg:132.36ms
step:992/1395 train_time:129985ms step_avg:132.37ms
step:993/1395 train_time:130134ms step_avg:132.38ms
step:994/1395 train_time:130271ms step_avg:132.39ms
step:995/1395 train_time:130409ms step_avg:132.39ms
step:996/1395 train_time:130545ms step_avg:132.40ms
step:997/1395 train_time:130683ms step_avg:132.40ms
step:998/1395 train_time:130821ms step_avg:132.41ms
step:999/1395 train_time:130960ms step_avg:132.42ms
step:1000/1395 train_time:131098ms step_avg:132.42ms
step:1000/1395 val_loss:3.4119 train_time:131209ms step_avg:132.53ms
step:1001/1395 train_time:131239ms step_avg:132.43ms
step:1002/1395 train_time:131378ms step_avg:132.44ms
step:1003/1395 train_time:131521ms step_avg:132.45ms
step:1004/1395 train_time:131661ms step_avg:132.46ms
step:1005/1395 train_time:131800ms step_avg:132.46ms
step:1006/1395 train_time:131937ms step_avg:132.47ms
step:1007/1395 train_time:132075ms step_avg:132.47ms
step:1008/1395 train_time:132215ms step_avg:132.48ms
step:1009/1395 train_time:132358ms step_avg:132.49ms
step:1010/1395 train_time:132496ms step_avg:132.50ms
step:1011/1395 train_time:132636ms step_avg:132.50ms
step:1012/1395 train_time:132776ms step_avg:132.51ms
step:1013/1395 train_time:132916ms step_avg:132.52ms
step:1014/1395 train_time:133054ms step_avg:132.52ms
step:1015/1395 train_time:133192ms step_avg:132.53ms
step:1016/1395 train_time:133331ms step_avg:132.54ms
step:1017/1395 train_time:133470ms step_avg:132.54ms
step:1018/1395 train_time:133608ms step_avg:132.55ms
step:1019/1395 train_time:133748ms step_avg:132.56ms
step:1020/1395 train_time:133889ms step_avg:132.56ms
step:1021/1395 train_time:134027ms step_avg:132.57ms
step:1022/1395 train_time:134166ms step_avg:132.57ms
step:1023/1395 train_time:134306ms step_avg:132.58ms
step:1024/1395 train_time:134444ms step_avg:132.59ms
step:1025/1395 train_time:134584ms step_avg:132.60ms
step:1026/1395 train_time:134723ms step_avg:132.60ms
step:1027/1395 train_time:134860ms step_avg:132.61ms
step:1028/1395 train_time:135000ms step_avg:132.61ms
step:1029/1395 train_time:135143ms step_avg:132.62ms
step:1030/1395 train_time:135283ms step_avg:132.63ms
step:1031/1395 train_time:135420ms step_avg:132.63ms
step:1032/1395 train_time:135557ms step_avg:132.64ms
step:1033/1395 train_time:135695ms step_avg:132.64ms
step:1034/1395 train_time:135834ms step_avg:132.65ms
step:1035/1395 train_time:135974ms step_avg:132.66ms
step:1036/1395 train_time:136112ms step_avg:132.66ms
step:1037/1395 train_time:136257ms step_avg:132.67ms
step:1038/1395 train_time:136396ms step_avg:132.68ms
step:1039/1395 train_time:136535ms step_avg:132.69ms
step:1040/1395 train_time:136674ms step_avg:132.69ms
step:1041/1395 train_time:136815ms step_avg:132.70ms
step:1042/1395 train_time:136953ms step_avg:132.71ms
step:1043/1395 train_time:137094ms step_avg:132.71ms
step:1044/1395 train_time:137238ms step_avg:132.72ms
step:1045/1395 train_time:137378ms step_avg:132.73ms
step:1046/1395 train_time:137519ms step_avg:132.74ms
step:1047/1395 train_time:137657ms step_avg:132.75ms
step:1048/1395 train_time:137796ms step_avg:132.75ms
step:1049/1395 train_time:137935ms step_avg:132.76ms
step:1050/1395 train_time:138077ms step_avg:132.77ms
step:1051/1395 train_time:138220ms step_avg:132.78ms
step:1052/1395 train_time:138360ms step_avg:132.78ms
step:1053/1395 train_time:138498ms step_avg:132.79ms
step:1054/1395 train_time:138637ms step_avg:132.79ms
step:1055/1395 train_time:138778ms step_avg:132.80ms
step:1056/1395 train_time:138916ms step_avg:132.81ms
step:1057/1395 train_time:139055ms step_avg:132.81ms
step:1058/1395 train_time:139197ms step_avg:132.82ms
step:1059/1395 train_time:139339ms step_avg:132.83ms
step:1060/1395 train_time:139481ms step_avg:132.84ms
step:1061/1395 train_time:139618ms step_avg:132.84ms
step:1062/1395 train_time:139759ms step_avg:132.85ms
step:1063/1395 train_time:139897ms step_avg:132.86ms
step:1064/1395 train_time:140036ms step_avg:132.86ms
step:1065/1395 train_time:140177ms step_avg:132.87ms
step:1066/1395 train_time:140319ms step_avg:132.88ms
step:1067/1395 train_time:140461ms step_avg:132.89ms
step:1068/1395 train_time:140600ms step_avg:132.89ms
step:1069/1395 train_time:140746ms step_avg:132.90ms
step:1070/1395 train_time:140884ms step_avg:132.91ms
step:1071/1395 train_time:141028ms step_avg:132.92ms
step:1072/1395 train_time:141166ms step_avg:132.93ms
step:1073/1395 train_time:141305ms step_avg:132.93ms
step:1074/1395 train_time:141443ms step_avg:132.94ms
step:1075/1395 train_time:141585ms step_avg:132.94ms
step:1076/1395 train_time:141723ms step_avg:132.95ms
step:1077/1395 train_time:141861ms step_avg:132.95ms
step:1078/1395 train_time:142004ms step_avg:132.96ms
step:1079/1395 train_time:142151ms step_avg:132.98ms
step:1080/1395 train_time:142291ms step_avg:132.98ms
step:1081/1395 train_time:142430ms step_avg:132.99ms
step:1082/1395 train_time:142568ms step_avg:132.99ms
step:1083/1395 train_time:142707ms step_avg:133.00ms
step:1084/1395 train_time:142852ms step_avg:133.01ms
step:1085/1395 train_time:142990ms step_avg:133.01ms
step:1086/1395 train_time:143131ms step_avg:133.02ms
step:1087/1395 train_time:143272ms step_avg:133.03ms
step:1088/1395 train_time:143411ms step_avg:133.03ms
step:1089/1395 train_time:143555ms step_avg:133.04ms
step:1090/1395 train_time:143700ms step_avg:133.06ms
step:1091/1395 train_time:143840ms step_avg:133.06ms
step:1092/1395 train_time:143979ms step_avg:133.07ms
step:1093/1395 train_time:144120ms step_avg:133.08ms
step:1094/1395 train_time:144259ms step_avg:133.08ms
step:1095/1395 train_time:144397ms step_avg:133.08ms
step:1096/1395 train_time:144539ms step_avg:133.09ms
step:1097/1395 train_time:144682ms step_avg:133.10ms
step:1098/1395 train_time:144824ms step_avg:133.11ms
step:1099/1395 train_time:144963ms step_avg:133.12ms
step:1100/1395 train_time:145101ms step_avg:133.12ms
step:1101/1395 train_time:145240ms step_avg:133.13ms
step:1102/1395 train_time:145382ms step_avg:133.13ms
step:1103/1395 train_time:145522ms step_avg:133.14ms
step:1104/1395 train_time:145662ms step_avg:133.15ms
step:1105/1395 train_time:145805ms step_avg:133.16ms
step:1106/1395 train_time:145945ms step_avg:133.16ms
step:1107/1395 train_time:146085ms step_avg:133.17ms
step:1108/1395 train_time:146231ms step_avg:133.18ms
step:1109/1395 train_time:146370ms step_avg:133.18ms
step:1110/1395 train_time:146510ms step_avg:133.19ms
step:1111/1395 train_time:146651ms step_avg:133.20ms
step:1112/1395 train_time:146790ms step_avg:133.20ms
step:1113/1395 train_time:146929ms step_avg:133.21ms
step:1114/1395 train_time:147072ms step_avg:133.22ms
step:1115/1395 train_time:147213ms step_avg:133.22ms
step:1116/1395 train_time:147352ms step_avg:133.23ms
step:1117/1395 train_time:147494ms step_avg:133.24ms
step:1118/1395 train_time:147640ms step_avg:133.25ms
step:1119/1395 train_time:147781ms step_avg:133.26ms
step:1120/1395 train_time:147921ms step_avg:133.26ms
step:1121/1395 train_time:148059ms step_avg:133.27ms
step:1122/1395 train_time:148198ms step_avg:133.27ms
step:1123/1395 train_time:148338ms step_avg:133.28ms
step:1124/1395 train_time:148479ms step_avg:133.28ms
step:1125/1395 train_time:148617ms step_avg:133.29ms
step:1125/1395 val_loss:3.3629 train_time:148732ms step_avg:133.39ms
step:1126/1395 train_time:148762ms step_avg:133.30ms
step:1127/1395 train_time:148903ms step_avg:133.31ms
step:1128/1395 train_time:149043ms step_avg:133.31ms
step:1129/1395 train_time:149186ms step_avg:133.32ms
step:1130/1395 train_time:149325ms step_avg:133.33ms
step:1131/1395 train_time:149467ms step_avg:133.33ms
step:1132/1395 train_time:149606ms step_avg:133.34ms
step:1133/1395 train_time:149744ms step_avg:133.34ms
step:1134/1395 train_time:149886ms step_avg:133.35ms
step:1135/1395 train_time:150026ms step_avg:133.36ms
step:1136/1395 train_time:150173ms step_avg:133.37ms
step:1137/1395 train_time:150311ms step_avg:133.37ms
step:1138/1395 train_time:150455ms step_avg:133.38ms
step:1139/1395 train_time:150597ms step_avg:133.39ms
step:1140/1395 train_time:150739ms step_avg:133.40ms
step:1141/1395 train_time:150879ms step_avg:133.40ms
step:1142/1395 train_time:151021ms step_avg:133.41ms
step:1143/1395 train_time:151165ms step_avg:133.42ms
step:1144/1395 train_time:151307ms step_avg:133.43ms
step:1145/1395 train_time:151446ms step_avg:133.43ms
step:1146/1395 train_time:151589ms step_avg:133.44ms
step:1147/1395 train_time:151731ms step_avg:133.45ms
step:1148/1395 train_time:151872ms step_avg:133.46ms
step:1149/1395 train_time:152015ms step_avg:133.46ms
step:1150/1395 train_time:152155ms step_avg:133.47ms
step:1151/1395 train_time:152299ms step_avg:133.48ms
step:1152/1395 train_time:152440ms step_avg:133.49ms
step:1153/1395 train_time:152585ms step_avg:133.50ms
step:1154/1395 train_time:152726ms step_avg:133.50ms
step:1155/1395 train_time:152867ms step_avg:133.51ms
step:1156/1395 train_time:153014ms step_avg:133.52ms
step:1157/1395 train_time:153156ms step_avg:133.53ms
step:1158/1395 train_time:153296ms step_avg:133.53ms
step:1159/1395 train_time:153436ms step_avg:133.54ms
step:1160/1395 train_time:153577ms step_avg:133.55ms
step:1161/1395 train_time:153718ms step_avg:133.55ms
step:1162/1395 train_time:153860ms step_avg:133.56ms
step:1163/1395 train_time:154001ms step_avg:133.57ms
step:1164/1395 train_time:154144ms step_avg:133.57ms
step:1165/1395 train_time:154283ms step_avg:133.58ms
step:1166/1395 train_time:154425ms step_avg:133.59ms
step:1167/1395 train_time:154564ms step_avg:133.59ms
step:1168/1395 train_time:154705ms step_avg:133.60ms
step:1169/1395 train_time:154846ms step_avg:133.60ms
step:1170/1395 train_time:154986ms step_avg:133.61ms
step:1171/1395 train_time:155128ms step_avg:133.62ms
step:1172/1395 train_time:155269ms step_avg:133.62ms
step:1173/1395 train_time:155409ms step_avg:133.63ms
step:1174/1395 train_time:155561ms step_avg:133.64ms
step:1175/1395 train_time:155703ms step_avg:133.65ms
step:1176/1395 train_time:155846ms step_avg:133.66ms
step:1177/1395 train_time:155995ms step_avg:133.67ms
step:1178/1395 train_time:156135ms step_avg:133.68ms
step:1179/1395 train_time:156275ms step_avg:133.68ms
step:1180/1395 train_time:156424ms step_avg:133.70ms
step:1181/1395 train_time:156568ms step_avg:133.70ms
step:1182/1395 train_time:156708ms step_avg:133.71ms
step:1183/1395 train_time:156850ms step_avg:133.72ms
step:1184/1395 train_time:156991ms step_avg:133.72ms
step:1185/1395 train_time:157135ms step_avg:133.73ms
step:1186/1395 train_time:157275ms step_avg:133.74ms
step:1187/1395 train_time:157428ms step_avg:133.75ms
step:1188/1395 train_time:157567ms step_avg:133.76ms
step:1189/1395 train_time:157710ms step_avg:133.77ms
step:1190/1395 train_time:157851ms step_avg:133.77ms
step:1191/1395 train_time:157994ms step_avg:133.78ms
step:1192/1395 train_time:158134ms step_avg:133.79ms
step:1193/1395 train_time:158275ms step_avg:133.79ms
step:1194/1395 train_time:158415ms step_avg:133.80ms
step:1195/1395 train_time:158557ms step_avg:133.80ms
step:1196/1395 train_time:158698ms step_avg:133.81ms
step:1197/1395 train_time:158841ms step_avg:133.82ms
step:1198/1395 train_time:158991ms step_avg:133.83ms
step:1199/1395 train_time:159131ms step_avg:133.84ms
step:1200/1395 train_time:159270ms step_avg:133.84ms
step:1201/1395 train_time:159410ms step_avg:133.85ms
step:1202/1395 train_time:159565ms step_avg:133.86ms
step:1203/1395 train_time:159714ms step_avg:133.88ms
step:1204/1395 train_time:159858ms step_avg:133.88ms
step:1205/1395 train_time:160001ms step_avg:133.89ms
step:1206/1395 train_time:160143ms step_avg:133.90ms
step:1207/1395 train_time:160284ms step_avg:133.90ms
step:1208/1395 train_time:160428ms step_avg:133.91ms
step:1209/1395 train_time:160569ms step_avg:133.92ms
step:1210/1395 train_time:160714ms step_avg:133.93ms
step:1211/1395 train_time:160856ms step_avg:133.94ms
step:1212/1395 train_time:160998ms step_avg:133.94ms
step:1213/1395 train_time:161139ms step_avg:133.95ms
step:1214/1395 train_time:161282ms step_avg:133.95ms
step:1215/1395 train_time:161424ms step_avg:133.96ms
step:1216/1395 train_time:161562ms step_avg:133.97ms
step:1217/1395 train_time:161705ms step_avg:133.97ms
step:1218/1395 train_time:161843ms step_avg:133.98ms
step:1219/1395 train_time:161982ms step_avg:133.98ms
step:1220/1395 train_time:162123ms step_avg:133.99ms
step:1221/1395 train_time:162261ms step_avg:133.99ms
step:1222/1395 train_time:162401ms step_avg:133.99ms
step:1223/1395 train_time:162543ms step_avg:134.00ms
step:1224/1395 train_time:162685ms step_avg:134.01ms
step:1225/1395 train_time:162829ms step_avg:134.02ms
step:1226/1395 train_time:162970ms step_avg:134.02ms
step:1227/1395 train_time:163111ms step_avg:134.03ms
step:1228/1395 train_time:163252ms step_avg:134.03ms
step:1229/1395 train_time:163393ms step_avg:134.04ms
step:1230/1395 train_time:163539ms step_avg:134.05ms
step:1231/1395 train_time:163683ms step_avg:134.06ms
step:1232/1395 train_time:163828ms step_avg:134.07ms
step:1233/1395 train_time:163968ms step_avg:134.07ms
step:1234/1395 train_time:164109ms step_avg:134.08ms
step:1235/1395 train_time:164249ms step_avg:134.08ms
step:1236/1395 train_time:164392ms step_avg:134.09ms
step:1237/1395 train_time:164530ms step_avg:134.09ms
step:1238/1395 train_time:164682ms step_avg:134.11ms
step:1239/1395 train_time:164822ms step_avg:134.11ms
step:1240/1395 train_time:164965ms step_avg:134.12ms
step:1241/1395 train_time:165111ms step_avg:134.13ms
step:1242/1395 train_time:165249ms step_avg:134.13ms
step:1243/1395 train_time:165393ms step_avg:134.14ms
step:1244/1395 train_time:165534ms step_avg:134.14ms
step:1245/1395 train_time:165676ms step_avg:134.15ms
step:1246/1395 train_time:165816ms step_avg:134.15ms
step:1247/1395 train_time:165958ms step_avg:134.16ms
step:1248/1395 train_time:166098ms step_avg:134.17ms
step:1249/1395 train_time:166237ms step_avg:134.17ms
step:1250/1395 train_time:166378ms step_avg:134.18ms
step:1250/1395 val_loss:3.3166 train_time:166493ms step_avg:134.27ms
step:1251/1395 train_time:166526ms step_avg:134.19ms
step:1252/1395 train_time:166672ms step_avg:134.20ms
step:1253/1395 train_time:166813ms step_avg:134.20ms
step:1254/1395 train_time:166951ms step_avg:134.21ms
step:1255/1395 train_time:167105ms step_avg:134.22ms
step:1256/1395 train_time:167247ms step_avg:134.23ms
step:1257/1395 train_time:167388ms step_avg:134.23ms
step:1258/1395 train_time:167531ms step_avg:134.24ms
step:1259/1395 train_time:167675ms step_avg:134.25ms
step:1260/1395 train_time:167815ms step_avg:134.25ms
step:1261/1395 train_time:167955ms step_avg:134.26ms
step:1262/1395 train_time:168101ms step_avg:134.27ms
step:1263/1395 train_time:168244ms step_avg:134.27ms
step:1264/1395 train_time:168385ms step_avg:134.28ms
step:1265/1395 train_time:168526ms step_avg:134.28ms
step:1266/1395 train_time:168668ms step_avg:134.29ms
step:1267/1395 train_time:168810ms step_avg:134.30ms
step:1268/1395 train_time:168952ms step_avg:134.30ms
step:1269/1395 train_time:169099ms step_avg:134.31ms
step:1270/1395 train_time:169240ms step_avg:134.32ms
step:1271/1395 train_time:169383ms step_avg:134.32ms
step:1272/1395 train_time:169522ms step_avg:134.33ms
step:1273/1395 train_time:169663ms step_avg:134.33ms
step:1274/1395 train_time:169803ms step_avg:134.34ms
step:1275/1395 train_time:169947ms step_avg:134.35ms
step:1276/1395 train_time:170085ms step_avg:134.35ms
step:1277/1395 train_time:170227ms step_avg:134.35ms
step:1278/1395 train_time:170367ms step_avg:134.36ms
step:1279/1395 train_time:170508ms step_avg:134.36ms
step:1280/1395 train_time:170656ms step_avg:134.37ms
step:1281/1395 train_time:170797ms step_avg:134.38ms
step:1282/1395 train_time:170936ms step_avg:134.38ms
step:1283/1395 train_time:171078ms step_avg:134.39ms
step:1284/1395 train_time:171221ms step_avg:134.40ms
step:1285/1395 train_time:171362ms step_avg:134.40ms
step:1286/1395 train_time:171504ms step_avg:134.41ms
step:1287/1395 train_time:171647ms step_avg:134.41ms
step:1288/1395 train_time:171789ms step_avg:134.42ms
step:1289/1395 train_time:171938ms step_avg:134.43ms
step:1290/1395 train_time:172084ms step_avg:134.44ms
step:1291/1395 train_time:172231ms step_avg:134.45ms
step:1292/1395 train_time:172374ms step_avg:134.46ms
step:1293/1395 train_time:172521ms step_avg:134.47ms
step:1294/1395 train_time:172662ms step_avg:134.47ms
step:1295/1395 train_time:172805ms step_avg:134.48ms
step:1296/1395 train_time:172948ms step_avg:134.49ms
step:1297/1395 train_time:173092ms step_avg:134.49ms
step:1298/1395 train_time:173232ms step_avg:134.50ms
step:1299/1395 train_time:173374ms step_avg:134.50ms
step:1300/1395 train_time:173515ms step_avg:134.51ms
step:1301/1395 train_time:173654ms step_avg:134.51ms
step:1302/1395 train_time:173796ms step_avg:134.52ms
step:1303/1395 train_time:173942ms step_avg:134.53ms
step:1304/1395 train_time:174085ms step_avg:134.53ms
step:1305/1395 train_time:174226ms step_avg:134.54ms
step:1306/1395 train_time:174371ms step_avg:134.55ms
step:1307/1395 train_time:174512ms step_avg:134.55ms
step:1308/1395 train_time:174656ms step_avg:134.56ms
step:1309/1395 train_time:174801ms step_avg:134.57ms
step:1310/1395 train_time:174943ms step_avg:134.57ms
step:1311/1395 train_time:175083ms step_avg:134.58ms
step:1312/1395 train_time:175223ms step_avg:134.58ms
step:1313/1395 train_time:175365ms step_avg:134.59ms
step:1314/1395 train_time:175505ms step_avg:134.59ms
step:1315/1395 train_time:175651ms step_avg:134.60ms
step:1316/1395 train_time:175792ms step_avg:134.60ms
step:1317/1395 train_time:175933ms step_avg:134.61ms
step:1318/1395 train_time:176080ms step_avg:134.62ms
step:1319/1395 train_time:176224ms step_avg:134.62ms
step:1320/1395 train_time:176365ms step_avg:134.63ms
step:1321/1395 train_time:176506ms step_avg:134.63ms
step:1322/1395 train_time:176655ms step_avg:134.65ms
step:1323/1395 train_time:176798ms step_avg:134.65ms
step:1324/1395 train_time:176940ms step_avg:134.66ms
step:1325/1395 train_time:177084ms step_avg:134.66ms
step:1326/1395 train_time:177229ms step_avg:134.67ms
step:1327/1395 train_time:177370ms step_avg:134.68ms
step:1328/1395 train_time:177510ms step_avg:134.68ms
step:1329/1395 train_time:177667ms step_avg:134.70ms
step:1330/1395 train_time:177813ms step_avg:134.71ms
step:1331/1395 train_time:177959ms step_avg:134.72ms
step:1332/1395 train_time:178109ms step_avg:134.73ms
step:1333/1395 train_time:178253ms step_avg:134.73ms
step:1334/1395 train_time:178394ms step_avg:134.74ms
step:1335/1395 train_time:178532ms step_avg:134.74ms
step:1336/1395 train_time:178682ms step_avg:134.75ms
step:1337/1395 train_time:178827ms step_avg:134.76ms
step:1338/1395 train_time:178968ms step_avg:134.76ms
step:1339/1395 train_time:179112ms step_avg:134.77ms
step:1340/1395 train_time:179258ms step_avg:134.78ms
step:1341/1395 train_time:179400ms step_avg:134.79ms
step:1342/1395 train_time:179543ms step_avg:134.79ms
step:1343/1395 train_time:179684ms step_avg:134.80ms
step:1344/1395 train_time:179823ms step_avg:134.80ms
step:1345/1395 train_time:179965ms step_avg:134.81ms
step:1346/1395 train_time:180107ms step_avg:134.81ms
step:1347/1395 train_time:180251ms step_avg:134.82ms
step:1348/1395 train_time:180392ms step_avg:134.82ms
step:1349/1395 train_time:180535ms step_avg:134.83ms
step:1350/1395 train_time:180674ms step_avg:134.83ms
step:1351/1395 train_time:180816ms step_avg:134.84ms
step:1352/1395 train_time:180967ms step_avg:134.85ms
step:1353/1395 train_time:181114ms step_avg:134.86ms
step:1354/1395 train_time:181258ms step_avg:134.86ms
step:1355/1395 train_time:181399ms step_avg:134.87ms
step:1356/1395 train_time:181538ms step_avg:134.87ms
step:1357/1395 train_time:181682ms step_avg:134.88ms
step:1358/1395 train_time:181826ms step_avg:134.89ms
step:1359/1395 train_time:181969ms step_avg:134.89ms
step:1360/1395 train_time:182115ms step_avg:134.90ms
step:1361/1395 train_time:182260ms step_avg:134.91ms
step:1362/1395 train_time:182405ms step_avg:134.91ms
step:1363/1395 train_time:182553ms step_avg:134.92ms
step:1364/1395 train_time:182697ms step_avg:134.93ms
step:1365/1395 train_time:182836ms step_avg:134.93ms
step:1366/1395 train_time:182979ms step_avg:134.94ms
step:1367/1395 train_time:183123ms step_avg:134.95ms
step:1368/1395 train_time:183268ms step_avg:134.95ms
step:1369/1395 train_time:183418ms step_avg:134.97ms
step:1370/1395 train_time:183564ms step_avg:134.97ms
step:1371/1395 train_time:183708ms step_avg:134.98ms
step:1372/1395 train_time:183857ms step_avg:134.99ms
step:1373/1395 train_time:184000ms step_avg:135.00ms
step:1374/1395 train_time:184147ms step_avg:135.00ms
step:1375/1395 train_time:184287ms step_avg:135.01ms
step:1375/1395 val_loss:3.2821 train_time:184400ms step_avg:135.09ms
step:1376/1395 train_time:184431ms step_avg:135.02ms
step:1377/1395 train_time:184574ms step_avg:135.02ms
step:1378/1395 train_time:184716ms step_avg:135.03ms
step:1379/1395 train_time:184859ms step_avg:135.03ms
step:1380/1395 train_time:185002ms step_avg:135.04ms
step:1381/1395 train_time:185148ms step_avg:135.05ms
step:1382/1395 train_time:185292ms step_avg:135.05ms
step:1383/1395 train_time:185435ms step_avg:135.06ms
step:1384/1395 train_time:185583ms step_avg:135.07ms
step:1385/1395 train_time:185722ms step_avg:135.07ms
step:1386/1395 train_time:185863ms step_avg:135.08ms
step:1387/1395 train_time:186009ms step_avg:135.08ms
step:1388/1395 train_time:186149ms step_avg:135.09ms
step:1389/1395 train_time:186293ms step_avg:135.09ms
step:1390/1395 train_time:186435ms step_avg:135.10ms
step:1391/1395 train_time:186576ms step_avg:135.10ms
step:1392/1395 train_time:186720ms step_avg:135.11ms
step:1393/1395 train_time:186860ms step_avg:135.11ms
step:1394/1395 train_time:187003ms step_avg:135.12ms
step:1395/1395 train_time:187142ms step_avg:135.12ms
step:1395/1395 val_loss:3.2779 train_time:187260ms step_avg:135.21ms
peak memory allocated: 37620 MiB reserved: 39114 MiB
