import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)  # unet pattern attention scale by @leloykun

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Wed Jan 15 21:31:38 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             121W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             119W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   30C    P0             113W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27791ms step_avg:nanms
step:2/1375 train_time:27876ms step_avg:nanms
step:3/1375 train_time:28062ms step_avg:nanms
step:4/1375 train_time:28195ms step_avg:nanms
step:5/1375 train_time:28328ms step_avg:nanms
step:6/1375 train_time:28460ms step_avg:nanms
step:7/1375 train_time:28592ms step_avg:nanms
step:8/1375 train_time:28724ms step_avg:nanms
step:9/1375 train_time:28856ms step_avg:nanms
step:10/1375 train_time:28992ms step_avg:nanms
step:11/1375 train_time:135ms step_avg:nanms
step:12/1375 train_time:270ms step_avg:nanms
step:13/1375 train_time:402ms step_avg:134.04ms
step:14/1375 train_time:536ms step_avg:133.99ms
step:15/1375 train_time:668ms step_avg:133.70ms
step:16/1375 train_time:803ms step_avg:133.76ms
step:17/1375 train_time:936ms step_avg:133.77ms
step:18/1375 train_time:1070ms step_avg:133.81ms
step:19/1375 train_time:1205ms step_avg:133.88ms
step:20/1375 train_time:1341ms step_avg:134.10ms
step:21/1375 train_time:1475ms step_avg:134.13ms
step:22/1375 train_time:1608ms step_avg:134.02ms
step:23/1375 train_time:1742ms step_avg:133.99ms
step:24/1375 train_time:1877ms step_avg:134.09ms
step:25/1375 train_time:2010ms step_avg:133.98ms
step:26/1375 train_time:2144ms step_avg:134.02ms
step:27/1375 train_time:2279ms step_avg:134.08ms
step:28/1375 train_time:2414ms step_avg:134.09ms
step:29/1375 train_time:2546ms step_avg:133.99ms
step:30/1375 train_time:2680ms step_avg:134.02ms
step:31/1375 train_time:2814ms step_avg:134.00ms
step:32/1375 train_time:2947ms step_avg:133.95ms
step:33/1375 train_time:3082ms step_avg:134.01ms
step:34/1375 train_time:3217ms step_avg:134.04ms
step:35/1375 train_time:3350ms step_avg:134.01ms
step:36/1375 train_time:3484ms step_avg:134.00ms
step:37/1375 train_time:3617ms step_avg:133.98ms
step:38/1375 train_time:3751ms step_avg:133.98ms
step:39/1375 train_time:3886ms step_avg:133.98ms
step:40/1375 train_time:4020ms step_avg:134.01ms
step:41/1375 train_time:4155ms step_avg:134.03ms
step:42/1375 train_time:4288ms step_avg:134.01ms
step:43/1375 train_time:4423ms step_avg:134.03ms
step:44/1375 train_time:4557ms step_avg:134.03ms
step:45/1375 train_time:4691ms step_avg:134.02ms
step:46/1375 train_time:4824ms step_avg:134.01ms
step:47/1375 train_time:4961ms step_avg:134.07ms
step:48/1375 train_time:5094ms step_avg:134.05ms
step:49/1375 train_time:5227ms step_avg:134.02ms
step:50/1375 train_time:5362ms step_avg:134.05ms
step:51/1375 train_time:5497ms step_avg:134.07ms
step:52/1375 train_time:5629ms step_avg:134.02ms
step:53/1375 train_time:5763ms step_avg:134.02ms
step:54/1375 train_time:5898ms step_avg:134.04ms
step:55/1375 train_time:6032ms step_avg:134.05ms
step:56/1375 train_time:6165ms step_avg:134.03ms
step:57/1375 train_time:6300ms step_avg:134.04ms
step:58/1375 train_time:6433ms step_avg:134.02ms
step:59/1375 train_time:6568ms step_avg:134.04ms
step:60/1375 train_time:6702ms step_avg:134.05ms
step:61/1375 train_time:6836ms step_avg:134.04ms
step:62/1375 train_time:6968ms step_avg:134.01ms
step:63/1375 train_time:7103ms step_avg:134.02ms
step:64/1375 train_time:7236ms step_avg:134.00ms
step:65/1375 train_time:7369ms step_avg:133.99ms
step:66/1375 train_time:7503ms step_avg:133.99ms
step:67/1375 train_time:7637ms step_avg:133.98ms
step:68/1375 train_time:7771ms step_avg:133.98ms
step:69/1375 train_time:7904ms step_avg:133.97ms
step:70/1375 train_time:8039ms step_avg:133.98ms
step:71/1375 train_time:8172ms step_avg:133.97ms
step:72/1375 train_time:8307ms step_avg:133.98ms
step:73/1375 train_time:8441ms step_avg:133.99ms
step:74/1375 train_time:8575ms step_avg:133.98ms
step:75/1375 train_time:8708ms step_avg:133.97ms
step:76/1375 train_time:8843ms step_avg:133.99ms
step:77/1375 train_time:8977ms step_avg:133.98ms
step:78/1375 train_time:9111ms step_avg:133.98ms
step:79/1375 train_time:9243ms step_avg:133.96ms
step:80/1375 train_time:9378ms step_avg:133.98ms
step:81/1375 train_time:9514ms step_avg:134.00ms
step:82/1375 train_time:9647ms step_avg:133.99ms
step:83/1375 train_time:9781ms step_avg:133.98ms
step:84/1375 train_time:9914ms step_avg:133.98ms
step:85/1375 train_time:10048ms step_avg:133.97ms
step:86/1375 train_time:10183ms step_avg:133.99ms
step:87/1375 train_time:10318ms step_avg:134.00ms
step:88/1375 train_time:10453ms step_avg:134.02ms
step:89/1375 train_time:10586ms step_avg:134.00ms
step:90/1375 train_time:10719ms step_avg:133.99ms
step:91/1375 train_time:10853ms step_avg:133.99ms
step:92/1375 train_time:10988ms step_avg:133.99ms
step:93/1375 train_time:11124ms step_avg:134.02ms
step:94/1375 train_time:11261ms step_avg:134.05ms
step:95/1375 train_time:11394ms step_avg:134.05ms
step:96/1375 train_time:11529ms step_avg:134.05ms
step:97/1375 train_time:11662ms step_avg:134.05ms
step:98/1375 train_time:11796ms step_avg:134.05ms
step:99/1375 train_time:11930ms step_avg:134.05ms
step:100/1375 train_time:12065ms step_avg:134.06ms
step:101/1375 train_time:12200ms step_avg:134.06ms
step:102/1375 train_time:12334ms step_avg:134.06ms
step:103/1375 train_time:12468ms step_avg:134.07ms
step:104/1375 train_time:12607ms step_avg:134.12ms
step:105/1375 train_time:12744ms step_avg:134.15ms
step:106/1375 train_time:12880ms step_avg:134.17ms
step:107/1375 train_time:13016ms step_avg:134.19ms
step:108/1375 train_time:13156ms step_avg:134.24ms
step:109/1375 train_time:13292ms step_avg:134.26ms
step:110/1375 train_time:13429ms step_avg:134.29ms
step:111/1375 train_time:13567ms step_avg:134.33ms
step:112/1375 train_time:13705ms step_avg:134.36ms
step:113/1375 train_time:13841ms step_avg:134.38ms
step:114/1375 train_time:13979ms step_avg:134.41ms
step:115/1375 train_time:14115ms step_avg:134.43ms
step:116/1375 train_time:14253ms step_avg:134.46ms
step:117/1375 train_time:14389ms step_avg:134.47ms
step:118/1375 train_time:14526ms step_avg:134.50ms
step:119/1375 train_time:14664ms step_avg:134.53ms
step:120/1375 train_time:14801ms step_avg:134.55ms
step:121/1375 train_time:14938ms step_avg:134.57ms
step:122/1375 train_time:15074ms step_avg:134.59ms
step:123/1375 train_time:15211ms step_avg:134.61ms
step:124/1375 train_time:15349ms step_avg:134.64ms
step:125/1375 train_time:15486ms step_avg:134.66ms
step:125/1375 val_loss:4.3606 train_time:15553ms step_avg:135.24ms
step:126/1375 train_time:15629ms step_avg:134.73ms
step:127/1375 train_time:15768ms step_avg:134.77ms
step:128/1375 train_time:15906ms step_avg:134.80ms
step:129/1375 train_time:16041ms step_avg:134.80ms
step:130/1375 train_time:16178ms step_avg:134.82ms
step:131/1375 train_time:16314ms step_avg:134.83ms
step:132/1375 train_time:16451ms step_avg:134.84ms
step:133/1375 train_time:16590ms step_avg:134.88ms
step:134/1375 train_time:16729ms step_avg:134.91ms
step:135/1375 train_time:16866ms step_avg:134.93ms
step:136/1375 train_time:17004ms step_avg:134.95ms
step:137/1375 train_time:17142ms step_avg:134.98ms
step:138/1375 train_time:17279ms step_avg:135.00ms
step:139/1375 train_time:17416ms step_avg:135.01ms
step:140/1375 train_time:17552ms step_avg:135.02ms
step:141/1375 train_time:17692ms step_avg:135.05ms
step:142/1375 train_time:17829ms step_avg:135.07ms
step:143/1375 train_time:17966ms step_avg:135.09ms
step:144/1375 train_time:18103ms step_avg:135.10ms
step:145/1375 train_time:18243ms step_avg:135.13ms
step:146/1375 train_time:18380ms step_avg:135.14ms
step:147/1375 train_time:18516ms step_avg:135.15ms
step:148/1375 train_time:18653ms step_avg:135.17ms
step:149/1375 train_time:18791ms step_avg:135.19ms
step:150/1375 train_time:18929ms step_avg:135.21ms
step:151/1375 train_time:19068ms step_avg:135.23ms
step:152/1375 train_time:19206ms step_avg:135.25ms
step:153/1375 train_time:19344ms step_avg:135.27ms
step:154/1375 train_time:19481ms step_avg:135.29ms
step:155/1375 train_time:19617ms step_avg:135.29ms
step:156/1375 train_time:19754ms step_avg:135.30ms
step:157/1375 train_time:19891ms step_avg:135.31ms
step:158/1375 train_time:20029ms step_avg:135.33ms
step:159/1375 train_time:20167ms step_avg:135.35ms
step:160/1375 train_time:20306ms step_avg:135.37ms
step:161/1375 train_time:20443ms step_avg:135.39ms
step:162/1375 train_time:20581ms step_avg:135.40ms
step:163/1375 train_time:20717ms step_avg:135.41ms
step:164/1375 train_time:20855ms step_avg:135.42ms
step:165/1375 train_time:20992ms step_avg:135.43ms
step:166/1375 train_time:21131ms step_avg:135.46ms
step:167/1375 train_time:21269ms step_avg:135.47ms
step:168/1375 train_time:21406ms step_avg:135.48ms
step:169/1375 train_time:21543ms step_avg:135.49ms
step:170/1375 train_time:21681ms step_avg:135.51ms
step:171/1375 train_time:21817ms step_avg:135.51ms
step:172/1375 train_time:21955ms step_avg:135.53ms
step:173/1375 train_time:22092ms step_avg:135.54ms
step:174/1375 train_time:22231ms step_avg:135.55ms
step:175/1375 train_time:22370ms step_avg:135.58ms
step:176/1375 train_time:22508ms step_avg:135.59ms
step:177/1375 train_time:22648ms step_avg:135.62ms
step:178/1375 train_time:22786ms step_avg:135.63ms
step:179/1375 train_time:22923ms step_avg:135.64ms
step:180/1375 train_time:23060ms step_avg:135.65ms
step:181/1375 train_time:23196ms step_avg:135.65ms
step:182/1375 train_time:23334ms step_avg:135.67ms
step:183/1375 train_time:23472ms step_avg:135.68ms
step:184/1375 train_time:23610ms step_avg:135.69ms
step:185/1375 train_time:23748ms step_avg:135.70ms
step:186/1375 train_time:23886ms step_avg:135.71ms
step:187/1375 train_time:24023ms step_avg:135.72ms
step:188/1375 train_time:24161ms step_avg:135.74ms
step:189/1375 train_time:24301ms step_avg:135.76ms
step:190/1375 train_time:24439ms step_avg:135.77ms
step:191/1375 train_time:24629ms step_avg:136.07ms
step:192/1375 train_time:24765ms step_avg:136.07ms
step:193/1375 train_time:24901ms step_avg:136.07ms
step:194/1375 train_time:25038ms step_avg:136.07ms
step:195/1375 train_time:25174ms step_avg:136.07ms
step:196/1375 train_time:25310ms step_avg:136.07ms
step:197/1375 train_time:25446ms step_avg:136.08ms
step:198/1375 train_time:25590ms step_avg:136.12ms
step:199/1375 train_time:25728ms step_avg:136.12ms
step:200/1375 train_time:25866ms step_avg:136.14ms
step:201/1375 train_time:26002ms step_avg:136.14ms
step:202/1375 train_time:26139ms step_avg:136.14ms
step:203/1375 train_time:26276ms step_avg:136.14ms
step:204/1375 train_time:26412ms step_avg:136.15ms
step:205/1375 train_time:26552ms step_avg:136.16ms
step:206/1375 train_time:26693ms step_avg:136.19ms
step:207/1375 train_time:26834ms step_avg:136.21ms
step:208/1375 train_time:26975ms step_avg:136.24ms
step:209/1375 train_time:27115ms step_avg:136.26ms
step:210/1375 train_time:27255ms step_avg:136.27ms
step:211/1375 train_time:27394ms step_avg:136.29ms
step:212/1375 train_time:27535ms step_avg:136.31ms
step:213/1375 train_time:27674ms step_avg:136.32ms
step:214/1375 train_time:27815ms step_avg:136.35ms
step:215/1375 train_time:27957ms step_avg:136.37ms
step:216/1375 train_time:28096ms step_avg:136.39ms
step:217/1375 train_time:28235ms step_avg:136.40ms
step:218/1375 train_time:28375ms step_avg:136.42ms
step:219/1375 train_time:28515ms step_avg:136.44ms
step:220/1375 train_time:28656ms step_avg:136.46ms
step:221/1375 train_time:28796ms step_avg:136.47ms
step:222/1375 train_time:28936ms step_avg:136.49ms
step:223/1375 train_time:29076ms step_avg:136.51ms
step:224/1375 train_time:29215ms step_avg:136.52ms
step:225/1375 train_time:29357ms step_avg:136.54ms
step:226/1375 train_time:29496ms step_avg:136.56ms
step:227/1375 train_time:29636ms step_avg:136.57ms
step:228/1375 train_time:29775ms step_avg:136.58ms
step:229/1375 train_time:29915ms step_avg:136.60ms
step:230/1375 train_time:30056ms step_avg:136.62ms
step:231/1375 train_time:30196ms step_avg:136.63ms
step:232/1375 train_time:30337ms step_avg:136.65ms
step:233/1375 train_time:30476ms step_avg:136.66ms
step:234/1375 train_time:30616ms step_avg:136.68ms
step:235/1375 train_time:30757ms step_avg:136.70ms
step:236/1375 train_time:30897ms step_avg:136.71ms
step:237/1375 train_time:31038ms step_avg:136.73ms
step:238/1375 train_time:31178ms step_avg:136.74ms
step:239/1375 train_time:31318ms step_avg:136.76ms
step:240/1375 train_time:31459ms step_avg:136.78ms
step:241/1375 train_time:31599ms step_avg:136.79ms
step:242/1375 train_time:31739ms step_avg:136.81ms
step:243/1375 train_time:31879ms step_avg:136.82ms
step:244/1375 train_time:32019ms step_avg:136.83ms
step:245/1375 train_time:32158ms step_avg:136.84ms
step:246/1375 train_time:32299ms step_avg:136.86ms
step:247/1375 train_time:32437ms step_avg:136.87ms
step:248/1375 train_time:32579ms step_avg:136.89ms
step:249/1375 train_time:32720ms step_avg:136.90ms
step:250/1375 train_time:32860ms step_avg:136.92ms
step:250/1375 val_loss:3.9534 train_time:32927ms step_avg:137.20ms
step:251/1375 train_time:33005ms step_avg:136.95ms
step:252/1375 train_time:33147ms step_avg:136.97ms
step:253/1375 train_time:33288ms step_avg:136.99ms
step:254/1375 train_time:33428ms step_avg:137.00ms
step:255/1375 train_time:33567ms step_avg:137.01ms
step:256/1375 train_time:33706ms step_avg:137.02ms
step:257/1375 train_time:33846ms step_avg:137.03ms
step:258/1375 train_time:33987ms step_avg:137.05ms
step:259/1375 train_time:34129ms step_avg:137.06ms
step:260/1375 train_time:34270ms step_avg:137.08ms
step:261/1375 train_time:34410ms step_avg:137.09ms
step:262/1375 train_time:34549ms step_avg:137.10ms
step:263/1375 train_time:34688ms step_avg:137.11ms
step:264/1375 train_time:34828ms step_avg:137.12ms
step:265/1375 train_time:34969ms step_avg:137.13ms
step:266/1375 train_time:35110ms step_avg:137.15ms
step:267/1375 train_time:35250ms step_avg:137.16ms
step:268/1375 train_time:35390ms step_avg:137.17ms
step:269/1375 train_time:35529ms step_avg:137.18ms
step:270/1375 train_time:35668ms step_avg:137.19ms
step:271/1375 train_time:35808ms step_avg:137.20ms
step:272/1375 train_time:35948ms step_avg:137.21ms
step:273/1375 train_time:36089ms step_avg:137.22ms
step:274/1375 train_time:36229ms step_avg:137.23ms
step:275/1375 train_time:36369ms step_avg:137.24ms
step:276/1375 train_time:36510ms step_avg:137.25ms
step:277/1375 train_time:36650ms step_avg:137.27ms
step:278/1375 train_time:36790ms step_avg:137.28ms
step:279/1375 train_time:36931ms step_avg:137.29ms
step:280/1375 train_time:37071ms step_avg:137.30ms
step:281/1375 train_time:37211ms step_avg:137.31ms
step:282/1375 train_time:37352ms step_avg:137.32ms
step:283/1375 train_time:37492ms step_avg:137.33ms
step:284/1375 train_time:37633ms step_avg:137.35ms
step:285/1375 train_time:37773ms step_avg:137.36ms
step:286/1375 train_time:37913ms step_avg:137.37ms
step:287/1375 train_time:38054ms step_avg:137.38ms
step:288/1375 train_time:38194ms step_avg:137.39ms
step:289/1375 train_time:38336ms step_avg:137.40ms
step:290/1375 train_time:38477ms step_avg:137.42ms
step:291/1375 train_time:38617ms step_avg:137.43ms
step:292/1375 train_time:38757ms step_avg:137.44ms
step:293/1375 train_time:38897ms step_avg:137.44ms
step:294/1375 train_time:39037ms step_avg:137.45ms
step:295/1375 train_time:39176ms step_avg:137.46ms
step:296/1375 train_time:39317ms step_avg:137.47ms
step:297/1375 train_time:39456ms step_avg:137.48ms
step:298/1375 train_time:39597ms step_avg:137.49ms
step:299/1375 train_time:39738ms step_avg:137.50ms
step:300/1375 train_time:39878ms step_avg:137.51ms
step:301/1375 train_time:40018ms step_avg:137.52ms
step:302/1375 train_time:40157ms step_avg:137.53ms
step:303/1375 train_time:40297ms step_avg:137.53ms
step:304/1375 train_time:40438ms step_avg:137.54ms
step:305/1375 train_time:40578ms step_avg:137.55ms
step:306/1375 train_time:40718ms step_avg:137.56ms
step:307/1375 train_time:40858ms step_avg:137.57ms
step:308/1375 train_time:41001ms step_avg:137.59ms
step:309/1375 train_time:41143ms step_avg:137.60ms
step:310/1375 train_time:41286ms step_avg:137.62ms
step:311/1375 train_time:41428ms step_avg:137.63ms
step:312/1375 train_time:41571ms step_avg:137.65ms
step:313/1375 train_time:41713ms step_avg:137.67ms
step:314/1375 train_time:41855ms step_avg:137.68ms
step:315/1375 train_time:41997ms step_avg:137.70ms
step:316/1375 train_time:42139ms step_avg:137.71ms
step:317/1375 train_time:42280ms step_avg:137.72ms
step:318/1375 train_time:42422ms step_avg:137.74ms
step:319/1375 train_time:42565ms step_avg:137.75ms
step:320/1375 train_time:42708ms step_avg:137.77ms
step:321/1375 train_time:42850ms step_avg:137.78ms
step:322/1375 train_time:42992ms step_avg:137.79ms
step:323/1375 train_time:43134ms step_avg:137.81ms
step:324/1375 train_time:43276ms step_avg:137.82ms
step:325/1375 train_time:43418ms step_avg:137.84ms
step:326/1375 train_time:43561ms step_avg:137.85ms
step:327/1375 train_time:43703ms step_avg:137.86ms
step:328/1375 train_time:43845ms step_avg:137.88ms
step:329/1375 train_time:43988ms step_avg:137.89ms
step:330/1375 train_time:44129ms step_avg:137.90ms
step:331/1375 train_time:44272ms step_avg:137.92ms
step:332/1375 train_time:44413ms step_avg:137.93ms
step:333/1375 train_time:44556ms step_avg:137.94ms
step:334/1375 train_time:44698ms step_avg:137.96ms
step:335/1375 train_time:44840ms step_avg:137.97ms
step:336/1375 train_time:44981ms step_avg:137.98ms
step:337/1375 train_time:45125ms step_avg:138.00ms
step:338/1375 train_time:45267ms step_avg:138.01ms
step:339/1375 train_time:45410ms step_avg:138.02ms
step:340/1375 train_time:45552ms step_avg:138.04ms
step:341/1375 train_time:45694ms step_avg:138.05ms
step:342/1375 train_time:45837ms step_avg:138.06ms
step:343/1375 train_time:45978ms step_avg:138.07ms
step:344/1375 train_time:46120ms step_avg:138.08ms
step:345/1375 train_time:46261ms step_avg:138.09ms
step:346/1375 train_time:46404ms step_avg:138.11ms
step:347/1375 train_time:46545ms step_avg:138.12ms
step:348/1375 train_time:46687ms step_avg:138.13ms
step:349/1375 train_time:46829ms step_avg:138.14ms
step:350/1375 train_time:46972ms step_avg:138.15ms
step:351/1375 train_time:47114ms step_avg:138.16ms
step:352/1375 train_time:47255ms step_avg:138.17ms
step:353/1375 train_time:47397ms step_avg:138.18ms
step:354/1375 train_time:47540ms step_avg:138.20ms
step:355/1375 train_time:47683ms step_avg:138.21ms
step:356/1375 train_time:47824ms step_avg:138.22ms
step:357/1375 train_time:47967ms step_avg:138.23ms
step:358/1375 train_time:48110ms step_avg:138.25ms
step:359/1375 train_time:48252ms step_avg:138.26ms
step:360/1375 train_time:48395ms step_avg:138.27ms
step:361/1375 train_time:48537ms step_avg:138.28ms
step:362/1375 train_time:48679ms step_avg:138.29ms
step:363/1375 train_time:48821ms step_avg:138.30ms
step:364/1375 train_time:48964ms step_avg:138.32ms
step:365/1375 train_time:49105ms step_avg:138.33ms
step:366/1375 train_time:49248ms step_avg:138.34ms
step:367/1375 train_time:49390ms step_avg:138.35ms
step:368/1375 train_time:49532ms step_avg:138.36ms
step:369/1375 train_time:49675ms step_avg:138.37ms
step:370/1375 train_time:49816ms step_avg:138.38ms
step:371/1375 train_time:49959ms step_avg:138.39ms
step:372/1375 train_time:50101ms step_avg:138.40ms
step:373/1375 train_time:50243ms step_avg:138.41ms
step:374/1375 train_time:50384ms step_avg:138.42ms
step:375/1375 train_time:50527ms step_avg:138.43ms
step:375/1375 val_loss:3.7680 train_time:50596ms step_avg:138.62ms
step:376/1375 train_time:50674ms step_avg:138.45ms
step:377/1375 train_time:50818ms step_avg:138.47ms
step:378/1375 train_time:50959ms step_avg:138.47ms
step:379/1375 train_time:51100ms step_avg:138.48ms
step:380/1375 train_time:51241ms step_avg:138.49ms
step:381/1375 train_time:51423ms step_avg:138.61ms
step:382/1375 train_time:51564ms step_avg:138.61ms
step:383/1375 train_time:51705ms step_avg:138.62ms
step:384/1375 train_time:51846ms step_avg:138.63ms
step:385/1375 train_time:51986ms step_avg:138.63ms
step:386/1375 train_time:52127ms step_avg:138.64ms
step:387/1375 train_time:52271ms step_avg:138.65ms
step:388/1375 train_time:52416ms step_avg:138.67ms
step:389/1375 train_time:52559ms step_avg:138.68ms
step:390/1375 train_time:52700ms step_avg:138.68ms
step:391/1375 train_time:52841ms step_avg:138.69ms
step:392/1375 train_time:52983ms step_avg:138.70ms
step:393/1375 train_time:53124ms step_avg:138.70ms
step:394/1375 train_time:53265ms step_avg:138.71ms
step:395/1375 train_time:53407ms step_avg:138.72ms
step:396/1375 train_time:53550ms step_avg:138.73ms
step:397/1375 train_time:53692ms step_avg:138.74ms
step:398/1375 train_time:53835ms step_avg:138.75ms
step:399/1375 train_time:53977ms step_avg:138.76ms
step:400/1375 train_time:54118ms step_avg:138.76ms
step:401/1375 train_time:54260ms step_avg:138.77ms
step:402/1375 train_time:54402ms step_avg:138.78ms
step:403/1375 train_time:54545ms step_avg:138.79ms
step:404/1375 train_time:54687ms step_avg:138.80ms
step:405/1375 train_time:54828ms step_avg:138.81ms
step:406/1375 train_time:54971ms step_avg:138.82ms
step:407/1375 train_time:55114ms step_avg:138.83ms
step:408/1375 train_time:55255ms step_avg:138.83ms
step:409/1375 train_time:55398ms step_avg:138.84ms
step:410/1375 train_time:55541ms step_avg:138.85ms
step:411/1375 train_time:55685ms step_avg:138.87ms
step:412/1375 train_time:55828ms step_avg:138.87ms
step:413/1375 train_time:55972ms step_avg:138.89ms
step:414/1375 train_time:56117ms step_avg:138.90ms
step:415/1375 train_time:56260ms step_avg:138.91ms
step:416/1375 train_time:56403ms step_avg:138.92ms
step:417/1375 train_time:56546ms step_avg:138.93ms
step:418/1375 train_time:56691ms step_avg:138.95ms
step:419/1375 train_time:56836ms step_avg:138.96ms
step:420/1375 train_time:56980ms step_avg:138.98ms
step:421/1375 train_time:57123ms step_avg:138.99ms
step:422/1375 train_time:57266ms step_avg:138.99ms
step:423/1375 train_time:57409ms step_avg:139.00ms
step:424/1375 train_time:57554ms step_avg:139.02ms
step:425/1375 train_time:57699ms step_avg:139.03ms
step:426/1375 train_time:57843ms step_avg:139.05ms
step:427/1375 train_time:57985ms step_avg:139.05ms
step:428/1375 train_time:58130ms step_avg:139.07ms
step:429/1375 train_time:58273ms step_avg:139.08ms
step:430/1375 train_time:58418ms step_avg:139.09ms
step:431/1375 train_time:58561ms step_avg:139.10ms
step:432/1375 train_time:58704ms step_avg:139.11ms
step:433/1375 train_time:58847ms step_avg:139.12ms
step:434/1375 train_time:58990ms step_avg:139.13ms
step:435/1375 train_time:59135ms step_avg:139.14ms
step:436/1375 train_time:59279ms step_avg:139.15ms
step:437/1375 train_time:59423ms step_avg:139.16ms
step:438/1375 train_time:59565ms step_avg:139.17ms
step:439/1375 train_time:59708ms step_avg:139.18ms
step:440/1375 train_time:59852ms step_avg:139.19ms
step:441/1375 train_time:59996ms step_avg:139.20ms
step:442/1375 train_time:60140ms step_avg:139.21ms
step:443/1375 train_time:60283ms step_avg:139.22ms
step:444/1375 train_time:60427ms step_avg:139.23ms
step:445/1375 train_time:60571ms step_avg:139.24ms
step:446/1375 train_time:60715ms step_avg:139.26ms
step:447/1375 train_time:60859ms step_avg:139.26ms
step:448/1375 train_time:61002ms step_avg:139.28ms
step:449/1375 train_time:61145ms step_avg:139.28ms
step:450/1375 train_time:61289ms step_avg:139.29ms
step:451/1375 train_time:61433ms step_avg:139.30ms
step:452/1375 train_time:61578ms step_avg:139.32ms
step:453/1375 train_time:61721ms step_avg:139.33ms
step:454/1375 train_time:61864ms step_avg:139.33ms
step:455/1375 train_time:62007ms step_avg:139.34ms
step:456/1375 train_time:62150ms step_avg:139.35ms
step:457/1375 train_time:62294ms step_avg:139.36ms
step:458/1375 train_time:62438ms step_avg:139.37ms
step:459/1375 train_time:62581ms step_avg:139.38ms
step:460/1375 train_time:62724ms step_avg:139.39ms
step:461/1375 train_time:62868ms step_avg:139.40ms
step:462/1375 train_time:63012ms step_avg:139.41ms
step:463/1375 train_time:63157ms step_avg:139.42ms
step:464/1375 train_time:63301ms step_avg:139.43ms
step:465/1375 train_time:63443ms step_avg:139.44ms
step:466/1375 train_time:63586ms step_avg:139.44ms
step:467/1375 train_time:63730ms step_avg:139.45ms
step:468/1375 train_time:63875ms step_avg:139.47ms
step:469/1375 train_time:64020ms step_avg:139.48ms
step:470/1375 train_time:64163ms step_avg:139.48ms
step:471/1375 train_time:64306ms step_avg:139.49ms
step:472/1375 train_time:64451ms step_avg:139.50ms
step:473/1375 train_time:64595ms step_avg:139.51ms
step:474/1375 train_time:64739ms step_avg:139.52ms
step:475/1375 train_time:64882ms step_avg:139.53ms
step:476/1375 train_time:65024ms step_avg:139.54ms
step:477/1375 train_time:65170ms step_avg:139.55ms
step:478/1375 train_time:65314ms step_avg:139.56ms
step:479/1375 train_time:65458ms step_avg:139.57ms
step:480/1375 train_time:65601ms step_avg:139.58ms
step:481/1375 train_time:65744ms step_avg:139.58ms
step:482/1375 train_time:65889ms step_avg:139.59ms
step:483/1375 train_time:66033ms step_avg:139.60ms
step:484/1375 train_time:66178ms step_avg:139.62ms
step:485/1375 train_time:66322ms step_avg:139.63ms
step:486/1375 train_time:66467ms step_avg:139.64ms
step:487/1375 train_time:66611ms step_avg:139.65ms
step:488/1375 train_time:66755ms step_avg:139.65ms
step:489/1375 train_time:66899ms step_avg:139.66ms
step:490/1375 train_time:67042ms step_avg:139.67ms
step:491/1375 train_time:67185ms step_avg:139.68ms
step:492/1375 train_time:67328ms step_avg:139.68ms
step:493/1375 train_time:67472ms step_avg:139.69ms
step:494/1375 train_time:67617ms step_avg:139.70ms
step:495/1375 train_time:67760ms step_avg:139.71ms
step:496/1375 train_time:67904ms step_avg:139.72ms
step:497/1375 train_time:68047ms step_avg:139.73ms
step:498/1375 train_time:68190ms step_avg:139.73ms
step:499/1375 train_time:68333ms step_avg:139.74ms
step:500/1375 train_time:68479ms step_avg:139.75ms
step:500/1375 val_loss:3.6527 train_time:68547ms step_avg:139.89ms
step:501/1375 train_time:68624ms step_avg:139.76ms
step:502/1375 train_time:68770ms step_avg:139.78ms
step:503/1375 train_time:68914ms step_avg:139.79ms
step:504/1375 train_time:69057ms step_avg:139.79ms
step:505/1375 train_time:69201ms step_avg:139.80ms
step:506/1375 train_time:69341ms step_avg:139.80ms
step:507/1375 train_time:69485ms step_avg:139.81ms
step:508/1375 train_time:69631ms step_avg:139.82ms
step:509/1375 train_time:69776ms step_avg:139.83ms
step:510/1375 train_time:69918ms step_avg:139.84ms
step:511/1375 train_time:70062ms step_avg:139.84ms
step:512/1375 train_time:70208ms step_avg:139.86ms
step:513/1375 train_time:70353ms step_avg:139.87ms
step:514/1375 train_time:70499ms step_avg:139.88ms
step:515/1375 train_time:70644ms step_avg:139.89ms
step:516/1375 train_time:70791ms step_avg:139.90ms
step:517/1375 train_time:70937ms step_avg:139.92ms
step:518/1375 train_time:71082ms step_avg:139.92ms
step:519/1375 train_time:71227ms step_avg:139.94ms
step:520/1375 train_time:71374ms step_avg:139.95ms
step:521/1375 train_time:71518ms step_avg:139.96ms
step:522/1375 train_time:71662ms step_avg:139.96ms
step:523/1375 train_time:71807ms step_avg:139.97ms
step:524/1375 train_time:71954ms step_avg:139.99ms
step:525/1375 train_time:72099ms step_avg:140.00ms
step:526/1375 train_time:72245ms step_avg:140.01ms
step:527/1375 train_time:72391ms step_avg:140.02ms
step:528/1375 train_time:72536ms step_avg:140.03ms
step:529/1375 train_time:72679ms step_avg:140.04ms
step:530/1375 train_time:72825ms step_avg:140.05ms
step:531/1375 train_time:72972ms step_avg:140.06ms
step:532/1375 train_time:73117ms step_avg:140.07ms
step:533/1375 train_time:73263ms step_avg:140.08ms
step:534/1375 train_time:73409ms step_avg:140.09ms
step:535/1375 train_time:73555ms step_avg:140.10ms
step:536/1375 train_time:73700ms step_avg:140.11ms
step:537/1375 train_time:73844ms step_avg:140.12ms
step:538/1375 train_time:73990ms step_avg:140.13ms
step:539/1375 train_time:74137ms step_avg:140.14ms
step:540/1375 train_time:74284ms step_avg:140.16ms
step:541/1375 train_time:74429ms step_avg:140.17ms
step:542/1375 train_time:74575ms step_avg:140.18ms
step:543/1375 train_time:74719ms step_avg:140.19ms
step:544/1375 train_time:74863ms step_avg:140.19ms
step:545/1375 train_time:75010ms step_avg:140.21ms
step:546/1375 train_time:75156ms step_avg:140.22ms
step:547/1375 train_time:75300ms step_avg:140.22ms
step:548/1375 train_time:75446ms step_avg:140.23ms
step:549/1375 train_time:75592ms step_avg:140.24ms
step:550/1375 train_time:75737ms step_avg:140.25ms
step:551/1375 train_time:75882ms step_avg:140.26ms
step:552/1375 train_time:76028ms step_avg:140.27ms
step:553/1375 train_time:76174ms step_avg:140.28ms
step:554/1375 train_time:76318ms step_avg:140.29ms
step:555/1375 train_time:76464ms step_avg:140.30ms
step:556/1375 train_time:76610ms step_avg:140.31ms
step:557/1375 train_time:76756ms step_avg:140.32ms
step:558/1375 train_time:76900ms step_avg:140.33ms
step:559/1375 train_time:77044ms step_avg:140.34ms
step:560/1375 train_time:77189ms step_avg:140.34ms
step:561/1375 train_time:77335ms step_avg:140.35ms
step:562/1375 train_time:77480ms step_avg:140.36ms
step:563/1375 train_time:77624ms step_avg:140.37ms
step:564/1375 train_time:77770ms step_avg:140.38ms
step:565/1375 train_time:77916ms step_avg:140.39ms
step:566/1375 train_time:78060ms step_avg:140.40ms
step:567/1375 train_time:78205ms step_avg:140.40ms
step:568/1375 train_time:78352ms step_avg:140.42ms
step:569/1375 train_time:78496ms step_avg:140.42ms
step:570/1375 train_time:78642ms step_avg:140.43ms
step:571/1375 train_time:78836ms step_avg:140.53ms
step:572/1375 train_time:78980ms step_avg:140.53ms
step:573/1375 train_time:79125ms step_avg:140.54ms
step:574/1375 train_time:79272ms step_avg:140.55ms
step:575/1375 train_time:79416ms step_avg:140.56ms
step:576/1375 train_time:79559ms step_avg:140.56ms
step:577/1375 train_time:79703ms step_avg:140.57ms
step:578/1375 train_time:79851ms step_avg:140.58ms
step:579/1375 train_time:79996ms step_avg:140.59ms
step:580/1375 train_time:80141ms step_avg:140.60ms
step:581/1375 train_time:80285ms step_avg:140.60ms
step:582/1375 train_time:80431ms step_avg:140.61ms
step:583/1375 train_time:80575ms step_avg:140.62ms
step:584/1375 train_time:80721ms step_avg:140.63ms
step:585/1375 train_time:80867ms step_avg:140.64ms
step:586/1375 train_time:81014ms step_avg:140.65ms
step:587/1375 train_time:81158ms step_avg:140.66ms
step:588/1375 train_time:81303ms step_avg:140.66ms
step:589/1375 train_time:81448ms step_avg:140.67ms
step:590/1375 train_time:81593ms step_avg:140.68ms
step:591/1375 train_time:81737ms step_avg:140.68ms
step:592/1375 train_time:81883ms step_avg:140.69ms
step:593/1375 train_time:82031ms step_avg:140.71ms
step:594/1375 train_time:82177ms step_avg:140.71ms
step:595/1375 train_time:82323ms step_avg:140.72ms
step:596/1375 train_time:82472ms step_avg:140.74ms
step:597/1375 train_time:82617ms step_avg:140.74ms
step:598/1375 train_time:82760ms step_avg:140.75ms
step:599/1375 train_time:82906ms step_avg:140.76ms
step:600/1375 train_time:83052ms step_avg:140.77ms
step:601/1375 train_time:83197ms step_avg:140.77ms
step:602/1375 train_time:83341ms step_avg:140.78ms
step:603/1375 train_time:83488ms step_avg:140.79ms
step:604/1375 train_time:83634ms step_avg:140.80ms
step:605/1375 train_time:83779ms step_avg:140.80ms
step:606/1375 train_time:83924ms step_avg:140.81ms
step:607/1375 train_time:84071ms step_avg:140.82ms
step:608/1375 train_time:84216ms step_avg:140.83ms
step:609/1375 train_time:84362ms step_avg:140.84ms
step:610/1375 train_time:84507ms step_avg:140.84ms
step:611/1375 train_time:84653ms step_avg:140.85ms
step:612/1375 train_time:84797ms step_avg:140.86ms
step:613/1375 train_time:84943ms step_avg:140.87ms
step:614/1375 train_time:85091ms step_avg:140.88ms
step:615/1375 train_time:85237ms step_avg:140.89ms
step:616/1375 train_time:85383ms step_avg:140.90ms
step:617/1375 train_time:85530ms step_avg:140.91ms
step:618/1375 train_time:85676ms step_avg:140.92ms
step:619/1375 train_time:85823ms step_avg:140.92ms
step:620/1375 train_time:85970ms step_avg:140.93ms
step:621/1375 train_time:86116ms step_avg:140.94ms
step:622/1375 train_time:86262ms step_avg:140.95ms
step:623/1375 train_time:86408ms step_avg:140.96ms
step:624/1375 train_time:86555ms step_avg:140.97ms
step:625/1375 train_time:86702ms step_avg:140.98ms
step:625/1375 val_loss:3.5721 train_time:86774ms step_avg:141.10ms
step:626/1375 train_time:86851ms step_avg:140.99ms
step:627/1375 train_time:86999ms step_avg:141.00ms
step:628/1375 train_time:87146ms step_avg:141.01ms
step:629/1375 train_time:87294ms step_avg:141.02ms
step:630/1375 train_time:87439ms step_avg:141.03ms
step:631/1375 train_time:87584ms step_avg:141.04ms
step:632/1375 train_time:87733ms step_avg:141.05ms
step:633/1375 train_time:87880ms step_avg:141.06ms
step:634/1375 train_time:88028ms step_avg:141.07ms
step:635/1375 train_time:88177ms step_avg:141.08ms
step:636/1375 train_time:88324ms step_avg:141.09ms
step:637/1375 train_time:88470ms step_avg:141.10ms
step:638/1375 train_time:88616ms step_avg:141.11ms
step:639/1375 train_time:88761ms step_avg:141.11ms
step:640/1375 train_time:88908ms step_avg:141.12ms
step:641/1375 train_time:89056ms step_avg:141.14ms
step:642/1375 train_time:89201ms step_avg:141.14ms
step:643/1375 train_time:89348ms step_avg:141.15ms
step:644/1375 train_time:89496ms step_avg:141.16ms
step:645/1375 train_time:89642ms step_avg:141.17ms
step:646/1375 train_time:89788ms step_avg:141.18ms
step:647/1375 train_time:89935ms step_avg:141.19ms
step:648/1375 train_time:90083ms step_avg:141.20ms
step:649/1375 train_time:90232ms step_avg:141.21ms
step:650/1375 train_time:90380ms step_avg:141.22ms
step:651/1375 train_time:90525ms step_avg:141.22ms
step:652/1375 train_time:90673ms step_avg:141.24ms
step:653/1375 train_time:90819ms step_avg:141.24ms
step:654/1375 train_time:90965ms step_avg:141.25ms
step:655/1375 train_time:91112ms step_avg:141.26ms
step:656/1375 train_time:91259ms step_avg:141.27ms
step:657/1375 train_time:91406ms step_avg:141.28ms
step:658/1375 train_time:91554ms step_avg:141.29ms
step:659/1375 train_time:91700ms step_avg:141.29ms
step:660/1375 train_time:91847ms step_avg:141.30ms
step:661/1375 train_time:91995ms step_avg:141.31ms
step:662/1375 train_time:92140ms step_avg:141.32ms
step:663/1375 train_time:92286ms step_avg:141.33ms
step:664/1375 train_time:92433ms step_avg:141.33ms
step:665/1375 train_time:92580ms step_avg:141.34ms
step:666/1375 train_time:92726ms step_avg:141.35ms
step:667/1375 train_time:92874ms step_avg:141.36ms
step:668/1375 train_time:93020ms step_avg:141.37ms
step:669/1375 train_time:93167ms step_avg:141.38ms
step:670/1375 train_time:93314ms step_avg:141.39ms
step:671/1375 train_time:93461ms step_avg:141.39ms
step:672/1375 train_time:93607ms step_avg:141.40ms
step:673/1375 train_time:93753ms step_avg:141.41ms
step:674/1375 train_time:93900ms step_avg:141.42ms
step:675/1375 train_time:94048ms step_avg:141.43ms
step:676/1375 train_time:94197ms step_avg:141.44ms
step:677/1375 train_time:94343ms step_avg:141.44ms
step:678/1375 train_time:94490ms step_avg:141.45ms
step:679/1375 train_time:94637ms step_avg:141.46ms
step:680/1375 train_time:94783ms step_avg:141.47ms
step:681/1375 train_time:94931ms step_avg:141.48ms
step:682/1375 train_time:95078ms step_avg:141.48ms
step:683/1375 train_time:95225ms step_avg:141.49ms
step:684/1375 train_time:95374ms step_avg:141.50ms
step:685/1375 train_time:95520ms step_avg:141.51ms
step:686/1375 train_time:95667ms step_avg:141.52ms
step:687/1375 train_time:95813ms step_avg:141.53ms
step:688/1375 train_time:95961ms step_avg:141.53ms
step:689/1375 train_time:96108ms step_avg:141.54ms
step:690/1375 train_time:96257ms step_avg:141.56ms
step:691/1375 train_time:96403ms step_avg:141.56ms
step:692/1375 train_time:96550ms step_avg:141.57ms
step:693/1375 train_time:96697ms step_avg:141.58ms
step:694/1375 train_time:96842ms step_avg:141.58ms
step:695/1375 train_time:96988ms step_avg:141.59ms
step:696/1375 train_time:97135ms step_avg:141.60ms
step:697/1375 train_time:97280ms step_avg:141.60ms
step:698/1375 train_time:97426ms step_avg:141.61ms
step:699/1375 train_time:97573ms step_avg:141.62ms
step:700/1375 train_time:97720ms step_avg:141.62ms
step:701/1375 train_time:97865ms step_avg:141.63ms
step:702/1375 train_time:98012ms step_avg:141.64ms
step:703/1375 train_time:98159ms step_avg:141.64ms
step:704/1375 train_time:98305ms step_avg:141.65ms
step:705/1375 train_time:98454ms step_avg:141.66ms
step:706/1375 train_time:98602ms step_avg:141.67ms
step:707/1375 train_time:98748ms step_avg:141.68ms
step:708/1375 train_time:98897ms step_avg:141.69ms
step:709/1375 train_time:99043ms step_avg:141.69ms
step:710/1375 train_time:99192ms step_avg:141.70ms
step:711/1375 train_time:99339ms step_avg:141.71ms
step:712/1375 train_time:99485ms step_avg:141.72ms
step:713/1375 train_time:99633ms step_avg:141.73ms
step:714/1375 train_time:99780ms step_avg:141.73ms
step:715/1375 train_time:99930ms step_avg:141.74ms
step:716/1375 train_time:100080ms step_avg:141.76ms
step:717/1375 train_time:100227ms step_avg:141.76ms
step:718/1375 train_time:100376ms step_avg:141.77ms
step:719/1375 train_time:100523ms step_avg:141.78ms
step:720/1375 train_time:100670ms step_avg:141.79ms
step:721/1375 train_time:100820ms step_avg:141.80ms
step:722/1375 train_time:100967ms step_avg:141.81ms
step:723/1375 train_time:101116ms step_avg:141.82ms
step:724/1375 train_time:101264ms step_avg:141.83ms
step:725/1375 train_time:101414ms step_avg:141.84ms
step:726/1375 train_time:101561ms step_avg:141.84ms
step:727/1375 train_time:101709ms step_avg:141.85ms
step:728/1375 train_time:101857ms step_avg:141.86ms
step:729/1375 train_time:102004ms step_avg:141.87ms
step:730/1375 train_time:102153ms step_avg:141.88ms
step:731/1375 train_time:102302ms step_avg:141.89ms
step:732/1375 train_time:102450ms step_avg:141.90ms
step:733/1375 train_time:102598ms step_avg:141.91ms
step:734/1375 train_time:102745ms step_avg:141.91ms
step:735/1375 train_time:102894ms step_avg:141.92ms
step:736/1375 train_time:103042ms step_avg:141.93ms
step:737/1375 train_time:103190ms step_avg:141.94ms
step:738/1375 train_time:103338ms step_avg:141.95ms
step:739/1375 train_time:103487ms step_avg:141.96ms
step:740/1375 train_time:103639ms step_avg:141.97ms
step:741/1375 train_time:103789ms step_avg:141.98ms
step:742/1375 train_time:103937ms step_avg:141.99ms
step:743/1375 train_time:104084ms step_avg:142.00ms
step:744/1375 train_time:104233ms step_avg:142.01ms
step:745/1375 train_time:104382ms step_avg:142.02ms
step:746/1375 train_time:104528ms step_avg:142.02ms
step:747/1375 train_time:104678ms step_avg:142.03ms
step:748/1375 train_time:104824ms step_avg:142.04ms
step:749/1375 train_time:104973ms step_avg:142.05ms
step:750/1375 train_time:105121ms step_avg:142.06ms
step:750/1375 val_loss:3.5191 train_time:105197ms step_avg:142.16ms
step:751/1375 train_time:105275ms step_avg:142.07ms
step:752/1375 train_time:105424ms step_avg:142.08ms
step:753/1375 train_time:105570ms step_avg:142.09ms
step:754/1375 train_time:105716ms step_avg:142.09ms
step:755/1375 train_time:105864ms step_avg:142.10ms
step:756/1375 train_time:106010ms step_avg:142.10ms
step:757/1375 train_time:106159ms step_avg:142.11ms
step:758/1375 train_time:106309ms step_avg:142.12ms
step:759/1375 train_time:106456ms step_avg:142.13ms
step:760/1375 train_time:106606ms step_avg:142.14ms
step:761/1375 train_time:106796ms step_avg:142.20ms
step:762/1375 train_time:106943ms step_avg:142.21ms
step:763/1375 train_time:107090ms step_avg:142.22ms
step:764/1375 train_time:107239ms step_avg:142.23ms
step:765/1375 train_time:107386ms step_avg:142.23ms
step:766/1375 train_time:107535ms step_avg:142.24ms
step:767/1375 train_time:107686ms step_avg:142.25ms
step:768/1375 train_time:107833ms step_avg:142.26ms
step:769/1375 train_time:107981ms step_avg:142.27ms
step:770/1375 train_time:108129ms step_avg:142.27ms
step:771/1375 train_time:108275ms step_avg:142.28ms
step:772/1375 train_time:108423ms step_avg:142.29ms
step:773/1375 train_time:108572ms step_avg:142.30ms
step:774/1375 train_time:108722ms step_avg:142.31ms
step:775/1375 train_time:108870ms step_avg:142.31ms
step:776/1375 train_time:109018ms step_avg:142.32ms
step:777/1375 train_time:109169ms step_avg:142.33ms
step:778/1375 train_time:109316ms step_avg:142.34ms
step:779/1375 train_time:109465ms step_avg:142.35ms
step:780/1375 train_time:109611ms step_avg:142.35ms
step:781/1375 train_time:109760ms step_avg:142.36ms
step:782/1375 train_time:109908ms step_avg:142.37ms
step:783/1375 train_time:110056ms step_avg:142.37ms
step:784/1375 train_time:110205ms step_avg:142.38ms
step:785/1375 train_time:110354ms step_avg:142.39ms
step:786/1375 train_time:110502ms step_avg:142.40ms
step:787/1375 train_time:110650ms step_avg:142.41ms
step:788/1375 train_time:110798ms step_avg:142.41ms
step:789/1375 train_time:110946ms step_avg:142.42ms
step:790/1375 train_time:111091ms step_avg:142.42ms
step:791/1375 train_time:111239ms step_avg:142.43ms
step:792/1375 train_time:111389ms step_avg:142.44ms
step:793/1375 train_time:111537ms step_avg:142.45ms
step:794/1375 train_time:111686ms step_avg:142.46ms
step:795/1375 train_time:111836ms step_avg:142.47ms
step:796/1375 train_time:111986ms step_avg:142.48ms
step:797/1375 train_time:112135ms step_avg:142.48ms
step:798/1375 train_time:112285ms step_avg:142.49ms
step:799/1375 train_time:112433ms step_avg:142.50ms
step:800/1375 train_time:112583ms step_avg:142.51ms
step:801/1375 train_time:112729ms step_avg:142.51ms
step:802/1375 train_time:112878ms step_avg:142.52ms
step:803/1375 train_time:113026ms step_avg:142.53ms
step:804/1375 train_time:113173ms step_avg:142.54ms
step:805/1375 train_time:113324ms step_avg:142.55ms
step:806/1375 train_time:113471ms step_avg:142.55ms
step:807/1375 train_time:113619ms step_avg:142.56ms
step:808/1375 train_time:113767ms step_avg:142.56ms
step:809/1375 train_time:113914ms step_avg:142.57ms
step:810/1375 train_time:114063ms step_avg:142.58ms
step:811/1375 train_time:114209ms step_avg:142.58ms
step:812/1375 train_time:114357ms step_avg:142.59ms
step:813/1375 train_time:114505ms step_avg:142.60ms
step:814/1375 train_time:114654ms step_avg:142.60ms
step:815/1375 train_time:114803ms step_avg:142.61ms
step:816/1375 train_time:114953ms step_avg:142.62ms
step:817/1375 train_time:115104ms step_avg:142.63ms
step:818/1375 train_time:115252ms step_avg:142.64ms
step:819/1375 train_time:115403ms step_avg:142.65ms
step:820/1375 train_time:115551ms step_avg:142.66ms
step:821/1375 train_time:115700ms step_avg:142.66ms
step:822/1375 train_time:115848ms step_avg:142.67ms
step:823/1375 train_time:115997ms step_avg:142.68ms
step:824/1375 train_time:116147ms step_avg:142.69ms
step:825/1375 train_time:116298ms step_avg:142.70ms
step:826/1375 train_time:116450ms step_avg:142.71ms
step:827/1375 train_time:116599ms step_avg:142.72ms
step:828/1375 train_time:116749ms step_avg:142.73ms
step:829/1375 train_time:116899ms step_avg:142.73ms
step:830/1375 train_time:117050ms step_avg:142.74ms
step:831/1375 train_time:117199ms step_avg:142.75ms
step:832/1375 train_time:117350ms step_avg:142.76ms
step:833/1375 train_time:117498ms step_avg:142.77ms
step:834/1375 train_time:117650ms step_avg:142.78ms
step:835/1375 train_time:117800ms step_avg:142.79ms
step:836/1375 train_time:117950ms step_avg:142.80ms
step:837/1375 train_time:118100ms step_avg:142.81ms
step:838/1375 train_time:118249ms step_avg:142.81ms
step:839/1375 train_time:118397ms step_avg:142.82ms
step:840/1375 train_time:118547ms step_avg:142.83ms
step:841/1375 train_time:118696ms step_avg:142.84ms
step:842/1375 train_time:118845ms step_avg:142.84ms
step:843/1375 train_time:118993ms step_avg:142.85ms
step:844/1375 train_time:119142ms step_avg:142.86ms
step:845/1375 train_time:119289ms step_avg:142.86ms
step:846/1375 train_time:119441ms step_avg:142.87ms
step:847/1375 train_time:119591ms step_avg:142.88ms
step:848/1375 train_time:119741ms step_avg:142.89ms
step:849/1375 train_time:119890ms step_avg:142.90ms
step:850/1375 train_time:120041ms step_avg:142.91ms
step:851/1375 train_time:120192ms step_avg:142.92ms
step:852/1375 train_time:120343ms step_avg:142.92ms
step:853/1375 train_time:120490ms step_avg:142.93ms
step:854/1375 train_time:120639ms step_avg:142.94ms
step:855/1375 train_time:120788ms step_avg:142.94ms
step:856/1375 train_time:120936ms step_avg:142.95ms
step:857/1375 train_time:121086ms step_avg:142.96ms
step:858/1375 train_time:121239ms step_avg:142.97ms
step:859/1375 train_time:121389ms step_avg:142.98ms
step:860/1375 train_time:121538ms step_avg:142.99ms
step:861/1375 train_time:121689ms step_avg:143.00ms
step:862/1375 train_time:121838ms step_avg:143.00ms
step:863/1375 train_time:121990ms step_avg:143.01ms
step:864/1375 train_time:122139ms step_avg:143.02ms
step:865/1375 train_time:122289ms step_avg:143.03ms
step:866/1375 train_time:122445ms step_avg:143.04ms
step:867/1375 train_time:122593ms step_avg:143.05ms
step:868/1375 train_time:122743ms step_avg:143.06ms
step:869/1375 train_time:122891ms step_avg:143.06ms
step:870/1375 train_time:123043ms step_avg:143.07ms
step:871/1375 train_time:123191ms step_avg:143.08ms
step:872/1375 train_time:123339ms step_avg:143.08ms
step:873/1375 train_time:123489ms step_avg:143.09ms
step:874/1375 train_time:123640ms step_avg:143.10ms
step:875/1375 train_time:123793ms step_avg:143.11ms
step:875/1375 val_loss:3.4671 train_time:123866ms step_avg:143.20ms
step:876/1375 train_time:123944ms step_avg:143.12ms
step:877/1375 train_time:124091ms step_avg:143.13ms
step:878/1375 train_time:124240ms step_avg:143.13ms
step:879/1375 train_time:124388ms step_avg:143.14ms
step:880/1375 train_time:124537ms step_avg:143.15ms
step:881/1375 train_time:124685ms step_avg:143.15ms
step:882/1375 train_time:124836ms step_avg:143.16ms
step:883/1375 train_time:124986ms step_avg:143.17ms
step:884/1375 train_time:125137ms step_avg:143.18ms
step:885/1375 train_time:125287ms step_avg:143.18ms
step:886/1375 train_time:125438ms step_avg:143.19ms
step:887/1375 train_time:125586ms step_avg:143.20ms
step:888/1375 train_time:125739ms step_avg:143.21ms
step:889/1375 train_time:125891ms step_avg:143.22ms
step:890/1375 train_time:126040ms step_avg:143.23ms
step:891/1375 train_time:126187ms step_avg:143.23ms
step:892/1375 train_time:126339ms step_avg:143.24ms
step:893/1375 train_time:126486ms step_avg:143.25ms
step:894/1375 train_time:126637ms step_avg:143.25ms
step:895/1375 train_time:126788ms step_avg:143.26ms
step:896/1375 train_time:126937ms step_avg:143.27ms
step:897/1375 train_time:127086ms step_avg:143.28ms
step:898/1375 train_time:127236ms step_avg:143.28ms
step:899/1375 train_time:127387ms step_avg:143.29ms
step:900/1375 train_time:127537ms step_avg:143.30ms
step:901/1375 train_time:127686ms step_avg:143.31ms
step:902/1375 train_time:127834ms step_avg:143.31ms
step:903/1375 train_time:127985ms step_avg:143.32ms
step:904/1375 train_time:128134ms step_avg:143.33ms
step:905/1375 train_time:128283ms step_avg:143.33ms
step:906/1375 train_time:128433ms step_avg:143.34ms
step:907/1375 train_time:128585ms step_avg:143.35ms
step:908/1375 train_time:128734ms step_avg:143.36ms
step:909/1375 train_time:128884ms step_avg:143.36ms
step:910/1375 train_time:129037ms step_avg:143.37ms
step:911/1375 train_time:129185ms step_avg:143.38ms
step:912/1375 train_time:129333ms step_avg:143.38ms
step:913/1375 train_time:129484ms step_avg:143.39ms
step:914/1375 train_time:129634ms step_avg:143.40ms
step:915/1375 train_time:129785ms step_avg:143.41ms
step:916/1375 train_time:129935ms step_avg:143.42ms
step:917/1375 train_time:130085ms step_avg:143.42ms
step:918/1375 train_time:130235ms step_avg:143.43ms
step:919/1375 train_time:130392ms step_avg:143.45ms
step:920/1375 train_time:130544ms step_avg:143.46ms
step:921/1375 train_time:130695ms step_avg:143.46ms
step:922/1375 train_time:130847ms step_avg:143.47ms
step:923/1375 train_time:130997ms step_avg:143.48ms
step:924/1375 train_time:131148ms step_avg:143.49ms
step:925/1375 train_time:131301ms step_avg:143.50ms
step:926/1375 train_time:131451ms step_avg:143.51ms
step:927/1375 train_time:131602ms step_avg:143.51ms
step:928/1375 train_time:131753ms step_avg:143.52ms
step:929/1375 train_time:131904ms step_avg:143.53ms
step:930/1375 train_time:132055ms step_avg:143.54ms
step:931/1375 train_time:132205ms step_avg:143.55ms
step:932/1375 train_time:132355ms step_avg:143.55ms
step:933/1375 train_time:132506ms step_avg:143.56ms
step:934/1375 train_time:132658ms step_avg:143.57ms
step:935/1375 train_time:132810ms step_avg:143.58ms
step:936/1375 train_time:132961ms step_avg:143.59ms
step:937/1375 train_time:133113ms step_avg:143.60ms
step:938/1375 train_time:133265ms step_avg:143.60ms
step:939/1375 train_time:133417ms step_avg:143.61ms
step:940/1375 train_time:133569ms step_avg:143.62ms
step:941/1375 train_time:133720ms step_avg:143.63ms
step:942/1375 train_time:133868ms step_avg:143.64ms
step:943/1375 train_time:134019ms step_avg:143.64ms
step:944/1375 train_time:134173ms step_avg:143.65ms
step:945/1375 train_time:134324ms step_avg:143.66ms
step:946/1375 train_time:134477ms step_avg:143.67ms
step:947/1375 train_time:134630ms step_avg:143.68ms
step:948/1375 train_time:134782ms step_avg:143.69ms
step:949/1375 train_time:134934ms step_avg:143.70ms
step:950/1375 train_time:135085ms step_avg:143.71ms
step:951/1375 train_time:135286ms step_avg:143.77ms
step:952/1375 train_time:135435ms step_avg:143.77ms
step:953/1375 train_time:135585ms step_avg:143.78ms
step:954/1375 train_time:135735ms step_avg:143.79ms
step:955/1375 train_time:135884ms step_avg:143.79ms
step:956/1375 train_time:136037ms step_avg:143.80ms
step:957/1375 train_time:136188ms step_avg:143.81ms
step:958/1375 train_time:136341ms step_avg:143.82ms
step:959/1375 train_time:136495ms step_avg:143.83ms
step:960/1375 train_time:136647ms step_avg:143.84ms
step:961/1375 train_time:136797ms step_avg:143.85ms
step:962/1375 train_time:136949ms step_avg:143.85ms
step:963/1375 train_time:137105ms step_avg:143.87ms
step:964/1375 train_time:137257ms step_avg:143.88ms
step:965/1375 train_time:137408ms step_avg:143.88ms
step:966/1375 train_time:137559ms step_avg:143.89ms
step:967/1375 train_time:137709ms step_avg:143.90ms
step:968/1375 train_time:137859ms step_avg:143.90ms
step:969/1375 train_time:138010ms step_avg:143.91ms
step:970/1375 train_time:138160ms step_avg:143.92ms
step:971/1375 train_time:138310ms step_avg:143.92ms
step:972/1375 train_time:138462ms step_avg:143.93ms
step:973/1375 train_time:138611ms step_avg:143.94ms
step:974/1375 train_time:138763ms step_avg:143.94ms
step:975/1375 train_time:138913ms step_avg:143.95ms
step:976/1375 train_time:139064ms step_avg:143.96ms
step:977/1375 train_time:139214ms step_avg:143.96ms
step:978/1375 train_time:139364ms step_avg:143.97ms
step:979/1375 train_time:139513ms step_avg:143.98ms
step:980/1375 train_time:139664ms step_avg:143.98ms
step:981/1375 train_time:139812ms step_avg:143.99ms
step:982/1375 train_time:139963ms step_avg:143.99ms
step:983/1375 train_time:140112ms step_avg:144.00ms
step:984/1375 train_time:140263ms step_avg:144.01ms
step:985/1375 train_time:140415ms step_avg:144.02ms
step:986/1375 train_time:140570ms step_avg:144.03ms
step:987/1375 train_time:140720ms step_avg:144.03ms
step:988/1375 train_time:140871ms step_avg:144.04ms
step:989/1375 train_time:141024ms step_avg:144.05ms
step:990/1375 train_time:141176ms step_avg:144.06ms
step:991/1375 train_time:141326ms step_avg:144.06ms
step:992/1375 train_time:141480ms step_avg:144.07ms
step:993/1375 train_time:141637ms step_avg:144.09ms
step:994/1375 train_time:141787ms step_avg:144.09ms
step:995/1375 train_time:141939ms step_avg:144.10ms
step:996/1375 train_time:142086ms step_avg:144.10ms
step:997/1375 train_time:142236ms step_avg:144.11ms
step:998/1375 train_time:142385ms step_avg:144.11ms
step:999/1375 train_time:142536ms step_avg:144.12ms
step:1000/1375 train_time:142686ms step_avg:144.13ms
step:1000/1375 val_loss:3.4030 train_time:142760ms step_avg:144.20ms
step:1001/1375 train_time:142837ms step_avg:144.13ms
step:1002/1375 train_time:142988ms step_avg:144.14ms
step:1003/1375 train_time:143141ms step_avg:144.15ms
step:1004/1375 train_time:143293ms step_avg:144.16ms
step:1005/1375 train_time:143443ms step_avg:144.16ms
step:1006/1375 train_time:143592ms step_avg:144.17ms
step:1007/1375 train_time:143740ms step_avg:144.17ms
step:1008/1375 train_time:143893ms step_avg:144.18ms
step:1009/1375 train_time:144047ms step_avg:144.19ms
step:1010/1375 train_time:144198ms step_avg:144.20ms
step:1011/1375 train_time:144348ms step_avg:144.20ms
step:1012/1375 train_time:144498ms step_avg:144.21ms
step:1013/1375 train_time:144649ms step_avg:144.22ms
step:1014/1375 train_time:144798ms step_avg:144.22ms
step:1015/1375 train_time:144949ms step_avg:144.23ms
step:1016/1375 train_time:145098ms step_avg:144.23ms
step:1017/1375 train_time:145250ms step_avg:144.24ms
step:1018/1375 train_time:145400ms step_avg:144.25ms
step:1019/1375 train_time:145553ms step_avg:144.25ms
step:1020/1375 train_time:145707ms step_avg:144.26ms
step:1021/1375 train_time:145857ms step_avg:144.27ms
step:1022/1375 train_time:146009ms step_avg:144.28ms
step:1023/1375 train_time:146161ms step_avg:144.29ms
step:1024/1375 train_time:146316ms step_avg:144.30ms
step:1025/1375 train_time:146470ms step_avg:144.30ms
step:1026/1375 train_time:146621ms step_avg:144.31ms
step:1027/1375 train_time:146774ms step_avg:144.32ms
step:1028/1375 train_time:146928ms step_avg:144.33ms
step:1029/1375 train_time:147082ms step_avg:144.34ms
step:1030/1375 train_time:147234ms step_avg:144.35ms
step:1031/1375 train_time:147382ms step_avg:144.35ms
step:1032/1375 train_time:147533ms step_avg:144.36ms
step:1033/1375 train_time:147683ms step_avg:144.36ms
step:1034/1375 train_time:147835ms step_avg:144.37ms
step:1035/1375 train_time:147989ms step_avg:144.38ms
step:1036/1375 train_time:148139ms step_avg:144.39ms
step:1037/1375 train_time:148293ms step_avg:144.39ms
step:1038/1375 train_time:148447ms step_avg:144.40ms
step:1039/1375 train_time:148597ms step_avg:144.41ms
step:1040/1375 train_time:148747ms step_avg:144.41ms
step:1041/1375 train_time:148899ms step_avg:144.42ms
step:1042/1375 train_time:149050ms step_avg:144.43ms
step:1043/1375 train_time:149203ms step_avg:144.44ms
step:1044/1375 train_time:149358ms step_avg:144.45ms
step:1045/1375 train_time:149511ms step_avg:144.46ms
step:1046/1375 train_time:149663ms step_avg:144.46ms
step:1047/1375 train_time:149813ms step_avg:144.47ms
step:1048/1375 train_time:149966ms step_avg:144.48ms
step:1049/1375 train_time:150120ms step_avg:144.48ms
step:1050/1375 train_time:150273ms step_avg:144.49ms
step:1051/1375 train_time:150426ms step_avg:144.50ms
step:1052/1375 train_time:150579ms step_avg:144.51ms
step:1053/1375 train_time:150731ms step_avg:144.52ms
step:1054/1375 train_time:150882ms step_avg:144.52ms
step:1055/1375 train_time:151034ms step_avg:144.53ms
step:1056/1375 train_time:151183ms step_avg:144.53ms
step:1057/1375 train_time:151334ms step_avg:144.54ms
step:1058/1375 train_time:151488ms step_avg:144.55ms
step:1059/1375 train_time:151642ms step_avg:144.56ms
step:1060/1375 train_time:151797ms step_avg:144.57ms
step:1061/1375 train_time:151947ms step_avg:144.57ms
step:1062/1375 train_time:152099ms step_avg:144.58ms
step:1063/1375 train_time:152251ms step_avg:144.59ms
step:1064/1375 train_time:152401ms step_avg:144.59ms
step:1065/1375 train_time:152554ms step_avg:144.60ms
step:1066/1375 train_time:152710ms step_avg:144.61ms
step:1067/1375 train_time:152863ms step_avg:144.62ms
step:1068/1375 train_time:153016ms step_avg:144.63ms
step:1069/1375 train_time:153174ms step_avg:144.64ms
step:1070/1375 train_time:153323ms step_avg:144.64ms
step:1071/1375 train_time:153478ms step_avg:144.65ms
step:1072/1375 train_time:153629ms step_avg:144.66ms
step:1073/1375 train_time:153778ms step_avg:144.66ms
step:1074/1375 train_time:153928ms step_avg:144.67ms
step:1075/1375 train_time:154080ms step_avg:144.68ms
step:1076/1375 train_time:154232ms step_avg:144.68ms
step:1077/1375 train_time:154381ms step_avg:144.69ms
step:1078/1375 train_time:154539ms step_avg:144.70ms
step:1079/1375 train_time:154695ms step_avg:144.71ms
step:1080/1375 train_time:154848ms step_avg:144.72ms
step:1081/1375 train_time:154999ms step_avg:144.72ms
step:1082/1375 train_time:155150ms step_avg:144.73ms
step:1083/1375 train_time:155300ms step_avg:144.73ms
step:1084/1375 train_time:155456ms step_avg:144.75ms
step:1085/1375 train_time:155608ms step_avg:144.75ms
step:1086/1375 train_time:155760ms step_avg:144.76ms
step:1087/1375 train_time:155913ms step_avg:144.77ms
step:1088/1375 train_time:156064ms step_avg:144.77ms
step:1089/1375 train_time:156220ms step_avg:144.78ms
step:1090/1375 train_time:156375ms step_avg:144.79ms
step:1091/1375 train_time:156527ms step_avg:144.80ms
step:1092/1375 train_time:156678ms step_avg:144.80ms
step:1093/1375 train_time:156831ms step_avg:144.81ms
step:1094/1375 train_time:156981ms step_avg:144.82ms
step:1095/1375 train_time:157133ms step_avg:144.82ms
step:1096/1375 train_time:157286ms step_avg:144.83ms
step:1097/1375 train_time:157441ms step_avg:144.84ms
step:1098/1375 train_time:157595ms step_avg:144.85ms
step:1099/1375 train_time:157746ms step_avg:144.85ms
step:1100/1375 train_time:157897ms step_avg:144.86ms
step:1101/1375 train_time:158050ms step_avg:144.87ms
step:1102/1375 train_time:158203ms step_avg:144.87ms
step:1103/1375 train_time:158357ms step_avg:144.88ms
step:1104/1375 train_time:158509ms step_avg:144.89ms
step:1105/1375 train_time:158663ms step_avg:144.90ms
step:1106/1375 train_time:158816ms step_avg:144.90ms
step:1107/1375 train_time:158967ms step_avg:144.91ms
step:1108/1375 train_time:159123ms step_avg:144.92ms
step:1109/1375 train_time:159275ms step_avg:144.93ms
step:1110/1375 train_time:159428ms step_avg:144.93ms
step:1111/1375 train_time:159581ms step_avg:144.94ms
step:1112/1375 train_time:159731ms step_avg:144.95ms
step:1113/1375 train_time:159881ms step_avg:144.95ms
step:1114/1375 train_time:160035ms step_avg:144.96ms
step:1115/1375 train_time:160187ms step_avg:144.97ms
step:1116/1375 train_time:160338ms step_avg:144.97ms
step:1117/1375 train_time:160493ms step_avg:144.98ms
step:1118/1375 train_time:160652ms step_avg:144.99ms
step:1119/1375 train_time:160803ms step_avg:145.00ms
step:1120/1375 train_time:160955ms step_avg:145.00ms
step:1121/1375 train_time:161107ms step_avg:145.01ms
step:1122/1375 train_time:161258ms step_avg:145.02ms
step:1123/1375 train_time:161411ms step_avg:145.02ms
step:1124/1375 train_time:161565ms step_avg:145.03ms
step:1125/1375 train_time:161717ms step_avg:145.04ms
step:1125/1375 val_loss:3.3493 train_time:161793ms step_avg:145.11ms
step:1126/1375 train_time:161871ms step_avg:145.05ms
step:1127/1375 train_time:162022ms step_avg:145.05ms
step:1128/1375 train_time:162176ms step_avg:145.06ms
step:1129/1375 train_time:162332ms step_avg:145.07ms
step:1130/1375 train_time:162481ms step_avg:145.07ms
step:1131/1375 train_time:162635ms step_avg:145.08ms
step:1132/1375 train_time:162786ms step_avg:145.09ms
step:1133/1375 train_time:162938ms step_avg:145.09ms
step:1134/1375 train_time:163094ms step_avg:145.10ms
step:1135/1375 train_time:163246ms step_avg:145.11ms
step:1136/1375 train_time:163407ms step_avg:145.12ms
step:1137/1375 train_time:163557ms step_avg:145.13ms
step:1138/1375 train_time:163713ms step_avg:145.14ms
step:1139/1375 train_time:163866ms step_avg:145.14ms
step:1140/1375 train_time:164018ms step_avg:145.15ms
step:1141/1375 train_time:164212ms step_avg:145.19ms
step:1142/1375 train_time:164363ms step_avg:145.20ms
step:1143/1375 train_time:164518ms step_avg:145.21ms
step:1144/1375 train_time:164672ms step_avg:145.21ms
step:1145/1375 train_time:164823ms step_avg:145.22ms
step:1146/1375 train_time:164976ms step_avg:145.23ms
step:1147/1375 train_time:165131ms step_avg:145.23ms
step:1148/1375 train_time:165283ms step_avg:145.24ms
step:1149/1375 train_time:165436ms step_avg:145.25ms
step:1150/1375 train_time:165588ms step_avg:145.25ms
step:1151/1375 train_time:165743ms step_avg:145.26ms
step:1152/1375 train_time:165896ms step_avg:145.27ms
step:1153/1375 train_time:166053ms step_avg:145.28ms
step:1154/1375 train_time:166204ms step_avg:145.28ms
step:1155/1375 train_time:166360ms step_avg:145.29ms
step:1156/1375 train_time:166521ms step_avg:145.31ms
step:1157/1375 train_time:166676ms step_avg:145.31ms
step:1158/1375 train_time:166828ms step_avg:145.32ms
step:1159/1375 train_time:166982ms step_avg:145.33ms
step:1160/1375 train_time:167133ms step_avg:145.33ms
step:1161/1375 train_time:167286ms step_avg:145.34ms
step:1162/1375 train_time:167441ms step_avg:145.35ms
step:1163/1375 train_time:167596ms step_avg:145.36ms
step:1164/1375 train_time:167751ms step_avg:145.36ms
step:1165/1375 train_time:167901ms step_avg:145.37ms
step:1166/1375 train_time:168053ms step_avg:145.37ms
step:1167/1375 train_time:168204ms step_avg:145.38ms
step:1168/1375 train_time:168357ms step_avg:145.39ms
step:1169/1375 train_time:168511ms step_avg:145.39ms
step:1170/1375 train_time:168663ms step_avg:145.40ms
step:1171/1375 train_time:168815ms step_avg:145.41ms
step:1172/1375 train_time:168967ms step_avg:145.41ms
step:1173/1375 train_time:169119ms step_avg:145.42ms
step:1174/1375 train_time:169285ms step_avg:145.43ms
step:1175/1375 train_time:169438ms step_avg:145.44ms
step:1176/1375 train_time:169595ms step_avg:145.45ms
step:1177/1375 train_time:169753ms step_avg:145.46ms
step:1178/1375 train_time:169905ms step_avg:145.47ms
step:1179/1375 train_time:170055ms step_avg:145.47ms
step:1180/1375 train_time:170216ms step_avg:145.48ms
step:1181/1375 train_time:170371ms step_avg:145.49ms
step:1182/1375 train_time:170522ms step_avg:145.50ms
step:1183/1375 train_time:170677ms step_avg:145.51ms
step:1184/1375 train_time:170831ms step_avg:145.51ms
step:1185/1375 train_time:170986ms step_avg:145.52ms
step:1186/1375 train_time:171139ms step_avg:145.53ms
step:1187/1375 train_time:171302ms step_avg:145.54ms
step:1188/1375 train_time:171454ms step_avg:145.55ms
step:1189/1375 train_time:171608ms step_avg:145.55ms
step:1190/1375 train_time:171761ms step_avg:145.56ms
step:1191/1375 train_time:171915ms step_avg:145.57ms
step:1192/1375 train_time:172066ms step_avg:145.57ms
step:1193/1375 train_time:172217ms step_avg:145.58ms
step:1194/1375 train_time:172371ms step_avg:145.58ms
step:1195/1375 train_time:172523ms step_avg:145.59ms
step:1196/1375 train_time:172676ms step_avg:145.60ms
step:1197/1375 train_time:172830ms step_avg:145.60ms
step:1198/1375 train_time:172990ms step_avg:145.61ms
step:1199/1375 train_time:173142ms step_avg:145.62ms
step:1200/1375 train_time:173295ms step_avg:145.63ms
step:1201/1375 train_time:173448ms step_avg:145.63ms
step:1202/1375 train_time:173617ms step_avg:145.65ms
step:1203/1375 train_time:173775ms step_avg:145.66ms
step:1204/1375 train_time:173930ms step_avg:145.67ms
step:1205/1375 train_time:174083ms step_avg:145.68ms
step:1206/1375 train_time:174239ms step_avg:145.68ms
step:1207/1375 train_time:174390ms step_avg:145.69ms
step:1208/1375 train_time:174545ms step_avg:145.70ms
step:1209/1375 train_time:174700ms step_avg:145.70ms
step:1210/1375 train_time:174856ms step_avg:145.71ms
step:1211/1375 train_time:175010ms step_avg:145.72ms
step:1212/1375 train_time:175163ms step_avg:145.73ms
step:1213/1375 train_time:175318ms step_avg:145.73ms
step:1214/1375 train_time:175475ms step_avg:145.74ms
step:1215/1375 train_time:175629ms step_avg:145.75ms
step:1216/1375 train_time:175778ms step_avg:145.75ms
step:1217/1375 train_time:175931ms step_avg:145.76ms
step:1218/1375 train_time:176081ms step_avg:145.76ms
step:1219/1375 train_time:176233ms step_avg:145.77ms
step:1220/1375 train_time:176383ms step_avg:145.77ms
step:1221/1375 train_time:176536ms step_avg:145.78ms
step:1222/1375 train_time:176689ms step_avg:145.78ms
step:1223/1375 train_time:176841ms step_avg:145.79ms
step:1224/1375 train_time:176997ms step_avg:145.80ms
step:1225/1375 train_time:177153ms step_avg:145.81ms
step:1226/1375 train_time:177308ms step_avg:145.81ms
step:1227/1375 train_time:177464ms step_avg:145.82ms
step:1228/1375 train_time:177620ms step_avg:145.83ms
step:1229/1375 train_time:177773ms step_avg:145.84ms
step:1230/1375 train_time:177934ms step_avg:145.85ms
step:1231/1375 train_time:178091ms step_avg:145.86ms
step:1232/1375 train_time:178248ms step_avg:145.87ms
step:1233/1375 train_time:178400ms step_avg:145.87ms
step:1234/1375 train_time:178554ms step_avg:145.88ms
step:1235/1375 train_time:178707ms step_avg:145.88ms
step:1236/1375 train_time:178861ms step_avg:145.89ms
step:1237/1375 train_time:179015ms step_avg:145.90ms
step:1238/1375 train_time:179178ms step_avg:145.91ms
step:1239/1375 train_time:179333ms step_avg:145.92ms
step:1240/1375 train_time:179487ms step_avg:145.92ms
step:1241/1375 train_time:179646ms step_avg:145.93ms
step:1242/1375 train_time:179799ms step_avg:145.94ms
step:1243/1375 train_time:179955ms step_avg:145.95ms
step:1244/1375 train_time:180110ms step_avg:145.96ms
step:1245/1375 train_time:180262ms step_avg:145.96ms
step:1246/1375 train_time:180415ms step_avg:145.97ms
step:1247/1375 train_time:180570ms step_avg:145.97ms
step:1248/1375 train_time:180721ms step_avg:145.98ms
step:1249/1375 train_time:180874ms step_avg:145.98ms
step:1250/1375 train_time:181027ms step_avg:145.99ms
step:1250/1375 val_loss:3.3037 train_time:181105ms step_avg:146.05ms
step:1251/1375 train_time:181185ms step_avg:146.00ms
step:1252/1375 train_time:181339ms step_avg:146.01ms
step:1253/1375 train_time:181491ms step_avg:146.01ms
step:1254/1375 train_time:181643ms step_avg:146.02ms
step:1255/1375 train_time:181806ms step_avg:146.03ms
step:1256/1375 train_time:181960ms step_avg:146.04ms
step:1257/1375 train_time:182111ms step_avg:146.04ms
step:1258/1375 train_time:182269ms step_avg:146.05ms
step:1259/1375 train_time:182424ms step_avg:146.06ms
step:1260/1375 train_time:182576ms step_avg:146.06ms
step:1261/1375 train_time:182729ms step_avg:146.07ms
step:1262/1375 train_time:182886ms step_avg:146.08ms
step:1263/1375 train_time:183040ms step_avg:146.08ms
step:1264/1375 train_time:183192ms step_avg:146.09ms
step:1265/1375 train_time:183345ms step_avg:146.09ms
step:1266/1375 train_time:183501ms step_avg:146.10ms
step:1267/1375 train_time:183653ms step_avg:146.10ms
step:1268/1375 train_time:183809ms step_avg:146.11ms
step:1269/1375 train_time:183969ms step_avg:146.12ms
step:1270/1375 train_time:184123ms step_avg:146.13ms
step:1271/1375 train_time:184276ms step_avg:146.13ms
step:1272/1375 train_time:184429ms step_avg:146.14ms
step:1273/1375 train_time:184581ms step_avg:146.15ms
step:1274/1375 train_time:184732ms step_avg:146.15ms
step:1275/1375 train_time:184890ms step_avg:146.16ms
step:1276/1375 train_time:185042ms step_avg:146.16ms
step:1277/1375 train_time:185195ms step_avg:146.17ms
step:1278/1375 train_time:185348ms step_avg:146.17ms
step:1279/1375 train_time:185502ms step_avg:146.18ms
step:1280/1375 train_time:185660ms step_avg:146.19ms
step:1281/1375 train_time:185813ms step_avg:146.19ms
step:1282/1375 train_time:185966ms step_avg:146.20ms
step:1283/1375 train_time:186121ms step_avg:146.21ms
step:1284/1375 train_time:186275ms step_avg:146.21ms
step:1285/1375 train_time:186430ms step_avg:146.22ms
step:1286/1375 train_time:186582ms step_avg:146.22ms
step:1287/1375 train_time:186736ms step_avg:146.23ms
step:1288/1375 train_time:186889ms step_avg:146.24ms
step:1289/1375 train_time:187051ms step_avg:146.25ms
step:1290/1375 train_time:187210ms step_avg:146.26ms
step:1291/1375 train_time:187366ms step_avg:146.27ms
step:1292/1375 train_time:187520ms step_avg:146.27ms
step:1293/1375 train_time:187679ms step_avg:146.28ms
step:1294/1375 train_time:187832ms step_avg:146.29ms
step:1295/1375 train_time:187987ms step_avg:146.29ms
step:1296/1375 train_time:188141ms step_avg:146.30ms
step:1297/1375 train_time:188298ms step_avg:146.31ms
step:1298/1375 train_time:188451ms step_avg:146.31ms
step:1299/1375 train_time:188605ms step_avg:146.32ms
step:1300/1375 train_time:188757ms step_avg:146.32ms
step:1301/1375 train_time:188910ms step_avg:146.33ms
step:1302/1375 train_time:189065ms step_avg:146.33ms
step:1303/1375 train_time:189223ms step_avg:146.34ms
step:1304/1375 train_time:189380ms step_avg:146.35ms
step:1305/1375 train_time:189534ms step_avg:146.36ms
step:1306/1375 train_time:189688ms step_avg:146.36ms
step:1307/1375 train_time:189840ms step_avg:146.37ms
step:1308/1375 train_time:189997ms step_avg:146.38ms
step:1309/1375 train_time:190154ms step_avg:146.38ms
step:1310/1375 train_time:190307ms step_avg:146.39ms
step:1311/1375 train_time:190461ms step_avg:146.40ms
step:1312/1375 train_time:190612ms step_avg:146.40ms
step:1313/1375 train_time:190766ms step_avg:146.41ms
step:1314/1375 train_time:190919ms step_avg:146.41ms
step:1315/1375 train_time:191075ms step_avg:146.42ms
step:1316/1375 train_time:191227ms step_avg:146.42ms
step:1317/1375 train_time:191380ms step_avg:146.43ms
step:1318/1375 train_time:191541ms step_avg:146.44ms
step:1319/1375 train_time:191697ms step_avg:146.45ms
step:1320/1375 train_time:191851ms step_avg:146.45ms
step:1321/1375 train_time:192006ms step_avg:146.46ms
step:1322/1375 train_time:192165ms step_avg:146.47ms
step:1323/1375 train_time:192319ms step_avg:146.47ms
step:1324/1375 train_time:192471ms step_avg:146.48ms
step:1325/1375 train_time:192627ms step_avg:146.48ms
step:1326/1375 train_time:192786ms step_avg:146.49ms
step:1327/1375 train_time:192941ms step_avg:146.50ms
step:1328/1375 train_time:193093ms step_avg:146.50ms
step:1329/1375 train_time:193265ms step_avg:146.52ms
step:1330/1375 train_time:193425ms step_avg:146.53ms
step:1331/1375 train_time:193627ms step_avg:146.58ms
step:1332/1375 train_time:193788ms step_avg:146.59ms
step:1333/1375 train_time:193942ms step_avg:146.59ms
step:1334/1375 train_time:194096ms step_avg:146.60ms
step:1335/1375 train_time:194248ms step_avg:146.60ms
step:1336/1375 train_time:194408ms step_avg:146.61ms
step:1337/1375 train_time:194565ms step_avg:146.62ms
step:1338/1375 train_time:194719ms step_avg:146.63ms
step:1339/1375 train_time:194875ms step_avg:146.63ms
step:1340/1375 train_time:195036ms step_avg:146.64ms
step:1341/1375 train_time:195189ms step_avg:146.65ms
step:1342/1375 train_time:195345ms step_avg:146.66ms
step:1343/1375 train_time:195499ms step_avg:146.66ms
step:1344/1375 train_time:195651ms step_avg:146.66ms
step:1345/1375 train_time:195806ms step_avg:146.67ms
step:1346/1375 train_time:195960ms step_avg:146.68ms
step:1347/1375 train_time:196119ms step_avg:146.69ms
step:1348/1375 train_time:196273ms step_avg:146.69ms
step:1349/1375 train_time:196427ms step_avg:146.70ms
step:1350/1375 train_time:196579ms step_avg:146.70ms
step:1351/1375 train_time:196732ms step_avg:146.71ms
step:1352/1375 train_time:196894ms step_avg:146.72ms
step:1353/1375 train_time:197053ms step_avg:146.73ms
step:1354/1375 train_time:197207ms step_avg:146.73ms
step:1355/1375 train_time:197360ms step_avg:146.74ms
step:1356/1375 train_time:197512ms step_avg:146.74ms
step:1357/1375 train_time:197668ms step_avg:146.75ms
step:1358/1375 train_time:197824ms step_avg:146.75ms
step:1359/1375 train_time:197976ms step_avg:146.76ms
step:1360/1375 train_time:198136ms step_avg:146.77ms
step:1361/1375 train_time:198294ms step_avg:146.78ms
step:1362/1375 train_time:198452ms step_avg:146.78ms
step:1363/1375 train_time:198611ms step_avg:146.79ms
step:1364/1375 train_time:198764ms step_avg:146.80ms
step:1365/1375 train_time:198914ms step_avg:146.80ms
step:1366/1375 train_time:199068ms step_avg:146.81ms
step:1367/1375 train_time:199223ms step_avg:146.81ms
step:1368/1375 train_time:199378ms step_avg:146.82ms
step:1369/1375 train_time:199543ms step_avg:146.83ms
step:1370/1375 train_time:199702ms step_avg:146.84ms
step:1371/1375 train_time:199857ms step_avg:146.85ms
step:1372/1375 train_time:200016ms step_avg:146.85ms
step:1373/1375 train_time:200171ms step_avg:146.86ms
step:1374/1375 train_time:200331ms step_avg:146.87ms
step:1375/1375 train_time:200484ms step_avg:146.87ms
step:1375/1375 val_loss:3.2783 train_time:200559ms step_avg:146.93ms
peak memory consumption: 31565 MiB
