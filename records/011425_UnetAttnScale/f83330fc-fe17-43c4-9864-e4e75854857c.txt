import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import glob
import subprocess
import contextlib
from dataclasses import dataclass

import torch
torch.empty(1, device='cuda', requires_grad=True).backward()
from torch import nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G, steps):
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert len(G.shape) == 2
    a, b, c = (3.4445, -4.7750,  2.0315)
    X = G.bfloat16()
    if G.size(0) > G.size(1):
        X = X.T

    # Ensure spectral norm is at most 1
    X = X / (X.norm() + 1e-7)
    # Perform the NS iterations
    for _ in range(steps):
        A = X @ X.T
        B = b * A + c * A @ A # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(0) > G.size(1):
        X = X.T
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer assumes that all parameters passed in are 2D.
    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D
    parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.
    - We believe it is unlikely to work well for training with small batch size.
    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.
    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5):
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.rank = int(os.environ['RANK'])
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        assert all(isinstance(p, torch.Tensor) for p in params)
        sizes = {p.numel() for p in params}
        param_groups = [dict(params=[p for p in params if p.numel() == size],
                             update_buffer=[torch.empty(size, device='cuda', dtype=torch.bfloat16) for _ in range(self.world_size)])
                        for size in sizes]
        super().__init__(param_groups, defaults)

    def step(self):

        for group in self.param_groups:

            lr = group['lr']
            momentum = group['momentum']
            nesterov = group['nesterov']
            ns_steps = group['ns_steps']
            update_buffers = group['update_buffer']
            # generate weight updates in distributed fashion
            params = group['params']
            handle = None
            params_world = None
            def update_prev():
                if params_world is None:
                    return
                assert handle is not None
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffers):
                    p_world.data.add_(
                        g_world.view_as(p_world),
                        alpha=-lr * max(1, p_world.size(0) / p_world.size(1)) ** 0.5,
                    )
            for base_i in range(len(params))[::self.world_size]:
                if base_i + rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if 'momentum_buffer' not in state:
                        state['momentum_buffer'] = torch.zeros_like(g)
                    buf = state['momentum_buffer']
                    buf.lerp_(g, 1 - momentum)
                    g = g.lerp_(buf, momentum) if nesterov else buf
                    g = zeropower_via_newtonschulz5(g, steps=ns_steps).flatten()
                else:
                    g = update_buffers[rank]
                update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather(update_buffers, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the GPT-2 model

def norm(x):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):

    def __init__(self, in_features, out_features):
        super().__init__(in_features, out_features, bias=False)

    def forward(self, x):
        return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):

    def __init__(self, dim, max_seq_len=65536):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum('i,j -> ij', t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x):
        cos, sin = self.cos[None, :x.size(-3), None, :], self.sin[None, :x.size(-3), None, :]
        x1, x2 = x.float().chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x)

class CausalSelfAttention(nn.Module):

    def __init__(self, dim, num_heads, layer_id):
        super().__init__()
        assert dim % num_heads == 0
        self.num_heads = num_heads
        self.layer_id = layer_id
        self.c_q = CastedLinear(dim, dim)
        self.c_k = CastedLinear(dim, dim)
        self.c_v = CastedLinear(dim, dim)
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(dim // num_heads) # dim // num_heads = head_dim
        self.c_proj = CastedLinear(dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977
        self.attn_scale = 0.13 + 0.01 * min(layer_id, 11 - layer_id)  # unet pattern attention scale by @leloykun

    def forward(self, x, ve, block_mask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, 'Must use batch size = 1 for FlexAttention'
        q = self.c_q(x).view(B, T, self.num_heads, -1)
        k = self.c_k(x).view(B, T, self.num_heads, -1)
        v = self.c_v(x).view(B, T, self.num_heads, -1)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale)
        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):

    def __init__(self, dim):
        super().__init__()
        self.c_fc = CastedLinear(dim, 4 * dim)
        self.c_proj = CastedLinear(4 * dim, dim)
        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977

    def forward(self, x):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):

    def __init__(self, model_dim, num_heads, layer_id, use_attn=True):
        super().__init__()
        self.attn = CausalSelfAttention(model_dim, num_heads, layer_id) if use_attn else None
        self.mlp = MLP(model_dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x, ve, x0, block_mask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

class ValueEmbedding(nn.Module):
    def __init__(self, vocab_size, model_dim):
        super().__init__()
        self.embed = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])

    def forward(self, inputs):
        ve = [emb(inputs).bfloat16() for emb in self.embed]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2], None, None, None, None, None, None, ve[0], ve[1], ve[2]]
        return ve

# -----------------------------------------------------------------------------
# The main GPT-2 model

class GPT(nn.Module):

    def __init__(self, vocab_size, num_layers, num_heads, model_dim):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, layer_id=layer_id, use_attn=(layer_id != 7))
                                     for layer_id in range(num_layers)])
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual learning
        # U-net structure on token value embeddings by @leloykun
        self.value_embeds = ValueEmbedding(vocab_size, model_dim)
        self.lm_head = CastedLinear(model_dim, vocab_size)
        self.lm_head.weight.data.zero_() # @Grad62304977
        # U-net design by @brendanh0gan
        self.num_encoder_layers = num_layers // 2 # Half of the layers for encoder
        self.num_decoder_layers = num_layers - self.num_encoder_layers # Remaining for decoder
        # Add learnable skip connection weights for decoder layers
        self.skip_weights = nn.Parameter(torch.ones(self.num_decoder_layers))

    def forward(self, inputs, targets, sliding_window_num_blocks):
        BLOCK_SIZE = 128
        seq_len = len(inputs)
        assert seq_len % BLOCK_SIZE == 0
        total_num_blocks = seq_len // BLOCK_SIZE
        assert inputs.ndim == 1
        docs = (inputs == 50256).cumsum(0)
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_mask):
            num_blocks = dense_mask.sum(dim=-1, dtype=torch.int32)
            indices = dense_mask.argsort(dim=-1, descending=True, stable=True).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        def create_doc_swc_block_mask(sliding_window_num_blocks):
            kv_idx = block_idx = torch.arange(total_num_blocks, dtype=torch.int32, device='cuda')
            q_idx = block_idx[:, None]
            causal_bm = q_idx >= kv_idx
            causal_full_bm = q_idx > kv_idx
            window_bm = q_idx - kv_idx < sliding_window_num_blocks
            window_full_bm = window_bm # block-wise sliding window by @YouJiacheng
            # document_bm = (docs_low[q_idx] <= docs_high[kv_idx]) & (docs_low[kv_idx] <= docs_high[q_idx])
            document_bm = (docs_low[:, None] <= docs_high) & (docs_low <= docs_high[:, None])
            document_full_bm = (docs_low[:, None] == docs_high) & (docs_low == docs_high[:, None])
            nonzero_bm = causal_bm & window_bm & document_bm
            full_bm  = causal_full_bm & window_full_bm & document_full_bm
            kv_num_blocks, kv_indices = dense_to_ordered(nonzero_bm & ~full_bm)
            full_kv_num_blocks, full_kv_indices = dense_to_ordered(full_bm)
            return BlockMask.from_kv_blocks(
                kv_num_blocks,
                kv_indices,
                full_kv_num_blocks,
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )

        block_mask = create_doc_swc_block_mask(sliding_window_num_blocks)

        x0 = norm(self.embed(inputs[None]).bfloat16()) # use of norm here by @Grad62304977
        x = x0
        ve = self.value_embeds(inputs)
        assert len(ve) == len(self.blocks)
        ve_enc, ve_dec = ve[:self.num_encoder_layers], ve[self.num_encoder_layers:]

        # Store outputs for U-Net skip connections
        skip_connections = []
        # Encoder pass - process only the first half of the blocks
        for i in range(self.num_encoder_layers):
            x = self.blocks[i](x, ve_enc[i], x0, block_mask)
            skip_connections.append(x)
        # Decoder pass - process the remaining blocks with weighted skip connections
        for i in range(self.num_decoder_layers):
            x = x + self.skip_weights[i] * skip_connections.pop()
            # U-net structure on token value embeddings by @leloykun
            x = self.blocks[self.num_encoder_layers + i](x, ve_dec[i], x0, block_mask)

        x = norm(x)
        logits = self.lm_head(x)
        logits = 15 * torch.tanh(logits / 15) # @Grad62304977 added tanh softcapping, @KoszarskyB reduced it from 30 to 15
        logits = logits.float()
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(path):
    # only reads the header, returns header data
    # header is 256 int32
    header = torch.from_file(path, False, 256, dtype=torch.int32)
    assert header[0] == 20240520, 'magic number mismatch in the data .bin file'
    assert header[1] == 1, 'unsupported version'
    num_tokens = int(header[2]) # number of tokens (claimed)
    with open(path, 'rb', buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, 'number of tokens read does not match header'
    return tokens

class DistributedDataLoader:

    def __init__(self, filename_pattern):
        self.rank = int(os.environ['RANK'])
        self.world_size = int(os.environ['WORLD_SIZE'])
        self.files = sorted(glob.glob(filename_pattern))
        self.reset()

    def reset(self):
        self.current_shard = -1
        self.advance()

    def advance(self):
        self.current_shard = (self.current_shard + 1) % len(self.files)
        self.current_position = 0
        self.tokens = _load_data_shard(self.files[self.current_shard])

    def next_batch(self, batch_size):
        assert batch_size % self.world_size == 0
        device_batch_size = batch_size // self.world_size
        # load next shard if necessary
        if self.current_position + batch_size + 1 >= len(self.tokens):
            self.advance()
        pos = self.current_position + self.rank * device_batch_size
        device_batch_tokens = self.tokens[pos:pos+device_batch_size+1]
        # advance current position
        self.current_position += batch_size
        inputs = device_batch_tokens[:-1].to(device='cuda', dtype=torch.int32, non_blocking=True)
        targets = device_batch_tokens[1:].to(device='cuda', dtype=torch.int64, non_blocking=True)
        return inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = 'data/fineweb10B/fineweb_train_*.bin' # input .bin to train on
    val_files = 'data/fineweb10B/fineweb_val_*.bin' # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # optimization
    batch_size = 8*64*1024 # batch size in tokens
    num_iterations = 1375 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    bf16_embeds = True
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    # implementation
    max_device_batch_size = 64*1024 # batch size per device in tokens
    save_checkpoint = False
args = Hyperparameters()

micro_bs = args.max_device_batch_size

# set up DDP (distributed data parallel). torchrun sets this env variable
rank = int(os.environ['RANK'])
local_rank = int(os.environ['LOCAL_RANK'])
world_size = int(os.environ['WORLD_SIZE'])
assert torch.cuda.is_available()
torch.cuda.set_device(local_rank)
dist.init_process_group(backend='nccl', device_id=torch.device(local_rank))
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs('logs', exist_ok=True)
    logfile = f'logs/{run_id}.txt'
    print(logfile)

def print0(s, console=False):
    if master_process:
        with open(logfile, 'a') as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0('='*100)
# log information about the hardware/software environment this is running on
print0(f'Running Python {sys.version}')
print0(f'Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}')
print0(subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout)
print0('='*100)

# load data
train_loader = DistributedDataLoader(args.train_files)
val_loader = DistributedDataLoader(args.val_files)
print0(f'Training dataloader files: {train_loader.files}')
print0(f'Validation dataloader files: {val_loader.files}')
print0('='*100)

# there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. suggested to me by @Grad62304977.
# this originates from Karpathy's experiments.
model = GPT(vocab_size=50304, num_layers=12, num_heads=6, model_dim=768)
model = model.cuda()
if args.bf16_embeds:
    for m in model.modules():
        if isinstance(m, nn.Embedding):
            m.bfloat16()
model: GPT = torch.compile(model)
ddp_model = DDP(model, device_ids=[local_rank], broadcast_buffers=False, gradient_as_bucket_view=True)

# collect the parameters to optimize
hidden_matrix_params = [p for p in model.blocks.parameters() if p.ndim == 2]
embed_params = [model.embed.weight, *model.value_embeds.parameters()]
scalar_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" not in n]
attn_scale_params = [p for n, p in model.named_parameters() if p.ndim < 2 and "attn_scale" in n]
head_params = [model.lm_head.weight]

# init the optimizer(s)
optimizer1 = torch.optim.Adam([dict(params=embed_params, lr=0.6),
                               dict(params=head_params, lr=0.008),
                               dict(params=scalar_params, lr=0.04),
                               dict(params=attn_scale_params, lr=0.01)],
                              betas=(0.8, 0.95), fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95)
optimizers = [optimizer1, optimizer2]

# learning rate schedule: stable then decay
def get_lr(it):
    t = 1 - it / args.num_iterations # time remaining in training
    assert 1 >= t > 0
    # 1) constant lr for first part of training
    if t >= args.cooldown_frac:
        return 1.0
    # 2) then linear cooldown
    else:
        return t / args.cooldown_frac
schedulers = [torch.optim.lr_scheduler.LambdaLR(opt, get_lr) for opt in optimizers]

# sliding window size schedule: linear increase over training in chunks of 128 from 128 -> 1792. By @fernbear.bsky.social
def get_sliding_window_blocks(it):
    x = it / args.num_iterations # training progress
    assert 0 <= x <= 1
    return int(((1 - x) * 128 + x * 1856) // 128)
sliding_window_num_blocks = torch.tensor(1, dtype=torch.int32, device='cuda')

# Start training loop
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    # This effectively ignores timing first 10 steps, which are slower for weird reasons.
    # Alternately, and slightly more correctly in terms of benchmarking, we could do 10
    # steps with dummy data first, and then re-initialize the model and reset the loader.
    if step == 10:
        training_time_ms = 0
        t0 = time.perf_counter()
    timed_steps = float('nan') if step <= 11 else (step - 10) + 1 # <= 11 to avoid bug in val

    sliding_window_num_blocks.copy_(get_sliding_window_blocks(step))

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        # run validation batches
        model.eval()
        val_loader.reset()
        val_loss = 0.0
        # calculate the number of steps to take in the val loop.
        val_batch_size = world_size * micro_bs
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        for _ in range(val_steps):
            with torch.no_grad():
                inputs_val, targets_val = val_loader.next_batch(val_batch_size)
                val_loss += ddp_model(inputs_val, targets_val, sliding_window_num_blocks)
        # Print attention scales
        for n, p in model.named_parameters():
            if p.ndim < 2 and "attn_scale" in n:
                print0(f'{n}: {p.item()}', console=True)
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        val_loss /= val_steps
        # logging
        print0(f'step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/(timed_steps-1):.2f}ms', console=True)
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f'logs/{run_id}', exist_ok=True)
            torch.save(log, f'logs/{run_id}/state_step{step:06d}.pt')
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    model.train()
    batch_size = args.batch_size
    assert batch_size % world_size == 0
    inputs_train, targets_train = train_loader.next_batch(batch_size)
    assert len(inputs_train) <= micro_bs or len(inputs_train) % micro_bs == 0
    for micro_inputs_train, micro_targets_train in zip(inputs_train.split(micro_bs), targets_train.split(micro_bs)):
        ddp_model(micro_inputs_train, micro_targets_train, sliding_window_num_blocks).backward()
    # momentum warmup for Muon
    frac = min(step/300, 1)
    for group in optimizer2.param_groups:
        group['momentum'] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers and schedulers
    for opt, sched in zip(optimizers, schedulers):
        opt.step()
        if step != train_steps-1:
            sched.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_time = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f'step:{step+1}/{train_steps} train_time:{approx_time:.0f}ms step_avg:{approx_time/timed_steps:.2f}ms', console=True)

print0(f'peak memory consumption: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB')
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250110+cu126 compiled for CUDA 12.6
Wed Jan 15 21:26:47 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.183.06             Driver Version: 535.183.06   CUDA Version: 12.4     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA H100 80GB HBM3          On  | 00000000:19:00.0 Off |                    0 |
| N/A   38C    P0             122W / 700W |   7713MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  | 00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  | 00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0             115W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  | 00000000:5D:00.0 Off |                    0 |
| N/A   38C    P0             118W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  | 00000000:9B:00.0 Off |                    0 |
| N/A   38C    P0             120W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  | 00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0             114W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  | 00000000:CB:00.0 Off |                    0 |
| N/A   37C    P0             116W / 700W |   3451MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  | 00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0             113W / 700W |   3211MiB / 81559MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+

====================================================================================================
Training dataloader files: ['data/fineweb10B/fineweb_train_000001.bin', 'data/fineweb10B/fineweb_train_000002.bin', 'data/fineweb10B/fineweb_train_000003.bin', 'data/fineweb10B/fineweb_train_000004.bin', 'data/fineweb10B/fineweb_train_000005.bin', 'data/fineweb10B/fineweb_train_000006.bin', 'data/fineweb10B/fineweb_train_000007.bin', 'data/fineweb10B/fineweb_train_000008.bin', 'data/fineweb10B/fineweb_train_000009.bin']
Validation dataloader files: ['data/fineweb10B/fineweb_val_000000.bin']
====================================================================================================
step:0/1375 val_loss:10.8258 train_time:0ms step_avg:nanms
step:1/1375 train_time:27843ms step_avg:nanms
step:2/1375 train_time:27928ms step_avg:nanms
step:3/1375 train_time:28113ms step_avg:nanms
step:4/1375 train_time:28245ms step_avg:nanms
step:5/1375 train_time:28378ms step_avg:nanms
step:6/1375 train_time:28511ms step_avg:nanms
step:7/1375 train_time:28643ms step_avg:nanms
step:8/1375 train_time:28774ms step_avg:nanms
step:9/1375 train_time:28907ms step_avg:nanms
step:10/1375 train_time:29042ms step_avg:nanms
step:11/1375 train_time:135ms step_avg:nanms
step:12/1375 train_time:268ms step_avg:nanms
step:13/1375 train_time:402ms step_avg:134.01ms
step:14/1375 train_time:534ms step_avg:133.53ms
step:15/1375 train_time:669ms step_avg:133.85ms
step:16/1375 train_time:802ms step_avg:133.70ms
step:17/1375 train_time:937ms step_avg:133.84ms
step:18/1375 train_time:1071ms step_avg:133.91ms
step:19/1375 train_time:1208ms step_avg:134.19ms
step:20/1375 train_time:1342ms step_avg:134.20ms
step:21/1375 train_time:1476ms step_avg:134.20ms
step:22/1375 train_time:1609ms step_avg:134.11ms
step:23/1375 train_time:1743ms step_avg:134.08ms
step:24/1375 train_time:1875ms step_avg:133.92ms
step:25/1375 train_time:2009ms step_avg:133.95ms
step:26/1375 train_time:2143ms step_avg:133.96ms
step:27/1375 train_time:2279ms step_avg:134.05ms
step:28/1375 train_time:2412ms step_avg:134.01ms
step:29/1375 train_time:2547ms step_avg:134.06ms
step:30/1375 train_time:2681ms step_avg:134.04ms
step:31/1375 train_time:2815ms step_avg:134.02ms
step:32/1375 train_time:2948ms step_avg:134.00ms
step:33/1375 train_time:3081ms step_avg:133.96ms
step:34/1375 train_time:3214ms step_avg:133.91ms
step:35/1375 train_time:3348ms step_avg:133.94ms
step:36/1375 train_time:3483ms step_avg:133.94ms
step:37/1375 train_time:3618ms step_avg:134.00ms
step:38/1375 train_time:3751ms step_avg:133.96ms
step:39/1375 train_time:3885ms step_avg:133.95ms
step:40/1375 train_time:4018ms step_avg:133.94ms
step:41/1375 train_time:4151ms step_avg:133.90ms
step:42/1375 train_time:4285ms step_avg:133.90ms
step:43/1375 train_time:4419ms step_avg:133.90ms
step:44/1375 train_time:4552ms step_avg:133.89ms
step:45/1375 train_time:4687ms step_avg:133.93ms
step:46/1375 train_time:4820ms step_avg:133.90ms
step:47/1375 train_time:4954ms step_avg:133.88ms
step:48/1375 train_time:5088ms step_avg:133.89ms
step:49/1375 train_time:5223ms step_avg:133.91ms
step:50/1375 train_time:5356ms step_avg:133.91ms
step:51/1375 train_time:5491ms step_avg:133.93ms
step:52/1375 train_time:5626ms step_avg:133.95ms
step:53/1375 train_time:5760ms step_avg:133.96ms
step:54/1375 train_time:5894ms step_avg:133.95ms
step:55/1375 train_time:6028ms step_avg:133.96ms
step:56/1375 train_time:6162ms step_avg:133.96ms
step:57/1375 train_time:6296ms step_avg:133.96ms
step:58/1375 train_time:6431ms step_avg:133.98ms
step:59/1375 train_time:6568ms step_avg:134.03ms
step:60/1375 train_time:6701ms step_avg:134.02ms
step:61/1375 train_time:6836ms step_avg:134.03ms
step:62/1375 train_time:6970ms step_avg:134.05ms
step:63/1375 train_time:7106ms step_avg:134.07ms
step:64/1375 train_time:7239ms step_avg:134.05ms
step:65/1375 train_time:7373ms step_avg:134.06ms
step:66/1375 train_time:7507ms step_avg:134.06ms
step:67/1375 train_time:7642ms step_avg:134.08ms
step:68/1375 train_time:7776ms step_avg:134.07ms
step:69/1375 train_time:7910ms step_avg:134.07ms
step:70/1375 train_time:8043ms step_avg:134.05ms
step:71/1375 train_time:8176ms step_avg:134.04ms
step:72/1375 train_time:8311ms step_avg:134.04ms
step:73/1375 train_time:8446ms step_avg:134.07ms
step:74/1375 train_time:8581ms step_avg:134.08ms
step:75/1375 train_time:8715ms step_avg:134.08ms
step:76/1375 train_time:8849ms step_avg:134.08ms
step:77/1375 train_time:8983ms step_avg:134.08ms
step:78/1375 train_time:9118ms step_avg:134.08ms
step:79/1375 train_time:9251ms step_avg:134.07ms
step:80/1375 train_time:9385ms step_avg:134.07ms
step:81/1375 train_time:9519ms step_avg:134.07ms
step:82/1375 train_time:9653ms step_avg:134.07ms
step:83/1375 train_time:9787ms step_avg:134.07ms
step:84/1375 train_time:9921ms step_avg:134.07ms
step:85/1375 train_time:10054ms step_avg:134.06ms
step:86/1375 train_time:10188ms step_avg:134.05ms
step:87/1375 train_time:10323ms step_avg:134.06ms
step:88/1375 train_time:10457ms step_avg:134.07ms
step:89/1375 train_time:10592ms step_avg:134.07ms
step:90/1375 train_time:10727ms step_avg:134.08ms
step:91/1375 train_time:10862ms step_avg:134.10ms
step:92/1375 train_time:10996ms step_avg:134.10ms
step:93/1375 train_time:11131ms step_avg:134.11ms
step:94/1375 train_time:11266ms step_avg:134.12ms
step:95/1375 train_time:11401ms step_avg:134.13ms
step:96/1375 train_time:11534ms step_avg:134.12ms
step:97/1375 train_time:11669ms step_avg:134.13ms
step:98/1375 train_time:11803ms step_avg:134.13ms
step:99/1375 train_time:11938ms step_avg:134.14ms
step:100/1375 train_time:12072ms step_avg:134.13ms
step:101/1375 train_time:12207ms step_avg:134.15ms
step:102/1375 train_time:12341ms step_avg:134.14ms
step:103/1375 train_time:12476ms step_avg:134.15ms
step:104/1375 train_time:12614ms step_avg:134.19ms
step:105/1375 train_time:12752ms step_avg:134.23ms
step:106/1375 train_time:12890ms step_avg:134.27ms
step:107/1375 train_time:13027ms step_avg:134.30ms
step:108/1375 train_time:13165ms step_avg:134.33ms
step:109/1375 train_time:13301ms step_avg:134.35ms
step:110/1375 train_time:13437ms step_avg:134.37ms
step:111/1375 train_time:13575ms step_avg:134.40ms
step:112/1375 train_time:13712ms step_avg:134.43ms
step:113/1375 train_time:13850ms step_avg:134.46ms
step:114/1375 train_time:13988ms step_avg:134.50ms
step:115/1375 train_time:14125ms step_avg:134.52ms
step:116/1375 train_time:14262ms step_avg:134.55ms
step:117/1375 train_time:14398ms step_avg:134.57ms
step:118/1375 train_time:14536ms step_avg:134.59ms
step:119/1375 train_time:14674ms step_avg:134.62ms
step:120/1375 train_time:14813ms step_avg:134.66ms
step:121/1375 train_time:14950ms step_avg:134.69ms
step:122/1375 train_time:15088ms step_avg:134.71ms
step:123/1375 train_time:15225ms step_avg:134.73ms
step:124/1375 train_time:15361ms step_avg:134.75ms
step:125/1375 train_time:15498ms step_avg:134.77ms
step:125/1375 val_loss:4.3629 train_time:15565ms step_avg:135.34ms
step:126/1375 train_time:15641ms step_avg:134.84ms
step:127/1375 train_time:15780ms step_avg:134.87ms
step:128/1375 train_time:15918ms step_avg:134.90ms
step:129/1375 train_time:16054ms step_avg:134.91ms
step:130/1375 train_time:16190ms step_avg:134.92ms
step:131/1375 train_time:16326ms step_avg:134.93ms
step:132/1375 train_time:16462ms step_avg:134.94ms
step:133/1375 train_time:16600ms step_avg:134.96ms
step:134/1375 train_time:16740ms step_avg:135.00ms
step:135/1375 train_time:16880ms step_avg:135.04ms
step:136/1375 train_time:17018ms step_avg:135.07ms
step:137/1375 train_time:17155ms step_avg:135.08ms
step:138/1375 train_time:17292ms step_avg:135.10ms
step:139/1375 train_time:17429ms step_avg:135.11ms
step:140/1375 train_time:17564ms step_avg:135.11ms
step:141/1375 train_time:17702ms step_avg:135.13ms
step:142/1375 train_time:17840ms step_avg:135.15ms
step:143/1375 train_time:17978ms step_avg:135.18ms
step:144/1375 train_time:18117ms step_avg:135.20ms
step:145/1375 train_time:18254ms step_avg:135.22ms
step:146/1375 train_time:18391ms step_avg:135.23ms
step:147/1375 train_time:18529ms step_avg:135.25ms
step:148/1375 train_time:18666ms step_avg:135.26ms
step:149/1375 train_time:18804ms step_avg:135.28ms
step:150/1375 train_time:18942ms step_avg:135.30ms
step:151/1375 train_time:19080ms step_avg:135.32ms
step:152/1375 train_time:19218ms step_avg:135.34ms
step:153/1375 train_time:19355ms step_avg:135.35ms
step:154/1375 train_time:19492ms step_avg:135.36ms
step:155/1375 train_time:19630ms step_avg:135.38ms
step:156/1375 train_time:19767ms step_avg:135.39ms
step:157/1375 train_time:19904ms step_avg:135.40ms
step:158/1375 train_time:20042ms step_avg:135.42ms
step:159/1375 train_time:20181ms step_avg:135.44ms
step:160/1375 train_time:20319ms step_avg:135.46ms
step:161/1375 train_time:20457ms step_avg:135.47ms
step:162/1375 train_time:20593ms step_avg:135.48ms
step:163/1375 train_time:20730ms step_avg:135.49ms
step:164/1375 train_time:20869ms step_avg:135.51ms
step:165/1375 train_time:21009ms step_avg:135.54ms
step:166/1375 train_time:21146ms step_avg:135.55ms
step:167/1375 train_time:21283ms step_avg:135.56ms
step:168/1375 train_time:21421ms step_avg:135.58ms
step:169/1375 train_time:21558ms step_avg:135.59ms
step:170/1375 train_time:21695ms step_avg:135.59ms
step:171/1375 train_time:21833ms step_avg:135.61ms
step:172/1375 train_time:21970ms step_avg:135.62ms
step:173/1375 train_time:22109ms step_avg:135.64ms
step:174/1375 train_time:22247ms step_avg:135.65ms
step:175/1375 train_time:22386ms step_avg:135.67ms
step:176/1375 train_time:22524ms step_avg:135.69ms
step:177/1375 train_time:22661ms step_avg:135.70ms
step:178/1375 train_time:22799ms step_avg:135.71ms
step:179/1375 train_time:22936ms step_avg:135.71ms
step:180/1375 train_time:23074ms step_avg:135.73ms
step:181/1375 train_time:23212ms step_avg:135.74ms
step:182/1375 train_time:23350ms step_avg:135.76ms
step:183/1375 train_time:23489ms step_avg:135.78ms
step:184/1375 train_time:23626ms step_avg:135.78ms
step:185/1375 train_time:23764ms step_avg:135.79ms
step:186/1375 train_time:23901ms step_avg:135.80ms
step:187/1375 train_time:24038ms step_avg:135.81ms
step:188/1375 train_time:24174ms step_avg:135.81ms
step:189/1375 train_time:24312ms step_avg:135.82ms
step:190/1375 train_time:24450ms step_avg:135.83ms
step:191/1375 train_time:24633ms step_avg:136.09ms
step:192/1375 train_time:24769ms step_avg:136.09ms
step:193/1375 train_time:24905ms step_avg:136.09ms
step:194/1375 train_time:25042ms step_avg:136.10ms
step:195/1375 train_time:25178ms step_avg:136.10ms
step:196/1375 train_time:25316ms step_avg:136.11ms
step:197/1375 train_time:25455ms step_avg:136.12ms
step:198/1375 train_time:25595ms step_avg:136.14ms
step:199/1375 train_time:25734ms step_avg:136.16ms
step:200/1375 train_time:25870ms step_avg:136.16ms
step:201/1375 train_time:26007ms step_avg:136.16ms
step:202/1375 train_time:26145ms step_avg:136.17ms
step:203/1375 train_time:26282ms step_avg:136.18ms
step:204/1375 train_time:26420ms step_avg:136.19ms
step:205/1375 train_time:26560ms step_avg:136.21ms
step:206/1375 train_time:26701ms step_avg:136.23ms
step:207/1375 train_time:26841ms step_avg:136.25ms
step:208/1375 train_time:26981ms step_avg:136.27ms
step:209/1375 train_time:27121ms step_avg:136.29ms
step:210/1375 train_time:27262ms step_avg:136.31ms
step:211/1375 train_time:27403ms step_avg:136.33ms
step:212/1375 train_time:27544ms step_avg:136.35ms
step:213/1375 train_time:27684ms step_avg:136.37ms
step:214/1375 train_time:27824ms step_avg:136.39ms
step:215/1375 train_time:27964ms step_avg:136.41ms
step:216/1375 train_time:28104ms step_avg:136.43ms
step:217/1375 train_time:28244ms step_avg:136.45ms
step:218/1375 train_time:28385ms step_avg:136.46ms
step:219/1375 train_time:28525ms step_avg:136.48ms
step:220/1375 train_time:28666ms step_avg:136.50ms
step:221/1375 train_time:28806ms step_avg:136.52ms
step:222/1375 train_time:28946ms step_avg:136.54ms
step:223/1375 train_time:29085ms step_avg:136.55ms
step:224/1375 train_time:29225ms step_avg:136.56ms
step:225/1375 train_time:29365ms step_avg:136.58ms
step:226/1375 train_time:29505ms step_avg:136.60ms
step:227/1375 train_time:29645ms step_avg:136.61ms
step:228/1375 train_time:29786ms step_avg:136.63ms
step:229/1375 train_time:29926ms step_avg:136.65ms
step:230/1375 train_time:30065ms step_avg:136.66ms
step:231/1375 train_time:30206ms step_avg:136.68ms
step:232/1375 train_time:30346ms step_avg:136.69ms
step:233/1375 train_time:30486ms step_avg:136.71ms
step:234/1375 train_time:30626ms step_avg:136.72ms
step:235/1375 train_time:30767ms step_avg:136.74ms
step:236/1375 train_time:30908ms step_avg:136.76ms
step:237/1375 train_time:31048ms step_avg:136.78ms
step:238/1375 train_time:31188ms step_avg:136.79ms
step:239/1375 train_time:31330ms step_avg:136.81ms
step:240/1375 train_time:31470ms step_avg:136.83ms
step:241/1375 train_time:31610ms step_avg:136.84ms
step:242/1375 train_time:31750ms step_avg:136.85ms
step:243/1375 train_time:31889ms step_avg:136.86ms
step:244/1375 train_time:32029ms step_avg:136.88ms
step:245/1375 train_time:32168ms step_avg:136.89ms
step:246/1375 train_time:32310ms step_avg:136.91ms
step:247/1375 train_time:32453ms step_avg:136.93ms
step:248/1375 train_time:32593ms step_avg:136.95ms
step:249/1375 train_time:32734ms step_avg:136.96ms
step:250/1375 train_time:32874ms step_avg:136.98ms
step:250/1375 val_loss:3.9516 train_time:32941ms step_avg:137.25ms
step:251/1375 train_time:33018ms step_avg:137.00ms
step:252/1375 train_time:33160ms step_avg:137.03ms
step:253/1375 train_time:33300ms step_avg:137.04ms
step:254/1375 train_time:33440ms step_avg:137.05ms
step:255/1375 train_time:33578ms step_avg:137.05ms
step:256/1375 train_time:33717ms step_avg:137.06ms
step:257/1375 train_time:33857ms step_avg:137.07ms
step:258/1375 train_time:33999ms step_avg:137.09ms
step:259/1375 train_time:34141ms step_avg:137.11ms
step:260/1375 train_time:34281ms step_avg:137.12ms
step:261/1375 train_time:34422ms step_avg:137.14ms
step:262/1375 train_time:34562ms step_avg:137.15ms
step:263/1375 train_time:34700ms step_avg:137.15ms
step:264/1375 train_time:34840ms step_avg:137.17ms
step:265/1375 train_time:34981ms step_avg:137.18ms
step:266/1375 train_time:35121ms step_avg:137.19ms
step:267/1375 train_time:35262ms step_avg:137.21ms
step:268/1375 train_time:35402ms step_avg:137.22ms
step:269/1375 train_time:35543ms step_avg:137.23ms
step:270/1375 train_time:35683ms step_avg:137.24ms
step:271/1375 train_time:35822ms step_avg:137.25ms
step:272/1375 train_time:35962ms step_avg:137.26ms
step:273/1375 train_time:36102ms step_avg:137.27ms
step:274/1375 train_time:36242ms step_avg:137.28ms
step:275/1375 train_time:36383ms step_avg:137.30ms
step:276/1375 train_time:36522ms step_avg:137.30ms
step:277/1375 train_time:36662ms step_avg:137.31ms
step:278/1375 train_time:36802ms step_avg:137.32ms
step:279/1375 train_time:36943ms step_avg:137.34ms
step:280/1375 train_time:37083ms step_avg:137.34ms
step:281/1375 train_time:37223ms step_avg:137.35ms
step:282/1375 train_time:37364ms step_avg:137.37ms
step:283/1375 train_time:37504ms step_avg:137.38ms
step:284/1375 train_time:37644ms step_avg:137.39ms
step:285/1375 train_time:37784ms step_avg:137.40ms
step:286/1375 train_time:37924ms step_avg:137.40ms
step:287/1375 train_time:38065ms step_avg:137.42ms
step:288/1375 train_time:38205ms step_avg:137.43ms
step:289/1375 train_time:38345ms step_avg:137.44ms
step:290/1375 train_time:38486ms step_avg:137.45ms
step:291/1375 train_time:38626ms step_avg:137.46ms
step:292/1375 train_time:38766ms step_avg:137.47ms
step:293/1375 train_time:38905ms step_avg:137.47ms
step:294/1375 train_time:39046ms step_avg:137.49ms
step:295/1375 train_time:39186ms step_avg:137.50ms
step:296/1375 train_time:39327ms step_avg:137.51ms
step:297/1375 train_time:39468ms step_avg:137.52ms
step:298/1375 train_time:39607ms step_avg:137.53ms
step:299/1375 train_time:39748ms step_avg:137.53ms
step:300/1375 train_time:39888ms step_avg:137.55ms
step:301/1375 train_time:40028ms step_avg:137.55ms
step:302/1375 train_time:40168ms step_avg:137.56ms
step:303/1375 train_time:40308ms step_avg:137.57ms
step:304/1375 train_time:40448ms step_avg:137.58ms
step:305/1375 train_time:40589ms step_avg:137.59ms
step:306/1375 train_time:40729ms step_avg:137.60ms
step:307/1375 train_time:40870ms step_avg:137.61ms
step:308/1375 train_time:41012ms step_avg:137.62ms
step:309/1375 train_time:41155ms step_avg:137.64ms
step:310/1375 train_time:41298ms step_avg:137.66ms
step:311/1375 train_time:41439ms step_avg:137.67ms
step:312/1375 train_time:41581ms step_avg:137.69ms
step:313/1375 train_time:41723ms step_avg:137.70ms
step:314/1375 train_time:41865ms step_avg:137.71ms
step:315/1375 train_time:42008ms step_avg:137.73ms
step:316/1375 train_time:42149ms step_avg:137.74ms
step:317/1375 train_time:42292ms step_avg:137.76ms
step:318/1375 train_time:42434ms step_avg:137.77ms
step:319/1375 train_time:42577ms step_avg:137.79ms
step:320/1375 train_time:42719ms step_avg:137.80ms
step:321/1375 train_time:42862ms step_avg:137.82ms
step:322/1375 train_time:43004ms step_avg:137.83ms
step:323/1375 train_time:43145ms step_avg:137.84ms
step:324/1375 train_time:43287ms step_avg:137.86ms
step:325/1375 train_time:43429ms step_avg:137.87ms
step:326/1375 train_time:43572ms step_avg:137.88ms
step:327/1375 train_time:43714ms step_avg:137.90ms
step:328/1375 train_time:43857ms step_avg:137.92ms
step:329/1375 train_time:44000ms step_avg:137.93ms
step:330/1375 train_time:44141ms step_avg:137.94ms
step:331/1375 train_time:44283ms step_avg:137.95ms
step:332/1375 train_time:44425ms step_avg:137.97ms
step:333/1375 train_time:44567ms step_avg:137.98ms
step:334/1375 train_time:44709ms step_avg:137.99ms
step:335/1375 train_time:44852ms step_avg:138.01ms
step:336/1375 train_time:44995ms step_avg:138.02ms
step:337/1375 train_time:45137ms step_avg:138.03ms
step:338/1375 train_time:45279ms step_avg:138.05ms
step:339/1375 train_time:45421ms step_avg:138.06ms
step:340/1375 train_time:45563ms step_avg:138.07ms
step:341/1375 train_time:45706ms step_avg:138.09ms
step:342/1375 train_time:45848ms step_avg:138.10ms
step:343/1375 train_time:45990ms step_avg:138.11ms
step:344/1375 train_time:46130ms step_avg:138.11ms
step:345/1375 train_time:46273ms step_avg:138.13ms
step:346/1375 train_time:46415ms step_avg:138.14ms
step:347/1375 train_time:46558ms step_avg:138.15ms
step:348/1375 train_time:46699ms step_avg:138.16ms
step:349/1375 train_time:46841ms step_avg:138.18ms
step:350/1375 train_time:46985ms step_avg:138.19ms
step:351/1375 train_time:47127ms step_avg:138.20ms
step:352/1375 train_time:47269ms step_avg:138.21ms
step:353/1375 train_time:47411ms step_avg:138.22ms
step:354/1375 train_time:47554ms step_avg:138.24ms
step:355/1375 train_time:47696ms step_avg:138.25ms
step:356/1375 train_time:47839ms step_avg:138.26ms
step:357/1375 train_time:47981ms step_avg:138.27ms
step:358/1375 train_time:48124ms step_avg:138.29ms
step:359/1375 train_time:48265ms step_avg:138.29ms
step:360/1375 train_time:48408ms step_avg:138.31ms
step:361/1375 train_time:48551ms step_avg:138.32ms
step:362/1375 train_time:48694ms step_avg:138.34ms
step:363/1375 train_time:48836ms step_avg:138.35ms
step:364/1375 train_time:48978ms step_avg:138.36ms
step:365/1375 train_time:49119ms step_avg:138.36ms
step:366/1375 train_time:49261ms step_avg:138.37ms
step:367/1375 train_time:49403ms step_avg:138.38ms
step:368/1375 train_time:49546ms step_avg:138.40ms
step:369/1375 train_time:49688ms step_avg:138.41ms
step:370/1375 train_time:49830ms step_avg:138.42ms
step:371/1375 train_time:49973ms step_avg:138.43ms
step:372/1375 train_time:50115ms step_avg:138.44ms
step:373/1375 train_time:50257ms step_avg:138.45ms
step:374/1375 train_time:50399ms step_avg:138.46ms
step:375/1375 train_time:50541ms step_avg:138.47ms
step:375/1375 val_loss:3.7739 train_time:50611ms step_avg:138.66ms
step:376/1375 train_time:50688ms step_avg:138.49ms
step:377/1375 train_time:50833ms step_avg:138.51ms
step:378/1375 train_time:50974ms step_avg:138.52ms
step:379/1375 train_time:51116ms step_avg:138.53ms
step:380/1375 train_time:51257ms step_avg:138.53ms
step:381/1375 train_time:51439ms step_avg:138.65ms
step:382/1375 train_time:51581ms step_avg:138.66ms
step:383/1375 train_time:51722ms step_avg:138.67ms
step:384/1375 train_time:51864ms step_avg:138.67ms
step:385/1375 train_time:52005ms step_avg:138.68ms
step:386/1375 train_time:52146ms step_avg:138.69ms
step:387/1375 train_time:52288ms step_avg:138.70ms
step:388/1375 train_time:52434ms step_avg:138.71ms
step:389/1375 train_time:52577ms step_avg:138.73ms
step:390/1375 train_time:52719ms step_avg:138.73ms
step:391/1375 train_time:52860ms step_avg:138.74ms
step:392/1375 train_time:53002ms step_avg:138.75ms
step:393/1375 train_time:53143ms step_avg:138.76ms
step:394/1375 train_time:53286ms step_avg:138.77ms
step:395/1375 train_time:53431ms step_avg:138.78ms
step:396/1375 train_time:53574ms step_avg:138.79ms
step:397/1375 train_time:53717ms step_avg:138.80ms
step:398/1375 train_time:53860ms step_avg:138.81ms
step:399/1375 train_time:54002ms step_avg:138.82ms
step:400/1375 train_time:54144ms step_avg:138.83ms
step:401/1375 train_time:54286ms step_avg:138.84ms
step:402/1375 train_time:54430ms step_avg:138.85ms
step:403/1375 train_time:54573ms step_avg:138.86ms
step:404/1375 train_time:54715ms step_avg:138.87ms
step:405/1375 train_time:54857ms step_avg:138.88ms
step:406/1375 train_time:54998ms step_avg:138.88ms
step:407/1375 train_time:55140ms step_avg:138.89ms
step:408/1375 train_time:55281ms step_avg:138.90ms
step:409/1375 train_time:55423ms step_avg:138.90ms
step:410/1375 train_time:55566ms step_avg:138.92ms
step:411/1375 train_time:55712ms step_avg:138.93ms
step:412/1375 train_time:55856ms step_avg:138.94ms
step:413/1375 train_time:55999ms step_avg:138.96ms
step:414/1375 train_time:56143ms step_avg:138.97ms
step:415/1375 train_time:56288ms step_avg:138.98ms
step:416/1375 train_time:56433ms step_avg:139.00ms
step:417/1375 train_time:56578ms step_avg:139.01ms
step:418/1375 train_time:56722ms step_avg:139.02ms
step:419/1375 train_time:56864ms step_avg:139.03ms
step:420/1375 train_time:57008ms step_avg:139.04ms
step:421/1375 train_time:57152ms step_avg:139.06ms
step:422/1375 train_time:57296ms step_avg:139.07ms
step:423/1375 train_time:57440ms step_avg:139.08ms
step:424/1375 train_time:57585ms step_avg:139.09ms
step:425/1375 train_time:57729ms step_avg:139.10ms
step:426/1375 train_time:57874ms step_avg:139.12ms
step:427/1375 train_time:58018ms step_avg:139.13ms
step:428/1375 train_time:58162ms step_avg:139.14ms
step:429/1375 train_time:58306ms step_avg:139.15ms
step:430/1375 train_time:58451ms step_avg:139.17ms
step:431/1375 train_time:58596ms step_avg:139.18ms
step:432/1375 train_time:58740ms step_avg:139.19ms
step:433/1375 train_time:58884ms step_avg:139.21ms
step:434/1375 train_time:59027ms step_avg:139.21ms
step:435/1375 train_time:59172ms step_avg:139.23ms
step:436/1375 train_time:59316ms step_avg:139.24ms
step:437/1375 train_time:59460ms step_avg:139.25ms
step:438/1375 train_time:59603ms step_avg:139.26ms
step:439/1375 train_time:59746ms step_avg:139.27ms
step:440/1375 train_time:59891ms step_avg:139.28ms
step:441/1375 train_time:60036ms step_avg:139.29ms
step:442/1375 train_time:60179ms step_avg:139.30ms
step:443/1375 train_time:60322ms step_avg:139.31ms
step:444/1375 train_time:60467ms step_avg:139.32ms
step:445/1375 train_time:60611ms step_avg:139.34ms
step:446/1375 train_time:60756ms step_avg:139.35ms
step:447/1375 train_time:60900ms step_avg:139.36ms
step:448/1375 train_time:61044ms step_avg:139.37ms
step:449/1375 train_time:61188ms step_avg:139.38ms
step:450/1375 train_time:61333ms step_avg:139.39ms
step:451/1375 train_time:61478ms step_avg:139.40ms
step:452/1375 train_time:61620ms step_avg:139.41ms
step:453/1375 train_time:61763ms step_avg:139.42ms
step:454/1375 train_time:61906ms step_avg:139.43ms
step:455/1375 train_time:62051ms step_avg:139.44ms
step:456/1375 train_time:62195ms step_avg:139.45ms
step:457/1375 train_time:62338ms step_avg:139.46ms
step:458/1375 train_time:62482ms step_avg:139.47ms
step:459/1375 train_time:62625ms step_avg:139.48ms
step:460/1375 train_time:62769ms step_avg:139.49ms
step:461/1375 train_time:62913ms step_avg:139.50ms
step:462/1375 train_time:63058ms step_avg:139.51ms
step:463/1375 train_time:63201ms step_avg:139.52ms
step:464/1375 train_time:63345ms step_avg:139.53ms
step:465/1375 train_time:63488ms step_avg:139.53ms
step:466/1375 train_time:63633ms step_avg:139.55ms
step:467/1375 train_time:63776ms step_avg:139.55ms
step:468/1375 train_time:63919ms step_avg:139.56ms
step:469/1375 train_time:64063ms step_avg:139.57ms
step:470/1375 train_time:64206ms step_avg:139.58ms
step:471/1375 train_time:64351ms step_avg:139.59ms
step:472/1375 train_time:64496ms step_avg:139.60ms
step:473/1375 train_time:64639ms step_avg:139.61ms
step:474/1375 train_time:64782ms step_avg:139.62ms
step:475/1375 train_time:64925ms step_avg:139.62ms
step:476/1375 train_time:65070ms step_avg:139.63ms
step:477/1375 train_time:65216ms step_avg:139.65ms
step:478/1375 train_time:65359ms step_avg:139.66ms
step:479/1375 train_time:65502ms step_avg:139.66ms
step:480/1375 train_time:65645ms step_avg:139.67ms
step:481/1375 train_time:65789ms step_avg:139.68ms
step:482/1375 train_time:65934ms step_avg:139.69ms
step:483/1375 train_time:66077ms step_avg:139.70ms
step:484/1375 train_time:66221ms step_avg:139.71ms
step:485/1375 train_time:66365ms step_avg:139.72ms
step:486/1375 train_time:66509ms step_avg:139.73ms
step:487/1375 train_time:66653ms step_avg:139.73ms
step:488/1375 train_time:66798ms step_avg:139.74ms
step:489/1375 train_time:66941ms step_avg:139.75ms
step:490/1375 train_time:67085ms step_avg:139.76ms
step:491/1375 train_time:67229ms step_avg:139.77ms
step:492/1375 train_time:67374ms step_avg:139.78ms
step:493/1375 train_time:67517ms step_avg:139.79ms
step:494/1375 train_time:67659ms step_avg:139.79ms
step:495/1375 train_time:67804ms step_avg:139.80ms
step:496/1375 train_time:67948ms step_avg:139.81ms
step:497/1375 train_time:68093ms step_avg:139.82ms
step:498/1375 train_time:68238ms step_avg:139.83ms
step:499/1375 train_time:68381ms step_avg:139.84ms
step:500/1375 train_time:68525ms step_avg:139.85ms
step:500/1375 val_loss:3.6540 train_time:68595ms step_avg:139.99ms
step:501/1375 train_time:68671ms step_avg:139.86ms
step:502/1375 train_time:68815ms step_avg:139.87ms
step:503/1375 train_time:68958ms step_avg:139.87ms
step:504/1375 train_time:69101ms step_avg:139.88ms
step:505/1375 train_time:69245ms step_avg:139.89ms
step:506/1375 train_time:69388ms step_avg:139.90ms
step:507/1375 train_time:69532ms step_avg:139.90ms
step:508/1375 train_time:69677ms step_avg:139.91ms
step:509/1375 train_time:69821ms step_avg:139.92ms
step:510/1375 train_time:69965ms step_avg:139.93ms
step:511/1375 train_time:70111ms step_avg:139.94ms
step:512/1375 train_time:70256ms step_avg:139.95ms
step:513/1375 train_time:70401ms step_avg:139.96ms
step:514/1375 train_time:70547ms step_avg:139.97ms
step:515/1375 train_time:70694ms step_avg:139.99ms
step:516/1375 train_time:70838ms step_avg:140.00ms
step:517/1375 train_time:70986ms step_avg:140.01ms
step:518/1375 train_time:71131ms step_avg:140.02ms
step:519/1375 train_time:71276ms step_avg:140.03ms
step:520/1375 train_time:71421ms step_avg:140.04ms
step:521/1375 train_time:71567ms step_avg:140.05ms
step:522/1375 train_time:71713ms step_avg:140.06ms
step:523/1375 train_time:71858ms step_avg:140.07ms
step:524/1375 train_time:72004ms step_avg:140.09ms
step:525/1375 train_time:72151ms step_avg:140.10ms
step:526/1375 train_time:72295ms step_avg:140.11ms
step:527/1375 train_time:72440ms step_avg:140.12ms
step:528/1375 train_time:72585ms step_avg:140.13ms
step:529/1375 train_time:72732ms step_avg:140.14ms
step:530/1375 train_time:72877ms step_avg:140.15ms
step:531/1375 train_time:73022ms step_avg:140.16ms
step:532/1375 train_time:73170ms step_avg:140.17ms
step:533/1375 train_time:73316ms step_avg:140.18ms
step:534/1375 train_time:73463ms step_avg:140.20ms
step:535/1375 train_time:73609ms step_avg:140.21ms
step:536/1375 train_time:73754ms step_avg:140.22ms
step:537/1375 train_time:73899ms step_avg:140.23ms
step:538/1375 train_time:74045ms step_avg:140.24ms
step:539/1375 train_time:74194ms step_avg:140.25ms
step:540/1375 train_time:74339ms step_avg:140.26ms
step:541/1375 train_time:74484ms step_avg:140.27ms
step:542/1375 train_time:74630ms step_avg:140.28ms
step:543/1375 train_time:74775ms step_avg:140.29ms
step:544/1375 train_time:74919ms step_avg:140.30ms
step:545/1375 train_time:75065ms step_avg:140.31ms
step:546/1375 train_time:75213ms step_avg:140.32ms
step:547/1375 train_time:75356ms step_avg:140.33ms
step:548/1375 train_time:75502ms step_avg:140.34ms
step:549/1375 train_time:75649ms step_avg:140.35ms
step:550/1375 train_time:75795ms step_avg:140.36ms
step:551/1375 train_time:75940ms step_avg:140.37ms
step:552/1375 train_time:76085ms step_avg:140.38ms
step:553/1375 train_time:76231ms step_avg:140.39ms
step:554/1375 train_time:76376ms step_avg:140.40ms
step:555/1375 train_time:76521ms step_avg:140.41ms
step:556/1375 train_time:76666ms step_avg:140.41ms
step:557/1375 train_time:76813ms step_avg:140.43ms
step:558/1375 train_time:76957ms step_avg:140.43ms
step:559/1375 train_time:77103ms step_avg:140.44ms
step:560/1375 train_time:77249ms step_avg:140.45ms
step:561/1375 train_time:77394ms step_avg:140.46ms
step:562/1375 train_time:77540ms step_avg:140.47ms
step:563/1375 train_time:77687ms step_avg:140.48ms
step:564/1375 train_time:77833ms step_avg:140.49ms
step:565/1375 train_time:77977ms step_avg:140.50ms
step:566/1375 train_time:78123ms step_avg:140.51ms
step:567/1375 train_time:78270ms step_avg:140.52ms
step:568/1375 train_time:78415ms step_avg:140.53ms
step:569/1375 train_time:78560ms step_avg:140.54ms
step:570/1375 train_time:78705ms step_avg:140.55ms
step:571/1375 train_time:78893ms step_avg:140.63ms
step:572/1375 train_time:79037ms step_avg:140.63ms
step:573/1375 train_time:79182ms step_avg:140.64ms
step:574/1375 train_time:79329ms step_avg:140.65ms
step:575/1375 train_time:79474ms step_avg:140.66ms
step:576/1375 train_time:79619ms step_avg:140.67ms
step:577/1375 train_time:79764ms step_avg:140.68ms
step:578/1375 train_time:79913ms step_avg:140.69ms
step:579/1375 train_time:80057ms step_avg:140.70ms
step:580/1375 train_time:80202ms step_avg:140.71ms
step:581/1375 train_time:80349ms step_avg:140.72ms
step:582/1375 train_time:80493ms step_avg:140.72ms
step:583/1375 train_time:80638ms step_avg:140.73ms
step:584/1375 train_time:80785ms step_avg:140.74ms
step:585/1375 train_time:80932ms step_avg:140.75ms
step:586/1375 train_time:81077ms step_avg:140.76ms
step:587/1375 train_time:81222ms step_avg:140.77ms
step:588/1375 train_time:81369ms step_avg:140.78ms
step:589/1375 train_time:81514ms step_avg:140.78ms
step:590/1375 train_time:81659ms step_avg:140.79ms
step:591/1375 train_time:81805ms step_avg:140.80ms
step:592/1375 train_time:81952ms step_avg:140.81ms
step:593/1375 train_time:82097ms step_avg:140.82ms
step:594/1375 train_time:82244ms step_avg:140.83ms
step:595/1375 train_time:82391ms step_avg:140.84ms
step:596/1375 train_time:82536ms step_avg:140.85ms
step:597/1375 train_time:82680ms step_avg:140.85ms
step:598/1375 train_time:82826ms step_avg:140.86ms
step:599/1375 train_time:82973ms step_avg:140.87ms
step:600/1375 train_time:83117ms step_avg:140.88ms
step:601/1375 train_time:83262ms step_avg:140.88ms
step:602/1375 train_time:83409ms step_avg:140.89ms
step:603/1375 train_time:83554ms step_avg:140.90ms
step:604/1375 train_time:83700ms step_avg:140.91ms
step:605/1375 train_time:83846ms step_avg:140.92ms
step:606/1375 train_time:83993ms step_avg:140.93ms
step:607/1375 train_time:84138ms step_avg:140.93ms
step:608/1375 train_time:84284ms step_avg:140.94ms
step:609/1375 train_time:84431ms step_avg:140.95ms
step:610/1375 train_time:84576ms step_avg:140.96ms
step:611/1375 train_time:84720ms step_avg:140.97ms
step:612/1375 train_time:84866ms step_avg:140.97ms
step:613/1375 train_time:85013ms step_avg:140.98ms
step:614/1375 train_time:85160ms step_avg:140.99ms
step:615/1375 train_time:85306ms step_avg:141.00ms
step:616/1375 train_time:85453ms step_avg:141.01ms
step:617/1375 train_time:85599ms step_avg:141.02ms
step:618/1375 train_time:85746ms step_avg:141.03ms
step:619/1375 train_time:85895ms step_avg:141.04ms
step:620/1375 train_time:86040ms step_avg:141.05ms
step:621/1375 train_time:86187ms step_avg:141.06ms
step:622/1375 train_time:86334ms step_avg:141.07ms
step:623/1375 train_time:86480ms step_avg:141.08ms
step:624/1375 train_time:86629ms step_avg:141.09ms
step:625/1375 train_time:86775ms step_avg:141.10ms
step:625/1375 val_loss:3.5742 train_time:86848ms step_avg:141.22ms
step:626/1375 train_time:86926ms step_avg:141.11ms
step:627/1375 train_time:87075ms step_avg:141.13ms
step:628/1375 train_time:87220ms step_avg:141.13ms
step:629/1375 train_time:87367ms step_avg:141.14ms
step:630/1375 train_time:87514ms step_avg:141.15ms
step:631/1375 train_time:87659ms step_avg:141.16ms
step:632/1375 train_time:87806ms step_avg:141.17ms
step:633/1375 train_time:87955ms step_avg:141.18ms
step:634/1375 train_time:88101ms step_avg:141.19ms
step:635/1375 train_time:88249ms step_avg:141.20ms
step:636/1375 train_time:88396ms step_avg:141.21ms
step:637/1375 train_time:88542ms step_avg:141.22ms
step:638/1375 train_time:88688ms step_avg:141.22ms
step:639/1375 train_time:88836ms step_avg:141.23ms
step:640/1375 train_time:88983ms step_avg:141.24ms
step:641/1375 train_time:89131ms step_avg:141.25ms
step:642/1375 train_time:89277ms step_avg:141.26ms
step:643/1375 train_time:89423ms step_avg:141.27ms
step:644/1375 train_time:89571ms step_avg:141.28ms
step:645/1375 train_time:89718ms step_avg:141.29ms
step:646/1375 train_time:89864ms step_avg:141.30ms
step:647/1375 train_time:90013ms step_avg:141.31ms
step:648/1375 train_time:90161ms step_avg:141.32ms
step:649/1375 train_time:90310ms step_avg:141.33ms
step:650/1375 train_time:90457ms step_avg:141.34ms
step:651/1375 train_time:90605ms step_avg:141.35ms
step:652/1375 train_time:90751ms step_avg:141.36ms
step:653/1375 train_time:90898ms step_avg:141.37ms
step:654/1375 train_time:91044ms step_avg:141.37ms
step:655/1375 train_time:91192ms step_avg:141.38ms
step:656/1375 train_time:91337ms step_avg:141.39ms
step:657/1375 train_time:91484ms step_avg:141.40ms
step:658/1375 train_time:91633ms step_avg:141.41ms
step:659/1375 train_time:91778ms step_avg:141.42ms
step:660/1375 train_time:91924ms step_avg:141.42ms
step:661/1375 train_time:92073ms step_avg:141.43ms
step:662/1375 train_time:92218ms step_avg:141.44ms
step:663/1375 train_time:92363ms step_avg:141.44ms
step:664/1375 train_time:92511ms step_avg:141.45ms
step:665/1375 train_time:92658ms step_avg:141.46ms
step:666/1375 train_time:92803ms step_avg:141.47ms
step:667/1375 train_time:92951ms step_avg:141.48ms
step:668/1375 train_time:93098ms step_avg:141.49ms
step:669/1375 train_time:93245ms step_avg:141.49ms
step:670/1375 train_time:93393ms step_avg:141.50ms
step:671/1375 train_time:93539ms step_avg:141.51ms
step:672/1375 train_time:93687ms step_avg:141.52ms
step:673/1375 train_time:93835ms step_avg:141.53ms
step:674/1375 train_time:93982ms step_avg:141.54ms
step:675/1375 train_time:94132ms step_avg:141.55ms
step:676/1375 train_time:94279ms step_avg:141.56ms
step:677/1375 train_time:94425ms step_avg:141.57ms
step:678/1375 train_time:94572ms step_avg:141.57ms
step:679/1375 train_time:94719ms step_avg:141.58ms
step:680/1375 train_time:94866ms step_avg:141.59ms
step:681/1375 train_time:95013ms step_avg:141.60ms
step:682/1375 train_time:95160ms step_avg:141.61ms
step:683/1375 train_time:95307ms step_avg:141.62ms
step:684/1375 train_time:95456ms step_avg:141.63ms
step:685/1375 train_time:95602ms step_avg:141.63ms
step:686/1375 train_time:95751ms step_avg:141.64ms
step:687/1375 train_time:95897ms step_avg:141.65ms
step:688/1375 train_time:96044ms step_avg:141.66ms
step:689/1375 train_time:96194ms step_avg:141.67ms
step:690/1375 train_time:96342ms step_avg:141.68ms
step:691/1375 train_time:96489ms step_avg:141.69ms
step:692/1375 train_time:96635ms step_avg:141.69ms
step:693/1375 train_time:96781ms step_avg:141.70ms
step:694/1375 train_time:96928ms step_avg:141.71ms
step:695/1375 train_time:97075ms step_avg:141.72ms
step:696/1375 train_time:97220ms step_avg:141.72ms
step:697/1375 train_time:97368ms step_avg:141.73ms
step:698/1375 train_time:97515ms step_avg:141.74ms
step:699/1375 train_time:97662ms step_avg:141.74ms
step:700/1375 train_time:97809ms step_avg:141.75ms
step:701/1375 train_time:97956ms step_avg:141.76ms
step:702/1375 train_time:98104ms step_avg:141.77ms
step:703/1375 train_time:98253ms step_avg:141.78ms
step:704/1375 train_time:98398ms step_avg:141.78ms
step:705/1375 train_time:98548ms step_avg:141.80ms
step:706/1375 train_time:98697ms step_avg:141.81ms
step:707/1375 train_time:98843ms step_avg:141.81ms
step:708/1375 train_time:98993ms step_avg:141.82ms
step:709/1375 train_time:99140ms step_avg:141.83ms
step:710/1375 train_time:99287ms step_avg:141.84ms
step:711/1375 train_time:99434ms step_avg:141.85ms
step:712/1375 train_time:99580ms step_avg:141.85ms
step:713/1375 train_time:99729ms step_avg:141.86ms
step:714/1375 train_time:99876ms step_avg:141.87ms
step:715/1375 train_time:100026ms step_avg:141.88ms
step:716/1375 train_time:100176ms step_avg:141.89ms
step:717/1375 train_time:100322ms step_avg:141.90ms
step:718/1375 train_time:100471ms step_avg:141.91ms
step:719/1375 train_time:100618ms step_avg:141.91ms
step:720/1375 train_time:100765ms step_avg:141.92ms
step:721/1375 train_time:100917ms step_avg:141.94ms
step:722/1375 train_time:101066ms step_avg:141.95ms
step:723/1375 train_time:101215ms step_avg:141.96ms
step:724/1375 train_time:101362ms step_avg:141.96ms
step:725/1375 train_time:101513ms step_avg:141.98ms
step:726/1375 train_time:101660ms step_avg:141.98ms
step:727/1375 train_time:101809ms step_avg:141.99ms
step:728/1375 train_time:101958ms step_avg:142.00ms
step:729/1375 train_time:102105ms step_avg:142.01ms
step:730/1375 train_time:102257ms step_avg:142.02ms
step:731/1375 train_time:102405ms step_avg:142.03ms
step:732/1375 train_time:102553ms step_avg:142.04ms
step:733/1375 train_time:102701ms step_avg:142.05ms
step:734/1375 train_time:102850ms step_avg:142.06ms
step:735/1375 train_time:102998ms step_avg:142.07ms
step:736/1375 train_time:103147ms step_avg:142.08ms
step:737/1375 train_time:103296ms step_avg:142.09ms
step:738/1375 train_time:103444ms step_avg:142.09ms
step:739/1375 train_time:103593ms step_avg:142.10ms
step:740/1375 train_time:103741ms step_avg:142.11ms
step:741/1375 train_time:103893ms step_avg:142.12ms
step:742/1375 train_time:104041ms step_avg:142.13ms
step:743/1375 train_time:104190ms step_avg:142.14ms
step:744/1375 train_time:104339ms step_avg:142.15ms
step:745/1375 train_time:104488ms step_avg:142.16ms
step:746/1375 train_time:104636ms step_avg:142.17ms
step:747/1375 train_time:104783ms step_avg:142.18ms
step:748/1375 train_time:104933ms step_avg:142.19ms
step:749/1375 train_time:105081ms step_avg:142.19ms
step:750/1375 train_time:105232ms step_avg:142.21ms
step:750/1375 val_loss:3.5202 train_time:105307ms step_avg:142.31ms
step:751/1375 train_time:105385ms step_avg:142.22ms
step:752/1375 train_time:105533ms step_avg:142.23ms
step:753/1375 train_time:105682ms step_avg:142.24ms
step:754/1375 train_time:105828ms step_avg:142.24ms
step:755/1375 train_time:105976ms step_avg:142.25ms
step:756/1375 train_time:106124ms step_avg:142.26ms
step:757/1375 train_time:106275ms step_avg:142.27ms
step:758/1375 train_time:106425ms step_avg:142.28ms
step:759/1375 train_time:106574ms step_avg:142.29ms
step:760/1375 train_time:106724ms step_avg:142.30ms
step:761/1375 train_time:106913ms step_avg:142.36ms
step:762/1375 train_time:107061ms step_avg:142.37ms
step:763/1375 train_time:107209ms step_avg:142.38ms
step:764/1375 train_time:107358ms step_avg:142.39ms
step:765/1375 train_time:107505ms step_avg:142.39ms
step:766/1375 train_time:107654ms step_avg:142.40ms
step:767/1375 train_time:107804ms step_avg:142.41ms
step:768/1375 train_time:107954ms step_avg:142.42ms
step:769/1375 train_time:108104ms step_avg:142.43ms
step:770/1375 train_time:108252ms step_avg:142.44ms
step:771/1375 train_time:108401ms step_avg:142.45ms
step:772/1375 train_time:108547ms step_avg:142.45ms
step:773/1375 train_time:108697ms step_avg:142.46ms
step:774/1375 train_time:108845ms step_avg:142.47ms
step:775/1375 train_time:108994ms step_avg:142.48ms
step:776/1375 train_time:109144ms step_avg:142.49ms
step:777/1375 train_time:109293ms step_avg:142.49ms
step:778/1375 train_time:109443ms step_avg:142.50ms
step:779/1375 train_time:109589ms step_avg:142.51ms
step:780/1375 train_time:109738ms step_avg:142.52ms
step:781/1375 train_time:109886ms step_avg:142.52ms
step:782/1375 train_time:110035ms step_avg:142.53ms
step:783/1375 train_time:110185ms step_avg:142.54ms
step:784/1375 train_time:110332ms step_avg:142.55ms
step:785/1375 train_time:110481ms step_avg:142.56ms
step:786/1375 train_time:110628ms step_avg:142.56ms
step:787/1375 train_time:110777ms step_avg:142.57ms
step:788/1375 train_time:110925ms step_avg:142.58ms
step:789/1375 train_time:111072ms step_avg:142.58ms
step:790/1375 train_time:111222ms step_avg:142.59ms
step:791/1375 train_time:111370ms step_avg:142.60ms
step:792/1375 train_time:111520ms step_avg:142.61ms
step:793/1375 train_time:111667ms step_avg:142.61ms
step:794/1375 train_time:111816ms step_avg:142.62ms
step:795/1375 train_time:111967ms step_avg:142.63ms
step:796/1375 train_time:112116ms step_avg:142.64ms
step:797/1375 train_time:112266ms step_avg:142.65ms
step:798/1375 train_time:112416ms step_avg:142.66ms
step:799/1375 train_time:112566ms step_avg:142.67ms
step:800/1375 train_time:112714ms step_avg:142.68ms
step:801/1375 train_time:112862ms step_avg:142.68ms
step:802/1375 train_time:113010ms step_avg:142.69ms
step:803/1375 train_time:113157ms step_avg:142.70ms
step:804/1375 train_time:113305ms step_avg:142.70ms
step:805/1375 train_time:113456ms step_avg:142.71ms
step:806/1375 train_time:113604ms step_avg:142.72ms
step:807/1375 train_time:113751ms step_avg:142.72ms
step:808/1375 train_time:113901ms step_avg:142.73ms
step:809/1375 train_time:114049ms step_avg:142.74ms
step:810/1375 train_time:114198ms step_avg:142.75ms
step:811/1375 train_time:114346ms step_avg:142.75ms
step:812/1375 train_time:114494ms step_avg:142.76ms
step:813/1375 train_time:114642ms step_avg:142.77ms
step:814/1375 train_time:114790ms step_avg:142.77ms
step:815/1375 train_time:114938ms step_avg:142.78ms
step:816/1375 train_time:115090ms step_avg:142.79ms
step:817/1375 train_time:115239ms step_avg:142.80ms
step:818/1375 train_time:115388ms step_avg:142.81ms
step:819/1375 train_time:115538ms step_avg:142.82ms
step:820/1375 train_time:115687ms step_avg:142.82ms
step:821/1375 train_time:115835ms step_avg:142.83ms
step:822/1375 train_time:115985ms step_avg:142.84ms
step:823/1375 train_time:116134ms step_avg:142.85ms
step:824/1375 train_time:116286ms step_avg:142.86ms
step:825/1375 train_time:116438ms step_avg:142.87ms
step:826/1375 train_time:116589ms step_avg:142.88ms
step:827/1375 train_time:116741ms step_avg:142.89ms
step:828/1375 train_time:116890ms step_avg:142.90ms
step:829/1375 train_time:117040ms step_avg:142.91ms
step:830/1375 train_time:117190ms step_avg:142.91ms
step:831/1375 train_time:117340ms step_avg:142.92ms
step:832/1375 train_time:117490ms step_avg:142.93ms
step:833/1375 train_time:117640ms step_avg:142.94ms
step:834/1375 train_time:117791ms step_avg:142.95ms
step:835/1375 train_time:117942ms step_avg:142.96ms
step:836/1375 train_time:118093ms step_avg:142.97ms
step:837/1375 train_time:118244ms step_avg:142.98ms
step:838/1375 train_time:118391ms step_avg:142.98ms
step:839/1375 train_time:118541ms step_avg:142.99ms
step:840/1375 train_time:118690ms step_avg:143.00ms
step:841/1375 train_time:118840ms step_avg:143.01ms
step:842/1375 train_time:118988ms step_avg:143.01ms
step:843/1375 train_time:119138ms step_avg:143.02ms
step:844/1375 train_time:119287ms step_avg:143.03ms
step:845/1375 train_time:119435ms step_avg:143.04ms
step:846/1375 train_time:119586ms step_avg:143.05ms
step:847/1375 train_time:119737ms step_avg:143.05ms
step:848/1375 train_time:119886ms step_avg:143.06ms
step:849/1375 train_time:120036ms step_avg:143.07ms
step:850/1375 train_time:120187ms step_avg:143.08ms
step:851/1375 train_time:120339ms step_avg:143.09ms
step:852/1375 train_time:120489ms step_avg:143.10ms
step:853/1375 train_time:120639ms step_avg:143.11ms
step:854/1375 train_time:120786ms step_avg:143.11ms
step:855/1375 train_time:120934ms step_avg:143.12ms
step:856/1375 train_time:121084ms step_avg:143.13ms
step:857/1375 train_time:121233ms step_avg:143.13ms
step:858/1375 train_time:121385ms step_avg:143.14ms
step:859/1375 train_time:121535ms step_avg:143.15ms
step:860/1375 train_time:121685ms step_avg:143.16ms
step:861/1375 train_time:121836ms step_avg:143.17ms
step:862/1375 train_time:121987ms step_avg:143.18ms
step:863/1375 train_time:122137ms step_avg:143.19ms
step:864/1375 train_time:122287ms step_avg:143.19ms
step:865/1375 train_time:122437ms step_avg:143.20ms
step:866/1375 train_time:122592ms step_avg:143.21ms
step:867/1375 train_time:122744ms step_avg:143.23ms
step:868/1375 train_time:122891ms step_avg:143.23ms
step:869/1375 train_time:123041ms step_avg:143.24ms
step:870/1375 train_time:123190ms step_avg:143.24ms
step:871/1375 train_time:123340ms step_avg:143.25ms
step:872/1375 train_time:123488ms step_avg:143.26ms
step:873/1375 train_time:123638ms step_avg:143.27ms
step:874/1375 train_time:123791ms step_avg:143.28ms
step:875/1375 train_time:123943ms step_avg:143.29ms
step:875/1375 val_loss:3.4669 train_time:124016ms step_avg:143.37ms
step:876/1375 train_time:124093ms step_avg:143.29ms
step:877/1375 train_time:124243ms step_avg:143.30ms
step:878/1375 train_time:124393ms step_avg:143.31ms
step:879/1375 train_time:124542ms step_avg:143.32ms
step:880/1375 train_time:124691ms step_avg:143.32ms
step:881/1375 train_time:124839ms step_avg:143.33ms
step:882/1375 train_time:124991ms step_avg:143.34ms
step:883/1375 train_time:125141ms step_avg:143.35ms
step:884/1375 train_time:125292ms step_avg:143.35ms
step:885/1375 train_time:125441ms step_avg:143.36ms
step:886/1375 train_time:125591ms step_avg:143.37ms
step:887/1375 train_time:125739ms step_avg:143.37ms
step:888/1375 train_time:125891ms step_avg:143.38ms
step:889/1375 train_time:126042ms step_avg:143.39ms
step:890/1375 train_time:126192ms step_avg:143.40ms
step:891/1375 train_time:126340ms step_avg:143.40ms
step:892/1375 train_time:126490ms step_avg:143.41ms
step:893/1375 train_time:126639ms step_avg:143.42ms
step:894/1375 train_time:126788ms step_avg:143.43ms
step:895/1375 train_time:126938ms step_avg:143.43ms
step:896/1375 train_time:127089ms step_avg:143.44ms
step:897/1375 train_time:127238ms step_avg:143.45ms
step:898/1375 train_time:127389ms step_avg:143.46ms
step:899/1375 train_time:127539ms step_avg:143.46ms
step:900/1375 train_time:127690ms step_avg:143.47ms
step:901/1375 train_time:127840ms step_avg:143.48ms
step:902/1375 train_time:127989ms step_avg:143.49ms
step:903/1375 train_time:128139ms step_avg:143.49ms
step:904/1375 train_time:128288ms step_avg:143.50ms
step:905/1375 train_time:128437ms step_avg:143.50ms
step:906/1375 train_time:128588ms step_avg:143.51ms
step:907/1375 train_time:128739ms step_avg:143.52ms
step:908/1375 train_time:128889ms step_avg:143.53ms
step:909/1375 train_time:129036ms step_avg:143.53ms
step:910/1375 train_time:129191ms step_avg:143.55ms
step:911/1375 train_time:129339ms step_avg:143.55ms
step:912/1375 train_time:129489ms step_avg:143.56ms
step:913/1375 train_time:129641ms step_avg:143.57ms
step:914/1375 train_time:129793ms step_avg:143.58ms
step:915/1375 train_time:129943ms step_avg:143.58ms
step:916/1375 train_time:130093ms step_avg:143.59ms
step:917/1375 train_time:130245ms step_avg:143.60ms
step:918/1375 train_time:130394ms step_avg:143.61ms
step:919/1375 train_time:130552ms step_avg:143.62ms
step:920/1375 train_time:130703ms step_avg:143.63ms
step:921/1375 train_time:130853ms step_avg:143.64ms
step:922/1375 train_time:131008ms step_avg:143.65ms
step:923/1375 train_time:131156ms step_avg:143.65ms
step:924/1375 train_time:131307ms step_avg:143.66ms
step:925/1375 train_time:131459ms step_avg:143.67ms
step:926/1375 train_time:131610ms step_avg:143.68ms
step:927/1375 train_time:131760ms step_avg:143.69ms
step:928/1375 train_time:131911ms step_avg:143.69ms
step:929/1375 train_time:132062ms step_avg:143.70ms
step:930/1375 train_time:132213ms step_avg:143.71ms
step:931/1375 train_time:132365ms step_avg:143.72ms
step:932/1375 train_time:132514ms step_avg:143.72ms
step:933/1375 train_time:132665ms step_avg:143.73ms
step:934/1375 train_time:132815ms step_avg:143.74ms
step:935/1375 train_time:132968ms step_avg:143.75ms
step:936/1375 train_time:133117ms step_avg:143.76ms
step:937/1375 train_time:133272ms step_avg:143.77ms
step:938/1375 train_time:133423ms step_avg:143.78ms
step:939/1375 train_time:133576ms step_avg:143.78ms
step:940/1375 train_time:133730ms step_avg:143.80ms
step:941/1375 train_time:133879ms step_avg:143.80ms
step:942/1375 train_time:134030ms step_avg:143.81ms
step:943/1375 train_time:134181ms step_avg:143.82ms
step:944/1375 train_time:134334ms step_avg:143.83ms
step:945/1375 train_time:134487ms step_avg:143.84ms
step:946/1375 train_time:134638ms step_avg:143.84ms
step:947/1375 train_time:134792ms step_avg:143.86ms
step:948/1375 train_time:134944ms step_avg:143.86ms
step:949/1375 train_time:135096ms step_avg:143.87ms
step:950/1375 train_time:135247ms step_avg:143.88ms
step:951/1375 train_time:135443ms step_avg:143.94ms
step:952/1375 train_time:135592ms step_avg:143.94ms
step:953/1375 train_time:135744ms step_avg:143.95ms
step:954/1375 train_time:135893ms step_avg:143.95ms
step:955/1375 train_time:136043ms step_avg:143.96ms
step:956/1375 train_time:136196ms step_avg:143.97ms
step:957/1375 train_time:136348ms step_avg:143.98ms
step:958/1375 train_time:136500ms step_avg:143.99ms
step:959/1375 train_time:136655ms step_avg:144.00ms
step:960/1375 train_time:136807ms step_avg:144.01ms
step:961/1375 train_time:136956ms step_avg:144.01ms
step:962/1375 train_time:137107ms step_avg:144.02ms
step:963/1375 train_time:137264ms step_avg:144.03ms
step:964/1375 train_time:137416ms step_avg:144.04ms
step:965/1375 train_time:137570ms step_avg:144.05ms
step:966/1375 train_time:137719ms step_avg:144.06ms
step:967/1375 train_time:137872ms step_avg:144.07ms
step:968/1375 train_time:138020ms step_avg:144.07ms
step:969/1375 train_time:138173ms step_avg:144.08ms
step:970/1375 train_time:138323ms step_avg:144.09ms
step:971/1375 train_time:138475ms step_avg:144.09ms
step:972/1375 train_time:138628ms step_avg:144.10ms
step:973/1375 train_time:138777ms step_avg:144.11ms
step:974/1375 train_time:138930ms step_avg:144.12ms
step:975/1375 train_time:139081ms step_avg:144.13ms
step:976/1375 train_time:139232ms step_avg:144.13ms
step:977/1375 train_time:139383ms step_avg:144.14ms
step:978/1375 train_time:139535ms step_avg:144.15ms
step:979/1375 train_time:139687ms step_avg:144.16ms
step:980/1375 train_time:139836ms step_avg:144.16ms
step:981/1375 train_time:139985ms step_avg:144.17ms
step:982/1375 train_time:140134ms step_avg:144.17ms
step:983/1375 train_time:140285ms step_avg:144.18ms
step:984/1375 train_time:140434ms step_avg:144.18ms
step:985/1375 train_time:140589ms step_avg:144.19ms
step:986/1375 train_time:140744ms step_avg:144.21ms
step:987/1375 train_time:140894ms step_avg:144.21ms
step:988/1375 train_time:141045ms step_avg:144.22ms
step:989/1375 train_time:141195ms step_avg:144.22ms
step:990/1375 train_time:141348ms step_avg:144.23ms
step:991/1375 train_time:141500ms step_avg:144.24ms
step:992/1375 train_time:141655ms step_avg:144.25ms
step:993/1375 train_time:141814ms step_avg:144.27ms
step:994/1375 train_time:141965ms step_avg:144.27ms
step:995/1375 train_time:142113ms step_avg:144.28ms
step:996/1375 train_time:142262ms step_avg:144.28ms
step:997/1375 train_time:142412ms step_avg:144.29ms
step:998/1375 train_time:142562ms step_avg:144.29ms
step:999/1375 train_time:142714ms step_avg:144.30ms
step:1000/1375 train_time:142866ms step_avg:144.31ms
step:1000/1375 val_loss:3.4019 train_time:142939ms step_avg:144.38ms
step:1001/1375 train_time:143017ms step_avg:144.32ms
step:1002/1375 train_time:143167ms step_avg:144.32ms
step:1003/1375 train_time:143320ms step_avg:144.33ms
step:1004/1375 train_time:143470ms step_avg:144.34ms
step:1005/1375 train_time:143622ms step_avg:144.34ms
step:1006/1375 train_time:143769ms step_avg:144.35ms
step:1007/1375 train_time:143922ms step_avg:144.35ms
step:1008/1375 train_time:144073ms step_avg:144.36ms
step:1009/1375 train_time:144228ms step_avg:144.37ms
step:1010/1375 train_time:144380ms step_avg:144.38ms
step:1011/1375 train_time:144531ms step_avg:144.39ms
step:1012/1375 train_time:144683ms step_avg:144.39ms
step:1013/1375 train_time:144836ms step_avg:144.40ms
step:1014/1375 train_time:144986ms step_avg:144.41ms
step:1015/1375 train_time:145137ms step_avg:144.42ms
step:1016/1375 train_time:145288ms step_avg:144.42ms
step:1017/1375 train_time:145440ms step_avg:144.43ms
step:1018/1375 train_time:145591ms step_avg:144.44ms
step:1019/1375 train_time:145743ms step_avg:144.44ms
step:1020/1375 train_time:145897ms step_avg:144.45ms
step:1021/1375 train_time:146048ms step_avg:144.46ms
step:1022/1375 train_time:146200ms step_avg:144.47ms
step:1023/1375 train_time:146354ms step_avg:144.48ms
step:1024/1375 train_time:146507ms step_avg:144.48ms
step:1025/1375 train_time:146660ms step_avg:144.49ms
step:1026/1375 train_time:146812ms step_avg:144.50ms
step:1027/1375 train_time:146963ms step_avg:144.51ms
step:1028/1375 train_time:147117ms step_avg:144.52ms
step:1029/1375 train_time:147273ms step_avg:144.53ms
step:1030/1375 train_time:147427ms step_avg:144.54ms
step:1031/1375 train_time:147577ms step_avg:144.54ms
step:1032/1375 train_time:147726ms step_avg:144.55ms
step:1033/1375 train_time:147877ms step_avg:144.55ms
step:1034/1375 train_time:148030ms step_avg:144.56ms
step:1035/1375 train_time:148184ms step_avg:144.57ms
step:1036/1375 train_time:148336ms step_avg:144.58ms
step:1037/1375 train_time:148489ms step_avg:144.59ms
step:1038/1375 train_time:148640ms step_avg:144.59ms
step:1039/1375 train_time:148790ms step_avg:144.60ms
step:1040/1375 train_time:148942ms step_avg:144.60ms
step:1041/1375 train_time:149094ms step_avg:144.61ms
step:1042/1375 train_time:149246ms step_avg:144.62ms
step:1043/1375 train_time:149399ms step_avg:144.63ms
step:1044/1375 train_time:149556ms step_avg:144.64ms
step:1045/1375 train_time:149709ms step_avg:144.65ms
step:1046/1375 train_time:149862ms step_avg:144.65ms
step:1047/1375 train_time:150011ms step_avg:144.66ms
step:1048/1375 train_time:150164ms step_avg:144.67ms
step:1049/1375 train_time:150318ms step_avg:144.68ms
step:1050/1375 train_time:150471ms step_avg:144.68ms
step:1051/1375 train_time:150625ms step_avg:144.69ms
step:1052/1375 train_time:150776ms step_avg:144.70ms
step:1053/1375 train_time:150928ms step_avg:144.71ms
step:1054/1375 train_time:151081ms step_avg:144.71ms
step:1055/1375 train_time:151232ms step_avg:144.72ms
step:1056/1375 train_time:151382ms step_avg:144.73ms
step:1057/1375 train_time:151534ms step_avg:144.73ms
step:1058/1375 train_time:151688ms step_avg:144.74ms
step:1059/1375 train_time:151843ms step_avg:144.75ms
step:1060/1375 train_time:151996ms step_avg:144.76ms
step:1061/1375 train_time:152145ms step_avg:144.76ms
step:1062/1375 train_time:152300ms step_avg:144.77ms
step:1063/1375 train_time:152450ms step_avg:144.78ms
step:1064/1375 train_time:152602ms step_avg:144.78ms
step:1065/1375 train_time:152756ms step_avg:144.79ms
step:1066/1375 train_time:152911ms step_avg:144.80ms
step:1067/1375 train_time:153064ms step_avg:144.81ms
step:1068/1375 train_time:153216ms step_avg:144.82ms
step:1069/1375 train_time:153372ms step_avg:144.83ms
step:1070/1375 train_time:153523ms step_avg:144.83ms
step:1071/1375 train_time:153679ms step_avg:144.84ms
step:1072/1375 train_time:153829ms step_avg:144.85ms
step:1073/1375 train_time:153979ms step_avg:144.85ms
step:1074/1375 train_time:154129ms step_avg:144.86ms
step:1075/1375 train_time:154282ms step_avg:144.87ms
step:1076/1375 train_time:154431ms step_avg:144.87ms
step:1077/1375 train_time:154583ms step_avg:144.88ms
step:1078/1375 train_time:154738ms step_avg:144.89ms
step:1079/1375 train_time:154896ms step_avg:144.90ms
step:1080/1375 train_time:155048ms step_avg:144.90ms
step:1081/1375 train_time:155200ms step_avg:144.91ms
step:1082/1375 train_time:155351ms step_avg:144.92ms
step:1083/1375 train_time:155504ms step_avg:144.92ms
step:1084/1375 train_time:155660ms step_avg:144.93ms
step:1085/1375 train_time:155811ms step_avg:144.94ms
step:1086/1375 train_time:155963ms step_avg:144.95ms
step:1087/1375 train_time:156113ms step_avg:144.95ms
step:1088/1375 train_time:156266ms step_avg:144.96ms
step:1089/1375 train_time:156421ms step_avg:144.97ms
step:1090/1375 train_time:156577ms step_avg:144.98ms
step:1091/1375 train_time:156730ms step_avg:144.99ms
step:1092/1375 train_time:156882ms step_avg:144.99ms
step:1093/1375 train_time:157035ms step_avg:145.00ms
step:1094/1375 train_time:157187ms step_avg:145.01ms
step:1095/1375 train_time:157340ms step_avg:145.01ms
step:1096/1375 train_time:157494ms step_avg:145.02ms
step:1097/1375 train_time:157647ms step_avg:145.03ms
step:1098/1375 train_time:157800ms step_avg:145.04ms
step:1099/1375 train_time:157952ms step_avg:145.04ms
step:1100/1375 train_time:158104ms step_avg:145.05ms
step:1101/1375 train_time:158256ms step_avg:145.06ms
step:1102/1375 train_time:158409ms step_avg:145.06ms
step:1103/1375 train_time:158563ms step_avg:145.07ms
step:1104/1375 train_time:158717ms step_avg:145.08ms
step:1105/1375 train_time:158872ms step_avg:145.09ms
step:1106/1375 train_time:159025ms step_avg:145.10ms
step:1107/1375 train_time:159177ms step_avg:145.10ms
step:1108/1375 train_time:159334ms step_avg:145.11ms
step:1109/1375 train_time:159488ms step_avg:145.12ms
step:1110/1375 train_time:159643ms step_avg:145.13ms
step:1111/1375 train_time:159795ms step_avg:145.14ms
step:1112/1375 train_time:159946ms step_avg:145.14ms
step:1113/1375 train_time:160097ms step_avg:145.15ms
step:1114/1375 train_time:160249ms step_avg:145.15ms
step:1115/1375 train_time:160403ms step_avg:145.16ms
step:1116/1375 train_time:160554ms step_avg:145.17ms
step:1117/1375 train_time:160708ms step_avg:145.17ms
step:1118/1375 train_time:160866ms step_avg:145.19ms
step:1119/1375 train_time:161018ms step_avg:145.19ms
step:1120/1375 train_time:161169ms step_avg:145.20ms
step:1121/1375 train_time:161321ms step_avg:145.20ms
step:1122/1375 train_time:161471ms step_avg:145.21ms
step:1123/1375 train_time:161623ms step_avg:145.21ms
step:1124/1375 train_time:161778ms step_avg:145.22ms
step:1125/1375 train_time:161931ms step_avg:145.23ms
step:1125/1375 val_loss:3.3484 train_time:162011ms step_avg:145.30ms
step:1126/1375 train_time:162090ms step_avg:145.24ms
step:1127/1375 train_time:162242ms step_avg:145.25ms
step:1128/1375 train_time:162395ms step_avg:145.26ms
step:1129/1375 train_time:162552ms step_avg:145.26ms
step:1130/1375 train_time:162703ms step_avg:145.27ms
step:1131/1375 train_time:162855ms step_avg:145.28ms
step:1132/1375 train_time:163007ms step_avg:145.28ms
step:1133/1375 train_time:163160ms step_avg:145.29ms
step:1134/1375 train_time:163317ms step_avg:145.30ms
step:1135/1375 train_time:163470ms step_avg:145.31ms
step:1136/1375 train_time:163629ms step_avg:145.32ms
step:1137/1375 train_time:163780ms step_avg:145.32ms
step:1138/1375 train_time:163938ms step_avg:145.33ms
step:1139/1375 train_time:164092ms step_avg:145.34ms
step:1140/1375 train_time:164245ms step_avg:145.35ms
step:1141/1375 train_time:164436ms step_avg:145.39ms
step:1142/1375 train_time:164591ms step_avg:145.40ms
step:1143/1375 train_time:164747ms step_avg:145.41ms
step:1144/1375 train_time:164899ms step_avg:145.41ms
step:1145/1375 train_time:165051ms step_avg:145.42ms
step:1146/1375 train_time:165206ms step_avg:145.43ms
step:1147/1375 train_time:165360ms step_avg:145.44ms
step:1148/1375 train_time:165513ms step_avg:145.44ms
step:1149/1375 train_time:165668ms step_avg:145.45ms
step:1150/1375 train_time:165820ms step_avg:145.46ms
step:1151/1375 train_time:165977ms step_avg:145.47ms
step:1152/1375 train_time:166132ms step_avg:145.47ms
step:1153/1375 train_time:166288ms step_avg:145.48ms
step:1154/1375 train_time:166440ms step_avg:145.49ms
step:1155/1375 train_time:166594ms step_avg:145.50ms
step:1156/1375 train_time:166753ms step_avg:145.51ms
step:1157/1375 train_time:166908ms step_avg:145.52ms
step:1158/1375 train_time:167060ms step_avg:145.52ms
step:1159/1375 train_time:167214ms step_avg:145.53ms
step:1160/1375 train_time:167364ms step_avg:145.53ms
step:1161/1375 train_time:167519ms step_avg:145.54ms
step:1162/1375 train_time:167675ms step_avg:145.55ms
step:1163/1375 train_time:167829ms step_avg:145.56ms
step:1164/1375 train_time:167982ms step_avg:145.57ms
step:1165/1375 train_time:168133ms step_avg:145.57ms
step:1166/1375 train_time:168285ms step_avg:145.58ms
step:1167/1375 train_time:168436ms step_avg:145.58ms
step:1168/1375 train_time:168591ms step_avg:145.59ms
step:1169/1375 train_time:168744ms step_avg:145.59ms
step:1170/1375 train_time:168897ms step_avg:145.60ms
step:1171/1375 train_time:169049ms step_avg:145.61ms
step:1172/1375 train_time:169204ms step_avg:145.61ms
step:1173/1375 train_time:169357ms step_avg:145.62ms
step:1174/1375 train_time:169521ms step_avg:145.64ms
step:1175/1375 train_time:169675ms step_avg:145.64ms
step:1176/1375 train_time:169831ms step_avg:145.65ms
step:1177/1375 train_time:169990ms step_avg:145.66ms
step:1178/1375 train_time:170144ms step_avg:145.67ms
step:1179/1375 train_time:170294ms step_avg:145.68ms
step:1180/1375 train_time:170455ms step_avg:145.69ms
step:1181/1375 train_time:170610ms step_avg:145.70ms
step:1182/1375 train_time:170762ms step_avg:145.70ms
step:1183/1375 train_time:170915ms step_avg:145.71ms
step:1184/1375 train_time:171069ms step_avg:145.71ms
step:1185/1375 train_time:171225ms step_avg:145.72ms
step:1186/1375 train_time:171380ms step_avg:145.73ms
step:1187/1375 train_time:171545ms step_avg:145.75ms
step:1188/1375 train_time:171696ms step_avg:145.75ms
step:1189/1375 train_time:171851ms step_avg:145.76ms
step:1190/1375 train_time:172005ms step_avg:145.77ms
step:1191/1375 train_time:172159ms step_avg:145.77ms
step:1192/1375 train_time:172311ms step_avg:145.78ms
step:1193/1375 train_time:172462ms step_avg:145.78ms
step:1194/1375 train_time:172615ms step_avg:145.79ms
step:1195/1375 train_time:172769ms step_avg:145.80ms
step:1196/1375 train_time:172921ms step_avg:145.80ms
step:1197/1375 train_time:173076ms step_avg:145.81ms
step:1198/1375 train_time:173236ms step_avg:145.82ms
step:1199/1375 train_time:173392ms step_avg:145.83ms
step:1200/1375 train_time:173544ms step_avg:145.84ms
step:1201/1375 train_time:173697ms step_avg:145.84ms
step:1202/1375 train_time:173866ms step_avg:145.86ms
step:1203/1375 train_time:174025ms step_avg:145.87ms
step:1204/1375 train_time:174180ms step_avg:145.88ms
step:1205/1375 train_time:174336ms step_avg:145.89ms
step:1206/1375 train_time:174491ms step_avg:145.90ms
step:1207/1375 train_time:174644ms step_avg:145.90ms
step:1208/1375 train_time:174800ms step_avg:145.91ms
step:1209/1375 train_time:174954ms step_avg:145.92ms
step:1210/1375 train_time:175112ms step_avg:145.93ms
step:1211/1375 train_time:175266ms step_avg:145.93ms
step:1212/1375 train_time:175419ms step_avg:145.94ms
step:1213/1375 train_time:175574ms step_avg:145.95ms
step:1214/1375 train_time:175732ms step_avg:145.96ms
step:1215/1375 train_time:175886ms step_avg:145.96ms
step:1216/1375 train_time:176036ms step_avg:145.97ms
step:1217/1375 train_time:176190ms step_avg:145.97ms
step:1218/1375 train_time:176339ms step_avg:145.98ms
step:1219/1375 train_time:176492ms step_avg:145.98ms
step:1220/1375 train_time:176644ms step_avg:145.99ms
step:1221/1375 train_time:176796ms step_avg:145.99ms
step:1222/1375 train_time:176949ms step_avg:146.00ms
step:1223/1375 train_time:177103ms step_avg:146.00ms
step:1224/1375 train_time:177259ms step_avg:146.01ms
step:1225/1375 train_time:177416ms step_avg:146.02ms
step:1226/1375 train_time:177572ms step_avg:146.03ms
step:1227/1375 train_time:177728ms step_avg:146.04ms
step:1228/1375 train_time:177882ms step_avg:146.04ms
step:1229/1375 train_time:178035ms step_avg:146.05ms
step:1230/1375 train_time:178195ms step_avg:146.06ms
step:1231/1375 train_time:178354ms step_avg:146.07ms
step:1232/1375 train_time:178511ms step_avg:146.08ms
step:1233/1375 train_time:178664ms step_avg:146.09ms
step:1234/1375 train_time:178817ms step_avg:146.09ms
step:1235/1375 train_time:178972ms step_avg:146.10ms
step:1236/1375 train_time:179126ms step_avg:146.11ms
step:1237/1375 train_time:179278ms step_avg:146.11ms
step:1238/1375 train_time:179445ms step_avg:146.13ms
step:1239/1375 train_time:179599ms step_avg:146.13ms
step:1240/1375 train_time:179755ms step_avg:146.14ms
step:1241/1375 train_time:179914ms step_avg:146.15ms
step:1242/1375 train_time:180068ms step_avg:146.16ms
step:1243/1375 train_time:180223ms step_avg:146.17ms
step:1244/1375 train_time:180376ms step_avg:146.17ms
step:1245/1375 train_time:180531ms step_avg:146.18ms
step:1246/1375 train_time:180683ms step_avg:146.18ms
step:1247/1375 train_time:180837ms step_avg:146.19ms
step:1248/1375 train_time:180990ms step_avg:146.20ms
step:1249/1375 train_time:181142ms step_avg:146.20ms
step:1250/1375 train_time:181295ms step_avg:146.21ms
step:1250/1375 val_loss:3.3028 train_time:181372ms step_avg:146.27ms
step:1251/1375 train_time:181453ms step_avg:146.22ms
step:1252/1375 train_time:181609ms step_avg:146.22ms
step:1253/1375 train_time:181760ms step_avg:146.23ms
step:1254/1375 train_time:181913ms step_avg:146.23ms
step:1255/1375 train_time:182077ms step_avg:146.25ms
step:1256/1375 train_time:182231ms step_avg:146.25ms
step:1257/1375 train_time:182384ms step_avg:146.26ms
step:1258/1375 train_time:182540ms step_avg:146.27ms
step:1259/1375 train_time:182696ms step_avg:146.27ms
step:1260/1375 train_time:182848ms step_avg:146.28ms
step:1261/1375 train_time:183000ms step_avg:146.28ms
step:1262/1375 train_time:183157ms step_avg:146.29ms
step:1263/1375 train_time:183312ms step_avg:146.30ms
step:1264/1375 train_time:183464ms step_avg:146.30ms
step:1265/1375 train_time:183617ms step_avg:146.31ms
step:1266/1375 train_time:183773ms step_avg:146.32ms
step:1267/1375 train_time:183926ms step_avg:146.32ms
step:1268/1375 train_time:184082ms step_avg:146.33ms
step:1269/1375 train_time:184240ms step_avg:146.34ms
step:1270/1375 train_time:184395ms step_avg:146.35ms
step:1271/1375 train_time:184549ms step_avg:146.35ms
step:1272/1375 train_time:184701ms step_avg:146.36ms
step:1273/1375 train_time:184853ms step_avg:146.36ms
step:1274/1375 train_time:185003ms step_avg:146.36ms
step:1275/1375 train_time:185161ms step_avg:146.37ms
step:1276/1375 train_time:185313ms step_avg:146.38ms
step:1277/1375 train_time:185466ms step_avg:146.38ms
step:1278/1375 train_time:185619ms step_avg:146.39ms
step:1279/1375 train_time:185775ms step_avg:146.39ms
step:1280/1375 train_time:185935ms step_avg:146.41ms
step:1281/1375 train_time:186089ms step_avg:146.41ms
step:1282/1375 train_time:186240ms step_avg:146.41ms
step:1283/1375 train_time:186395ms step_avg:146.42ms
step:1284/1375 train_time:186550ms step_avg:146.43ms
step:1285/1375 train_time:186703ms step_avg:146.43ms
step:1286/1375 train_time:186857ms step_avg:146.44ms
step:1287/1375 train_time:187010ms step_avg:146.45ms
step:1288/1375 train_time:187165ms step_avg:146.45ms
step:1289/1375 train_time:187328ms step_avg:146.46ms
step:1290/1375 train_time:187490ms step_avg:146.48ms
step:1291/1375 train_time:187648ms step_avg:146.49ms
step:1292/1375 train_time:187803ms step_avg:146.49ms
step:1293/1375 train_time:187961ms step_avg:146.50ms
step:1294/1375 train_time:188116ms step_avg:146.51ms
step:1295/1375 train_time:188272ms step_avg:146.52ms
step:1296/1375 train_time:188426ms step_avg:146.52ms
step:1297/1375 train_time:188581ms step_avg:146.53ms
step:1298/1375 train_time:188735ms step_avg:146.53ms
step:1299/1375 train_time:188889ms step_avg:146.54ms
step:1300/1375 train_time:189040ms step_avg:146.54ms
step:1301/1375 train_time:189194ms step_avg:146.55ms
step:1302/1375 train_time:189348ms step_avg:146.55ms
step:1303/1375 train_time:189506ms step_avg:146.56ms
step:1304/1375 train_time:189665ms step_avg:146.57ms
step:1305/1375 train_time:189821ms step_avg:146.58ms
step:1306/1375 train_time:189977ms step_avg:146.59ms
step:1307/1375 train_time:190132ms step_avg:146.59ms
step:1308/1375 train_time:190288ms step_avg:146.60ms
step:1309/1375 train_time:190445ms step_avg:146.61ms
step:1310/1375 train_time:190598ms step_avg:146.61ms
step:1311/1375 train_time:190750ms step_avg:146.62ms
step:1312/1375 train_time:190903ms step_avg:146.62ms
step:1313/1375 train_time:191056ms step_avg:146.63ms
step:1314/1375 train_time:191212ms step_avg:146.64ms
step:1315/1375 train_time:191367ms step_avg:146.64ms
step:1316/1375 train_time:191518ms step_avg:146.64ms
step:1317/1375 train_time:191672ms step_avg:146.65ms
step:1318/1375 train_time:191834ms step_avg:146.66ms
step:1319/1375 train_time:191990ms step_avg:146.67ms
step:1320/1375 train_time:192144ms step_avg:146.67ms
step:1321/1375 train_time:192298ms step_avg:146.68ms
step:1322/1375 train_time:192458ms step_avg:146.69ms
step:1323/1375 train_time:192611ms step_avg:146.70ms
step:1324/1375 train_time:192764ms step_avg:146.70ms
step:1325/1375 train_time:192921ms step_avg:146.71ms
step:1326/1375 train_time:193078ms step_avg:146.72ms
step:1327/1375 train_time:193234ms step_avg:146.72ms
step:1328/1375 train_time:193387ms step_avg:146.73ms
step:1329/1375 train_time:193558ms step_avg:146.75ms
step:1330/1375 train_time:193718ms step_avg:146.76ms
step:1331/1375 train_time:193914ms step_avg:146.79ms
step:1332/1375 train_time:194077ms step_avg:146.81ms
step:1333/1375 train_time:194234ms step_avg:146.81ms
step:1334/1375 train_time:194386ms step_avg:146.82ms
step:1335/1375 train_time:194539ms step_avg:146.82ms
step:1336/1375 train_time:194700ms step_avg:146.83ms
step:1337/1375 train_time:194858ms step_avg:146.84ms
step:1338/1375 train_time:195014ms step_avg:146.85ms
step:1339/1375 train_time:195170ms step_avg:146.85ms
step:1340/1375 train_time:195330ms step_avg:146.86ms
step:1341/1375 train_time:195484ms step_avg:146.87ms
step:1342/1375 train_time:195641ms step_avg:146.88ms
step:1343/1375 train_time:195795ms step_avg:146.88ms
step:1344/1375 train_time:195947ms step_avg:146.89ms
step:1345/1375 train_time:196101ms step_avg:146.89ms
step:1346/1375 train_time:196255ms step_avg:146.90ms
step:1347/1375 train_time:196411ms step_avg:146.90ms
step:1348/1375 train_time:196566ms step_avg:146.91ms
step:1349/1375 train_time:196719ms step_avg:146.92ms
step:1350/1375 train_time:196873ms step_avg:146.92ms
step:1351/1375 train_time:197027ms step_avg:146.93ms
step:1352/1375 train_time:197190ms step_avg:146.94ms
step:1353/1375 train_time:197350ms step_avg:146.95ms
step:1354/1375 train_time:197505ms step_avg:146.95ms
step:1355/1375 train_time:197659ms step_avg:146.96ms
step:1356/1375 train_time:197812ms step_avg:146.96ms
step:1357/1375 train_time:197970ms step_avg:146.97ms
step:1358/1375 train_time:198126ms step_avg:146.98ms
step:1359/1375 train_time:198280ms step_avg:146.98ms
step:1360/1375 train_time:198438ms step_avg:146.99ms
step:1361/1375 train_time:198596ms step_avg:147.00ms
step:1362/1375 train_time:198754ms step_avg:147.01ms
step:1363/1375 train_time:198914ms step_avg:147.02ms
step:1364/1375 train_time:199068ms step_avg:147.02ms
step:1365/1375 train_time:199219ms step_avg:147.02ms
step:1366/1375 train_time:199374ms step_avg:147.03ms
step:1367/1375 train_time:199530ms step_avg:147.04ms
step:1368/1375 train_time:199687ms step_avg:147.05ms
step:1369/1375 train_time:199850ms step_avg:147.06ms
step:1370/1375 train_time:200009ms step_avg:147.07ms
step:1371/1375 train_time:200166ms step_avg:147.07ms
step:1372/1375 train_time:200326ms step_avg:147.08ms
step:1373/1375 train_time:200482ms step_avg:147.09ms
step:1374/1375 train_time:200641ms step_avg:147.10ms
step:1375/1375 train_time:200794ms step_avg:147.10ms
step:1375/1375 val_loss:3.2775 train_time:200869ms step_avg:147.16ms
peak memory consumption: 31563 MiB
