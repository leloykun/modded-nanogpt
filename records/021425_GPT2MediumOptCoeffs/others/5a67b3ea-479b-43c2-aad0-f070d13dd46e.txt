import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for a, b, c in [
        (3.9090, -4.0174, 1.1084),
        (3.7495, -3.8578, 1.1084),
        (3.5931, -3.7008, 1.1077),
        (3.4398, -3.5462, 1.1064),
        (3.2882, -3.3917, 1.1035),
        (3.1156, -3.2027, 1.0871),
        (2.7208, -2.6650, 0.9442),
    ]:
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.mul_(1 - group["lr"] * group["weight_decay"])
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=0.5, w_s=2**-9, grad_s=2**-19)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 7050 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=16, num_heads=8, model_dim=1024,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.1/1024**0.5), dict(params=embed_params, lr=0.3), dict(params=scalar_params, lr=0.015)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.025, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        return (1 - x) / args.cooldown_frac

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
for _ in range(warmup_steps):
    inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device="cuda")
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sun Feb 16 18:35:55 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   33C    P0            113W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   29C    P0            120W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   27C    P0            112W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   33C    P0            114W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   33C    P0            116W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   29C    P0            111W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   32C    P0            112W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   27C    P0            112W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/7050 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/7050 train_time:155ms step_avg:155.40ms
step:2/7050 train_time:349ms step_avg:174.30ms
step:3/7050 train_time:562ms step_avg:187.44ms
step:4/7050 train_time:786ms step_avg:196.44ms
step:5/7050 train_time:1010ms step_avg:201.98ms
step:6/7050 train_time:1232ms step_avg:205.27ms
step:7/7050 train_time:1458ms step_avg:208.26ms
step:8/7050 train_time:1684ms step_avg:210.46ms
step:9/7050 train_time:1909ms step_avg:212.08ms
step:10/7050 train_time:2132ms step_avg:213.19ms
step:11/7050 train_time:2356ms step_avg:214.21ms
step:12/7050 train_time:2582ms step_avg:215.17ms
step:13/7050 train_time:2808ms step_avg:216.01ms
step:14/7050 train_time:3031ms step_avg:216.51ms
step:15/7050 train_time:3257ms step_avg:217.11ms
step:16/7050 train_time:3482ms step_avg:217.60ms
step:17/7050 train_time:3710ms step_avg:218.22ms
step:18/7050 train_time:3931ms step_avg:218.41ms
step:19/7050 train_time:4156ms step_avg:218.75ms
step:20/7050 train_time:4383ms step_avg:219.15ms
step:21/7050 train_time:4608ms step_avg:219.45ms
step:22/7050 train_time:4833ms step_avg:219.70ms
step:23/7050 train_time:5056ms step_avg:219.83ms
step:24/7050 train_time:5283ms step_avg:220.13ms
step:25/7050 train_time:5508ms step_avg:220.31ms
step:26/7050 train_time:5733ms step_avg:220.49ms
step:27/7050 train_time:5957ms step_avg:220.63ms
step:28/7050 train_time:6184ms step_avg:220.85ms
step:29/7050 train_time:6410ms step_avg:221.04ms
step:30/7050 train_time:6636ms step_avg:221.18ms
step:31/7050 train_time:6859ms step_avg:221.27ms
step:32/7050 train_time:7086ms step_avg:221.43ms
step:33/7050 train_time:7311ms step_avg:221.56ms
step:34/7050 train_time:7538ms step_avg:221.70ms
step:35/7050 train_time:7763ms step_avg:221.81ms
step:36/7050 train_time:7989ms step_avg:221.90ms
step:37/7050 train_time:8214ms step_avg:222.00ms
step:38/7050 train_time:8440ms step_avg:222.10ms
step:39/7050 train_time:8664ms step_avg:222.15ms
step:40/7050 train_time:8889ms step_avg:222.23ms
step:41/7050 train_time:9114ms step_avg:222.29ms
step:42/7050 train_time:9338ms step_avg:222.34ms
step:43/7050 train_time:9564ms step_avg:222.42ms
step:44/7050 train_time:9788ms step_avg:222.46ms
step:45/7050 train_time:10013ms step_avg:222.51ms
step:46/7050 train_time:10238ms step_avg:222.56ms
step:47/7050 train_time:10463ms step_avg:222.61ms
step:48/7050 train_time:10688ms step_avg:222.67ms
step:49/7050 train_time:10915ms step_avg:222.75ms
step:50/7050 train_time:11138ms step_avg:222.76ms
step:51/7050 train_time:11365ms step_avg:222.84ms
step:52/7050 train_time:11590ms step_avg:222.88ms
step:53/7050 train_time:11816ms step_avg:222.95ms
step:54/7050 train_time:12044ms step_avg:223.03ms
step:55/7050 train_time:12269ms step_avg:223.07ms
step:56/7050 train_time:12493ms step_avg:223.09ms
step:57/7050 train_time:12719ms step_avg:223.13ms
step:58/7050 train_time:12943ms step_avg:223.15ms
step:59/7050 train_time:13168ms step_avg:223.19ms
step:60/7050 train_time:13392ms step_avg:223.20ms
step:61/7050 train_time:13618ms step_avg:223.24ms
step:62/7050 train_time:13842ms step_avg:223.26ms
step:63/7050 train_time:14068ms step_avg:223.30ms
step:64/7050 train_time:14293ms step_avg:223.33ms
step:65/7050 train_time:14517ms step_avg:223.34ms
step:66/7050 train_time:14742ms step_avg:223.36ms
step:67/7050 train_time:14967ms step_avg:223.39ms
step:68/7050 train_time:15193ms step_avg:223.42ms
step:69/7050 train_time:15416ms step_avg:223.43ms
step:70/7050 train_time:15641ms step_avg:223.44ms
step:71/7050 train_time:15864ms step_avg:223.44ms
step:72/7050 train_time:16089ms step_avg:223.46ms
step:73/7050 train_time:16317ms step_avg:223.52ms
step:74/7050 train_time:16540ms step_avg:223.51ms
step:75/7050 train_time:16764ms step_avg:223.52ms
step:76/7050 train_time:16989ms step_avg:223.53ms
step:77/7050 train_time:17216ms step_avg:223.59ms
step:78/7050 train_time:17440ms step_avg:223.59ms
step:79/7050 train_time:17664ms step_avg:223.59ms
step:80/7050 train_time:17889ms step_avg:223.61ms
step:81/7050 train_time:18113ms step_avg:223.61ms
step:82/7050 train_time:18338ms step_avg:223.64ms
step:83/7050 train_time:18565ms step_avg:223.67ms
step:84/7050 train_time:18788ms step_avg:223.67ms
step:85/7050 train_time:19016ms step_avg:223.71ms
step:86/7050 train_time:19241ms step_avg:223.73ms
step:87/7050 train_time:19467ms step_avg:223.76ms
step:88/7050 train_time:19690ms step_avg:223.75ms
step:89/7050 train_time:19915ms step_avg:223.76ms
step:90/7050 train_time:20141ms step_avg:223.79ms
step:91/7050 train_time:20366ms step_avg:223.80ms
step:92/7050 train_time:20591ms step_avg:223.81ms
step:93/7050 train_time:20817ms step_avg:223.84ms
step:94/7050 train_time:21041ms step_avg:223.84ms
step:95/7050 train_time:21265ms step_avg:223.84ms
step:96/7050 train_time:21491ms step_avg:223.87ms
step:97/7050 train_time:21713ms step_avg:223.85ms
step:98/7050 train_time:21938ms step_avg:223.86ms
step:99/7050 train_time:22163ms step_avg:223.86ms
step:100/7050 train_time:22387ms step_avg:223.87ms
step:101/7050 train_time:22613ms step_avg:223.89ms
step:102/7050 train_time:22834ms step_avg:223.86ms
step:103/7050 train_time:23060ms step_avg:223.88ms
step:104/7050 train_time:23284ms step_avg:223.88ms
step:105/7050 train_time:23509ms step_avg:223.90ms
step:106/7050 train_time:23733ms step_avg:223.89ms
step:107/7050 train_time:23956ms step_avg:223.89ms
step:108/7050 train_time:24183ms step_avg:223.92ms
step:109/7050 train_time:24409ms step_avg:223.94ms
step:110/7050 train_time:24633ms step_avg:223.94ms
step:111/7050 train_time:24857ms step_avg:223.94ms
step:112/7050 train_time:25084ms step_avg:223.96ms
step:113/7050 train_time:25310ms step_avg:223.98ms
step:114/7050 train_time:25534ms step_avg:223.98ms
step:115/7050 train_time:25758ms step_avg:223.99ms
step:116/7050 train_time:25982ms step_avg:223.98ms
step:117/7050 train_time:26207ms step_avg:224.00ms
step:118/7050 train_time:26430ms step_avg:223.98ms
step:119/7050 train_time:26654ms step_avg:223.98ms
step:120/7050 train_time:26879ms step_avg:223.99ms
step:121/7050 train_time:27104ms step_avg:224.00ms
step:122/7050 train_time:27328ms step_avg:224.00ms
step:123/7050 train_time:27552ms step_avg:224.00ms
step:124/7050 train_time:27778ms step_avg:224.02ms
step:125/7050 train_time:28004ms step_avg:224.03ms
step:125/7050 val_loss:4.4906 train_time:28130ms step_avg:225.04ms
step:126/7050 train_time:28226ms step_avg:224.02ms
step:127/7050 train_time:28450ms step_avg:224.02ms
step:128/7050 train_time:28679ms step_avg:224.05ms
step:129/7050 train_time:28908ms step_avg:224.09ms
step:130/7050 train_time:29132ms step_avg:224.09ms
step:131/7050 train_time:29355ms step_avg:224.09ms
step:132/7050 train_time:29578ms step_avg:224.08ms
step:133/7050 train_time:29806ms step_avg:224.11ms
step:134/7050 train_time:30031ms step_avg:224.11ms
step:135/7050 train_time:30255ms step_avg:224.11ms
step:136/7050 train_time:30478ms step_avg:224.10ms
step:137/7050 train_time:30704ms step_avg:224.11ms
step:138/7050 train_time:30929ms step_avg:224.12ms
step:139/7050 train_time:31153ms step_avg:224.12ms
step:140/7050 train_time:31376ms step_avg:224.11ms
step:141/7050 train_time:31602ms step_avg:224.12ms
step:142/7050 train_time:31828ms step_avg:224.14ms
step:143/7050 train_time:32053ms step_avg:224.15ms
step:144/7050 train_time:32278ms step_avg:224.15ms
step:145/7050 train_time:32502ms step_avg:224.15ms
step:146/7050 train_time:32728ms step_avg:224.17ms
step:147/7050 train_time:32953ms step_avg:224.17ms
step:148/7050 train_time:33176ms step_avg:224.16ms
step:149/7050 train_time:33400ms step_avg:224.16ms
step:150/7050 train_time:33625ms step_avg:224.17ms
step:151/7050 train_time:33850ms step_avg:224.18ms
step:152/7050 train_time:34076ms step_avg:224.18ms
step:153/7050 train_time:34298ms step_avg:224.17ms
step:154/7050 train_time:34522ms step_avg:224.17ms
step:155/7050 train_time:34747ms step_avg:224.17ms
step:156/7050 train_time:34971ms step_avg:224.17ms
step:157/7050 train_time:35197ms step_avg:224.18ms
step:158/7050 train_time:35420ms step_avg:224.18ms
step:159/7050 train_time:35645ms step_avg:224.18ms
step:160/7050 train_time:35869ms step_avg:224.18ms
step:161/7050 train_time:36094ms step_avg:224.19ms
step:162/7050 train_time:36318ms step_avg:224.19ms
step:163/7050 train_time:36543ms step_avg:224.19ms
step:164/7050 train_time:36765ms step_avg:224.18ms
step:165/7050 train_time:36991ms step_avg:224.18ms
step:166/7050 train_time:37216ms step_avg:224.19ms
step:167/7050 train_time:37440ms step_avg:224.19ms
step:168/7050 train_time:37664ms step_avg:224.19ms
step:169/7050 train_time:37889ms step_avg:224.19ms
step:170/7050 train_time:38112ms step_avg:224.19ms
step:171/7050 train_time:38338ms step_avg:224.20ms
step:172/7050 train_time:38564ms step_avg:224.21ms
step:173/7050 train_time:38788ms step_avg:224.21ms
step:174/7050 train_time:39012ms step_avg:224.21ms
step:175/7050 train_time:39235ms step_avg:224.20ms
step:176/7050 train_time:39460ms step_avg:224.20ms
step:177/7050 train_time:39684ms step_avg:224.20ms
step:178/7050 train_time:39908ms step_avg:224.20ms
step:179/7050 train_time:40135ms step_avg:224.22ms
step:180/7050 train_time:40356ms step_avg:224.20ms
step:181/7050 train_time:40580ms step_avg:224.20ms
step:182/7050 train_time:40805ms step_avg:224.20ms
step:183/7050 train_time:41029ms step_avg:224.20ms
step:184/7050 train_time:41254ms step_avg:224.20ms
step:185/7050 train_time:41478ms step_avg:224.20ms
step:186/7050 train_time:41702ms step_avg:224.20ms
step:187/7050 train_time:41925ms step_avg:224.20ms
step:188/7050 train_time:42149ms step_avg:224.20ms
step:189/7050 train_time:42376ms step_avg:224.21ms
step:190/7050 train_time:42599ms step_avg:224.21ms
step:191/7050 train_time:42824ms step_avg:224.21ms
step:192/7050 train_time:43048ms step_avg:224.21ms
step:193/7050 train_time:43271ms step_avg:224.20ms
step:194/7050 train_time:43497ms step_avg:224.21ms
step:195/7050 train_time:43721ms step_avg:224.21ms
step:196/7050 train_time:43944ms step_avg:224.21ms
step:197/7050 train_time:44169ms step_avg:224.21ms
step:198/7050 train_time:44393ms step_avg:224.21ms
step:199/7050 train_time:44616ms step_avg:224.20ms
step:200/7050 train_time:44841ms step_avg:224.21ms
step:201/7050 train_time:45065ms step_avg:224.20ms
step:202/7050 train_time:45288ms step_avg:224.20ms
step:203/7050 train_time:45513ms step_avg:224.20ms
step:204/7050 train_time:45738ms step_avg:224.21ms
step:205/7050 train_time:45963ms step_avg:224.21ms
step:206/7050 train_time:46187ms step_avg:224.21ms
step:207/7050 train_time:46411ms step_avg:224.21ms
step:208/7050 train_time:46638ms step_avg:224.22ms
step:209/7050 train_time:46861ms step_avg:224.21ms
step:210/7050 train_time:47083ms step_avg:224.21ms
step:211/7050 train_time:47309ms step_avg:224.21ms
step:212/7050 train_time:47535ms step_avg:224.22ms
step:213/7050 train_time:47758ms step_avg:224.21ms
step:214/7050 train_time:47981ms step_avg:224.21ms
step:215/7050 train_time:48205ms step_avg:224.21ms
step:216/7050 train_time:48428ms step_avg:224.20ms
step:217/7050 train_time:48652ms step_avg:224.20ms
step:218/7050 train_time:48877ms step_avg:224.21ms
step:219/7050 train_time:49099ms step_avg:224.20ms
step:220/7050 train_time:49323ms step_avg:224.20ms
step:221/7050 train_time:49547ms step_avg:224.20ms
step:222/7050 train_time:49769ms step_avg:224.19ms
step:223/7050 train_time:49994ms step_avg:224.19ms
step:224/7050 train_time:50219ms step_avg:224.19ms
step:225/7050 train_time:50443ms step_avg:224.19ms
step:226/7050 train_time:50666ms step_avg:224.19ms
step:227/7050 train_time:50891ms step_avg:224.19ms
step:228/7050 train_time:51115ms step_avg:224.19ms
step:229/7050 train_time:51341ms step_avg:224.20ms
step:230/7050 train_time:51564ms step_avg:224.19ms
step:231/7050 train_time:51787ms step_avg:224.19ms
step:232/7050 train_time:52011ms step_avg:224.19ms
step:233/7050 train_time:52235ms step_avg:224.19ms
step:234/7050 train_time:52462ms step_avg:224.20ms
step:235/7050 train_time:52686ms step_avg:224.19ms
step:236/7050 train_time:52910ms step_avg:224.19ms
step:237/7050 train_time:53134ms step_avg:224.19ms
step:238/7050 train_time:53359ms step_avg:224.20ms
step:239/7050 train_time:53585ms step_avg:224.20ms
step:240/7050 train_time:53809ms step_avg:224.20ms
step:241/7050 train_time:54033ms step_avg:224.20ms
step:242/7050 train_time:54258ms step_avg:224.21ms
step:243/7050 train_time:54481ms step_avg:224.20ms
step:244/7050 train_time:54707ms step_avg:224.21ms
step:245/7050 train_time:54932ms step_avg:224.21ms
step:246/7050 train_time:55155ms step_avg:224.21ms
step:247/7050 train_time:55380ms step_avg:224.21ms
step:248/7050 train_time:55604ms step_avg:224.21ms
step:249/7050 train_time:55827ms step_avg:224.21ms
step:250/7050 train_time:56051ms step_avg:224.20ms
step:250/7050 val_loss:4.1768 train_time:56177ms step_avg:224.71ms
step:251/7050 train_time:56275ms step_avg:224.20ms
step:252/7050 train_time:56502ms step_avg:224.22ms
step:253/7050 train_time:56728ms step_avg:224.22ms
step:254/7050 train_time:56952ms step_avg:224.22ms
step:255/7050 train_time:57176ms step_avg:224.22ms
step:256/7050 train_time:57401ms step_avg:224.22ms
step:257/7050 train_time:57625ms step_avg:224.22ms
step:258/7050 train_time:57848ms step_avg:224.22ms
step:259/7050 train_time:58072ms step_avg:224.22ms
step:260/7050 train_time:58295ms step_avg:224.21ms
step:261/7050 train_time:58519ms step_avg:224.21ms
step:262/7050 train_time:58744ms step_avg:224.21ms
step:263/7050 train_time:58966ms step_avg:224.20ms
step:264/7050 train_time:59190ms step_avg:224.21ms
step:265/7050 train_time:59413ms step_avg:224.20ms
step:266/7050 train_time:59637ms step_avg:224.20ms
step:267/7050 train_time:59862ms step_avg:224.20ms
step:268/7050 train_time:60084ms step_avg:224.19ms
step:269/7050 train_time:60307ms step_avg:224.19ms
step:270/7050 train_time:60530ms step_avg:224.18ms
step:271/7050 train_time:60754ms step_avg:224.18ms
step:272/7050 train_time:60978ms step_avg:224.18ms
step:273/7050 train_time:61202ms step_avg:224.18ms
step:274/7050 train_time:61425ms step_avg:224.18ms
step:275/7050 train_time:61650ms step_avg:224.18ms
step:276/7050 train_time:61873ms step_avg:224.18ms
step:277/7050 train_time:62096ms step_avg:224.17ms
step:278/7050 train_time:62321ms step_avg:224.18ms
step:279/7050 train_time:62543ms step_avg:224.17ms
step:280/7050 train_time:62768ms step_avg:224.17ms
step:281/7050 train_time:62992ms step_avg:224.17ms
step:282/7050 train_time:63216ms step_avg:224.17ms
step:283/7050 train_time:63440ms step_avg:224.17ms
step:284/7050 train_time:63664ms step_avg:224.17ms
step:285/7050 train_time:63888ms step_avg:224.17ms
step:286/7050 train_time:64112ms step_avg:224.17ms
step:287/7050 train_time:64335ms step_avg:224.16ms
step:288/7050 train_time:64560ms step_avg:224.17ms
step:289/7050 train_time:64784ms step_avg:224.17ms
step:290/7050 train_time:65008ms step_avg:224.16ms
step:291/7050 train_time:65231ms step_avg:224.16ms
step:292/7050 train_time:65456ms step_avg:224.16ms
step:293/7050 train_time:65680ms step_avg:224.17ms
step:294/7050 train_time:65905ms step_avg:224.17ms
step:295/7050 train_time:66128ms step_avg:224.16ms
step:296/7050 train_time:66351ms step_avg:224.16ms
step:297/7050 train_time:66576ms step_avg:224.16ms
step:298/7050 train_time:66802ms step_avg:224.17ms
step:299/7050 train_time:67023ms step_avg:224.16ms
step:300/7050 train_time:67247ms step_avg:224.16ms
step:301/7050 train_time:67471ms step_avg:224.16ms
step:302/7050 train_time:67694ms step_avg:224.15ms
step:303/7050 train_time:67920ms step_avg:224.16ms
step:304/7050 train_time:68141ms step_avg:224.15ms
step:305/7050 train_time:68364ms step_avg:224.14ms
step:306/7050 train_time:68589ms step_avg:224.15ms
step:307/7050 train_time:68812ms step_avg:224.14ms
step:308/7050 train_time:69034ms step_avg:224.14ms
step:309/7050 train_time:69259ms step_avg:224.14ms
step:310/7050 train_time:69484ms step_avg:224.14ms
step:311/7050 train_time:69707ms step_avg:224.14ms
step:312/7050 train_time:69929ms step_avg:224.13ms
step:313/7050 train_time:70154ms step_avg:224.13ms
step:314/7050 train_time:70378ms step_avg:224.13ms
step:315/7050 train_time:70602ms step_avg:224.13ms
step:316/7050 train_time:70825ms step_avg:224.13ms
step:317/7050 train_time:71049ms step_avg:224.13ms
step:318/7050 train_time:71273ms step_avg:224.13ms
step:319/7050 train_time:71498ms step_avg:224.13ms
step:320/7050 train_time:71721ms step_avg:224.13ms
step:321/7050 train_time:71945ms step_avg:224.13ms
step:322/7050 train_time:72169ms step_avg:224.13ms
step:323/7050 train_time:72394ms step_avg:224.13ms
step:324/7050 train_time:72617ms step_avg:224.13ms
step:325/7050 train_time:72840ms step_avg:224.12ms
step:326/7050 train_time:73065ms step_avg:224.13ms
step:327/7050 train_time:73290ms step_avg:224.13ms
step:328/7050 train_time:73515ms step_avg:224.13ms
step:329/7050 train_time:73737ms step_avg:224.13ms
step:330/7050 train_time:73961ms step_avg:224.12ms
step:331/7050 train_time:74183ms step_avg:224.12ms
step:332/7050 train_time:74409ms step_avg:224.12ms
step:333/7050 train_time:74632ms step_avg:224.12ms
step:334/7050 train_time:74855ms step_avg:224.12ms
step:335/7050 train_time:75080ms step_avg:224.12ms
step:336/7050 train_time:75305ms step_avg:224.12ms
step:337/7050 train_time:75528ms step_avg:224.12ms
step:338/7050 train_time:75752ms step_avg:224.12ms
step:339/7050 train_time:75976ms step_avg:224.12ms
step:340/7050 train_time:76201ms step_avg:224.12ms
step:341/7050 train_time:76422ms step_avg:224.11ms
step:342/7050 train_time:76645ms step_avg:224.11ms
step:343/7050 train_time:76870ms step_avg:224.11ms
step:344/7050 train_time:77095ms step_avg:224.11ms
step:345/7050 train_time:77318ms step_avg:224.11ms
step:346/7050 train_time:77542ms step_avg:224.11ms
step:347/7050 train_time:77764ms step_avg:224.11ms
step:348/7050 train_time:77989ms step_avg:224.11ms
step:349/7050 train_time:78214ms step_avg:224.11ms
step:350/7050 train_time:78436ms step_avg:224.10ms
step:351/7050 train_time:78660ms step_avg:224.10ms
step:352/7050 train_time:78884ms step_avg:224.10ms
step:353/7050 train_time:79108ms step_avg:224.10ms
step:354/7050 train_time:79331ms step_avg:224.10ms
step:355/7050 train_time:79554ms step_avg:224.09ms
step:356/7050 train_time:79779ms step_avg:224.10ms
step:357/7050 train_time:80001ms step_avg:224.09ms
step:358/7050 train_time:80224ms step_avg:224.09ms
step:359/7050 train_time:80447ms step_avg:224.09ms
step:360/7050 train_time:80671ms step_avg:224.09ms
step:361/7050 train_time:80894ms step_avg:224.08ms
step:362/7050 train_time:81118ms step_avg:224.08ms
step:363/7050 train_time:81341ms step_avg:224.08ms
step:364/7050 train_time:81565ms step_avg:224.08ms
step:365/7050 train_time:81788ms step_avg:224.08ms
step:366/7050 train_time:82013ms step_avg:224.08ms
step:367/7050 train_time:82236ms step_avg:224.08ms
step:368/7050 train_time:82458ms step_avg:224.07ms
step:369/7050 train_time:82684ms step_avg:224.07ms
step:370/7050 train_time:82906ms step_avg:224.07ms
step:371/7050 train_time:83130ms step_avg:224.07ms
step:372/7050 train_time:83353ms step_avg:224.07ms
step:373/7050 train_time:83578ms step_avg:224.07ms
step:374/7050 train_time:83804ms step_avg:224.08ms
step:375/7050 train_time:84024ms step_avg:224.07ms
step:375/7050 val_loss:4.0338 train_time:84153ms step_avg:224.41ms
step:376/7050 train_time:84253ms step_avg:224.08ms
step:377/7050 train_time:84475ms step_avg:224.07ms
step:378/7050 train_time:84701ms step_avg:224.08ms
step:379/7050 train_time:84925ms step_avg:224.08ms
step:380/7050 train_time:85147ms step_avg:224.07ms
step:381/7050 train_time:85373ms step_avg:224.08ms
step:382/7050 train_time:85595ms step_avg:224.07ms
step:383/7050 train_time:85821ms step_avg:224.07ms
step:384/7050 train_time:86043ms step_avg:224.07ms
step:385/7050 train_time:86266ms step_avg:224.07ms
step:386/7050 train_time:86490ms step_avg:224.07ms
step:387/7050 train_time:86714ms step_avg:224.07ms
step:388/7050 train_time:86937ms step_avg:224.06ms
step:389/7050 train_time:87161ms step_avg:224.06ms
step:390/7050 train_time:87383ms step_avg:224.06ms
step:391/7050 train_time:87608ms step_avg:224.06ms
step:392/7050 train_time:87832ms step_avg:224.06ms
step:393/7050 train_time:88054ms step_avg:224.06ms
step:394/7050 train_time:88277ms step_avg:224.05ms
step:395/7050 train_time:88499ms step_avg:224.05ms
step:396/7050 train_time:88726ms step_avg:224.05ms
step:397/7050 train_time:88949ms step_avg:224.05ms
step:398/7050 train_time:89172ms step_avg:224.05ms
step:399/7050 train_time:89394ms step_avg:224.04ms
step:400/7050 train_time:89620ms step_avg:224.05ms
step:401/7050 train_time:89844ms step_avg:224.05ms
step:402/7050 train_time:90068ms step_avg:224.05ms
step:403/7050 train_time:90290ms step_avg:224.04ms
step:404/7050 train_time:90512ms step_avg:224.04ms
step:405/7050 train_time:90738ms step_avg:224.04ms
step:406/7050 train_time:90962ms step_avg:224.04ms
step:407/7050 train_time:91184ms step_avg:224.04ms
step:408/7050 train_time:91408ms step_avg:224.04ms
step:409/7050 train_time:91632ms step_avg:224.04ms
step:410/7050 train_time:91856ms step_avg:224.04ms
step:411/7050 train_time:92080ms step_avg:224.04ms
step:412/7050 train_time:92302ms step_avg:224.03ms
step:413/7050 train_time:92525ms step_avg:224.03ms
step:414/7050 train_time:92748ms step_avg:224.03ms
step:415/7050 train_time:92973ms step_avg:224.03ms
step:416/7050 train_time:93196ms step_avg:224.03ms
step:417/7050 train_time:93420ms step_avg:224.03ms
step:418/7050 train_time:93643ms step_avg:224.03ms
step:419/7050 train_time:93866ms step_avg:224.02ms
step:420/7050 train_time:94090ms step_avg:224.02ms
step:421/7050 train_time:94312ms step_avg:224.02ms
step:422/7050 train_time:94536ms step_avg:224.02ms
step:423/7050 train_time:94759ms step_avg:224.02ms
step:424/7050 train_time:94983ms step_avg:224.02ms
step:425/7050 train_time:95206ms step_avg:224.01ms
step:426/7050 train_time:95430ms step_avg:224.01ms
step:427/7050 train_time:95654ms step_avg:224.01ms
step:428/7050 train_time:95877ms step_avg:224.01ms
step:429/7050 train_time:96099ms step_avg:224.01ms
step:430/7050 train_time:96323ms step_avg:224.01ms
step:431/7050 train_time:96549ms step_avg:224.01ms
step:432/7050 train_time:96772ms step_avg:224.01ms
step:433/7050 train_time:96994ms step_avg:224.00ms
step:434/7050 train_time:97217ms step_avg:224.00ms
step:435/7050 train_time:97441ms step_avg:224.00ms
step:436/7050 train_time:97666ms step_avg:224.01ms
step:437/7050 train_time:97890ms step_avg:224.00ms
step:438/7050 train_time:98113ms step_avg:224.00ms
step:439/7050 train_time:98336ms step_avg:224.00ms
step:440/7050 train_time:98560ms step_avg:224.00ms
step:441/7050 train_time:98784ms step_avg:224.00ms
step:442/7050 train_time:99007ms step_avg:224.00ms
step:443/7050 train_time:99228ms step_avg:223.99ms
step:444/7050 train_time:99453ms step_avg:223.99ms
step:445/7050 train_time:99678ms step_avg:223.99ms
step:446/7050 train_time:99901ms step_avg:223.99ms
step:447/7050 train_time:100124ms step_avg:223.99ms
step:448/7050 train_time:100348ms step_avg:223.99ms
step:449/7050 train_time:100572ms step_avg:223.99ms
step:450/7050 train_time:100794ms step_avg:223.99ms
step:451/7050 train_time:101018ms step_avg:223.99ms
step:452/7050 train_time:101242ms step_avg:223.99ms
step:453/7050 train_time:101466ms step_avg:223.99ms
step:454/7050 train_time:101691ms step_avg:223.99ms
step:455/7050 train_time:101913ms step_avg:223.99ms
step:456/7050 train_time:102136ms step_avg:223.98ms
step:457/7050 train_time:102360ms step_avg:223.98ms
step:458/7050 train_time:102584ms step_avg:223.98ms
step:459/7050 train_time:102806ms step_avg:223.98ms
step:460/7050 train_time:103030ms step_avg:223.98ms
step:461/7050 train_time:103254ms step_avg:223.98ms
step:462/7050 train_time:103477ms step_avg:223.98ms
step:463/7050 train_time:103700ms step_avg:223.97ms
step:464/7050 train_time:103922ms step_avg:223.97ms
step:465/7050 train_time:104145ms step_avg:223.97ms
step:466/7050 train_time:104368ms step_avg:223.97ms
step:467/7050 train_time:104591ms step_avg:223.96ms
step:468/7050 train_time:104815ms step_avg:223.96ms
step:469/7050 train_time:105040ms step_avg:223.97ms
step:470/7050 train_time:105263ms step_avg:223.96ms
step:471/7050 train_time:105486ms step_avg:223.96ms
step:472/7050 train_time:105710ms step_avg:223.96ms
step:473/7050 train_time:105934ms step_avg:223.96ms
step:474/7050 train_time:106158ms step_avg:223.96ms
step:475/7050 train_time:106381ms step_avg:223.96ms
step:476/7050 train_time:106603ms step_avg:223.96ms
step:477/7050 train_time:106827ms step_avg:223.96ms
step:478/7050 train_time:107052ms step_avg:223.96ms
step:479/7050 train_time:107274ms step_avg:223.96ms
step:480/7050 train_time:107498ms step_avg:223.95ms
step:481/7050 train_time:107722ms step_avg:223.95ms
step:482/7050 train_time:107945ms step_avg:223.95ms
step:483/7050 train_time:108169ms step_avg:223.95ms
step:484/7050 train_time:108392ms step_avg:223.95ms
step:485/7050 train_time:108616ms step_avg:223.95ms
step:486/7050 train_time:108840ms step_avg:223.95ms
step:487/7050 train_time:109063ms step_avg:223.95ms
step:488/7050 train_time:109285ms step_avg:223.95ms
step:489/7050 train_time:109510ms step_avg:223.95ms
step:490/7050 train_time:109733ms step_avg:223.95ms
step:491/7050 train_time:109957ms step_avg:223.94ms
step:492/7050 train_time:110181ms step_avg:223.94ms
step:493/7050 train_time:110402ms step_avg:223.94ms
step:494/7050 train_time:110626ms step_avg:223.94ms
step:495/7050 train_time:110851ms step_avg:223.94ms
step:496/7050 train_time:111074ms step_avg:223.94ms
step:497/7050 train_time:111297ms step_avg:223.94ms
step:498/7050 train_time:111521ms step_avg:223.94ms
step:499/7050 train_time:111744ms step_avg:223.94ms
step:500/7050 train_time:111967ms step_avg:223.93ms
step:500/7050 val_loss:3.9369 train_time:112094ms step_avg:224.19ms
step:501/7050 train_time:112191ms step_avg:223.93ms
step:502/7050 train_time:112413ms step_avg:223.93ms
step:503/7050 train_time:112640ms step_avg:223.94ms
step:504/7050 train_time:112863ms step_avg:223.94ms
step:505/7050 train_time:113087ms step_avg:223.94ms
step:506/7050 train_time:113309ms step_avg:223.93ms
step:507/7050 train_time:113533ms step_avg:223.93ms
step:508/7050 train_time:113757ms step_avg:223.93ms
step:509/7050 train_time:113982ms step_avg:223.93ms
step:510/7050 train_time:114204ms step_avg:223.93ms
step:511/7050 train_time:114427ms step_avg:223.93ms
step:512/7050 train_time:114651ms step_avg:223.93ms
step:513/7050 train_time:114875ms step_avg:223.93ms
step:514/7050 train_time:115099ms step_avg:223.93ms
step:515/7050 train_time:115322ms step_avg:223.93ms
step:516/7050 train_time:115547ms step_avg:223.93ms
step:517/7050 train_time:115770ms step_avg:223.93ms
step:518/7050 train_time:115996ms step_avg:223.93ms
step:519/7050 train_time:116217ms step_avg:223.93ms
step:520/7050 train_time:116441ms step_avg:223.93ms
step:521/7050 train_time:116664ms step_avg:223.92ms
step:522/7050 train_time:116889ms step_avg:223.92ms
step:523/7050 train_time:117113ms step_avg:223.92ms
step:524/7050 train_time:117337ms step_avg:223.93ms
step:525/7050 train_time:117560ms step_avg:223.92ms
step:526/7050 train_time:117785ms step_avg:223.93ms
step:527/7050 train_time:118011ms step_avg:223.93ms
step:528/7050 train_time:118236ms step_avg:223.93ms
step:529/7050 train_time:118459ms step_avg:223.93ms
step:530/7050 train_time:118685ms step_avg:223.93ms
step:531/7050 train_time:118911ms step_avg:223.94ms
step:532/7050 train_time:119136ms step_avg:223.94ms
step:533/7050 train_time:119361ms step_avg:223.94ms
step:534/7050 train_time:119587ms step_avg:223.95ms
step:535/7050 train_time:119812ms step_avg:223.95ms
step:536/7050 train_time:120037ms step_avg:223.95ms
step:537/7050 train_time:120262ms step_avg:223.95ms
step:538/7050 train_time:120487ms step_avg:223.95ms
step:539/7050 train_time:120711ms step_avg:223.95ms
step:540/7050 train_time:120936ms step_avg:223.95ms
step:541/7050 train_time:121161ms step_avg:223.96ms
step:542/7050 train_time:121388ms step_avg:223.96ms
step:543/7050 train_time:121613ms step_avg:223.96ms
step:544/7050 train_time:121837ms step_avg:223.96ms
step:545/7050 train_time:122060ms step_avg:223.96ms
step:546/7050 train_time:122286ms step_avg:223.97ms
step:547/7050 train_time:122512ms step_avg:223.97ms
step:548/7050 train_time:122736ms step_avg:223.97ms
step:549/7050 train_time:122960ms step_avg:223.97ms
step:550/7050 train_time:123184ms step_avg:223.97ms
step:551/7050 train_time:123409ms step_avg:223.97ms
step:552/7050 train_time:123634ms step_avg:223.98ms
step:553/7050 train_time:123857ms step_avg:223.97ms
step:554/7050 train_time:124082ms step_avg:223.97ms
step:555/7050 train_time:124306ms step_avg:223.97ms
step:556/7050 train_time:124531ms step_avg:223.98ms
step:557/7050 train_time:124754ms step_avg:223.98ms
step:558/7050 train_time:124979ms step_avg:223.98ms
step:559/7050 train_time:125205ms step_avg:223.98ms
step:560/7050 train_time:125429ms step_avg:223.98ms
step:561/7050 train_time:125655ms step_avg:223.98ms
step:562/7050 train_time:125878ms step_avg:223.98ms
step:563/7050 train_time:126103ms step_avg:223.98ms
step:564/7050 train_time:126328ms step_avg:223.99ms
step:565/7050 train_time:126552ms step_avg:223.99ms
step:566/7050 train_time:126779ms step_avg:223.99ms
step:567/7050 train_time:127002ms step_avg:223.99ms
step:568/7050 train_time:127228ms step_avg:223.99ms
step:569/7050 train_time:127451ms step_avg:223.99ms
step:570/7050 train_time:127676ms step_avg:223.99ms
step:571/7050 train_time:127901ms step_avg:223.99ms
step:572/7050 train_time:128125ms step_avg:223.99ms
step:573/7050 train_time:128351ms step_avg:224.00ms
step:574/7050 train_time:128575ms step_avg:224.00ms
step:575/7050 train_time:128800ms step_avg:224.00ms
step:576/7050 train_time:129026ms step_avg:224.00ms
step:577/7050 train_time:129251ms step_avg:224.01ms
step:578/7050 train_time:129476ms step_avg:224.01ms
step:579/7050 train_time:129700ms step_avg:224.01ms
step:580/7050 train_time:129927ms step_avg:224.01ms
step:581/7050 train_time:130151ms step_avg:224.01ms
step:582/7050 train_time:130377ms step_avg:224.01ms
step:583/7050 train_time:130601ms step_avg:224.01ms
step:584/7050 train_time:130826ms step_avg:224.02ms
step:585/7050 train_time:131050ms step_avg:224.02ms
step:586/7050 train_time:131275ms step_avg:224.02ms
step:587/7050 train_time:131499ms step_avg:224.02ms
step:588/7050 train_time:131723ms step_avg:224.02ms
step:589/7050 train_time:131949ms step_avg:224.02ms
step:590/7050 train_time:132173ms step_avg:224.02ms
step:591/7050 train_time:132398ms step_avg:224.02ms
step:592/7050 train_time:132622ms step_avg:224.02ms
step:593/7050 train_time:132847ms step_avg:224.03ms
step:594/7050 train_time:133073ms step_avg:224.03ms
step:595/7050 train_time:133298ms step_avg:224.03ms
step:596/7050 train_time:133522ms step_avg:224.03ms
step:597/7050 train_time:133746ms step_avg:224.03ms
step:598/7050 train_time:133971ms step_avg:224.03ms
step:599/7050 train_time:134199ms step_avg:224.04ms
step:600/7050 train_time:134422ms step_avg:224.04ms
step:601/7050 train_time:134647ms step_avg:224.04ms
step:602/7050 train_time:134870ms step_avg:224.04ms
step:603/7050 train_time:135097ms step_avg:224.04ms
step:604/7050 train_time:135325ms step_avg:224.05ms
step:605/7050 train_time:135546ms step_avg:224.04ms
step:606/7050 train_time:135770ms step_avg:224.04ms
step:607/7050 train_time:135994ms step_avg:224.04ms
step:608/7050 train_time:136220ms step_avg:224.05ms
step:609/7050 train_time:136444ms step_avg:224.05ms
step:610/7050 train_time:136669ms step_avg:224.05ms
step:611/7050 train_time:136893ms step_avg:224.05ms
step:612/7050 train_time:137118ms step_avg:224.05ms
step:613/7050 train_time:137342ms step_avg:224.05ms
step:614/7050 train_time:137566ms step_avg:224.05ms
step:615/7050 train_time:137791ms step_avg:224.05ms
step:616/7050 train_time:138017ms step_avg:224.05ms
step:617/7050 train_time:138241ms step_avg:224.05ms
step:618/7050 train_time:138465ms step_avg:224.05ms
step:619/7050 train_time:138691ms step_avg:224.06ms
step:620/7050 train_time:138916ms step_avg:224.06ms
step:621/7050 train_time:139140ms step_avg:224.06ms
step:622/7050 train_time:139363ms step_avg:224.06ms
step:623/7050 train_time:139590ms step_avg:224.06ms
step:624/7050 train_time:139814ms step_avg:224.06ms
step:625/7050 train_time:140038ms step_avg:224.06ms
step:625/7050 val_loss:3.6587 train_time:140163ms step_avg:224.26ms
step:626/7050 train_time:140262ms step_avg:224.06ms
step:627/7050 train_time:140488ms step_avg:224.06ms
step:628/7050 train_time:140718ms step_avg:224.07ms
step:629/7050 train_time:140943ms step_avg:224.07ms
step:630/7050 train_time:141166ms step_avg:224.07ms
step:631/7050 train_time:141392ms step_avg:224.08ms
step:632/7050 train_time:141616ms step_avg:224.08ms
step:633/7050 train_time:141843ms step_avg:224.08ms
step:634/7050 train_time:142068ms step_avg:224.08ms
step:635/7050 train_time:142291ms step_avg:224.08ms
step:636/7050 train_time:142516ms step_avg:224.08ms
step:637/7050 train_time:142743ms step_avg:224.09ms
step:638/7050 train_time:142967ms step_avg:224.09ms
step:639/7050 train_time:143191ms step_avg:224.09ms
step:640/7050 train_time:143415ms step_avg:224.09ms
step:641/7050 train_time:143642ms step_avg:224.09ms
step:642/7050 train_time:143868ms step_avg:224.09ms
step:643/7050 train_time:144093ms step_avg:224.10ms
step:644/7050 train_time:144317ms step_avg:224.10ms
step:645/7050 train_time:144543ms step_avg:224.10ms
step:646/7050 train_time:144769ms step_avg:224.10ms
step:647/7050 train_time:144993ms step_avg:224.10ms
step:648/7050 train_time:145219ms step_avg:224.10ms
step:649/7050 train_time:145444ms step_avg:224.11ms
step:650/7050 train_time:145670ms step_avg:224.11ms
step:651/7050 train_time:145895ms step_avg:224.11ms
step:652/7050 train_time:146119ms step_avg:224.11ms
step:653/7050 train_time:146344ms step_avg:224.11ms
step:654/7050 train_time:146568ms step_avg:224.11ms
step:655/7050 train_time:146793ms step_avg:224.11ms
step:656/7050 train_time:147017ms step_avg:224.11ms
step:657/7050 train_time:147241ms step_avg:224.11ms
step:658/7050 train_time:147467ms step_avg:224.11ms
step:659/7050 train_time:147691ms step_avg:224.11ms
step:660/7050 train_time:147915ms step_avg:224.11ms
step:661/7050 train_time:148142ms step_avg:224.12ms
step:662/7050 train_time:148369ms step_avg:224.12ms
step:663/7050 train_time:148592ms step_avg:224.12ms
step:664/7050 train_time:148816ms step_avg:224.12ms
step:665/7050 train_time:149040ms step_avg:224.12ms
step:666/7050 train_time:149266ms step_avg:224.12ms
step:667/7050 train_time:149491ms step_avg:224.12ms
step:668/7050 train_time:149713ms step_avg:224.12ms
step:669/7050 train_time:149939ms step_avg:224.12ms
step:670/7050 train_time:150164ms step_avg:224.13ms
step:671/7050 train_time:150388ms step_avg:224.12ms
step:672/7050 train_time:150612ms step_avg:224.12ms
step:673/7050 train_time:150836ms step_avg:224.12ms
step:674/7050 train_time:151062ms step_avg:224.13ms
step:675/7050 train_time:151287ms step_avg:224.13ms
step:676/7050 train_time:151512ms step_avg:224.13ms
step:677/7050 train_time:151738ms step_avg:224.13ms
step:678/7050 train_time:151962ms step_avg:224.13ms
step:679/7050 train_time:152186ms step_avg:224.13ms
step:680/7050 train_time:152412ms step_avg:224.14ms
step:681/7050 train_time:152636ms step_avg:224.14ms
step:682/7050 train_time:152862ms step_avg:224.14ms
step:683/7050 train_time:153086ms step_avg:224.14ms
step:684/7050 train_time:153311ms step_avg:224.14ms
step:685/7050 train_time:153536ms step_avg:224.14ms
step:686/7050 train_time:153761ms step_avg:224.14ms
step:687/7050 train_time:153987ms step_avg:224.14ms
step:688/7050 train_time:154211ms step_avg:224.14ms
step:689/7050 train_time:154436ms step_avg:224.14ms
step:690/7050 train_time:154661ms step_avg:224.15ms
step:691/7050 train_time:154886ms step_avg:224.15ms
step:692/7050 train_time:155109ms step_avg:224.15ms
step:693/7050 train_time:155332ms step_avg:224.14ms
step:694/7050 train_time:155561ms step_avg:224.15ms
step:695/7050 train_time:155784ms step_avg:224.15ms
step:696/7050 train_time:156007ms step_avg:224.15ms
step:697/7050 train_time:156231ms step_avg:224.15ms
step:698/7050 train_time:156456ms step_avg:224.15ms
step:699/7050 train_time:156682ms step_avg:224.15ms
step:700/7050 train_time:156906ms step_avg:224.15ms
step:701/7050 train_time:157131ms step_avg:224.15ms
step:702/7050 train_time:157356ms step_avg:224.15ms
step:703/7050 train_time:157583ms step_avg:224.16ms
step:704/7050 train_time:157806ms step_avg:224.16ms
step:705/7050 train_time:158030ms step_avg:224.16ms
step:706/7050 train_time:158255ms step_avg:224.16ms
step:707/7050 train_time:158480ms step_avg:224.16ms
step:708/7050 train_time:158704ms step_avg:224.16ms
step:709/7050 train_time:158928ms step_avg:224.16ms
step:710/7050 train_time:159152ms step_avg:224.16ms
step:711/7050 train_time:159376ms step_avg:224.16ms
step:712/7050 train_time:159601ms step_avg:224.16ms
step:713/7050 train_time:159826ms step_avg:224.16ms
step:714/7050 train_time:160051ms step_avg:224.16ms
step:715/7050 train_time:160276ms step_avg:224.16ms
step:716/7050 train_time:160500ms step_avg:224.16ms
step:717/7050 train_time:160723ms step_avg:224.16ms
step:718/7050 train_time:160949ms step_avg:224.16ms
step:719/7050 train_time:161174ms step_avg:224.16ms
step:720/7050 train_time:161397ms step_avg:224.16ms
step:721/7050 train_time:161622ms step_avg:224.16ms
step:722/7050 train_time:161847ms step_avg:224.16ms
step:723/7050 train_time:162071ms step_avg:224.17ms
step:724/7050 train_time:162295ms step_avg:224.16ms
step:725/7050 train_time:162519ms step_avg:224.16ms
step:726/7050 train_time:162746ms step_avg:224.17ms
step:727/7050 train_time:162975ms step_avg:224.17ms
step:728/7050 train_time:163197ms step_avg:224.17ms
step:729/7050 train_time:163421ms step_avg:224.17ms
step:730/7050 train_time:163646ms step_avg:224.17ms
step:731/7050 train_time:163872ms step_avg:224.18ms
step:732/7050 train_time:164096ms step_avg:224.17ms
step:733/7050 train_time:164320ms step_avg:224.17ms
step:734/7050 train_time:164546ms step_avg:224.18ms
step:735/7050 train_time:164771ms step_avg:224.18ms
step:736/7050 train_time:164996ms step_avg:224.18ms
step:737/7050 train_time:165220ms step_avg:224.18ms
step:738/7050 train_time:165445ms step_avg:224.18ms
step:739/7050 train_time:165670ms step_avg:224.18ms
step:740/7050 train_time:165894ms step_avg:224.18ms
step:741/7050 train_time:166119ms step_avg:224.18ms
step:742/7050 train_time:166343ms step_avg:224.18ms
step:743/7050 train_time:166568ms step_avg:224.18ms
step:744/7050 train_time:166792ms step_avg:224.18ms
step:745/7050 train_time:167016ms step_avg:224.18ms
step:746/7050 train_time:167242ms step_avg:224.19ms
step:747/7050 train_time:167467ms step_avg:224.19ms
step:748/7050 train_time:167692ms step_avg:224.19ms
step:749/7050 train_time:167916ms step_avg:224.19ms
step:750/7050 train_time:168143ms step_avg:224.19ms
step:750/7050 val_loss:3.5913 train_time:168269ms step_avg:224.36ms
step:751/7050 train_time:168367ms step_avg:224.19ms
step:752/7050 train_time:168591ms step_avg:224.19ms
step:753/7050 train_time:168819ms step_avg:224.20ms
step:754/7050 train_time:169044ms step_avg:224.20ms
step:755/7050 train_time:169267ms step_avg:224.20ms
step:756/7050 train_time:169493ms step_avg:224.20ms
step:757/7050 train_time:169719ms step_avg:224.20ms
step:758/7050 train_time:169945ms step_avg:224.20ms
step:759/7050 train_time:170169ms step_avg:224.20ms
step:760/7050 train_time:170394ms step_avg:224.20ms
step:761/7050 train_time:170618ms step_avg:224.20ms
step:762/7050 train_time:170844ms step_avg:224.20ms
step:763/7050 train_time:171069ms step_avg:224.21ms
step:764/7050 train_time:171294ms step_avg:224.21ms
step:765/7050 train_time:171518ms step_avg:224.21ms
step:766/7050 train_time:171744ms step_avg:224.21ms
step:767/7050 train_time:171969ms step_avg:224.21ms
step:768/7050 train_time:172195ms step_avg:224.21ms
step:769/7050 train_time:172421ms step_avg:224.21ms
step:770/7050 train_time:172644ms step_avg:224.21ms
step:771/7050 train_time:172869ms step_avg:224.21ms
step:772/7050 train_time:173095ms step_avg:224.22ms
step:773/7050 train_time:173319ms step_avg:224.22ms
step:774/7050 train_time:173543ms step_avg:224.22ms
step:775/7050 train_time:173769ms step_avg:224.22ms
step:776/7050 train_time:173995ms step_avg:224.22ms
step:777/7050 train_time:174218ms step_avg:224.22ms
step:778/7050 train_time:174442ms step_avg:224.22ms
step:779/7050 train_time:174666ms step_avg:224.22ms
step:780/7050 train_time:174891ms step_avg:224.22ms
step:781/7050 train_time:175117ms step_avg:224.22ms
step:782/7050 train_time:175342ms step_avg:224.22ms
step:783/7050 train_time:175566ms step_avg:224.22ms
step:784/7050 train_time:175793ms step_avg:224.23ms
step:785/7050 train_time:176018ms step_avg:224.23ms
step:786/7050 train_time:176243ms step_avg:224.23ms
step:787/7050 train_time:176468ms step_avg:224.23ms
step:788/7050 train_time:176694ms step_avg:224.23ms
step:789/7050 train_time:176919ms step_avg:224.23ms
step:790/7050 train_time:177142ms step_avg:224.23ms
step:791/7050 train_time:177366ms step_avg:224.23ms
step:792/7050 train_time:177590ms step_avg:224.23ms
step:793/7050 train_time:177815ms step_avg:224.23ms
step:794/7050 train_time:178040ms step_avg:224.23ms
step:795/7050 train_time:178264ms step_avg:224.23ms
step:796/7050 train_time:178490ms step_avg:224.23ms
step:797/7050 train_time:178716ms step_avg:224.24ms
step:798/7050 train_time:178940ms step_avg:224.24ms
step:799/7050 train_time:179165ms step_avg:224.24ms
step:800/7050 train_time:179389ms step_avg:224.24ms
step:801/7050 train_time:179615ms step_avg:224.24ms
step:802/7050 train_time:179839ms step_avg:224.24ms
step:803/7050 train_time:180063ms step_avg:224.24ms
step:804/7050 train_time:180289ms step_avg:224.24ms
step:805/7050 train_time:180515ms step_avg:224.24ms
step:806/7050 train_time:180739ms step_avg:224.24ms
step:807/7050 train_time:180964ms step_avg:224.24ms
step:808/7050 train_time:181190ms step_avg:224.25ms
step:809/7050 train_time:181414ms step_avg:224.25ms
step:810/7050 train_time:181640ms step_avg:224.25ms
step:811/7050 train_time:181864ms step_avg:224.25ms
step:812/7050 train_time:182090ms step_avg:224.25ms
step:813/7050 train_time:182317ms step_avg:224.25ms
step:814/7050 train_time:182541ms step_avg:224.25ms
step:815/7050 train_time:182765ms step_avg:224.25ms
step:816/7050 train_time:182990ms step_avg:224.25ms
step:817/7050 train_time:183217ms step_avg:224.26ms
step:818/7050 train_time:183441ms step_avg:224.26ms
step:819/7050 train_time:183664ms step_avg:224.25ms
step:820/7050 train_time:183889ms step_avg:224.25ms
step:821/7050 train_time:184115ms step_avg:224.26ms
step:822/7050 train_time:184340ms step_avg:224.26ms
step:823/7050 train_time:184563ms step_avg:224.26ms
step:824/7050 train_time:184788ms step_avg:224.26ms
step:825/7050 train_time:185012ms step_avg:224.26ms
step:826/7050 train_time:185237ms step_avg:224.26ms
step:827/7050 train_time:185462ms step_avg:224.26ms
step:828/7050 train_time:185686ms step_avg:224.26ms
step:829/7050 train_time:185911ms step_avg:224.26ms
step:830/7050 train_time:186136ms step_avg:224.26ms
step:831/7050 train_time:186360ms step_avg:224.26ms
step:832/7050 train_time:186585ms step_avg:224.26ms
step:833/7050 train_time:186811ms step_avg:224.26ms
step:834/7050 train_time:187036ms step_avg:224.26ms
step:835/7050 train_time:187261ms step_avg:224.26ms
step:836/7050 train_time:187485ms step_avg:224.26ms
step:837/7050 train_time:187712ms step_avg:224.27ms
step:838/7050 train_time:187938ms step_avg:224.27ms
step:839/7050 train_time:188163ms step_avg:224.27ms
step:840/7050 train_time:188387ms step_avg:224.27ms
step:841/7050 train_time:188612ms step_avg:224.27ms
step:842/7050 train_time:188836ms step_avg:224.27ms
step:843/7050 train_time:189061ms step_avg:224.27ms
step:844/7050 train_time:189284ms step_avg:224.27ms
step:845/7050 train_time:189511ms step_avg:224.27ms
step:846/7050 train_time:189736ms step_avg:224.27ms
step:847/7050 train_time:189960ms step_avg:224.27ms
step:848/7050 train_time:190186ms step_avg:224.28ms
step:849/7050 train_time:190410ms step_avg:224.28ms
step:850/7050 train_time:190635ms step_avg:224.28ms
step:851/7050 train_time:190859ms step_avg:224.28ms
step:852/7050 train_time:191083ms step_avg:224.28ms
step:853/7050 train_time:191308ms step_avg:224.28ms
step:854/7050 train_time:191532ms step_avg:224.28ms
step:855/7050 train_time:191758ms step_avg:224.28ms
step:856/7050 train_time:191982ms step_avg:224.28ms
step:857/7050 train_time:192206ms step_avg:224.28ms
step:858/7050 train_time:192431ms step_avg:224.28ms
step:859/7050 train_time:192657ms step_avg:224.28ms
step:860/7050 train_time:192883ms step_avg:224.28ms
step:861/7050 train_time:193108ms step_avg:224.28ms
step:862/7050 train_time:193335ms step_avg:224.29ms
step:863/7050 train_time:193559ms step_avg:224.29ms
step:864/7050 train_time:193785ms step_avg:224.29ms
step:865/7050 train_time:194012ms step_avg:224.29ms
step:866/7050 train_time:194237ms step_avg:224.29ms
step:867/7050 train_time:194461ms step_avg:224.29ms
step:868/7050 train_time:194685ms step_avg:224.29ms
step:869/7050 train_time:194912ms step_avg:224.29ms
step:870/7050 train_time:195136ms step_avg:224.29ms
step:871/7050 train_time:195360ms step_avg:224.29ms
step:872/7050 train_time:195585ms step_avg:224.29ms
step:873/7050 train_time:195809ms step_avg:224.29ms
step:874/7050 train_time:196035ms step_avg:224.30ms
step:875/7050 train_time:196258ms step_avg:224.30ms
step:875/7050 val_loss:3.5365 train_time:196386ms step_avg:224.44ms
step:876/7050 train_time:196487ms step_avg:224.30ms
step:877/7050 train_time:196709ms step_avg:224.30ms
step:878/7050 train_time:196937ms step_avg:224.30ms
step:879/7050 train_time:197165ms step_avg:224.31ms
step:880/7050 train_time:197388ms step_avg:224.31ms
step:881/7050 train_time:197611ms step_avg:224.30ms
step:882/7050 train_time:197837ms step_avg:224.30ms
step:883/7050 train_time:198064ms step_avg:224.31ms
step:884/7050 train_time:198288ms step_avg:224.31ms
step:885/7050 train_time:198512ms step_avg:224.31ms
step:886/7050 train_time:198736ms step_avg:224.31ms
step:887/7050 train_time:198962ms step_avg:224.31ms
step:888/7050 train_time:199186ms step_avg:224.31ms
step:889/7050 train_time:199412ms step_avg:224.31ms
step:890/7050 train_time:199635ms step_avg:224.31ms
step:891/7050 train_time:199861ms step_avg:224.31ms
step:892/7050 train_time:200087ms step_avg:224.31ms
step:893/7050 train_time:200310ms step_avg:224.31ms
step:894/7050 train_time:200535ms step_avg:224.31ms
step:895/7050 train_time:200759ms step_avg:224.31ms
step:896/7050 train_time:200985ms step_avg:224.31ms
step:897/7050 train_time:201208ms step_avg:224.31ms
step:898/7050 train_time:201434ms step_avg:224.31ms
step:899/7050 train_time:201660ms step_avg:224.32ms
step:900/7050 train_time:201884ms step_avg:224.32ms
step:901/7050 train_time:202109ms step_avg:224.32ms
step:902/7050 train_time:202335ms step_avg:224.32ms
step:903/7050 train_time:202558ms step_avg:224.32ms
step:904/7050 train_time:202785ms step_avg:224.32ms
step:905/7050 train_time:203007ms step_avg:224.32ms
step:906/7050 train_time:203231ms step_avg:224.32ms
step:907/7050 train_time:203456ms step_avg:224.32ms
step:908/7050 train_time:203683ms step_avg:224.32ms
step:909/7050 train_time:203909ms step_avg:224.32ms
step:910/7050 train_time:204132ms step_avg:224.32ms
step:911/7050 train_time:204357ms step_avg:224.32ms
step:912/7050 train_time:204582ms step_avg:224.32ms
step:913/7050 train_time:204807ms step_avg:224.32ms
step:914/7050 train_time:205031ms step_avg:224.32ms
step:915/7050 train_time:205256ms step_avg:224.32ms
step:916/7050 train_time:205482ms step_avg:224.32ms
step:917/7050 train_time:205707ms step_avg:224.33ms
step:918/7050 train_time:205932ms step_avg:224.33ms
step:919/7050 train_time:206159ms step_avg:224.33ms
step:920/7050 train_time:206382ms step_avg:224.33ms
step:921/7050 train_time:206607ms step_avg:224.33ms
step:922/7050 train_time:206833ms step_avg:224.33ms
step:923/7050 train_time:207059ms step_avg:224.33ms
step:924/7050 train_time:207283ms step_avg:224.33ms
step:925/7050 train_time:207506ms step_avg:224.33ms
step:926/7050 train_time:207731ms step_avg:224.33ms
step:927/7050 train_time:207957ms step_avg:224.33ms
step:928/7050 train_time:208183ms step_avg:224.34ms
step:929/7050 train_time:208407ms step_avg:224.33ms
step:930/7050 train_time:208631ms step_avg:224.33ms
step:931/7050 train_time:208855ms step_avg:224.33ms
step:932/7050 train_time:209081ms step_avg:224.34ms
step:933/7050 train_time:209304ms step_avg:224.33ms
step:934/7050 train_time:209530ms step_avg:224.34ms
step:935/7050 train_time:209756ms step_avg:224.34ms
step:936/7050 train_time:209980ms step_avg:224.34ms
step:937/7050 train_time:210205ms step_avg:224.34ms
step:938/7050 train_time:210429ms step_avg:224.34ms
step:939/7050 train_time:210654ms step_avg:224.34ms
step:940/7050 train_time:210879ms step_avg:224.34ms
step:941/7050 train_time:211104ms step_avg:224.34ms
step:942/7050 train_time:211327ms step_avg:224.34ms
step:943/7050 train_time:211552ms step_avg:224.34ms
step:944/7050 train_time:211776ms step_avg:224.34ms
step:945/7050 train_time:212000ms step_avg:224.34ms
step:946/7050 train_time:212225ms step_avg:224.34ms
step:947/7050 train_time:212451ms step_avg:224.34ms
step:948/7050 train_time:212675ms step_avg:224.34ms
step:949/7050 train_time:212899ms step_avg:224.34ms
step:950/7050 train_time:213124ms step_avg:224.34ms
step:951/7050 train_time:213349ms step_avg:224.34ms
step:952/7050 train_time:213577ms step_avg:224.35ms
step:953/7050 train_time:213800ms step_avg:224.34ms
step:954/7050 train_time:214023ms step_avg:224.34ms
step:955/7050 train_time:214247ms step_avg:224.34ms
step:956/7050 train_time:214474ms step_avg:224.34ms
step:957/7050 train_time:214699ms step_avg:224.35ms
step:958/7050 train_time:214922ms step_avg:224.34ms
step:959/7050 train_time:215147ms step_avg:224.35ms
step:960/7050 train_time:215372ms step_avg:224.35ms
step:961/7050 train_time:215597ms step_avg:224.35ms
step:962/7050 train_time:215823ms step_avg:224.35ms
step:963/7050 train_time:216048ms step_avg:224.35ms
step:964/7050 train_time:216274ms step_avg:224.35ms
step:965/7050 train_time:216498ms step_avg:224.35ms
step:966/7050 train_time:216723ms step_avg:224.35ms
step:967/7050 train_time:216947ms step_avg:224.35ms
step:968/7050 train_time:217173ms step_avg:224.35ms
step:969/7050 train_time:217398ms step_avg:224.35ms
step:970/7050 train_time:217623ms step_avg:224.35ms
step:971/7050 train_time:217846ms step_avg:224.35ms
step:972/7050 train_time:218071ms step_avg:224.35ms
step:973/7050 train_time:218297ms step_avg:224.35ms
step:974/7050 train_time:218522ms step_avg:224.36ms
step:975/7050 train_time:218745ms step_avg:224.35ms
step:976/7050 train_time:218970ms step_avg:224.35ms
step:977/7050 train_time:219195ms step_avg:224.36ms
step:978/7050 train_time:219419ms step_avg:224.36ms
step:979/7050 train_time:219645ms step_avg:224.36ms
step:980/7050 train_time:219869ms step_avg:224.36ms
step:981/7050 train_time:220096ms step_avg:224.36ms
step:982/7050 train_time:220320ms step_avg:224.36ms
step:983/7050 train_time:220543ms step_avg:224.36ms
step:984/7050 train_time:220769ms step_avg:224.36ms
step:985/7050 train_time:220996ms step_avg:224.36ms
step:986/7050 train_time:221219ms step_avg:224.36ms
step:987/7050 train_time:221444ms step_avg:224.36ms
step:988/7050 train_time:221668ms step_avg:224.36ms
step:989/7050 train_time:221892ms step_avg:224.36ms
step:990/7050 train_time:222116ms step_avg:224.36ms
step:991/7050 train_time:222341ms step_avg:224.36ms
step:992/7050 train_time:222564ms step_avg:224.36ms
step:993/7050 train_time:222789ms step_avg:224.36ms
step:994/7050 train_time:223015ms step_avg:224.36ms
step:995/7050 train_time:223239ms step_avg:224.36ms
step:996/7050 train_time:223463ms step_avg:224.36ms
step:997/7050 train_time:223689ms step_avg:224.36ms
step:998/7050 train_time:223914ms step_avg:224.36ms
step:999/7050 train_time:224139ms step_avg:224.36ms
step:1000/7050 train_time:224363ms step_avg:224.36ms
step:1000/7050 val_loss:3.4893 train_time:224490ms step_avg:224.49ms
step:1001/7050 train_time:224588ms step_avg:224.36ms
step:1002/7050 train_time:224812ms step_avg:224.36ms
step:1003/7050 train_time:225042ms step_avg:224.37ms
step:1004/7050 train_time:225266ms step_avg:224.37ms
step:1005/7050 train_time:225490ms step_avg:224.37ms
step:1006/7050 train_time:225714ms step_avg:224.37ms
step:1007/7050 train_time:225939ms step_avg:224.37ms
step:1008/7050 train_time:226165ms step_avg:224.37ms
step:1009/7050 train_time:226390ms step_avg:224.37ms
step:1010/7050 train_time:226614ms step_avg:224.37ms
step:1011/7050 train_time:226838ms step_avg:224.37ms
step:1012/7050 train_time:227063ms step_avg:224.37ms
step:1013/7050 train_time:227287ms step_avg:224.37ms
step:1014/7050 train_time:227513ms step_avg:224.37ms
step:1015/7050 train_time:227736ms step_avg:224.37ms
step:1016/7050 train_time:227961ms step_avg:224.37ms
step:1017/7050 train_time:228187ms step_avg:224.37ms
step:1018/7050 train_time:228413ms step_avg:224.37ms
step:1019/7050 train_time:228637ms step_avg:224.37ms
step:1020/7050 train_time:228861ms step_avg:224.37ms
step:1021/7050 train_time:229087ms step_avg:224.37ms
step:1022/7050 train_time:229311ms step_avg:224.38ms
step:1023/7050 train_time:229538ms step_avg:224.38ms
step:1024/7050 train_time:229761ms step_avg:224.38ms
step:1025/7050 train_time:229987ms step_avg:224.38ms
step:1026/7050 train_time:230212ms step_avg:224.38ms
step:1027/7050 train_time:230437ms step_avg:224.38ms
step:1028/7050 train_time:230661ms step_avg:224.38ms
step:1029/7050 train_time:230885ms step_avg:224.38ms
step:1030/7050 train_time:231109ms step_avg:224.38ms
step:1031/7050 train_time:231335ms step_avg:224.38ms
step:1032/7050 train_time:231558ms step_avg:224.38ms
step:1033/7050 train_time:231783ms step_avg:224.38ms
step:1034/7050 train_time:232009ms step_avg:224.38ms
step:1035/7050 train_time:232234ms step_avg:224.38ms
step:1036/7050 train_time:232458ms step_avg:224.38ms
step:1037/7050 train_time:232682ms step_avg:224.38ms
step:1038/7050 train_time:232908ms step_avg:224.38ms
step:1039/7050 train_time:233132ms step_avg:224.38ms
step:1040/7050 train_time:233356ms step_avg:224.38ms
step:1041/7050 train_time:233580ms step_avg:224.38ms
step:1042/7050 train_time:233805ms step_avg:224.38ms
step:1043/7050 train_time:234034ms step_avg:224.39ms
step:1044/7050 train_time:234258ms step_avg:224.39ms
step:1045/7050 train_time:234484ms step_avg:224.39ms
step:1046/7050 train_time:234710ms step_avg:224.39ms
step:1047/7050 train_time:234936ms step_avg:224.39ms
step:1048/7050 train_time:235161ms step_avg:224.39ms
step:1049/7050 train_time:235388ms step_avg:224.39ms
step:1050/7050 train_time:235614ms step_avg:224.39ms
step:1051/7050 train_time:235841ms step_avg:224.40ms
step:1052/7050 train_time:236067ms step_avg:224.40ms
step:1053/7050 train_time:236292ms step_avg:224.40ms
step:1054/7050 train_time:236518ms step_avg:224.40ms
step:1055/7050 train_time:236744ms step_avg:224.40ms
step:1056/7050 train_time:236969ms step_avg:224.40ms
step:1057/7050 train_time:237195ms step_avg:224.40ms
step:1058/7050 train_time:237420ms step_avg:224.41ms
step:1059/7050 train_time:237647ms step_avg:224.41ms
step:1060/7050 train_time:237873ms step_avg:224.41ms
step:1061/7050 train_time:238099ms step_avg:224.41ms
step:1062/7050 train_time:238325ms step_avg:224.41ms
step:1063/7050 train_time:238551ms step_avg:224.41ms
step:1064/7050 train_time:238777ms step_avg:224.41ms
step:1065/7050 train_time:239002ms step_avg:224.42ms
step:1066/7050 train_time:239228ms step_avg:224.42ms
step:1067/7050 train_time:239455ms step_avg:224.42ms
step:1068/7050 train_time:239680ms step_avg:224.42ms
step:1069/7050 train_time:239906ms step_avg:224.42ms
step:1070/7050 train_time:240132ms step_avg:224.42ms
step:1071/7050 train_time:240358ms step_avg:224.42ms
step:1072/7050 train_time:240583ms step_avg:224.42ms
step:1073/7050 train_time:240810ms step_avg:224.43ms
step:1074/7050 train_time:241036ms step_avg:224.43ms
step:1075/7050 train_time:241261ms step_avg:224.43ms
step:1076/7050 train_time:241488ms step_avg:224.43ms
step:1077/7050 train_time:241715ms step_avg:224.43ms
step:1078/7050 train_time:241941ms step_avg:224.43ms
step:1079/7050 train_time:242167ms step_avg:224.44ms
step:1080/7050 train_time:242392ms step_avg:224.44ms
step:1081/7050 train_time:242620ms step_avg:224.44ms
step:1082/7050 train_time:242848ms step_avg:224.44ms
step:1083/7050 train_time:243074ms step_avg:224.45ms
step:1084/7050 train_time:243299ms step_avg:224.45ms
step:1085/7050 train_time:243524ms step_avg:224.45ms
step:1086/7050 train_time:243751ms step_avg:224.45ms
step:1087/7050 train_time:243977ms step_avg:224.45ms
step:1088/7050 train_time:244201ms step_avg:224.45ms
step:1089/7050 train_time:244428ms step_avg:224.45ms
step:1090/7050 train_time:244652ms step_avg:224.45ms
step:1091/7050 train_time:244879ms step_avg:224.45ms
step:1092/7050 train_time:245105ms step_avg:224.45ms
step:1093/7050 train_time:245330ms step_avg:224.46ms
step:1094/7050 train_time:245555ms step_avg:224.46ms
step:1095/7050 train_time:245780ms step_avg:224.46ms
step:1096/7050 train_time:246007ms step_avg:224.46ms
step:1097/7050 train_time:246234ms step_avg:224.46ms
step:1098/7050 train_time:246459ms step_avg:224.46ms
step:1099/7050 train_time:246684ms step_avg:224.46ms
step:1100/7050 train_time:246911ms step_avg:224.46ms
step:1101/7050 train_time:247138ms step_avg:224.47ms
step:1102/7050 train_time:247362ms step_avg:224.47ms
step:1103/7050 train_time:247588ms step_avg:224.47ms
step:1104/7050 train_time:247817ms step_avg:224.47ms
step:1105/7050 train_time:248041ms step_avg:224.47ms
step:1106/7050 train_time:248267ms step_avg:224.47ms
step:1107/7050 train_time:248493ms step_avg:224.47ms
step:1108/7050 train_time:248719ms step_avg:224.48ms
step:1109/7050 train_time:248946ms step_avg:224.48ms
step:1110/7050 train_time:249172ms step_avg:224.48ms
step:1111/7050 train_time:249398ms step_avg:224.48ms
step:1112/7050 train_time:249623ms step_avg:224.48ms
step:1113/7050 train_time:249850ms step_avg:224.48ms
step:1114/7050 train_time:250075ms step_avg:224.48ms
step:1115/7050 train_time:250299ms step_avg:224.48ms
step:1116/7050 train_time:250526ms step_avg:224.49ms
step:1117/7050 train_time:250752ms step_avg:224.49ms
step:1118/7050 train_time:250978ms step_avg:224.49ms
step:1119/7050 train_time:251202ms step_avg:224.49ms
step:1120/7050 train_time:251429ms step_avg:224.49ms
step:1121/7050 train_time:251656ms step_avg:224.49ms
step:1122/7050 train_time:251880ms step_avg:224.49ms
step:1123/7050 train_time:252108ms step_avg:224.50ms
step:1124/7050 train_time:252334ms step_avg:224.50ms
step:1125/7050 train_time:252559ms step_avg:224.50ms
step:1125/7050 val_loss:3.4268 train_time:252686ms step_avg:224.61ms
step:1126/7050 train_time:252784ms step_avg:224.50ms
step:1127/7050 train_time:253009ms step_avg:224.50ms
step:1128/7050 train_time:253240ms step_avg:224.50ms
step:1129/7050 train_time:253466ms step_avg:224.51ms
step:1130/7050 train_time:253692ms step_avg:224.51ms
step:1131/7050 train_time:253916ms step_avg:224.51ms
step:1132/7050 train_time:254143ms step_avg:224.51ms
step:1133/7050 train_time:254371ms step_avg:224.51ms
step:1134/7050 train_time:254599ms step_avg:224.51ms
step:1135/7050 train_time:254823ms step_avg:224.51ms
step:1136/7050 train_time:255048ms step_avg:224.51ms
step:1137/7050 train_time:255279ms step_avg:224.52ms
step:1138/7050 train_time:255503ms step_avg:224.52ms
step:1139/7050 train_time:255730ms step_avg:224.52ms
step:1140/7050 train_time:255955ms step_avg:224.52ms
step:1141/7050 train_time:256184ms step_avg:224.53ms
step:1142/7050 train_time:256408ms step_avg:224.53ms
step:1143/7050 train_time:256635ms step_avg:224.53ms
step:1144/7050 train_time:256861ms step_avg:224.53ms
step:1145/7050 train_time:257090ms step_avg:224.53ms
step:1146/7050 train_time:257313ms step_avg:224.53ms
step:1147/7050 train_time:257540ms step_avg:224.53ms
step:1148/7050 train_time:257766ms step_avg:224.53ms
step:1149/7050 train_time:257992ms step_avg:224.54ms
step:1150/7050 train_time:258217ms step_avg:224.54ms
step:1151/7050 train_time:258444ms step_avg:224.54ms
step:1152/7050 train_time:258670ms step_avg:224.54ms
step:1153/7050 train_time:258895ms step_avg:224.54ms
step:1154/7050 train_time:259121ms step_avg:224.54ms
step:1155/7050 train_time:259346ms step_avg:224.54ms
step:1156/7050 train_time:259571ms step_avg:224.54ms
step:1157/7050 train_time:259798ms step_avg:224.54ms
step:1158/7050 train_time:260024ms step_avg:224.55ms
step:1159/7050 train_time:260248ms step_avg:224.55ms
step:1160/7050 train_time:260476ms step_avg:224.55ms
step:1161/7050 train_time:260702ms step_avg:224.55ms
step:1162/7050 train_time:260927ms step_avg:224.55ms
step:1163/7050 train_time:261154ms step_avg:224.55ms
step:1164/7050 train_time:261379ms step_avg:224.55ms
step:1165/7050 train_time:261605ms step_avg:224.55ms
step:1166/7050 train_time:261831ms step_avg:224.55ms
step:1167/7050 train_time:262058ms step_avg:224.56ms
step:1168/7050 train_time:262285ms step_avg:224.56ms
step:1169/7050 train_time:262508ms step_avg:224.56ms
step:1170/7050 train_time:262734ms step_avg:224.56ms
step:1171/7050 train_time:262960ms step_avg:224.56ms
step:1172/7050 train_time:263185ms step_avg:224.56ms
step:1173/7050 train_time:263412ms step_avg:224.56ms
step:1174/7050 train_time:263637ms step_avg:224.56ms
step:1175/7050 train_time:263864ms step_avg:224.56ms
step:1176/7050 train_time:264091ms step_avg:224.57ms
step:1177/7050 train_time:264316ms step_avg:224.57ms
step:1178/7050 train_time:264543ms step_avg:224.57ms
step:1179/7050 train_time:264769ms step_avg:224.57ms
step:1180/7050 train_time:264997ms step_avg:224.57ms
step:1181/7050 train_time:265222ms step_avg:224.57ms
step:1182/7050 train_time:265447ms step_avg:224.57ms
step:1183/7050 train_time:265673ms step_avg:224.58ms
step:1184/7050 train_time:265898ms step_avg:224.58ms
step:1185/7050 train_time:266124ms step_avg:224.58ms
step:1186/7050 train_time:266349ms step_avg:224.58ms
step:1187/7050 train_time:266576ms step_avg:224.58ms
step:1188/7050 train_time:266802ms step_avg:224.58ms
step:1189/7050 train_time:267029ms step_avg:224.58ms
step:1190/7050 train_time:267254ms step_avg:224.58ms
step:1191/7050 train_time:267479ms step_avg:224.58ms
step:1192/7050 train_time:267705ms step_avg:224.58ms
step:1193/7050 train_time:267931ms step_avg:224.59ms
step:1194/7050 train_time:268156ms step_avg:224.59ms
step:1195/7050 train_time:268382ms step_avg:224.59ms
step:1196/7050 train_time:268607ms step_avg:224.59ms
step:1197/7050 train_time:268832ms step_avg:224.59ms
step:1198/7050 train_time:269061ms step_avg:224.59ms
step:1199/7050 train_time:269287ms step_avg:224.59ms
step:1200/7050 train_time:269512ms step_avg:224.59ms
step:1201/7050 train_time:269737ms step_avg:224.59ms
step:1202/7050 train_time:269966ms step_avg:224.60ms
step:1203/7050 train_time:270190ms step_avg:224.60ms
step:1204/7050 train_time:270416ms step_avg:224.60ms
step:1205/7050 train_time:270642ms step_avg:224.60ms
step:1206/7050 train_time:270868ms step_avg:224.60ms
step:1207/7050 train_time:271095ms step_avg:224.60ms
step:1208/7050 train_time:271320ms step_avg:224.60ms
step:1209/7050 train_time:271546ms step_avg:224.60ms
step:1210/7050 train_time:271773ms step_avg:224.61ms
step:1211/7050 train_time:271999ms step_avg:224.61ms
step:1212/7050 train_time:272225ms step_avg:224.61ms
step:1213/7050 train_time:272451ms step_avg:224.61ms
step:1214/7050 train_time:272676ms step_avg:224.61ms
step:1215/7050 train_time:272903ms step_avg:224.61ms
step:1216/7050 train_time:273128ms step_avg:224.61ms
step:1217/7050 train_time:273356ms step_avg:224.62ms
step:1218/7050 train_time:273581ms step_avg:224.62ms
step:1219/7050 train_time:273807ms step_avg:224.62ms
step:1220/7050 train_time:274034ms step_avg:224.62ms
step:1221/7050 train_time:274260ms step_avg:224.62ms
step:1222/7050 train_time:274487ms step_avg:224.62ms
step:1223/7050 train_time:274713ms step_avg:224.62ms
step:1224/7050 train_time:274939ms step_avg:224.62ms
step:1225/7050 train_time:275166ms step_avg:224.63ms
step:1226/7050 train_time:275392ms step_avg:224.63ms
step:1227/7050 train_time:275618ms step_avg:224.63ms
step:1228/7050 train_time:275844ms step_avg:224.63ms
step:1229/7050 train_time:276069ms step_avg:224.63ms
step:1230/7050 train_time:276298ms step_avg:224.63ms
step:1231/7050 train_time:276523ms step_avg:224.63ms
step:1232/7050 train_time:276748ms step_avg:224.63ms
step:1233/7050 train_time:276976ms step_avg:224.64ms
step:1234/7050 train_time:277200ms step_avg:224.64ms
step:1235/7050 train_time:277427ms step_avg:224.64ms
step:1236/7050 train_time:277652ms step_avg:224.64ms
step:1237/7050 train_time:277879ms step_avg:224.64ms
step:1238/7050 train_time:278104ms step_avg:224.64ms
step:1239/7050 train_time:278330ms step_avg:224.64ms
step:1240/7050 train_time:278557ms step_avg:224.64ms
step:1241/7050 train_time:278783ms step_avg:224.64ms
step:1242/7050 train_time:279009ms step_avg:224.65ms
step:1243/7050 train_time:279235ms step_avg:224.65ms
step:1244/7050 train_time:279463ms step_avg:224.65ms
step:1245/7050 train_time:279689ms step_avg:224.65ms
step:1246/7050 train_time:279913ms step_avg:224.65ms
step:1247/7050 train_time:280138ms step_avg:224.65ms
step:1248/7050 train_time:280365ms step_avg:224.65ms
step:1249/7050 train_time:280590ms step_avg:224.65ms
step:1250/7050 train_time:280818ms step_avg:224.65ms
step:1250/7050 val_loss:3.3957 train_time:280945ms step_avg:224.76ms
step:1251/7050 train_time:281043ms step_avg:224.66ms
step:1252/7050 train_time:281269ms step_avg:224.66ms
step:1253/7050 train_time:281499ms step_avg:224.66ms
step:1254/7050 train_time:281725ms step_avg:224.66ms
step:1255/7050 train_time:281951ms step_avg:224.66ms
step:1256/7050 train_time:282174ms step_avg:224.66ms
step:1257/7050 train_time:282401ms step_avg:224.66ms
step:1258/7050 train_time:282630ms step_avg:224.67ms
step:1259/7050 train_time:282853ms step_avg:224.66ms
step:1260/7050 train_time:283078ms step_avg:224.66ms
step:1261/7050 train_time:283304ms step_avg:224.67ms
step:1262/7050 train_time:283531ms step_avg:224.67ms
step:1263/7050 train_time:283757ms step_avg:224.67ms
step:1264/7050 train_time:283984ms step_avg:224.67ms
step:1265/7050 train_time:284210ms step_avg:224.67ms
step:1266/7050 train_time:284437ms step_avg:224.67ms
step:1267/7050 train_time:284662ms step_avg:224.67ms
step:1268/7050 train_time:284890ms step_avg:224.68ms
step:1269/7050 train_time:285118ms step_avg:224.68ms
step:1270/7050 train_time:285344ms step_avg:224.68ms
step:1271/7050 train_time:285571ms step_avg:224.68ms
step:1272/7050 train_time:285797ms step_avg:224.68ms
step:1273/7050 train_time:286023ms step_avg:224.68ms
step:1274/7050 train_time:286249ms step_avg:224.69ms
step:1275/7050 train_time:286476ms step_avg:224.69ms
step:1276/7050 train_time:286701ms step_avg:224.69ms
step:1277/7050 train_time:286929ms step_avg:224.69ms
step:1278/7050 train_time:287154ms step_avg:224.69ms
step:1279/7050 train_time:287380ms step_avg:224.69ms
step:1280/7050 train_time:287607ms step_avg:224.69ms
step:1281/7050 train_time:287833ms step_avg:224.69ms
step:1282/7050 train_time:288059ms step_avg:224.69ms
step:1283/7050 train_time:288285ms step_avg:224.70ms
step:1284/7050 train_time:288511ms step_avg:224.70ms
step:1285/7050 train_time:288737ms step_avg:224.70ms
step:1286/7050 train_time:288963ms step_avg:224.70ms
step:1287/7050 train_time:289189ms step_avg:224.70ms
step:1288/7050 train_time:289416ms step_avg:224.70ms
step:1289/7050 train_time:289642ms step_avg:224.70ms
step:1290/7050 train_time:289867ms step_avg:224.70ms
step:1291/7050 train_time:290092ms step_avg:224.70ms
step:1292/7050 train_time:290319ms step_avg:224.71ms
step:1293/7050 train_time:290544ms step_avg:224.71ms
step:1294/7050 train_time:290769ms step_avg:224.71ms
step:1295/7050 train_time:290994ms step_avg:224.71ms
step:1296/7050 train_time:291220ms step_avg:224.71ms
step:1297/7050 train_time:291447ms step_avg:224.71ms
step:1298/7050 train_time:291674ms step_avg:224.71ms
step:1299/7050 train_time:291900ms step_avg:224.71ms
step:1300/7050 train_time:292126ms step_avg:224.71ms
step:1301/7050 train_time:292350ms step_avg:224.71ms
step:1302/7050 train_time:292576ms step_avg:224.71ms
step:1303/7050 train_time:292805ms step_avg:224.72ms
step:1304/7050 train_time:293031ms step_avg:224.72ms
step:1305/7050 train_time:293255ms step_avg:224.72ms
step:1306/7050 train_time:293481ms step_avg:224.72ms
step:1307/7050 train_time:293706ms step_avg:224.72ms
step:1308/7050 train_time:293932ms step_avg:224.72ms
step:1309/7050 train_time:294157ms step_avg:224.72ms
step:1310/7050 train_time:294383ms step_avg:224.72ms
step:1311/7050 train_time:294609ms step_avg:224.72ms
step:1312/7050 train_time:294835ms step_avg:224.72ms
step:1313/7050 train_time:295062ms step_avg:224.72ms
step:1314/7050 train_time:295287ms step_avg:224.72ms
step:1315/7050 train_time:295512ms step_avg:224.72ms
step:1316/7050 train_time:295740ms step_avg:224.73ms
step:1317/7050 train_time:295967ms step_avg:224.73ms
step:1318/7050 train_time:296193ms step_avg:224.73ms
step:1319/7050 train_time:296419ms step_avg:224.73ms
step:1320/7050 train_time:296645ms step_avg:224.73ms
step:1321/7050 train_time:296870ms step_avg:224.73ms
step:1322/7050 train_time:297097ms step_avg:224.73ms
step:1323/7050 train_time:297322ms step_avg:224.73ms
step:1324/7050 train_time:297549ms step_avg:224.73ms
step:1325/7050 train_time:297773ms step_avg:224.73ms
step:1326/7050 train_time:298000ms step_avg:224.74ms
step:1327/7050 train_time:298226ms step_avg:224.74ms
step:1328/7050 train_time:298453ms step_avg:224.74ms
step:1329/7050 train_time:298677ms step_avg:224.74ms
step:1330/7050 train_time:298904ms step_avg:224.74ms
step:1331/7050 train_time:299130ms step_avg:224.74ms
step:1332/7050 train_time:299354ms step_avg:224.74ms
step:1333/7050 train_time:299580ms step_avg:224.74ms
step:1334/7050 train_time:299807ms step_avg:224.74ms
step:1335/7050 train_time:300032ms step_avg:224.74ms
step:1336/7050 train_time:300260ms step_avg:224.75ms
step:1337/7050 train_time:300485ms step_avg:224.75ms
step:1338/7050 train_time:300712ms step_avg:224.75ms
step:1339/7050 train_time:300938ms step_avg:224.75ms
step:1340/7050 train_time:301165ms step_avg:224.75ms
step:1341/7050 train_time:301391ms step_avg:224.75ms
step:1342/7050 train_time:301616ms step_avg:224.75ms
step:1343/7050 train_time:301842ms step_avg:224.75ms
step:1344/7050 train_time:302067ms step_avg:224.75ms
step:1345/7050 train_time:302294ms step_avg:224.75ms
step:1346/7050 train_time:302518ms step_avg:224.75ms
step:1347/7050 train_time:302744ms step_avg:224.75ms
step:1348/7050 train_time:302971ms step_avg:224.76ms
step:1349/7050 train_time:303197ms step_avg:224.76ms
step:1350/7050 train_time:303423ms step_avg:224.76ms
step:1351/7050 train_time:303648ms step_avg:224.76ms
step:1352/7050 train_time:303874ms step_avg:224.76ms
step:1353/7050 train_time:304100ms step_avg:224.76ms
step:1354/7050 train_time:304329ms step_avg:224.76ms
step:1355/7050 train_time:304553ms step_avg:224.76ms
step:1356/7050 train_time:304779ms step_avg:224.76ms
step:1357/7050 train_time:305005ms step_avg:224.76ms
step:1358/7050 train_time:305232ms step_avg:224.77ms
step:1359/7050 train_time:305458ms step_avg:224.77ms
step:1360/7050 train_time:305684ms step_avg:224.77ms
step:1361/7050 train_time:305911ms step_avg:224.77ms
step:1362/7050 train_time:306138ms step_avg:224.77ms
step:1363/7050 train_time:306363ms step_avg:224.77ms
step:1364/7050 train_time:306590ms step_avg:224.77ms
step:1365/7050 train_time:306816ms step_avg:224.77ms
step:1366/7050 train_time:307042ms step_avg:224.77ms
step:1367/7050 train_time:307269ms step_avg:224.78ms
step:1368/7050 train_time:307493ms step_avg:224.78ms
step:1369/7050 train_time:307720ms step_avg:224.78ms
step:1370/7050 train_time:307947ms step_avg:224.78ms
step:1371/7050 train_time:308171ms step_avg:224.78ms
step:1372/7050 train_time:308397ms step_avg:224.78ms
step:1373/7050 train_time:308624ms step_avg:224.78ms
step:1374/7050 train_time:308849ms step_avg:224.78ms
step:1375/7050 train_time:309075ms step_avg:224.78ms
step:1375/7050 val_loss:3.3730 train_time:309202ms step_avg:224.87ms
step:1376/7050 train_time:309307ms step_avg:224.79ms
step:1377/7050 train_time:309530ms step_avg:224.79ms
step:1378/7050 train_time:309759ms step_avg:224.79ms
step:1379/7050 train_time:309987ms step_avg:224.79ms
step:1380/7050 train_time:310211ms step_avg:224.79ms
step:1381/7050 train_time:310434ms step_avg:224.79ms
step:1382/7050 train_time:310662ms step_avg:224.79ms
step:1383/7050 train_time:310889ms step_avg:224.79ms
step:1384/7050 train_time:311115ms step_avg:224.79ms
step:1385/7050 train_time:311340ms step_avg:224.79ms
step:1386/7050 train_time:311565ms step_avg:224.79ms
step:1387/7050 train_time:311793ms step_avg:224.80ms
step:1388/7050 train_time:312017ms step_avg:224.80ms
step:1389/7050 train_time:312244ms step_avg:224.80ms
step:1390/7050 train_time:312469ms step_avg:224.80ms
step:1391/7050 train_time:312695ms step_avg:224.80ms
step:1392/7050 train_time:312922ms step_avg:224.80ms
step:1393/7050 train_time:313148ms step_avg:224.80ms
step:1394/7050 train_time:313376ms step_avg:224.80ms
step:1395/7050 train_time:313600ms step_avg:224.80ms
step:1396/7050 train_time:313825ms step_avg:224.80ms
step:1397/7050 train_time:314052ms step_avg:224.80ms
step:1398/7050 train_time:314277ms step_avg:224.80ms
step:1399/7050 train_time:314504ms step_avg:224.81ms
step:1400/7050 train_time:314730ms step_avg:224.81ms
step:1401/7050 train_time:314954ms step_avg:224.81ms
step:1402/7050 train_time:315182ms step_avg:224.81ms
step:1403/7050 train_time:315407ms step_avg:224.81ms
step:1404/7050 train_time:315633ms step_avg:224.81ms
step:1405/7050 train_time:315860ms step_avg:224.81ms
step:1406/7050 train_time:316086ms step_avg:224.81ms
step:1407/7050 train_time:316311ms step_avg:224.81ms
step:1408/7050 train_time:316536ms step_avg:224.81ms
step:1409/7050 train_time:316762ms step_avg:224.81ms
step:1410/7050 train_time:316986ms step_avg:224.81ms
step:1411/7050 train_time:317212ms step_avg:224.81ms
step:1412/7050 train_time:317437ms step_avg:224.81ms
step:1413/7050 train_time:317665ms step_avg:224.82ms
step:1414/7050 train_time:317891ms step_avg:224.82ms
step:1415/7050 train_time:318116ms step_avg:224.82ms
step:1416/7050 train_time:318341ms step_avg:224.82ms
step:1417/7050 train_time:318569ms step_avg:224.82ms
step:1418/7050 train_time:318795ms step_avg:224.82ms
step:1419/7050 train_time:319020ms step_avg:224.82ms
step:1420/7050 train_time:319244ms step_avg:224.82ms
step:1421/7050 train_time:319470ms step_avg:224.82ms
step:1422/7050 train_time:319700ms step_avg:224.82ms
step:1423/7050 train_time:319926ms step_avg:224.82ms
step:1424/7050 train_time:320151ms step_avg:224.83ms
step:1425/7050 train_time:320377ms step_avg:224.83ms
step:1426/7050 train_time:320604ms step_avg:224.83ms
step:1427/7050 train_time:320830ms step_avg:224.83ms
step:1428/7050 train_time:321056ms step_avg:224.83ms
step:1429/7050 train_time:321283ms step_avg:224.83ms
step:1430/7050 train_time:321509ms step_avg:224.83ms
step:1431/7050 train_time:321735ms step_avg:224.83ms
step:1432/7050 train_time:321961ms step_avg:224.83ms
step:1433/7050 train_time:322188ms step_avg:224.83ms
step:1434/7050 train_time:322413ms step_avg:224.83ms
step:1435/7050 train_time:322639ms step_avg:224.84ms
step:1436/7050 train_time:322865ms step_avg:224.84ms
step:1437/7050 train_time:323093ms step_avg:224.84ms
step:1438/7050 train_time:323319ms step_avg:224.84ms
step:1439/7050 train_time:323544ms step_avg:224.84ms
step:1440/7050 train_time:323770ms step_avg:224.84ms
step:1441/7050 train_time:323996ms step_avg:224.84ms
step:1442/7050 train_time:324221ms step_avg:224.84ms
step:1443/7050 train_time:324448ms step_avg:224.84ms
step:1444/7050 train_time:324674ms step_avg:224.84ms
step:1445/7050 train_time:324899ms step_avg:224.84ms
step:1446/7050 train_time:325124ms step_avg:224.84ms
step:1447/7050 train_time:325349ms step_avg:224.84ms
step:1448/7050 train_time:325576ms step_avg:224.84ms
step:1449/7050 train_time:325804ms step_avg:224.85ms
step:1450/7050 train_time:326030ms step_avg:224.85ms
step:1451/7050 train_time:326255ms step_avg:224.85ms
step:1452/7050 train_time:326482ms step_avg:224.85ms
step:1453/7050 train_time:326708ms step_avg:224.85ms
step:1454/7050 train_time:326933ms step_avg:224.85ms
step:1455/7050 train_time:327159ms step_avg:224.85ms
step:1456/7050 train_time:327386ms step_avg:224.85ms
step:1457/7050 train_time:327611ms step_avg:224.85ms
step:1458/7050 train_time:327836ms step_avg:224.85ms
step:1459/7050 train_time:328060ms step_avg:224.85ms
step:1460/7050 train_time:328286ms step_avg:224.85ms
step:1461/7050 train_time:328514ms step_avg:224.86ms
step:1462/7050 train_time:328740ms step_avg:224.86ms
step:1463/7050 train_time:328967ms step_avg:224.86ms
step:1464/7050 train_time:329193ms step_avg:224.86ms
step:1465/7050 train_time:329417ms step_avg:224.86ms
step:1466/7050 train_time:329643ms step_avg:224.86ms
step:1467/7050 train_time:329869ms step_avg:224.86ms
step:1468/7050 train_time:330095ms step_avg:224.86ms
step:1469/7050 train_time:330321ms step_avg:224.86ms
step:1470/7050 train_time:330545ms step_avg:224.86ms
step:1471/7050 train_time:330771ms step_avg:224.86ms
step:1472/7050 train_time:330996ms step_avg:224.86ms
step:1473/7050 train_time:331223ms step_avg:224.86ms
step:1474/7050 train_time:331449ms step_avg:224.86ms
step:1475/7050 train_time:331676ms step_avg:224.87ms
step:1476/7050 train_time:331902ms step_avg:224.87ms
step:1477/7050 train_time:332129ms step_avg:224.87ms
step:1478/7050 train_time:332354ms step_avg:224.87ms
step:1479/7050 train_time:332580ms step_avg:224.87ms
step:1480/7050 train_time:332806ms step_avg:224.87ms
step:1481/7050 train_time:333032ms step_avg:224.87ms
step:1482/7050 train_time:333258ms step_avg:224.87ms
step:1483/7050 train_time:333484ms step_avg:224.87ms
step:1484/7050 train_time:333711ms step_avg:224.87ms
step:1485/7050 train_time:333936ms step_avg:224.87ms
step:1486/7050 train_time:334164ms step_avg:224.87ms
step:1487/7050 train_time:334391ms step_avg:224.88ms
step:1488/7050 train_time:334618ms step_avg:224.88ms
step:1489/7050 train_time:334845ms step_avg:224.88ms
step:1490/7050 train_time:335071ms step_avg:224.88ms
step:1491/7050 train_time:335297ms step_avg:224.88ms
step:1492/7050 train_time:335522ms step_avg:224.88ms
step:1493/7050 train_time:335747ms step_avg:224.88ms
step:1494/7050 train_time:335974ms step_avg:224.88ms
step:1495/7050 train_time:336199ms step_avg:224.88ms
step:1496/7050 train_time:336425ms step_avg:224.88ms
step:1497/7050 train_time:336651ms step_avg:224.88ms
step:1498/7050 train_time:336879ms step_avg:224.89ms
step:1499/7050 train_time:337104ms step_avg:224.89ms
step:1500/7050 train_time:337330ms step_avg:224.89ms
step:1500/7050 val_loss:3.3509 train_time:337457ms step_avg:224.97ms
step:1501/7050 train_time:337556ms step_avg:224.89ms
step:1502/7050 train_time:337783ms step_avg:224.89ms
step:1503/7050 train_time:338011ms step_avg:224.89ms
step:1504/7050 train_time:338239ms step_avg:224.89ms
step:1505/7050 train_time:338463ms step_avg:224.89ms
step:1506/7050 train_time:338687ms step_avg:224.89ms
step:1507/7050 train_time:338915ms step_avg:224.89ms
step:1508/7050 train_time:339142ms step_avg:224.90ms
step:1509/7050 train_time:339367ms step_avg:224.90ms
step:1510/7050 train_time:339591ms step_avg:224.90ms
step:1511/7050 train_time:339817ms step_avg:224.90ms
step:1512/7050 train_time:340044ms step_avg:224.90ms
step:1513/7050 train_time:340271ms step_avg:224.90ms
step:1514/7050 train_time:340496ms step_avg:224.90ms
step:1515/7050 train_time:340721ms step_avg:224.90ms
step:1516/7050 train_time:340949ms step_avg:224.90ms
step:1517/7050 train_time:341176ms step_avg:224.90ms
step:1518/7050 train_time:341401ms step_avg:224.90ms
step:1519/7050 train_time:341627ms step_avg:224.90ms
step:1520/7050 train_time:341852ms step_avg:224.90ms
