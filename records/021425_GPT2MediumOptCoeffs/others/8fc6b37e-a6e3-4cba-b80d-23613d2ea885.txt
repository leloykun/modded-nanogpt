import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for a, b, c in [
        (3.9090, -4.0174, 1.1084),
        (3.7495, -3.8578, 1.1084),
        (3.5931, -3.7008, 1.1077),
        (3.4398, -3.5462, 1.1064),
        (3.2882, -3.3917, 1.1035),
        (3.1156, -3.2027, 1.0871),
        (2.7208, -2.6650, 0.9442),
    ]:
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.mul_(1 - group["lr"] * group["weight_decay"])
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=0.5, w_s=2**-9, grad_s=2**-19)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 7050 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=16, num_heads=8, model_dim=1024,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.1/1024**0.5), dict(params=embed_params, lr=0.3), dict(params=scalar_params, lr=0.015)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.025, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        return (1 - x) / args.cooldown_frac

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
for _ in range(warmup_steps):
    inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device="cuda")
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sun Feb 16 18:46:04 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   37C    P0            116W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0            121W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            117W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0            118W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0            111W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            114W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/7050 val_loss:10.8258 train_time:0ms step_avg:0.06ms
step:1/7050 train_time:163ms step_avg:162.98ms
step:2/7050 train_time:348ms step_avg:174.07ms
step:3/7050 train_time:563ms step_avg:187.61ms
step:4/7050 train_time:786ms step_avg:196.43ms
step:5/7050 train_time:1009ms step_avg:201.74ms
step:6/7050 train_time:1231ms step_avg:205.25ms
step:7/7050 train_time:1462ms step_avg:208.88ms
step:8/7050 train_time:1688ms step_avg:210.99ms
step:9/7050 train_time:1914ms step_avg:212.67ms
step:10/7050 train_time:2138ms step_avg:213.81ms
step:11/7050 train_time:2362ms step_avg:214.70ms
step:12/7050 train_time:2588ms step_avg:215.69ms
step:13/7050 train_time:2813ms step_avg:216.42ms
step:14/7050 train_time:3039ms step_avg:217.04ms
step:15/7050 train_time:3262ms step_avg:217.48ms
step:16/7050 train_time:3487ms step_avg:217.94ms
step:17/7050 train_time:3713ms step_avg:218.43ms
step:18/7050 train_time:3938ms step_avg:218.79ms
step:19/7050 train_time:4165ms step_avg:219.19ms
step:20/7050 train_time:4387ms step_avg:219.37ms
step:21/7050 train_time:4613ms step_avg:219.67ms
step:22/7050 train_time:4839ms step_avg:219.94ms
step:23/7050 train_time:5064ms step_avg:220.16ms
step:24/7050 train_time:5291ms step_avg:220.47ms
step:25/7050 train_time:5515ms step_avg:220.60ms
step:26/7050 train_time:5741ms step_avg:220.81ms
step:27/7050 train_time:5969ms step_avg:221.07ms
step:28/7050 train_time:6194ms step_avg:221.23ms
step:29/7050 train_time:6420ms step_avg:221.37ms
step:30/7050 train_time:6645ms step_avg:221.51ms
step:31/7050 train_time:6871ms step_avg:221.64ms
step:32/7050 train_time:7096ms step_avg:221.76ms
step:33/7050 train_time:7322ms step_avg:221.87ms
step:34/7050 train_time:7546ms step_avg:221.95ms
step:35/7050 train_time:7773ms step_avg:222.08ms
step:36/7050 train_time:8000ms step_avg:222.21ms
step:37/7050 train_time:8225ms step_avg:222.29ms
step:38/7050 train_time:8448ms step_avg:222.32ms
step:39/7050 train_time:8675ms step_avg:222.44ms
step:40/7050 train_time:8902ms step_avg:222.55ms
step:41/7050 train_time:9128ms step_avg:222.63ms
step:42/7050 train_time:9353ms step_avg:222.70ms
step:43/7050 train_time:9579ms step_avg:222.76ms
step:44/7050 train_time:9804ms step_avg:222.82ms
step:45/7050 train_time:10029ms step_avg:222.88ms
step:46/7050 train_time:10256ms step_avg:222.96ms
step:47/7050 train_time:10482ms step_avg:223.02ms
step:48/7050 train_time:10706ms step_avg:223.05ms
step:49/7050 train_time:10932ms step_avg:223.10ms
step:50/7050 train_time:11158ms step_avg:223.16ms
step:51/7050 train_time:11383ms step_avg:223.19ms
step:52/7050 train_time:11607ms step_avg:223.22ms
step:53/7050 train_time:11833ms step_avg:223.27ms
step:54/7050 train_time:12059ms step_avg:223.32ms
step:55/7050 train_time:12283ms step_avg:223.33ms
step:56/7050 train_time:12507ms step_avg:223.34ms
step:57/7050 train_time:12733ms step_avg:223.38ms
step:58/7050 train_time:12958ms step_avg:223.41ms
step:59/7050 train_time:13184ms step_avg:223.45ms
step:60/7050 train_time:13410ms step_avg:223.50ms
step:61/7050 train_time:13635ms step_avg:223.53ms
step:62/7050 train_time:13861ms step_avg:223.57ms
step:63/7050 train_time:14086ms step_avg:223.59ms
step:64/7050 train_time:14311ms step_avg:223.61ms
step:65/7050 train_time:14536ms step_avg:223.64ms
step:66/7050 train_time:14764ms step_avg:223.69ms
step:67/7050 train_time:14989ms step_avg:223.71ms
step:68/7050 train_time:15215ms step_avg:223.74ms
step:69/7050 train_time:15439ms step_avg:223.75ms
step:70/7050 train_time:15663ms step_avg:223.76ms
step:71/7050 train_time:15888ms step_avg:223.77ms
step:72/7050 train_time:16115ms step_avg:223.82ms
step:73/7050 train_time:16342ms step_avg:223.86ms
step:74/7050 train_time:16566ms step_avg:223.86ms
step:75/7050 train_time:16791ms step_avg:223.88ms
step:76/7050 train_time:17016ms step_avg:223.90ms
step:77/7050 train_time:17241ms step_avg:223.91ms
step:78/7050 train_time:17466ms step_avg:223.92ms
step:79/7050 train_time:17692ms step_avg:223.95ms
step:80/7050 train_time:17917ms step_avg:223.97ms
step:81/7050 train_time:18141ms step_avg:223.96ms
step:82/7050 train_time:18366ms step_avg:223.97ms
step:83/7050 train_time:18591ms step_avg:223.99ms
step:84/7050 train_time:18816ms step_avg:224.00ms
step:85/7050 train_time:19043ms step_avg:224.04ms
step:86/7050 train_time:19269ms step_avg:224.06ms
step:87/7050 train_time:19495ms step_avg:224.08ms
step:88/7050 train_time:19720ms step_avg:224.09ms
step:89/7050 train_time:19944ms step_avg:224.09ms
step:90/7050 train_time:20172ms step_avg:224.13ms
step:91/7050 train_time:20395ms step_avg:224.12ms
step:92/7050 train_time:20621ms step_avg:224.14ms
step:93/7050 train_time:20845ms step_avg:224.14ms
step:94/7050 train_time:21072ms step_avg:224.17ms
step:95/7050 train_time:21296ms step_avg:224.17ms
step:96/7050 train_time:21521ms step_avg:224.18ms
step:97/7050 train_time:21745ms step_avg:224.18ms
step:98/7050 train_time:21972ms step_avg:224.21ms
step:99/7050 train_time:22197ms step_avg:224.21ms
step:100/7050 train_time:22421ms step_avg:224.21ms
step:101/7050 train_time:22645ms step_avg:224.20ms
step:102/7050 train_time:22871ms step_avg:224.23ms
step:103/7050 train_time:23096ms step_avg:224.24ms
step:104/7050 train_time:23321ms step_avg:224.24ms
step:105/7050 train_time:23548ms step_avg:224.26ms
step:106/7050 train_time:23773ms step_avg:224.27ms
step:107/7050 train_time:23999ms step_avg:224.29ms
step:108/7050 train_time:24222ms step_avg:224.28ms
step:109/7050 train_time:24448ms step_avg:224.29ms
step:110/7050 train_time:24674ms step_avg:224.31ms
step:111/7050 train_time:24900ms step_avg:224.33ms
step:112/7050 train_time:25122ms step_avg:224.31ms
step:113/7050 train_time:25346ms step_avg:224.30ms
step:114/7050 train_time:25572ms step_avg:224.32ms
step:115/7050 train_time:25797ms step_avg:224.33ms
step:116/7050 train_time:26022ms step_avg:224.33ms
step:117/7050 train_time:26246ms step_avg:224.32ms
step:118/7050 train_time:26470ms step_avg:224.32ms
step:119/7050 train_time:26695ms step_avg:224.33ms
step:120/7050 train_time:26921ms step_avg:224.34ms
step:121/7050 train_time:27145ms step_avg:224.34ms
step:122/7050 train_time:27370ms step_avg:224.34ms
step:123/7050 train_time:27595ms step_avg:224.35ms
step:124/7050 train_time:27821ms step_avg:224.36ms
step:125/7050 train_time:28043ms step_avg:224.35ms
step:125/7050 val_loss:4.4931 train_time:28171ms step_avg:225.37ms
step:126/7050 train_time:28268ms step_avg:224.35ms
step:127/7050 train_time:28493ms step_avg:224.35ms
step:128/7050 train_time:28715ms step_avg:224.34ms
step:129/7050 train_time:28942ms step_avg:224.35ms
step:130/7050 train_time:29167ms step_avg:224.36ms
step:131/7050 train_time:29394ms step_avg:224.38ms
step:132/7050 train_time:29617ms step_avg:224.37ms
step:133/7050 train_time:29842ms step_avg:224.38ms
step:134/7050 train_time:30066ms step_avg:224.37ms
step:135/7050 train_time:30294ms step_avg:224.40ms
step:136/7050 train_time:30517ms step_avg:224.39ms
step:137/7050 train_time:30743ms step_avg:224.40ms
step:138/7050 train_time:30965ms step_avg:224.38ms
step:139/7050 train_time:31189ms step_avg:224.38ms
step:140/7050 train_time:31416ms step_avg:224.40ms
step:141/7050 train_time:31639ms step_avg:224.39ms
step:142/7050 train_time:31866ms step_avg:224.41ms
step:143/7050 train_time:32087ms step_avg:224.38ms
step:144/7050 train_time:32311ms step_avg:224.38ms
step:145/7050 train_time:32536ms step_avg:224.39ms
step:146/7050 train_time:32761ms step_avg:224.39ms
step:147/7050 train_time:32987ms step_avg:224.40ms
step:148/7050 train_time:33210ms step_avg:224.39ms
step:149/7050 train_time:33434ms step_avg:224.39ms
step:150/7050 train_time:33659ms step_avg:224.40ms
step:151/7050 train_time:33886ms step_avg:224.41ms
step:152/7050 train_time:34108ms step_avg:224.39ms
step:153/7050 train_time:34332ms step_avg:224.39ms
step:154/7050 train_time:34556ms step_avg:224.39ms
step:155/7050 train_time:34782ms step_avg:224.40ms
step:156/7050 train_time:35008ms step_avg:224.41ms
step:157/7050 train_time:35233ms step_avg:224.42ms
step:158/7050 train_time:35456ms step_avg:224.41ms
step:159/7050 train_time:35680ms step_avg:224.40ms
step:160/7050 train_time:35904ms step_avg:224.40ms
step:161/7050 train_time:36128ms step_avg:224.40ms
step:162/7050 train_time:36354ms step_avg:224.41ms
step:163/7050 train_time:36577ms step_avg:224.40ms
step:164/7050 train_time:36801ms step_avg:224.40ms
step:165/7050 train_time:37025ms step_avg:224.40ms
step:166/7050 train_time:37251ms step_avg:224.40ms
step:167/7050 train_time:37474ms step_avg:224.40ms
step:168/7050 train_time:37699ms step_avg:224.40ms
step:169/7050 train_time:37923ms step_avg:224.40ms
step:170/7050 train_time:38147ms step_avg:224.40ms
step:171/7050 train_time:38371ms step_avg:224.39ms
step:172/7050 train_time:38597ms step_avg:224.40ms
step:173/7050 train_time:38819ms step_avg:224.39ms
step:174/7050 train_time:39044ms step_avg:224.39ms
step:175/7050 train_time:39268ms step_avg:224.39ms
step:176/7050 train_time:39492ms step_avg:224.39ms
step:177/7050 train_time:39715ms step_avg:224.38ms
step:178/7050 train_time:39940ms step_avg:224.38ms
step:179/7050 train_time:40164ms step_avg:224.38ms
step:180/7050 train_time:40388ms step_avg:224.38ms
step:181/7050 train_time:40611ms step_avg:224.37ms
step:182/7050 train_time:40835ms step_avg:224.37ms
step:183/7050 train_time:41060ms step_avg:224.37ms
step:184/7050 train_time:41283ms step_avg:224.36ms
step:185/7050 train_time:41507ms step_avg:224.36ms
step:186/7050 train_time:41730ms step_avg:224.35ms
step:187/7050 train_time:41955ms step_avg:224.36ms
step:188/7050 train_time:42180ms step_avg:224.36ms
step:189/7050 train_time:42403ms step_avg:224.35ms
step:190/7050 train_time:42627ms step_avg:224.35ms
step:191/7050 train_time:42852ms step_avg:224.36ms
step:192/7050 train_time:43077ms step_avg:224.36ms
step:193/7050 train_time:43300ms step_avg:224.35ms
step:194/7050 train_time:43523ms step_avg:224.35ms
step:195/7050 train_time:43747ms step_avg:224.34ms
step:196/7050 train_time:43972ms step_avg:224.35ms
step:197/7050 train_time:44196ms step_avg:224.35ms
step:198/7050 train_time:44421ms step_avg:224.35ms
step:199/7050 train_time:44645ms step_avg:224.35ms
step:200/7050 train_time:44871ms step_avg:224.35ms
step:201/7050 train_time:45094ms step_avg:224.35ms
step:202/7050 train_time:45318ms step_avg:224.35ms
step:203/7050 train_time:45543ms step_avg:224.35ms
step:204/7050 train_time:45767ms step_avg:224.35ms
step:205/7050 train_time:45990ms step_avg:224.34ms
step:206/7050 train_time:46214ms step_avg:224.34ms
step:207/7050 train_time:46438ms step_avg:224.34ms
step:208/7050 train_time:46662ms step_avg:224.34ms
step:209/7050 train_time:46886ms step_avg:224.34ms
step:210/7050 train_time:47109ms step_avg:224.33ms
step:211/7050 train_time:47333ms step_avg:224.33ms
step:212/7050 train_time:47557ms step_avg:224.33ms
step:213/7050 train_time:47783ms step_avg:224.33ms
step:214/7050 train_time:48005ms step_avg:224.32ms
step:215/7050 train_time:48228ms step_avg:224.32ms
step:216/7050 train_time:48453ms step_avg:224.32ms
step:217/7050 train_time:48677ms step_avg:224.32ms
step:218/7050 train_time:48899ms step_avg:224.31ms
step:219/7050 train_time:49123ms step_avg:224.31ms
step:220/7050 train_time:49347ms step_avg:224.31ms
step:221/7050 train_time:49572ms step_avg:224.31ms
step:222/7050 train_time:49796ms step_avg:224.30ms
step:223/7050 train_time:50019ms step_avg:224.30ms
step:224/7050 train_time:50244ms step_avg:224.30ms
step:225/7050 train_time:50468ms step_avg:224.30ms
step:226/7050 train_time:50693ms step_avg:224.30ms
step:227/7050 train_time:50916ms step_avg:224.30ms
step:228/7050 train_time:51140ms step_avg:224.30ms
step:229/7050 train_time:51363ms step_avg:224.29ms
step:230/7050 train_time:51587ms step_avg:224.29ms
step:231/7050 train_time:51813ms step_avg:224.30ms
step:232/7050 train_time:52037ms step_avg:224.30ms
step:233/7050 train_time:52260ms step_avg:224.29ms
step:234/7050 train_time:52485ms step_avg:224.30ms
step:235/7050 train_time:52709ms step_avg:224.29ms
step:236/7050 train_time:52932ms step_avg:224.29ms
step:237/7050 train_time:53156ms step_avg:224.29ms
step:238/7050 train_time:53381ms step_avg:224.29ms
step:239/7050 train_time:53606ms step_avg:224.29ms
step:240/7050 train_time:53831ms step_avg:224.29ms
step:241/7050 train_time:54054ms step_avg:224.29ms
step:242/7050 train_time:54279ms step_avg:224.29ms
step:243/7050 train_time:54503ms step_avg:224.29ms
step:244/7050 train_time:54728ms step_avg:224.29ms
step:245/7050 train_time:54952ms step_avg:224.29ms
step:246/7050 train_time:55176ms step_avg:224.29ms
step:247/7050 train_time:55400ms step_avg:224.29ms
step:248/7050 train_time:55623ms step_avg:224.29ms
step:249/7050 train_time:55849ms step_avg:224.29ms
step:250/7050 train_time:56073ms step_avg:224.29ms
step:250/7050 val_loss:4.1799 train_time:56198ms step_avg:224.79ms
step:251/7050 train_time:56295ms step_avg:224.28ms
step:252/7050 train_time:56519ms step_avg:224.28ms
step:253/7050 train_time:56752ms step_avg:224.31ms
step:254/7050 train_time:56975ms step_avg:224.31ms
step:255/7050 train_time:57199ms step_avg:224.31ms
step:256/7050 train_time:57422ms step_avg:224.30ms
step:257/7050 train_time:57649ms step_avg:224.32ms
step:258/7050 train_time:57873ms step_avg:224.31ms
step:259/7050 train_time:58096ms step_avg:224.31ms
step:260/7050 train_time:58320ms step_avg:224.31ms
step:261/7050 train_time:58542ms step_avg:224.30ms
step:262/7050 train_time:58766ms step_avg:224.30ms
step:263/7050 train_time:58990ms step_avg:224.30ms
step:264/7050 train_time:59215ms step_avg:224.30ms
step:265/7050 train_time:59438ms step_avg:224.29ms
step:266/7050 train_time:59660ms step_avg:224.29ms
step:267/7050 train_time:59885ms step_avg:224.29ms
step:268/7050 train_time:60109ms step_avg:224.29ms
step:269/7050 train_time:60332ms step_avg:224.28ms
step:270/7050 train_time:60556ms step_avg:224.28ms
step:271/7050 train_time:60780ms step_avg:224.28ms
step:272/7050 train_time:61005ms step_avg:224.28ms
step:273/7050 train_time:61228ms step_avg:224.28ms
step:274/7050 train_time:61451ms step_avg:224.28ms
step:275/7050 train_time:61673ms step_avg:224.27ms
step:276/7050 train_time:61898ms step_avg:224.27ms
step:277/7050 train_time:62124ms step_avg:224.27ms
step:278/7050 train_time:62347ms step_avg:224.27ms
step:279/7050 train_time:62570ms step_avg:224.27ms
step:280/7050 train_time:62793ms step_avg:224.26ms
step:281/7050 train_time:63018ms step_avg:224.26ms
step:282/7050 train_time:63240ms step_avg:224.26ms
step:283/7050 train_time:63465ms step_avg:224.26ms
step:284/7050 train_time:63688ms step_avg:224.25ms
step:285/7050 train_time:63911ms step_avg:224.25ms
step:286/7050 train_time:64136ms step_avg:224.25ms
step:287/7050 train_time:64359ms step_avg:224.25ms
step:288/7050 train_time:64583ms step_avg:224.25ms
step:289/7050 train_time:64808ms step_avg:224.25ms
step:290/7050 train_time:65030ms step_avg:224.24ms
step:291/7050 train_time:65254ms step_avg:224.24ms
step:292/7050 train_time:65477ms step_avg:224.24ms
step:293/7050 train_time:65700ms step_avg:224.23ms
step:294/7050 train_time:65925ms step_avg:224.23ms
step:295/7050 train_time:66147ms step_avg:224.23ms
step:296/7050 train_time:66372ms step_avg:224.23ms
step:297/7050 train_time:66594ms step_avg:224.22ms
step:298/7050 train_time:66820ms step_avg:224.23ms
step:299/7050 train_time:67043ms step_avg:224.22ms
step:300/7050 train_time:67267ms step_avg:224.22ms
step:301/7050 train_time:67490ms step_avg:224.22ms
step:302/7050 train_time:67713ms step_avg:224.21ms
step:303/7050 train_time:67937ms step_avg:224.21ms
step:304/7050 train_time:68160ms step_avg:224.21ms
step:305/7050 train_time:68384ms step_avg:224.21ms
step:306/7050 train_time:68608ms step_avg:224.21ms
step:307/7050 train_time:68832ms step_avg:224.21ms
step:308/7050 train_time:69056ms step_avg:224.21ms
step:309/7050 train_time:69280ms step_avg:224.21ms
step:310/7050 train_time:69504ms step_avg:224.21ms
step:311/7050 train_time:69729ms step_avg:224.21ms
step:312/7050 train_time:69953ms step_avg:224.21ms
step:313/7050 train_time:70177ms step_avg:224.21ms
step:314/7050 train_time:70400ms step_avg:224.21ms
step:315/7050 train_time:70624ms step_avg:224.20ms
step:316/7050 train_time:70848ms step_avg:224.20ms
step:317/7050 train_time:71072ms step_avg:224.20ms
step:318/7050 train_time:71297ms step_avg:224.20ms
step:319/7050 train_time:71519ms step_avg:224.20ms
step:320/7050 train_time:71743ms step_avg:224.20ms
step:321/7050 train_time:71968ms step_avg:224.20ms
step:322/7050 train_time:72192ms step_avg:224.20ms
step:323/7050 train_time:72417ms step_avg:224.20ms
step:324/7050 train_time:72641ms step_avg:224.20ms
step:325/7050 train_time:72863ms step_avg:224.19ms
step:326/7050 train_time:73088ms step_avg:224.20ms
step:327/7050 train_time:73312ms step_avg:224.20ms
step:328/7050 train_time:73535ms step_avg:224.19ms
step:329/7050 train_time:73758ms step_avg:224.19ms
step:330/7050 train_time:73980ms step_avg:224.18ms
step:331/7050 train_time:74205ms step_avg:224.18ms
step:332/7050 train_time:74429ms step_avg:224.18ms
step:333/7050 train_time:74653ms step_avg:224.18ms
step:334/7050 train_time:74876ms step_avg:224.18ms
step:335/7050 train_time:75100ms step_avg:224.18ms
step:336/7050 train_time:75324ms step_avg:224.18ms
step:337/7050 train_time:75547ms step_avg:224.18ms
step:338/7050 train_time:75770ms step_avg:224.17ms
step:339/7050 train_time:75995ms step_avg:224.17ms
step:340/7050 train_time:76219ms step_avg:224.17ms
step:341/7050 train_time:76443ms step_avg:224.17ms
step:342/7050 train_time:76668ms step_avg:224.17ms
step:343/7050 train_time:76891ms step_avg:224.17ms
step:344/7050 train_time:77115ms step_avg:224.17ms
step:345/7050 train_time:77339ms step_avg:224.17ms
step:346/7050 train_time:77562ms step_avg:224.17ms
step:347/7050 train_time:77786ms step_avg:224.17ms
step:348/7050 train_time:78010ms step_avg:224.17ms
step:349/7050 train_time:78233ms step_avg:224.16ms
step:350/7050 train_time:78457ms step_avg:224.16ms
step:351/7050 train_time:78680ms step_avg:224.16ms
step:352/7050 train_time:78904ms step_avg:224.16ms
step:353/7050 train_time:79128ms step_avg:224.16ms
step:354/7050 train_time:79351ms step_avg:224.16ms
step:355/7050 train_time:79574ms step_avg:224.15ms
step:356/7050 train_time:79800ms step_avg:224.16ms
step:357/7050 train_time:80024ms step_avg:224.16ms
step:358/7050 train_time:80248ms step_avg:224.16ms
step:359/7050 train_time:80470ms step_avg:224.15ms
step:360/7050 train_time:80693ms step_avg:224.15ms
step:361/7050 train_time:80918ms step_avg:224.15ms
step:362/7050 train_time:81142ms step_avg:224.15ms
step:363/7050 train_time:81365ms step_avg:224.14ms
step:364/7050 train_time:81587ms step_avg:224.14ms
step:365/7050 train_time:81810ms step_avg:224.14ms
step:366/7050 train_time:82035ms step_avg:224.14ms
step:367/7050 train_time:82259ms step_avg:224.14ms
step:368/7050 train_time:82481ms step_avg:224.13ms
step:369/7050 train_time:82704ms step_avg:224.13ms
step:370/7050 train_time:82928ms step_avg:224.13ms
step:371/7050 train_time:83151ms step_avg:224.13ms
step:372/7050 train_time:83375ms step_avg:224.13ms
step:373/7050 train_time:83599ms step_avg:224.13ms
step:374/7050 train_time:83823ms step_avg:224.13ms
step:375/7050 train_time:84048ms step_avg:224.13ms
step:375/7050 val_loss:4.0329 train_time:84174ms step_avg:224.46ms
step:376/7050 train_time:84271ms step_avg:224.12ms
step:377/7050 train_time:84498ms step_avg:224.13ms
step:378/7050 train_time:84727ms step_avg:224.15ms
step:379/7050 train_time:84952ms step_avg:224.15ms
step:380/7050 train_time:85174ms step_avg:224.14ms
step:381/7050 train_time:85398ms step_avg:224.14ms
step:382/7050 train_time:85624ms step_avg:224.15ms
step:383/7050 train_time:85848ms step_avg:224.15ms
step:384/7050 train_time:86071ms step_avg:224.14ms
step:385/7050 train_time:86293ms step_avg:224.14ms
step:386/7050 train_time:86517ms step_avg:224.14ms
step:387/7050 train_time:86742ms step_avg:224.14ms
step:388/7050 train_time:86967ms step_avg:224.14ms
step:389/7050 train_time:87188ms step_avg:224.13ms
step:390/7050 train_time:87413ms step_avg:224.13ms
step:391/7050 train_time:87635ms step_avg:224.13ms
step:392/7050 train_time:87860ms step_avg:224.13ms
step:393/7050 train_time:88085ms step_avg:224.13ms
step:394/7050 train_time:88307ms step_avg:224.13ms
step:395/7050 train_time:88529ms step_avg:224.12ms
step:396/7050 train_time:88754ms step_avg:224.13ms
step:397/7050 train_time:88978ms step_avg:224.13ms
step:398/7050 train_time:89202ms step_avg:224.13ms
step:399/7050 train_time:89425ms step_avg:224.12ms
step:400/7050 train_time:89648ms step_avg:224.12ms
step:401/7050 train_time:89873ms step_avg:224.12ms
step:402/7050 train_time:90096ms step_avg:224.12ms
step:403/7050 train_time:90319ms step_avg:224.12ms
step:404/7050 train_time:90543ms step_avg:224.12ms
step:405/7050 train_time:90766ms step_avg:224.11ms
step:406/7050 train_time:90990ms step_avg:224.11ms
step:407/7050 train_time:91212ms step_avg:224.11ms
step:408/7050 train_time:91436ms step_avg:224.11ms
step:409/7050 train_time:91661ms step_avg:224.11ms
step:410/7050 train_time:91884ms step_avg:224.11ms
step:411/7050 train_time:92108ms step_avg:224.11ms
step:412/7050 train_time:92330ms step_avg:224.10ms
step:413/7050 train_time:92555ms step_avg:224.10ms
step:414/7050 train_time:92779ms step_avg:224.10ms
step:415/7050 train_time:93003ms step_avg:224.10ms
step:416/7050 train_time:93225ms step_avg:224.10ms
step:417/7050 train_time:93449ms step_avg:224.10ms
step:418/7050 train_time:93675ms step_avg:224.10ms
step:419/7050 train_time:93898ms step_avg:224.10ms
step:420/7050 train_time:94123ms step_avg:224.10ms
step:421/7050 train_time:94346ms step_avg:224.10ms
step:422/7050 train_time:94570ms step_avg:224.10ms
step:423/7050 train_time:94794ms step_avg:224.10ms
step:424/7050 train_time:95015ms step_avg:224.09ms
step:425/7050 train_time:95239ms step_avg:224.09ms
step:426/7050 train_time:95464ms step_avg:224.09ms
step:427/7050 train_time:95687ms step_avg:224.09ms
step:428/7050 train_time:95910ms step_avg:224.09ms
step:429/7050 train_time:96133ms step_avg:224.09ms
step:430/7050 train_time:96356ms step_avg:224.08ms
step:431/7050 train_time:96579ms step_avg:224.08ms
step:432/7050 train_time:96803ms step_avg:224.08ms
step:433/7050 train_time:97027ms step_avg:224.08ms
step:434/7050 train_time:97250ms step_avg:224.08ms
step:435/7050 train_time:97475ms step_avg:224.08ms
step:436/7050 train_time:97697ms step_avg:224.08ms
step:437/7050 train_time:97922ms step_avg:224.08ms
step:438/7050 train_time:98144ms step_avg:224.07ms
step:439/7050 train_time:98368ms step_avg:224.07ms
step:440/7050 train_time:98592ms step_avg:224.07ms
step:441/7050 train_time:98815ms step_avg:224.07ms
step:442/7050 train_time:99039ms step_avg:224.07ms
step:443/7050 train_time:99264ms step_avg:224.07ms
step:444/7050 train_time:99487ms step_avg:224.07ms
step:445/7050 train_time:99709ms step_avg:224.06ms
step:446/7050 train_time:99932ms step_avg:224.06ms
step:447/7050 train_time:100156ms step_avg:224.06ms
step:448/7050 train_time:100380ms step_avg:224.06ms
step:449/7050 train_time:100603ms step_avg:224.06ms
step:450/7050 train_time:100825ms step_avg:224.05ms
step:451/7050 train_time:101048ms step_avg:224.05ms
step:452/7050 train_time:101272ms step_avg:224.05ms
step:453/7050 train_time:101496ms step_avg:224.05ms
step:454/7050 train_time:101720ms step_avg:224.05ms
step:455/7050 train_time:101943ms step_avg:224.05ms
step:456/7050 train_time:102166ms step_avg:224.05ms
step:457/7050 train_time:102389ms step_avg:224.05ms
step:458/7050 train_time:102613ms step_avg:224.05ms
step:459/7050 train_time:102836ms step_avg:224.04ms
step:460/7050 train_time:103060ms step_avg:224.04ms
step:461/7050 train_time:103284ms step_avg:224.04ms
step:462/7050 train_time:103507ms step_avg:224.04ms
step:463/7050 train_time:103731ms step_avg:224.04ms
step:464/7050 train_time:103955ms step_avg:224.04ms
step:465/7050 train_time:104178ms step_avg:224.04ms
step:466/7050 train_time:104402ms step_avg:224.04ms
step:467/7050 train_time:104625ms step_avg:224.04ms
step:468/7050 train_time:104848ms step_avg:224.03ms
step:469/7050 train_time:105072ms step_avg:224.03ms
step:470/7050 train_time:105296ms step_avg:224.03ms
step:471/7050 train_time:105519ms step_avg:224.03ms
step:472/7050 train_time:105744ms step_avg:224.03ms
step:473/7050 train_time:105969ms step_avg:224.03ms
step:474/7050 train_time:106192ms step_avg:224.03ms
step:475/7050 train_time:106416ms step_avg:224.03ms
step:476/7050 train_time:106639ms step_avg:224.03ms
step:477/7050 train_time:106864ms step_avg:224.03ms
step:478/7050 train_time:107088ms step_avg:224.03ms
step:479/7050 train_time:107310ms step_avg:224.03ms
step:480/7050 train_time:107534ms step_avg:224.03ms
step:481/7050 train_time:107759ms step_avg:224.03ms
step:482/7050 train_time:107984ms step_avg:224.03ms
step:483/7050 train_time:108206ms step_avg:224.03ms
step:484/7050 train_time:108430ms step_avg:224.03ms
step:485/7050 train_time:108655ms step_avg:224.03ms
step:486/7050 train_time:108879ms step_avg:224.03ms
step:487/7050 train_time:109103ms step_avg:224.03ms
step:488/7050 train_time:109325ms step_avg:224.03ms
step:489/7050 train_time:109549ms step_avg:224.03ms
step:490/7050 train_time:109772ms step_avg:224.03ms
step:491/7050 train_time:109995ms step_avg:224.02ms
step:492/7050 train_time:110219ms step_avg:224.02ms
step:493/7050 train_time:110443ms step_avg:224.02ms
step:494/7050 train_time:110666ms step_avg:224.02ms
step:495/7050 train_time:110892ms step_avg:224.02ms
step:496/7050 train_time:111114ms step_avg:224.02ms
step:497/7050 train_time:111337ms step_avg:224.02ms
step:498/7050 train_time:111563ms step_avg:224.02ms
step:499/7050 train_time:111785ms step_avg:224.02ms
step:500/7050 train_time:112008ms step_avg:224.02ms
step:500/7050 val_loss:3.9399 train_time:112134ms step_avg:224.27ms
step:501/7050 train_time:112232ms step_avg:224.02ms
step:502/7050 train_time:112454ms step_avg:224.01ms
step:503/7050 train_time:112683ms step_avg:224.02ms
step:504/7050 train_time:112908ms step_avg:224.02ms
step:505/7050 train_time:113131ms step_avg:224.02ms
step:506/7050 train_time:113352ms step_avg:224.02ms
step:507/7050 train_time:113575ms step_avg:224.01ms
step:508/7050 train_time:113800ms step_avg:224.02ms
step:509/7050 train_time:114023ms step_avg:224.01ms
step:510/7050 train_time:114246ms step_avg:224.01ms
step:511/7050 train_time:114467ms step_avg:224.01ms
step:512/7050 train_time:114693ms step_avg:224.01ms
step:513/7050 train_time:114917ms step_avg:224.01ms
step:514/7050 train_time:115142ms step_avg:224.01ms
step:515/7050 train_time:115363ms step_avg:224.01ms
step:516/7050 train_time:115586ms step_avg:224.00ms
step:517/7050 train_time:115810ms step_avg:224.00ms
step:518/7050 train_time:116035ms step_avg:224.01ms
step:519/7050 train_time:116258ms step_avg:224.00ms
step:520/7050 train_time:116480ms step_avg:224.00ms
step:521/7050 train_time:116704ms step_avg:224.00ms
step:522/7050 train_time:116927ms step_avg:224.00ms
step:523/7050 train_time:117151ms step_avg:224.00ms
step:524/7050 train_time:117374ms step_avg:224.00ms
step:525/7050 train_time:117599ms step_avg:224.00ms
step:526/7050 train_time:117825ms step_avg:224.00ms
step:527/7050 train_time:118049ms step_avg:224.00ms
step:528/7050 train_time:118274ms step_avg:224.00ms
step:529/7050 train_time:118499ms step_avg:224.01ms
step:530/7050 train_time:118724ms step_avg:224.01ms
step:531/7050 train_time:118949ms step_avg:224.01ms
step:532/7050 train_time:119174ms step_avg:224.01ms
step:533/7050 train_time:119399ms step_avg:224.01ms
step:534/7050 train_time:119624ms step_avg:224.02ms
step:535/7050 train_time:119848ms step_avg:224.02ms
step:536/7050 train_time:120073ms step_avg:224.02ms
step:537/7050 train_time:120297ms step_avg:224.02ms
step:538/7050 train_time:120521ms step_avg:224.02ms
step:539/7050 train_time:120746ms step_avg:224.02ms
step:540/7050 train_time:120970ms step_avg:224.02ms
step:541/7050 train_time:121195ms step_avg:224.02ms
step:542/7050 train_time:121421ms step_avg:224.02ms
step:543/7050 train_time:121646ms step_avg:224.03ms
step:544/7050 train_time:121870ms step_avg:224.03ms
step:545/7050 train_time:122095ms step_avg:224.03ms
step:546/7050 train_time:122320ms step_avg:224.03ms
step:547/7050 train_time:122547ms step_avg:224.03ms
step:548/7050 train_time:122770ms step_avg:224.03ms
step:549/7050 train_time:122995ms step_avg:224.03ms
step:550/7050 train_time:123220ms step_avg:224.04ms
step:551/7050 train_time:123447ms step_avg:224.04ms
step:552/7050 train_time:123671ms step_avg:224.04ms
step:553/7050 train_time:123895ms step_avg:224.04ms
step:554/7050 train_time:124121ms step_avg:224.05ms
step:555/7050 train_time:124347ms step_avg:224.05ms
step:556/7050 train_time:124572ms step_avg:224.05ms
step:557/7050 train_time:124796ms step_avg:224.05ms
step:558/7050 train_time:125020ms step_avg:224.05ms
step:559/7050 train_time:125246ms step_avg:224.05ms
step:560/7050 train_time:125471ms step_avg:224.06ms
step:561/7050 train_time:125696ms step_avg:224.06ms
step:562/7050 train_time:125922ms step_avg:224.06ms
step:563/7050 train_time:126146ms step_avg:224.06ms
step:564/7050 train_time:126372ms step_avg:224.06ms
step:565/7050 train_time:126595ms step_avg:224.06ms
step:566/7050 train_time:126821ms step_avg:224.06ms
step:567/7050 train_time:127047ms step_avg:224.07ms
step:568/7050 train_time:127271ms step_avg:224.07ms
step:569/7050 train_time:127496ms step_avg:224.07ms
step:570/7050 train_time:127721ms step_avg:224.07ms
step:571/7050 train_time:127948ms step_avg:224.08ms
step:572/7050 train_time:128172ms step_avg:224.08ms
step:573/7050 train_time:128397ms step_avg:224.08ms
step:574/7050 train_time:128621ms step_avg:224.08ms
step:575/7050 train_time:128845ms step_avg:224.08ms
step:576/7050 train_time:129070ms step_avg:224.08ms
step:577/7050 train_time:129295ms step_avg:224.08ms
step:578/7050 train_time:129520ms step_avg:224.08ms
step:579/7050 train_time:129747ms step_avg:224.09ms
step:580/7050 train_time:129970ms step_avg:224.09ms
step:581/7050 train_time:130195ms step_avg:224.09ms
step:582/7050 train_time:130420ms step_avg:224.09ms
step:583/7050 train_time:130646ms step_avg:224.09ms
step:584/7050 train_time:130869ms step_avg:224.09ms
step:585/7050 train_time:131094ms step_avg:224.09ms
step:586/7050 train_time:131321ms step_avg:224.10ms
step:587/7050 train_time:131546ms step_avg:224.10ms
step:588/7050 train_time:131770ms step_avg:224.10ms
step:589/7050 train_time:131994ms step_avg:224.10ms
step:590/7050 train_time:132219ms step_avg:224.10ms
step:591/7050 train_time:132445ms step_avg:224.10ms
step:592/7050 train_time:132671ms step_avg:224.11ms
step:593/7050 train_time:132896ms step_avg:224.11ms
step:594/7050 train_time:133121ms step_avg:224.11ms
step:595/7050 train_time:133347ms step_avg:224.11ms
step:596/7050 train_time:133572ms step_avg:224.11ms
step:597/7050 train_time:133795ms step_avg:224.11ms
step:598/7050 train_time:134021ms step_avg:224.11ms
step:599/7050 train_time:134247ms step_avg:224.12ms
step:600/7050 train_time:134471ms step_avg:224.12ms
step:601/7050 train_time:134695ms step_avg:224.12ms
step:602/7050 train_time:134920ms step_avg:224.12ms
step:603/7050 train_time:135146ms step_avg:224.12ms
step:604/7050 train_time:135370ms step_avg:224.12ms
step:605/7050 train_time:135595ms step_avg:224.12ms
step:606/7050 train_time:135820ms step_avg:224.13ms
step:607/7050 train_time:136047ms step_avg:224.13ms
step:608/7050 train_time:136270ms step_avg:224.13ms
step:609/7050 train_time:136495ms step_avg:224.13ms
step:610/7050 train_time:136719ms step_avg:224.13ms
step:611/7050 train_time:136944ms step_avg:224.13ms
step:612/7050 train_time:137169ms step_avg:224.13ms
step:613/7050 train_time:137394ms step_avg:224.13ms
step:614/7050 train_time:137620ms step_avg:224.14ms
step:615/7050 train_time:137845ms step_avg:224.14ms
step:616/7050 train_time:138070ms step_avg:224.14ms
step:617/7050 train_time:138295ms step_avg:224.14ms
step:618/7050 train_time:138519ms step_avg:224.14ms
step:619/7050 train_time:138745ms step_avg:224.14ms
step:620/7050 train_time:138970ms step_avg:224.15ms
step:621/7050 train_time:139195ms step_avg:224.15ms
step:622/7050 train_time:139420ms step_avg:224.15ms
step:623/7050 train_time:139646ms step_avg:224.15ms
step:624/7050 train_time:139870ms step_avg:224.15ms
step:625/7050 train_time:140095ms step_avg:224.15ms
step:625/7050 val_loss:3.6553 train_time:140223ms step_avg:224.36ms
step:626/7050 train_time:140322ms step_avg:224.16ms
step:627/7050 train_time:140544ms step_avg:224.15ms
step:628/7050 train_time:140773ms step_avg:224.16ms
step:629/7050 train_time:140999ms step_avg:224.16ms
step:630/7050 train_time:141222ms step_avg:224.16ms
step:631/7050 train_time:141445ms step_avg:224.16ms
step:632/7050 train_time:141671ms step_avg:224.16ms
step:633/7050 train_time:141897ms step_avg:224.17ms
step:634/7050 train_time:142121ms step_avg:224.17ms
step:635/7050 train_time:142345ms step_avg:224.17ms
step:636/7050 train_time:142570ms step_avg:224.17ms
step:637/7050 train_time:142794ms step_avg:224.17ms
step:638/7050 train_time:143023ms step_avg:224.17ms
step:639/7050 train_time:143248ms step_avg:224.18ms
step:640/7050 train_time:143474ms step_avg:224.18ms
step:641/7050 train_time:143698ms step_avg:224.18ms
step:642/7050 train_time:143923ms step_avg:224.18ms
step:643/7050 train_time:144149ms step_avg:224.18ms
step:644/7050 train_time:144375ms step_avg:224.18ms
step:645/7050 train_time:144599ms step_avg:224.18ms
step:646/7050 train_time:144823ms step_avg:224.18ms
step:647/7050 train_time:145049ms step_avg:224.19ms
step:648/7050 train_time:145274ms step_avg:224.19ms
step:649/7050 train_time:145498ms step_avg:224.19ms
step:650/7050 train_time:145723ms step_avg:224.19ms
step:651/7050 train_time:145948ms step_avg:224.19ms
step:652/7050 train_time:146175ms step_avg:224.19ms
step:653/7050 train_time:146400ms step_avg:224.20ms
step:654/7050 train_time:146624ms step_avg:224.20ms
step:655/7050 train_time:146849ms step_avg:224.20ms
step:656/7050 train_time:147074ms step_avg:224.20ms
step:657/7050 train_time:147300ms step_avg:224.20ms
step:658/7050 train_time:147523ms step_avg:224.20ms
step:659/7050 train_time:147748ms step_avg:224.20ms
step:660/7050 train_time:147974ms step_avg:224.20ms
step:661/7050 train_time:148198ms step_avg:224.20ms
step:662/7050 train_time:148423ms step_avg:224.20ms
step:663/7050 train_time:148648ms step_avg:224.20ms
step:664/7050 train_time:148873ms step_avg:224.21ms
step:665/7050 train_time:149097ms step_avg:224.21ms
step:666/7050 train_time:149322ms step_avg:224.21ms
step:667/7050 train_time:149546ms step_avg:224.21ms
step:668/7050 train_time:149773ms step_avg:224.21ms
step:669/7050 train_time:149997ms step_avg:224.21ms
step:670/7050 train_time:150222ms step_avg:224.21ms
step:671/7050 train_time:150448ms step_avg:224.21ms
step:672/7050 train_time:150672ms step_avg:224.21ms
step:673/7050 train_time:150896ms step_avg:224.21ms
step:674/7050 train_time:151122ms step_avg:224.22ms
step:675/7050 train_time:151346ms step_avg:224.22ms
step:676/7050 train_time:151572ms step_avg:224.22ms
step:677/7050 train_time:151796ms step_avg:224.22ms
step:678/7050 train_time:152022ms step_avg:224.22ms
step:679/7050 train_time:152245ms step_avg:224.22ms
step:680/7050 train_time:152471ms step_avg:224.22ms
step:681/7050 train_time:152694ms step_avg:224.22ms
step:682/7050 train_time:152919ms step_avg:224.22ms
step:683/7050 train_time:153144ms step_avg:224.22ms
step:684/7050 train_time:153370ms step_avg:224.23ms
step:685/7050 train_time:153596ms step_avg:224.23ms
step:686/7050 train_time:153821ms step_avg:224.23ms
step:687/7050 train_time:154047ms step_avg:224.23ms
step:688/7050 train_time:154272ms step_avg:224.23ms
step:689/7050 train_time:154497ms step_avg:224.23ms
step:690/7050 train_time:154721ms step_avg:224.23ms
step:691/7050 train_time:154948ms step_avg:224.24ms
step:692/7050 train_time:155172ms step_avg:224.24ms
step:693/7050 train_time:155396ms step_avg:224.24ms
step:694/7050 train_time:155621ms step_avg:224.24ms
step:695/7050 train_time:155845ms step_avg:224.24ms
step:696/7050 train_time:156073ms step_avg:224.24ms
step:697/7050 train_time:156297ms step_avg:224.24ms
step:698/7050 train_time:156520ms step_avg:224.24ms
step:699/7050 train_time:156744ms step_avg:224.24ms
step:700/7050 train_time:156969ms step_avg:224.24ms
step:701/7050 train_time:157196ms step_avg:224.24ms
step:702/7050 train_time:157419ms step_avg:224.24ms
step:703/7050 train_time:157644ms step_avg:224.24ms
step:704/7050 train_time:157867ms step_avg:224.24ms
step:705/7050 train_time:158093ms step_avg:224.25ms
step:706/7050 train_time:158318ms step_avg:224.25ms
step:707/7050 train_time:158543ms step_avg:224.25ms
step:708/7050 train_time:158767ms step_avg:224.25ms
step:709/7050 train_time:158993ms step_avg:224.25ms
step:710/7050 train_time:159218ms step_avg:224.25ms
step:711/7050 train_time:159444ms step_avg:224.25ms
step:712/7050 train_time:159669ms step_avg:224.25ms
step:713/7050 train_time:159895ms step_avg:224.26ms
step:714/7050 train_time:160119ms step_avg:224.26ms
step:715/7050 train_time:160344ms step_avg:224.26ms
step:716/7050 train_time:160569ms step_avg:224.26ms
step:717/7050 train_time:160795ms step_avg:224.26ms
step:718/7050 train_time:161018ms step_avg:224.26ms
step:719/7050 train_time:161244ms step_avg:224.26ms
step:720/7050 train_time:161469ms step_avg:224.26ms
step:721/7050 train_time:161694ms step_avg:224.26ms
step:722/7050 train_time:161919ms step_avg:224.26ms
step:723/7050 train_time:162142ms step_avg:224.26ms
step:724/7050 train_time:162369ms step_avg:224.27ms
step:725/7050 train_time:162593ms step_avg:224.27ms
step:726/7050 train_time:162817ms step_avg:224.27ms
step:727/7050 train_time:163043ms step_avg:224.27ms
step:728/7050 train_time:163268ms step_avg:224.27ms
step:729/7050 train_time:163493ms step_avg:224.27ms
step:730/7050 train_time:163717ms step_avg:224.27ms
step:731/7050 train_time:163941ms step_avg:224.27ms
step:732/7050 train_time:164166ms step_avg:224.27ms
step:733/7050 train_time:164392ms step_avg:224.27ms
step:734/7050 train_time:164615ms step_avg:224.27ms
step:735/7050 train_time:164840ms step_avg:224.27ms
step:736/7050 train_time:165064ms step_avg:224.27ms
step:737/7050 train_time:165289ms step_avg:224.27ms
step:738/7050 train_time:165514ms step_avg:224.27ms
step:739/7050 train_time:165740ms step_avg:224.28ms
step:740/7050 train_time:165965ms step_avg:224.28ms
step:741/7050 train_time:166188ms step_avg:224.28ms
step:742/7050 train_time:166413ms step_avg:224.28ms
step:743/7050 train_time:166636ms step_avg:224.27ms
step:744/7050 train_time:166861ms step_avg:224.28ms
step:745/7050 train_time:167086ms step_avg:224.28ms
step:746/7050 train_time:167311ms step_avg:224.28ms
step:747/7050 train_time:167534ms step_avg:224.28ms
step:748/7050 train_time:167761ms step_avg:224.28ms
step:749/7050 train_time:167984ms step_avg:224.28ms
step:750/7050 train_time:168208ms step_avg:224.28ms
step:750/7050 val_loss:3.5858 train_time:168334ms step_avg:224.45ms
step:751/7050 train_time:168432ms step_avg:224.28ms
step:752/7050 train_time:168657ms step_avg:224.28ms
step:753/7050 train_time:168890ms step_avg:224.29ms
step:754/7050 train_time:169117ms step_avg:224.29ms
step:755/7050 train_time:169341ms step_avg:224.29ms
step:756/7050 train_time:169565ms step_avg:224.29ms
step:757/7050 train_time:169793ms step_avg:224.30ms
step:758/7050 train_time:170018ms step_avg:224.30ms
step:759/7050 train_time:170243ms step_avg:224.30ms
step:760/7050 train_time:170466ms step_avg:224.30ms
step:761/7050 train_time:170691ms step_avg:224.30ms
step:762/7050 train_time:170917ms step_avg:224.30ms
step:763/7050 train_time:171142ms step_avg:224.30ms
step:764/7050 train_time:171368ms step_avg:224.30ms
step:765/7050 train_time:171594ms step_avg:224.31ms
step:766/7050 train_time:171820ms step_avg:224.31ms
step:767/7050 train_time:172044ms step_avg:224.31ms
step:768/7050 train_time:172269ms step_avg:224.31ms
step:769/7050 train_time:172495ms step_avg:224.31ms
step:770/7050 train_time:172719ms step_avg:224.31ms
step:771/7050 train_time:172945ms step_avg:224.31ms
step:772/7050 train_time:173170ms step_avg:224.31ms
step:773/7050 train_time:173394ms step_avg:224.31ms
step:774/7050 train_time:173620ms step_avg:224.32ms
step:775/7050 train_time:173845ms step_avg:224.32ms
step:776/7050 train_time:174070ms step_avg:224.32ms
step:777/7050 train_time:174294ms step_avg:224.32ms
step:778/7050 train_time:174518ms step_avg:224.32ms
step:779/7050 train_time:174745ms step_avg:224.32ms
step:780/7050 train_time:174969ms step_avg:224.32ms
step:781/7050 train_time:175196ms step_avg:224.32ms
step:782/7050 train_time:175420ms step_avg:224.32ms
step:783/7050 train_time:175646ms step_avg:224.32ms
step:784/7050 train_time:175869ms step_avg:224.32ms
step:785/7050 train_time:176095ms step_avg:224.32ms
step:786/7050 train_time:176320ms step_avg:224.33ms
step:787/7050 train_time:176546ms step_avg:224.33ms
step:788/7050 train_time:176770ms step_avg:224.33ms
step:789/7050 train_time:176994ms step_avg:224.33ms
step:790/7050 train_time:177219ms step_avg:224.33ms
step:791/7050 train_time:177446ms step_avg:224.33ms
step:792/7050 train_time:177671ms step_avg:224.33ms
step:793/7050 train_time:177894ms step_avg:224.33ms
step:794/7050 train_time:178122ms step_avg:224.33ms
step:795/7050 train_time:178344ms step_avg:224.33ms
step:796/7050 train_time:178569ms step_avg:224.33ms
step:797/7050 train_time:178794ms step_avg:224.33ms
step:798/7050 train_time:179018ms step_avg:224.33ms
step:799/7050 train_time:179241ms step_avg:224.33ms
step:800/7050 train_time:179466ms step_avg:224.33ms
step:801/7050 train_time:179693ms step_avg:224.34ms
step:802/7050 train_time:179919ms step_avg:224.34ms
step:803/7050 train_time:180144ms step_avg:224.34ms
step:804/7050 train_time:180369ms step_avg:224.34ms
step:805/7050 train_time:180593ms step_avg:224.34ms
step:806/7050 train_time:180819ms step_avg:224.34ms
step:807/7050 train_time:181045ms step_avg:224.34ms
step:808/7050 train_time:181270ms step_avg:224.34ms
step:809/7050 train_time:181494ms step_avg:224.34ms
step:810/7050 train_time:181719ms step_avg:224.34ms
step:811/7050 train_time:181946ms step_avg:224.35ms
step:812/7050 train_time:182169ms step_avg:224.35ms
step:813/7050 train_time:182393ms step_avg:224.35ms
step:814/7050 train_time:182618ms step_avg:224.35ms
step:815/7050 train_time:182844ms step_avg:224.35ms
step:816/7050 train_time:183068ms step_avg:224.35ms
step:817/7050 train_time:183295ms step_avg:224.35ms
step:818/7050 train_time:183519ms step_avg:224.35ms
step:819/7050 train_time:183744ms step_avg:224.35ms
step:820/7050 train_time:183969ms step_avg:224.35ms
step:821/7050 train_time:184194ms step_avg:224.35ms
step:822/7050 train_time:184421ms step_avg:224.36ms
step:823/7050 train_time:184644ms step_avg:224.35ms
step:824/7050 train_time:184869ms step_avg:224.36ms
step:825/7050 train_time:185094ms step_avg:224.36ms
step:826/7050 train_time:185319ms step_avg:224.36ms
step:827/7050 train_time:185544ms step_avg:224.36ms
step:828/7050 train_time:185767ms step_avg:224.36ms
step:829/7050 train_time:185993ms step_avg:224.36ms
step:830/7050 train_time:186218ms step_avg:224.36ms
step:831/7050 train_time:186445ms step_avg:224.36ms
step:832/7050 train_time:186669ms step_avg:224.36ms
step:833/7050 train_time:186895ms step_avg:224.36ms
step:834/7050 train_time:187122ms step_avg:224.37ms
step:835/7050 train_time:187348ms step_avg:224.37ms
step:836/7050 train_time:187573ms step_avg:224.37ms
step:837/7050 train_time:187800ms step_avg:224.37ms
step:838/7050 train_time:188026ms step_avg:224.37ms
step:839/7050 train_time:188251ms step_avg:224.38ms
step:840/7050 train_time:188476ms step_avg:224.38ms
step:841/7050 train_time:188700ms step_avg:224.38ms
step:842/7050 train_time:188924ms step_avg:224.38ms
step:843/7050 train_time:189149ms step_avg:224.38ms
step:844/7050 train_time:189375ms step_avg:224.38ms
step:845/7050 train_time:189600ms step_avg:224.38ms
step:846/7050 train_time:189825ms step_avg:224.38ms
step:847/7050 train_time:190049ms step_avg:224.38ms
step:848/7050 train_time:190274ms step_avg:224.38ms
step:849/7050 train_time:190499ms step_avg:224.38ms
step:850/7050 train_time:190724ms step_avg:224.38ms
step:851/7050 train_time:190949ms step_avg:224.38ms
step:852/7050 train_time:191175ms step_avg:224.38ms
step:853/7050 train_time:191400ms step_avg:224.38ms
step:854/7050 train_time:191625ms step_avg:224.39ms
step:855/7050 train_time:191850ms step_avg:224.39ms
step:856/7050 train_time:192076ms step_avg:224.39ms
step:857/7050 train_time:192299ms step_avg:224.39ms
step:858/7050 train_time:192525ms step_avg:224.39ms
step:859/7050 train_time:192750ms step_avg:224.39ms
step:860/7050 train_time:192976ms step_avg:224.39ms
step:861/7050 train_time:193202ms step_avg:224.39ms
step:862/7050 train_time:193426ms step_avg:224.39ms
step:863/7050 train_time:193650ms step_avg:224.39ms
step:864/7050 train_time:193875ms step_avg:224.39ms
step:865/7050 train_time:194100ms step_avg:224.39ms
step:866/7050 train_time:194325ms step_avg:224.39ms
step:867/7050 train_time:194550ms step_avg:224.40ms
step:868/7050 train_time:194777ms step_avg:224.40ms
step:869/7050 train_time:195002ms step_avg:224.40ms
step:870/7050 train_time:195229ms step_avg:224.40ms
step:871/7050 train_time:195454ms step_avg:224.40ms
step:872/7050 train_time:195679ms step_avg:224.40ms
step:873/7050 train_time:195905ms step_avg:224.40ms
step:874/7050 train_time:196129ms step_avg:224.40ms
step:875/7050 train_time:196355ms step_avg:224.41ms
step:875/7050 val_loss:3.5330 train_time:196481ms step_avg:224.55ms
step:876/7050 train_time:196580ms step_avg:224.41ms
step:877/7050 train_time:196803ms step_avg:224.41ms
step:878/7050 train_time:197034ms step_avg:224.41ms
step:879/7050 train_time:197260ms step_avg:224.41ms
step:880/7050 train_time:197483ms step_avg:224.41ms
step:881/7050 train_time:197706ms step_avg:224.41ms
step:882/7050 train_time:197931ms step_avg:224.41ms
step:883/7050 train_time:198159ms step_avg:224.42ms
step:884/7050 train_time:198383ms step_avg:224.42ms
step:885/7050 train_time:198606ms step_avg:224.41ms
step:886/7050 train_time:198829ms step_avg:224.41ms
step:887/7050 train_time:199054ms step_avg:224.41ms
step:888/7050 train_time:199280ms step_avg:224.41ms
step:889/7050 train_time:199504ms step_avg:224.41ms
step:890/7050 train_time:199727ms step_avg:224.41ms
step:891/7050 train_time:199951ms step_avg:224.41ms
step:892/7050 train_time:200178ms step_avg:224.41ms
step:893/7050 train_time:200403ms step_avg:224.42ms
step:894/7050 train_time:200628ms step_avg:224.42ms
step:895/7050 train_time:200852ms step_avg:224.42ms
step:896/7050 train_time:201079ms step_avg:224.42ms
step:897/7050 train_time:201304ms step_avg:224.42ms
step:898/7050 train_time:201528ms step_avg:224.42ms
step:899/7050 train_time:201752ms step_avg:224.42ms
step:900/7050 train_time:201979ms step_avg:224.42ms
step:901/7050 train_time:202203ms step_avg:224.42ms
step:902/7050 train_time:202428ms step_avg:224.42ms
step:903/7050 train_time:202653ms step_avg:224.42ms
step:904/7050 train_time:202879ms step_avg:224.42ms
step:905/7050 train_time:203104ms step_avg:224.42ms
step:906/7050 train_time:203328ms step_avg:224.42ms
step:907/7050 train_time:203553ms step_avg:224.42ms
step:908/7050 train_time:203779ms step_avg:224.43ms
step:909/7050 train_time:204003ms step_avg:224.43ms
step:910/7050 train_time:204228ms step_avg:224.43ms
step:911/7050 train_time:204454ms step_avg:224.43ms
step:912/7050 train_time:204679ms step_avg:224.43ms
step:913/7050 train_time:204904ms step_avg:224.43ms
step:914/7050 train_time:205128ms step_avg:224.43ms
step:915/7050 train_time:205353ms step_avg:224.43ms
step:916/7050 train_time:205579ms step_avg:224.43ms
step:917/7050 train_time:205803ms step_avg:224.43ms
step:918/7050 train_time:206027ms step_avg:224.43ms
step:919/7050 train_time:206252ms step_avg:224.43ms
step:920/7050 train_time:206478ms step_avg:224.43ms
step:921/7050 train_time:206702ms step_avg:224.43ms
step:922/7050 train_time:206927ms step_avg:224.43ms
step:923/7050 train_time:207151ms step_avg:224.43ms
step:924/7050 train_time:207377ms step_avg:224.43ms
step:925/7050 train_time:207602ms step_avg:224.43ms
step:926/7050 train_time:207827ms step_avg:224.44ms
step:927/7050 train_time:208052ms step_avg:224.44ms
step:928/7050 train_time:208278ms step_avg:224.44ms
step:929/7050 train_time:208503ms step_avg:224.44ms
step:930/7050 train_time:208728ms step_avg:224.44ms
step:931/7050 train_time:208952ms step_avg:224.44ms
step:932/7050 train_time:209179ms step_avg:224.44ms
step:933/7050 train_time:209403ms step_avg:224.44ms
step:934/7050 train_time:209627ms step_avg:224.44ms
step:935/7050 train_time:209852ms step_avg:224.44ms
step:936/7050 train_time:210077ms step_avg:224.44ms
step:937/7050 train_time:210302ms step_avg:224.44ms
step:938/7050 train_time:210527ms step_avg:224.44ms
step:939/7050 train_time:210753ms step_avg:224.44ms
step:940/7050 train_time:210977ms step_avg:224.44ms
step:941/7050 train_time:211201ms step_avg:224.44ms
step:942/7050 train_time:211427ms step_avg:224.44ms
step:943/7050 train_time:211651ms step_avg:224.44ms
step:944/7050 train_time:211877ms step_avg:224.45ms
step:945/7050 train_time:212102ms step_avg:224.45ms
step:946/7050 train_time:212326ms step_avg:224.45ms
step:947/7050 train_time:212551ms step_avg:224.45ms
step:948/7050 train_time:212778ms step_avg:224.45ms
step:949/7050 train_time:213002ms step_avg:224.45ms
step:950/7050 train_time:213227ms step_avg:224.45ms
step:951/7050 train_time:213452ms step_avg:224.45ms
step:952/7050 train_time:213678ms step_avg:224.45ms
step:953/7050 train_time:213902ms step_avg:224.45ms
step:954/7050 train_time:214127ms step_avg:224.45ms
step:955/7050 train_time:214351ms step_avg:224.45ms
step:956/7050 train_time:214577ms step_avg:224.45ms
step:957/7050 train_time:214804ms step_avg:224.46ms
step:958/7050 train_time:215027ms step_avg:224.45ms
step:959/7050 train_time:215251ms step_avg:224.45ms
step:960/7050 train_time:215477ms step_avg:224.46ms
step:961/7050 train_time:215703ms step_avg:224.46ms
step:962/7050 train_time:215928ms step_avg:224.46ms
step:963/7050 train_time:216152ms step_avg:224.46ms
step:964/7050 train_time:216378ms step_avg:224.46ms
step:965/7050 train_time:216602ms step_avg:224.46ms
step:966/7050 train_time:216827ms step_avg:224.46ms
step:967/7050 train_time:217052ms step_avg:224.46ms
step:968/7050 train_time:217279ms step_avg:224.46ms
step:969/7050 train_time:217502ms step_avg:224.46ms
step:970/7050 train_time:217726ms step_avg:224.46ms
step:971/7050 train_time:217951ms step_avg:224.46ms
step:972/7050 train_time:218176ms step_avg:224.46ms
step:973/7050 train_time:218401ms step_avg:224.46ms
step:974/7050 train_time:218625ms step_avg:224.46ms
step:975/7050 train_time:218848ms step_avg:224.46ms
step:976/7050 train_time:219073ms step_avg:224.46ms
step:977/7050 train_time:219300ms step_avg:224.46ms
step:978/7050 train_time:219524ms step_avg:224.46ms
step:979/7050 train_time:219748ms step_avg:224.46ms
step:980/7050 train_time:219973ms step_avg:224.46ms
step:981/7050 train_time:220200ms step_avg:224.46ms
step:982/7050 train_time:220423ms step_avg:224.46ms
step:983/7050 train_time:220647ms step_avg:224.46ms
step:984/7050 train_time:220872ms step_avg:224.46ms
step:985/7050 train_time:221095ms step_avg:224.46ms
step:986/7050 train_time:221318ms step_avg:224.46ms
step:987/7050 train_time:221542ms step_avg:224.46ms
step:988/7050 train_time:221768ms step_avg:224.46ms
step:989/7050 train_time:221992ms step_avg:224.46ms
step:990/7050 train_time:222217ms step_avg:224.46ms
step:991/7050 train_time:222441ms step_avg:224.46ms
step:992/7050 train_time:222667ms step_avg:224.46ms
step:993/7050 train_time:222892ms step_avg:224.46ms
step:994/7050 train_time:223118ms step_avg:224.47ms
step:995/7050 train_time:223344ms step_avg:224.47ms
step:996/7050 train_time:223568ms step_avg:224.47ms
step:997/7050 train_time:223796ms step_avg:224.47ms
step:998/7050 train_time:224020ms step_avg:224.47ms
step:999/7050 train_time:224245ms step_avg:224.47ms
step:1000/7050 train_time:224471ms step_avg:224.47ms
step:1000/7050 val_loss:3.4883 train_time:224598ms step_avg:224.60ms
step:1001/7050 train_time:224694ms step_avg:224.47ms
step:1002/7050 train_time:224920ms step_avg:224.47ms
step:1003/7050 train_time:225152ms step_avg:224.48ms
step:1004/7050 train_time:225377ms step_avg:224.48ms
step:1005/7050 train_time:225602ms step_avg:224.48ms
step:1006/7050 train_time:225824ms step_avg:224.48ms
step:1007/7050 train_time:226050ms step_avg:224.48ms
step:1008/7050 train_time:226277ms step_avg:224.48ms
step:1009/7050 train_time:226501ms step_avg:224.48ms
step:1010/7050 train_time:226725ms step_avg:224.48ms
step:1011/7050 train_time:226949ms step_avg:224.48ms
step:1012/7050 train_time:227174ms step_avg:224.48ms
step:1013/7050 train_time:227399ms step_avg:224.48ms
step:1014/7050 train_time:227624ms step_avg:224.48ms
step:1015/7050 train_time:227848ms step_avg:224.48ms
step:1016/7050 train_time:228073ms step_avg:224.48ms
step:1017/7050 train_time:228300ms step_avg:224.48ms
step:1018/7050 train_time:228524ms step_avg:224.48ms
step:1019/7050 train_time:228750ms step_avg:224.49ms
step:1020/7050 train_time:228975ms step_avg:224.49ms
step:1021/7050 train_time:229200ms step_avg:224.49ms
step:1022/7050 train_time:229426ms step_avg:224.49ms
step:1023/7050 train_time:229650ms step_avg:224.49ms
step:1024/7050 train_time:229875ms step_avg:224.49ms
step:1025/7050 train_time:230100ms step_avg:224.49ms
step:1026/7050 train_time:230324ms step_avg:224.49ms
step:1027/7050 train_time:230548ms step_avg:224.49ms
step:1028/7050 train_time:230772ms step_avg:224.49ms
step:1029/7050 train_time:230998ms step_avg:224.49ms
step:1030/7050 train_time:231223ms step_avg:224.49ms
step:1031/7050 train_time:231449ms step_avg:224.49ms
step:1032/7050 train_time:231674ms step_avg:224.49ms
step:1033/7050 train_time:231899ms step_avg:224.49ms
step:1034/7050 train_time:232124ms step_avg:224.49ms
step:1035/7050 train_time:232349ms step_avg:224.49ms
step:1036/7050 train_time:232573ms step_avg:224.49ms
step:1037/7050 train_time:232800ms step_avg:224.49ms
step:1038/7050 train_time:233025ms step_avg:224.49ms
step:1039/7050 train_time:233250ms step_avg:224.50ms
step:1040/7050 train_time:233474ms step_avg:224.49ms
step:1041/7050 train_time:233699ms step_avg:224.49ms
step:1042/7050 train_time:233924ms step_avg:224.50ms
step:1043/7050 train_time:234148ms step_avg:224.49ms
step:1044/7050 train_time:234374ms step_avg:224.50ms
step:1045/7050 train_time:234598ms step_avg:224.50ms
step:1046/7050 train_time:234824ms step_avg:224.50ms
step:1047/7050 train_time:235050ms step_avg:224.50ms
step:1048/7050 train_time:235276ms step_avg:224.50ms
step:1049/7050 train_time:235500ms step_avg:224.50ms
step:1050/7050 train_time:235728ms step_avg:224.50ms
step:1051/7050 train_time:235955ms step_avg:224.51ms
step:1052/7050 train_time:236183ms step_avg:224.51ms
step:1053/7050 train_time:236408ms step_avg:224.51ms
step:1054/7050 train_time:236633ms step_avg:224.51ms
step:1055/7050 train_time:236859ms step_avg:224.51ms
step:1056/7050 train_time:237085ms step_avg:224.51ms
step:1057/7050 train_time:237315ms step_avg:224.52ms
step:1058/7050 train_time:237540ms step_avg:224.52ms
step:1059/7050 train_time:237766ms step_avg:224.52ms
step:1060/7050 train_time:237991ms step_avg:224.52ms
step:1061/7050 train_time:238218ms step_avg:224.52ms
step:1062/7050 train_time:238446ms step_avg:224.53ms
step:1063/7050 train_time:238669ms step_avg:224.52ms
step:1064/7050 train_time:238895ms step_avg:224.53ms
step:1065/7050 train_time:239121ms step_avg:224.53ms
step:1066/7050 train_time:239348ms step_avg:224.53ms
step:1067/7050 train_time:239572ms step_avg:224.53ms
step:1068/7050 train_time:239800ms step_avg:224.53ms
step:1069/7050 train_time:240025ms step_avg:224.53ms
step:1070/7050 train_time:240251ms step_avg:224.53ms
step:1071/7050 train_time:240476ms step_avg:224.53ms
step:1072/7050 train_time:240703ms step_avg:224.54ms
step:1073/7050 train_time:240929ms step_avg:224.54ms
step:1074/7050 train_time:241156ms step_avg:224.54ms
step:1075/7050 train_time:241382ms step_avg:224.54ms
step:1076/7050 train_time:241609ms step_avg:224.54ms
step:1077/7050 train_time:241835ms step_avg:224.55ms
step:1078/7050 train_time:242060ms step_avg:224.55ms
step:1079/7050 train_time:242287ms step_avg:224.55ms
step:1080/7050 train_time:242512ms step_avg:224.55ms
step:1081/7050 train_time:242739ms step_avg:224.55ms
step:1082/7050 train_time:242963ms step_avg:224.55ms
step:1083/7050 train_time:243190ms step_avg:224.55ms
step:1084/7050 train_time:243414ms step_avg:224.55ms
step:1085/7050 train_time:243642ms step_avg:224.55ms
step:1086/7050 train_time:243867ms step_avg:224.56ms
step:1087/7050 train_time:244092ms step_avg:224.56ms
step:1088/7050 train_time:244321ms step_avg:224.56ms
step:1089/7050 train_time:244545ms step_avg:224.56ms
step:1090/7050 train_time:244771ms step_avg:224.56ms
step:1091/7050 train_time:244998ms step_avg:224.56ms
step:1092/7050 train_time:245226ms step_avg:224.57ms
step:1093/7050 train_time:245453ms step_avg:224.57ms
step:1094/7050 train_time:245679ms step_avg:224.57ms
step:1095/7050 train_time:245904ms step_avg:224.57ms
step:1096/7050 train_time:246131ms step_avg:224.57ms
step:1097/7050 train_time:246356ms step_avg:224.57ms
step:1098/7050 train_time:246582ms step_avg:224.57ms
step:1099/7050 train_time:246809ms step_avg:224.58ms
step:1100/7050 train_time:247035ms step_avg:224.58ms
step:1101/7050 train_time:247260ms step_avg:224.58ms
step:1102/7050 train_time:247487ms step_avg:224.58ms
step:1103/7050 train_time:247713ms step_avg:224.58ms
step:1104/7050 train_time:247939ms step_avg:224.58ms
step:1105/7050 train_time:248164ms step_avg:224.58ms
step:1106/7050 train_time:248389ms step_avg:224.58ms
step:1107/7050 train_time:248616ms step_avg:224.59ms
step:1108/7050 train_time:248843ms step_avg:224.59ms
step:1109/7050 train_time:249068ms step_avg:224.59ms
step:1110/7050 train_time:249297ms step_avg:224.59ms
step:1111/7050 train_time:249523ms step_avg:224.59ms
step:1112/7050 train_time:249750ms step_avg:224.60ms
step:1113/7050 train_time:249976ms step_avg:224.60ms
step:1114/7050 train_time:250202ms step_avg:224.60ms
step:1115/7050 train_time:250428ms step_avg:224.60ms
step:1116/7050 train_time:250653ms step_avg:224.60ms
step:1117/7050 train_time:250880ms step_avg:224.60ms
step:1118/7050 train_time:251107ms step_avg:224.60ms
step:1119/7050 train_time:251331ms step_avg:224.60ms
step:1120/7050 train_time:251557ms step_avg:224.60ms
step:1121/7050 train_time:251783ms step_avg:224.61ms
step:1122/7050 train_time:252011ms step_avg:224.61ms
step:1123/7050 train_time:252238ms step_avg:224.61ms
step:1124/7050 train_time:252463ms step_avg:224.61ms
step:1125/7050 train_time:252689ms step_avg:224.61ms
step:1125/7050 val_loss:3.4244 train_time:252817ms step_avg:224.73ms
step:1126/7050 train_time:252916ms step_avg:224.61ms
step:1127/7050 train_time:253142ms step_avg:224.62ms
step:1128/7050 train_time:253371ms step_avg:224.62ms
step:1129/7050 train_time:253596ms step_avg:224.62ms
step:1130/7050 train_time:253823ms step_avg:224.62ms
step:1131/7050 train_time:254047ms step_avg:224.62ms
step:1132/7050 train_time:254273ms step_avg:224.62ms
step:1133/7050 train_time:254502ms step_avg:224.63ms
step:1134/7050 train_time:254728ms step_avg:224.63ms
step:1135/7050 train_time:254954ms step_avg:224.63ms
step:1136/7050 train_time:255179ms step_avg:224.63ms
step:1137/7050 train_time:255405ms step_avg:224.63ms
step:1138/7050 train_time:255632ms step_avg:224.63ms
step:1139/7050 train_time:255859ms step_avg:224.64ms
step:1140/7050 train_time:256085ms step_avg:224.64ms
step:1141/7050 train_time:256311ms step_avg:224.64ms
step:1142/7050 train_time:256537ms step_avg:224.64ms
step:1143/7050 train_time:256763ms step_avg:224.64ms
step:1144/7050 train_time:256989ms step_avg:224.64ms
step:1145/7050 train_time:257215ms step_avg:224.64ms
step:1146/7050 train_time:257441ms step_avg:224.64ms
step:1147/7050 train_time:257668ms step_avg:224.64ms
step:1148/7050 train_time:257894ms step_avg:224.65ms
step:1149/7050 train_time:258120ms step_avg:224.65ms
step:1150/7050 train_time:258346ms step_avg:224.65ms
step:1151/7050 train_time:258573ms step_avg:224.65ms
step:1152/7050 train_time:258797ms step_avg:224.65ms
step:1153/7050 train_time:259024ms step_avg:224.65ms
step:1154/7050 train_time:259251ms step_avg:224.65ms
step:1155/7050 train_time:259477ms step_avg:224.66ms
step:1156/7050 train_time:259702ms step_avg:224.66ms
step:1157/7050 train_time:259928ms step_avg:224.66ms
step:1158/7050 train_time:260156ms step_avg:224.66ms
step:1159/7050 train_time:260381ms step_avg:224.66ms
step:1160/7050 train_time:260608ms step_avg:224.66ms
step:1161/7050 train_time:260834ms step_avg:224.66ms
step:1162/7050 train_time:261061ms step_avg:224.67ms
step:1163/7050 train_time:261287ms step_avg:224.67ms
step:1164/7050 train_time:261514ms step_avg:224.67ms
step:1165/7050 train_time:261739ms step_avg:224.67ms
step:1166/7050 train_time:261966ms step_avg:224.67ms
step:1167/7050 train_time:262193ms step_avg:224.67ms
step:1168/7050 train_time:262417ms step_avg:224.67ms
step:1169/7050 train_time:262645ms step_avg:224.67ms
step:1170/7050 train_time:262868ms step_avg:224.67ms
step:1171/7050 train_time:263096ms step_avg:224.68ms
step:1172/7050 train_time:263324ms step_avg:224.68ms
step:1173/7050 train_time:263552ms step_avg:224.68ms
step:1174/7050 train_time:263776ms step_avg:224.68ms
step:1175/7050 train_time:264001ms step_avg:224.68ms
step:1176/7050 train_time:264228ms step_avg:224.68ms
step:1177/7050 train_time:264455ms step_avg:224.69ms
step:1178/7050 train_time:264679ms step_avg:224.69ms
step:1179/7050 train_time:264906ms step_avg:224.69ms
step:1180/7050 train_time:265130ms step_avg:224.69ms
step:1181/7050 train_time:265356ms step_avg:224.69ms
step:1182/7050 train_time:265581ms step_avg:224.69ms
step:1183/7050 train_time:265807ms step_avg:224.69ms
step:1184/7050 train_time:266036ms step_avg:224.69ms
step:1185/7050 train_time:266261ms step_avg:224.69ms
step:1186/7050 train_time:266489ms step_avg:224.70ms
step:1187/7050 train_time:266714ms step_avg:224.70ms
step:1188/7050 train_time:266940ms step_avg:224.70ms
step:1189/7050 train_time:267166ms step_avg:224.70ms
step:1190/7050 train_time:267392ms step_avg:224.70ms
step:1191/7050 train_time:267617ms step_avg:224.70ms
step:1192/7050 train_time:267845ms step_avg:224.70ms
step:1193/7050 train_time:268070ms step_avg:224.70ms
step:1194/7050 train_time:268296ms step_avg:224.70ms
step:1195/7050 train_time:268521ms step_avg:224.70ms
step:1196/7050 train_time:268747ms step_avg:224.70ms
step:1197/7050 train_time:268976ms step_avg:224.71ms
step:1198/7050 train_time:269201ms step_avg:224.71ms
step:1199/7050 train_time:269428ms step_avg:224.71ms
step:1200/7050 train_time:269656ms step_avg:224.71ms
step:1201/7050 train_time:269881ms step_avg:224.71ms
step:1202/7050 train_time:270108ms step_avg:224.72ms
step:1203/7050 train_time:270334ms step_avg:224.72ms
step:1204/7050 train_time:270560ms step_avg:224.72ms
step:1205/7050 train_time:270787ms step_avg:224.72ms
step:1206/7050 train_time:271014ms step_avg:224.72ms
step:1207/7050 train_time:271240ms step_avg:224.72ms
step:1208/7050 train_time:271466ms step_avg:224.72ms
step:1209/7050 train_time:271693ms step_avg:224.73ms
step:1210/7050 train_time:271917ms step_avg:224.73ms
step:1211/7050 train_time:272144ms step_avg:224.73ms
step:1212/7050 train_time:272369ms step_avg:224.73ms
step:1213/7050 train_time:272595ms step_avg:224.73ms
step:1214/7050 train_time:272819ms step_avg:224.73ms
step:1215/7050 train_time:273044ms step_avg:224.73ms
step:1216/7050 train_time:273272ms step_avg:224.73ms
step:1217/7050 train_time:273497ms step_avg:224.73ms
step:1218/7050 train_time:273721ms step_avg:224.73ms
step:1219/7050 train_time:273949ms step_avg:224.73ms
step:1220/7050 train_time:274174ms step_avg:224.73ms
step:1221/7050 train_time:274401ms step_avg:224.73ms
step:1222/7050 train_time:274626ms step_avg:224.73ms
step:1223/7050 train_time:274853ms step_avg:224.74ms
step:1224/7050 train_time:275079ms step_avg:224.74ms
step:1225/7050 train_time:275305ms step_avg:224.74ms
step:1226/7050 train_time:275531ms step_avg:224.74ms
step:1227/7050 train_time:275757ms step_avg:224.74ms
step:1228/7050 train_time:275986ms step_avg:224.74ms
step:1229/7050 train_time:276213ms step_avg:224.75ms
step:1230/7050 train_time:276437ms step_avg:224.75ms
step:1231/7050 train_time:276662ms step_avg:224.75ms
step:1232/7050 train_time:276889ms step_avg:224.75ms
step:1233/7050 train_time:277116ms step_avg:224.75ms
step:1234/7050 train_time:277340ms step_avg:224.75ms
step:1235/7050 train_time:277566ms step_avg:224.75ms
step:1236/7050 train_time:277793ms step_avg:224.75ms
step:1237/7050 train_time:278019ms step_avg:224.75ms
step:1238/7050 train_time:278247ms step_avg:224.76ms
step:1239/7050 train_time:278472ms step_avg:224.76ms
step:1240/7050 train_time:278697ms step_avg:224.76ms
step:1241/7050 train_time:278925ms step_avg:224.76ms
step:1242/7050 train_time:279152ms step_avg:224.76ms
step:1243/7050 train_time:279377ms step_avg:224.76ms
step:1244/7050 train_time:279603ms step_avg:224.76ms
step:1245/7050 train_time:279829ms step_avg:224.76ms
step:1246/7050 train_time:280056ms step_avg:224.76ms
step:1247/7050 train_time:280282ms step_avg:224.76ms
step:1248/7050 train_time:280507ms step_avg:224.77ms
step:1249/7050 train_time:280734ms step_avg:224.77ms
step:1250/7050 train_time:280960ms step_avg:224.77ms
step:1250/7050 val_loss:3.3947 train_time:281087ms step_avg:224.87ms
step:1251/7050 train_time:281185ms step_avg:224.77ms
step:1252/7050 train_time:281413ms step_avg:224.77ms
step:1253/7050 train_time:281641ms step_avg:224.77ms
step:1254/7050 train_time:281868ms step_avg:224.78ms
step:1255/7050 train_time:282093ms step_avg:224.78ms
step:1256/7050 train_time:282320ms step_avg:224.78ms
step:1257/7050 train_time:282546ms step_avg:224.78ms
step:1258/7050 train_time:282773ms step_avg:224.78ms
step:1259/7050 train_time:282999ms step_avg:224.78ms
step:1260/7050 train_time:283225ms step_avg:224.78ms
step:1261/7050 train_time:283451ms step_avg:224.78ms
step:1262/7050 train_time:283679ms step_avg:224.79ms
step:1263/7050 train_time:283905ms step_avg:224.79ms
step:1264/7050 train_time:284130ms step_avg:224.79ms
step:1265/7050 train_time:284357ms step_avg:224.79ms
step:1266/7050 train_time:284585ms step_avg:224.79ms
step:1267/7050 train_time:284812ms step_avg:224.79ms
step:1268/7050 train_time:285037ms step_avg:224.79ms
step:1269/7050 train_time:285263ms step_avg:224.79ms
step:1270/7050 train_time:285490ms step_avg:224.80ms
step:1271/7050 train_time:285715ms step_avg:224.80ms
step:1272/7050 train_time:285941ms step_avg:224.80ms
step:1273/7050 train_time:286167ms step_avg:224.80ms
step:1274/7050 train_time:286394ms step_avg:224.80ms
step:1275/7050 train_time:286623ms step_avg:224.80ms
step:1276/7050 train_time:286848ms step_avg:224.80ms
step:1277/7050 train_time:287073ms step_avg:224.80ms
step:1278/7050 train_time:287300ms step_avg:224.80ms
step:1279/7050 train_time:287526ms step_avg:224.81ms
step:1280/7050 train_time:287752ms step_avg:224.81ms
step:1281/7050 train_time:287977ms step_avg:224.81ms
step:1282/7050 train_time:288203ms step_avg:224.81ms
step:1283/7050 train_time:288428ms step_avg:224.81ms
step:1284/7050 train_time:288655ms step_avg:224.81ms
step:1285/7050 train_time:288881ms step_avg:224.81ms
step:1286/7050 train_time:289109ms step_avg:224.81ms
step:1287/7050 train_time:289335ms step_avg:224.81ms
step:1288/7050 train_time:289561ms step_avg:224.81ms
step:1289/7050 train_time:289788ms step_avg:224.82ms
step:1290/7050 train_time:290012ms step_avg:224.82ms
step:1291/7050 train_time:290239ms step_avg:224.82ms
step:1292/7050 train_time:290465ms step_avg:224.82ms
step:1293/7050 train_time:290694ms step_avg:224.82ms
step:1294/7050 train_time:290921ms step_avg:224.82ms
step:1295/7050 train_time:291146ms step_avg:224.82ms
step:1296/7050 train_time:291371ms step_avg:224.82ms
step:1297/7050 train_time:291598ms step_avg:224.83ms
step:1298/7050 train_time:291823ms step_avg:224.83ms
step:1299/7050 train_time:292051ms step_avg:224.83ms
step:1300/7050 train_time:292274ms step_avg:224.83ms
step:1301/7050 train_time:292501ms step_avg:224.83ms
step:1302/7050 train_time:292727ms step_avg:224.83ms
step:1303/7050 train_time:292953ms step_avg:224.83ms
step:1304/7050 train_time:293181ms step_avg:224.83ms
step:1305/7050 train_time:293409ms step_avg:224.83ms
step:1306/7050 train_time:293633ms step_avg:224.83ms
step:1307/7050 train_time:293858ms step_avg:224.83ms
step:1308/7050 train_time:294086ms step_avg:224.84ms
step:1309/7050 train_time:294312ms step_avg:224.84ms
step:1310/7050 train_time:294538ms step_avg:224.84ms
step:1311/7050 train_time:294763ms step_avg:224.84ms
step:1312/7050 train_time:294990ms step_avg:224.84ms
step:1313/7050 train_time:295215ms step_avg:224.84ms
step:1314/7050 train_time:295441ms step_avg:224.84ms
step:1315/7050 train_time:295667ms step_avg:224.84ms
step:1316/7050 train_time:295893ms step_avg:224.84ms
step:1317/7050 train_time:296120ms step_avg:224.84ms
step:1318/7050 train_time:296346ms step_avg:224.84ms
step:1319/7050 train_time:296572ms step_avg:224.85ms
step:1320/7050 train_time:296798ms step_avg:224.85ms
step:1321/7050 train_time:297026ms step_avg:224.85ms
step:1322/7050 train_time:297251ms step_avg:224.85ms
step:1323/7050 train_time:297478ms step_avg:224.85ms
step:1324/7050 train_time:297704ms step_avg:224.85ms
step:1325/7050 train_time:297930ms step_avg:224.85ms
step:1326/7050 train_time:298157ms step_avg:224.85ms
step:1327/7050 train_time:298382ms step_avg:224.85ms
step:1328/7050 train_time:298608ms step_avg:224.86ms
step:1329/7050 train_time:298833ms step_avg:224.86ms
step:1330/7050 train_time:299058ms step_avg:224.86ms
step:1331/7050 train_time:299285ms step_avg:224.86ms
step:1332/7050 train_time:299510ms step_avg:224.86ms
step:1333/7050 train_time:299736ms step_avg:224.86ms
step:1334/7050 train_time:299962ms step_avg:224.86ms
step:1335/7050 train_time:300189ms step_avg:224.86ms
step:1336/7050 train_time:300417ms step_avg:224.86ms
step:1337/7050 train_time:300640ms step_avg:224.86ms
step:1338/7050 train_time:300865ms step_avg:224.86ms
step:1339/7050 train_time:301092ms step_avg:224.86ms
step:1340/7050 train_time:301318ms step_avg:224.86ms
step:1341/7050 train_time:301543ms step_avg:224.86ms
step:1342/7050 train_time:301769ms step_avg:224.87ms
step:1343/7050 train_time:301997ms step_avg:224.87ms
step:1344/7050 train_time:302222ms step_avg:224.87ms
step:1345/7050 train_time:302447ms step_avg:224.87ms
step:1346/7050 train_time:302672ms step_avg:224.87ms
step:1347/7050 train_time:302900ms step_avg:224.87ms
step:1348/7050 train_time:303125ms step_avg:224.87ms
step:1349/7050 train_time:303351ms step_avg:224.87ms
step:1350/7050 train_time:303576ms step_avg:224.87ms
step:1351/7050 train_time:303804ms step_avg:224.87ms
step:1352/7050 train_time:304030ms step_avg:224.87ms
step:1353/7050 train_time:304256ms step_avg:224.88ms
step:1354/7050 train_time:304482ms step_avg:224.88ms
step:1355/7050 train_time:304708ms step_avg:224.88ms
step:1356/7050 train_time:304935ms step_avg:224.88ms
step:1357/7050 train_time:305161ms step_avg:224.88ms
step:1358/7050 train_time:305387ms step_avg:224.88ms
step:1359/7050 train_time:305613ms step_avg:224.88ms
step:1360/7050 train_time:305838ms step_avg:224.88ms
step:1361/7050 train_time:306065ms step_avg:224.88ms
step:1362/7050 train_time:306294ms step_avg:224.89ms
step:1363/7050 train_time:306518ms step_avg:224.88ms
step:1364/7050 train_time:306745ms step_avg:224.89ms
step:1365/7050 train_time:306971ms step_avg:224.89ms
step:1366/7050 train_time:307198ms step_avg:224.89ms
step:1367/7050 train_time:307422ms step_avg:224.89ms
step:1368/7050 train_time:307648ms step_avg:224.89ms
step:1369/7050 train_time:307874ms step_avg:224.89ms
step:1370/7050 train_time:308100ms step_avg:224.89ms
step:1371/7050 train_time:308328ms step_avg:224.89ms
step:1372/7050 train_time:308551ms step_avg:224.89ms
step:1373/7050 train_time:308779ms step_avg:224.89ms
step:1374/7050 train_time:309004ms step_avg:224.89ms
step:1375/7050 train_time:309233ms step_avg:224.90ms
step:1375/7050 val_loss:3.3728 train_time:309360ms step_avg:224.99ms
step:1376/7050 train_time:309458ms step_avg:224.90ms
step:1377/7050 train_time:309686ms step_avg:224.90ms
step:1378/7050 train_time:309918ms step_avg:224.90ms
step:1379/7050 train_time:310144ms step_avg:224.91ms
step:1380/7050 train_time:310368ms step_avg:224.90ms
step:1381/7050 train_time:310593ms step_avg:224.90ms
step:1382/7050 train_time:310822ms step_avg:224.91ms
step:1383/7050 train_time:311051ms step_avg:224.91ms
step:1384/7050 train_time:311277ms step_avg:224.91ms
step:1385/7050 train_time:311501ms step_avg:224.91ms
step:1386/7050 train_time:311727ms step_avg:224.91ms
step:1387/7050 train_time:311955ms step_avg:224.91ms
step:1388/7050 train_time:312181ms step_avg:224.91ms
step:1389/7050 train_time:312405ms step_avg:224.91ms
step:1390/7050 train_time:312631ms step_avg:224.91ms
step:1391/7050 train_time:312858ms step_avg:224.92ms
step:1392/7050 train_time:313083ms step_avg:224.92ms
step:1393/7050 train_time:313311ms step_avg:224.92ms
step:1394/7050 train_time:313536ms step_avg:224.92ms
step:1395/7050 train_time:313762ms step_avg:224.92ms
step:1396/7050 train_time:313987ms step_avg:224.92ms
step:1397/7050 train_time:314213ms step_avg:224.92ms
step:1398/7050 train_time:314439ms step_avg:224.92ms
step:1399/7050 train_time:314666ms step_avg:224.92ms
step:1400/7050 train_time:314892ms step_avg:224.92ms
step:1401/7050 train_time:315117ms step_avg:224.92ms
step:1402/7050 train_time:315345ms step_avg:224.93ms
step:1403/7050 train_time:315572ms step_avg:224.93ms
step:1404/7050 train_time:315797ms step_avg:224.93ms
step:1405/7050 train_time:316022ms step_avg:224.93ms
step:1406/7050 train_time:316249ms step_avg:224.93ms
step:1407/7050 train_time:316477ms step_avg:224.93ms
step:1408/7050 train_time:316703ms step_avg:224.93ms
step:1409/7050 train_time:316928ms step_avg:224.93ms
step:1410/7050 train_time:317155ms step_avg:224.93ms
step:1411/7050 train_time:317380ms step_avg:224.93ms
step:1412/7050 train_time:317606ms step_avg:224.93ms
step:1413/7050 train_time:317833ms step_avg:224.94ms
step:1414/7050 train_time:318057ms step_avg:224.93ms
step:1415/7050 train_time:318282ms step_avg:224.93ms
step:1416/7050 train_time:318508ms step_avg:224.94ms
step:1417/7050 train_time:318737ms step_avg:224.94ms
step:1418/7050 train_time:318961ms step_avg:224.94ms
step:1419/7050 train_time:319187ms step_avg:224.94ms
step:1420/7050 train_time:319411ms step_avg:224.94ms
step:1421/7050 train_time:319639ms step_avg:224.94ms
step:1422/7050 train_time:319865ms step_avg:224.94ms
step:1423/7050 train_time:320089ms step_avg:224.94ms
step:1424/7050 train_time:320314ms step_avg:224.94ms
step:1425/7050 train_time:320540ms step_avg:224.94ms
step:1426/7050 train_time:320767ms step_avg:224.94ms
step:1427/7050 train_time:320992ms step_avg:224.94ms
step:1428/7050 train_time:321217ms step_avg:224.94ms
step:1429/7050 train_time:321443ms step_avg:224.94ms
step:1430/7050 train_time:321668ms step_avg:224.94ms
step:1431/7050 train_time:321895ms step_avg:224.94ms
step:1432/7050 train_time:322120ms step_avg:224.94ms
step:1433/7050 train_time:322347ms step_avg:224.95ms
step:1434/7050 train_time:322573ms step_avg:224.95ms
step:1435/7050 train_time:322798ms step_avg:224.95ms
step:1436/7050 train_time:323024ms step_avg:224.95ms
step:1437/7050 train_time:323252ms step_avg:224.95ms
step:1438/7050 train_time:323478ms step_avg:224.95ms
step:1439/7050 train_time:323705ms step_avg:224.95ms
step:1440/7050 train_time:323930ms step_avg:224.95ms
step:1441/7050 train_time:324156ms step_avg:224.95ms
step:1442/7050 train_time:324382ms step_avg:224.95ms
step:1443/7050 train_time:324608ms step_avg:224.95ms
step:1444/7050 train_time:324835ms step_avg:224.96ms
step:1445/7050 train_time:325060ms step_avg:224.96ms
step:1446/7050 train_time:325287ms step_avg:224.96ms
step:1447/7050 train_time:325513ms step_avg:224.96ms
step:1448/7050 train_time:325739ms step_avg:224.96ms
step:1449/7050 train_time:325966ms step_avg:224.96ms
step:1450/7050 train_time:326192ms step_avg:224.96ms
step:1451/7050 train_time:326417ms step_avg:224.96ms
step:1452/7050 train_time:326644ms step_avg:224.96ms
step:1453/7050 train_time:326870ms step_avg:224.96ms
step:1454/7050 train_time:327096ms step_avg:224.96ms
step:1455/7050 train_time:327322ms step_avg:224.96ms
step:1456/7050 train_time:327547ms step_avg:224.96ms
step:1457/7050 train_time:327775ms step_avg:224.97ms
step:1458/7050 train_time:327999ms step_avg:224.96ms
step:1459/7050 train_time:328224ms step_avg:224.97ms
step:1460/7050 train_time:328451ms step_avg:224.97ms
step:1461/7050 train_time:328677ms step_avg:224.97ms
step:1462/7050 train_time:328901ms step_avg:224.97ms
step:1463/7050 train_time:329127ms step_avg:224.97ms
step:1464/7050 train_time:329353ms step_avg:224.97ms
step:1465/7050 train_time:329579ms step_avg:224.97ms
step:1466/7050 train_time:329805ms step_avg:224.97ms
step:1467/7050 train_time:330031ms step_avg:224.97ms
step:1468/7050 train_time:330259ms step_avg:224.97ms
step:1469/7050 train_time:330486ms step_avg:224.97ms
step:1470/7050 train_time:330712ms step_avg:224.97ms
step:1471/7050 train_time:330938ms step_avg:224.97ms
step:1472/7050 train_time:331164ms step_avg:224.98ms
step:1473/7050 train_time:331391ms step_avg:224.98ms
step:1474/7050 train_time:331616ms step_avg:224.98ms
step:1475/7050 train_time:331842ms step_avg:224.98ms
step:1476/7050 train_time:332068ms step_avg:224.98ms
step:1477/7050 train_time:332294ms step_avg:224.98ms
step:1478/7050 train_time:332519ms step_avg:224.98ms
step:1479/7050 train_time:332746ms step_avg:224.98ms
step:1480/7050 train_time:332972ms step_avg:224.98ms
step:1481/7050 train_time:333197ms step_avg:224.98ms
step:1482/7050 train_time:333427ms step_avg:224.98ms
step:1483/7050 train_time:333654ms step_avg:224.99ms
step:1484/7050 train_time:333879ms step_avg:224.99ms
step:1485/7050 train_time:334104ms step_avg:224.99ms
step:1486/7050 train_time:334330ms step_avg:224.99ms
step:1487/7050 train_time:334558ms step_avg:224.99ms
step:1488/7050 train_time:334783ms step_avg:224.99ms
step:1489/7050 train_time:335009ms step_avg:224.99ms
step:1490/7050 train_time:335235ms step_avg:224.99ms
step:1491/7050 train_time:335463ms step_avg:224.99ms
step:1492/7050 train_time:335688ms step_avg:224.99ms
step:1493/7050 train_time:335914ms step_avg:224.99ms
step:1494/7050 train_time:336141ms step_avg:224.99ms
step:1495/7050 train_time:336369ms step_avg:225.00ms
step:1496/7050 train_time:336595ms step_avg:225.00ms
step:1497/7050 train_time:336820ms step_avg:225.00ms
step:1498/7050 train_time:337047ms step_avg:225.00ms
step:1499/7050 train_time:337273ms step_avg:225.00ms
step:1500/7050 train_time:337500ms step_avg:225.00ms
step:1500/7050 val_loss:3.3479 train_time:337628ms step_avg:225.09ms
step:1501/7050 train_time:337726ms step_avg:225.00ms
step:1502/7050 train_time:337950ms step_avg:225.00ms
step:1503/7050 train_time:338183ms step_avg:225.01ms
step:1504/7050 train_time:338410ms step_avg:225.01ms
step:1505/7050 train_time:338633ms step_avg:225.01ms
step:1506/7050 train_time:338857ms step_avg:225.00ms
step:1507/7050 train_time:339085ms step_avg:225.01ms
step:1508/7050 train_time:339312ms step_avg:225.01ms
step:1509/7050 train_time:339538ms step_avg:225.01ms
step:1510/7050 train_time:339762ms step_avg:225.01ms
step:1511/7050 train_time:339989ms step_avg:225.01ms
step:1512/7050 train_time:340217ms step_avg:225.01ms
step:1513/7050 train_time:340442ms step_avg:225.01ms
step:1514/7050 train_time:340669ms step_avg:225.01ms
step:1515/7050 train_time:340893ms step_avg:225.01ms
step:1516/7050 train_time:341120ms step_avg:225.01ms
step:1517/7050 train_time:341345ms step_avg:225.01ms
step:1518/7050 train_time:341573ms step_avg:225.01ms
step:1519/7050 train_time:341799ms step_avg:225.02ms
step:1520/7050 train_time:342024ms step_avg:225.02ms
