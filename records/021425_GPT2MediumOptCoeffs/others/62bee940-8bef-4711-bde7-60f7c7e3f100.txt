import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for a, b, c in [
        (3.8839, -3.9828, 1.0989),
        (3.7253, -3.8239, 1.0986),
        (3.5715, -3.6700, 1.0985),
        (3.4220, -3.5202, 1.0983),
        (3.2774, -3.3757, 1.0983),
        (3.1288, -3.2227, 1.0939),
        (2.7203, -2.6642, 0.9439),
    ]:
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X
    
    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=15/self.head_dim).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=(768**0.5)/448, w_s=2**-9, grad_s=1/448)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x).float()
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq, reduction='sum' if self.training else 'mean')
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 48*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 1750 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=12, num_heads=6, model_dim=768,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.22/768**0.5), dict(params=embed_params, lr=0.6), dict(params=scalar_params, lr=0.04)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.05, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        w = (1 - x) / args.cooldown_frac
        return w * 1.0 + (1 - w) * 0.1

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
for _ in range(warmup_steps):
    inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device="cuda")
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sun Feb 16 18:31:31 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   38C    P0            118W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   30C    P0            121W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0            115W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            117W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0            119W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0            111W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            114W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/1750 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/1750 train_time:65ms step_avg:65.26ms
step:2/1750 train_time:143ms step_avg:71.71ms
step:3/1750 train_time:236ms step_avg:78.63ms
step:4/1750 train_time:332ms step_avg:82.98ms
step:5/1750 train_time:428ms step_avg:85.66ms
step:6/1750 train_time:524ms step_avg:87.39ms
step:7/1750 train_time:621ms step_avg:88.67ms
step:8/1750 train_time:718ms step_avg:89.73ms
step:9/1750 train_time:814ms step_avg:90.46ms
step:10/1750 train_time:911ms step_avg:91.14ms
step:11/1750 train_time:1008ms step_avg:91.64ms
step:12/1750 train_time:1106ms step_avg:92.14ms
step:13/1750 train_time:1205ms step_avg:92.66ms
step:14/1750 train_time:1300ms step_avg:92.88ms
step:15/1750 train_time:1397ms step_avg:93.13ms
step:16/1750 train_time:1494ms step_avg:93.38ms
step:17/1750 train_time:1592ms step_avg:93.64ms
step:18/1750 train_time:1689ms step_avg:93.84ms
step:19/1750 train_time:1787ms step_avg:94.03ms
step:20/1750 train_time:1883ms step_avg:94.14ms
step:21/1750 train_time:1980ms step_avg:94.27ms
step:22/1750 train_time:2077ms step_avg:94.39ms
step:23/1750 train_time:2174ms step_avg:94.50ms
step:24/1750 train_time:2272ms step_avg:94.67ms
step:25/1750 train_time:2369ms step_avg:94.76ms
step:26/1750 train_time:2466ms step_avg:94.85ms
step:27/1750 train_time:2563ms step_avg:94.92ms
step:28/1750 train_time:2660ms step_avg:94.98ms
step:29/1750 train_time:2756ms step_avg:95.04ms
step:30/1750 train_time:2854ms step_avg:95.15ms
step:31/1750 train_time:2951ms step_avg:95.19ms
step:32/1750 train_time:3048ms step_avg:95.24ms
step:33/1750 train_time:3145ms step_avg:95.30ms
step:34/1750 train_time:3242ms step_avg:95.35ms
step:35/1750 train_time:3338ms step_avg:95.38ms
step:36/1750 train_time:3435ms step_avg:95.41ms
step:37/1750 train_time:3532ms step_avg:95.45ms
step:38/1750 train_time:3629ms step_avg:95.49ms
step:39/1750 train_time:3725ms step_avg:95.51ms
step:40/1750 train_time:3821ms step_avg:95.53ms
step:41/1750 train_time:3919ms step_avg:95.59ms
step:42/1750 train_time:4015ms step_avg:95.59ms
step:43/1750 train_time:4112ms step_avg:95.63ms
step:44/1750 train_time:4208ms step_avg:95.64ms
step:45/1750 train_time:4305ms step_avg:95.68ms
step:46/1750 train_time:4402ms step_avg:95.69ms
step:47/1750 train_time:4499ms step_avg:95.73ms
step:48/1750 train_time:4595ms step_avg:95.74ms
step:49/1750 train_time:4693ms step_avg:95.77ms
step:50/1750 train_time:4790ms step_avg:95.81ms
step:51/1750 train_time:4887ms step_avg:95.82ms
step:52/1750 train_time:4984ms step_avg:95.85ms
step:53/1750 train_time:5082ms step_avg:95.89ms
step:54/1750 train_time:5178ms step_avg:95.89ms
step:55/1750 train_time:5275ms step_avg:95.91ms
step:56/1750 train_time:5373ms step_avg:95.95ms
step:57/1750 train_time:5471ms step_avg:95.99ms
step:58/1750 train_time:5568ms step_avg:95.99ms
step:59/1750 train_time:5665ms step_avg:96.01ms
step:60/1750 train_time:5761ms step_avg:96.01ms
step:61/1750 train_time:5858ms step_avg:96.03ms
step:62/1750 train_time:5955ms step_avg:96.04ms
step:63/1750 train_time:6052ms step_avg:96.06ms
step:64/1750 train_time:6148ms step_avg:96.06ms
step:65/1750 train_time:6245ms step_avg:96.07ms
step:66/1750 train_time:6341ms step_avg:96.07ms
step:67/1750 train_time:6437ms step_avg:96.08ms
step:68/1750 train_time:6534ms step_avg:96.08ms
step:69/1750 train_time:6631ms step_avg:96.11ms
step:70/1750 train_time:6728ms step_avg:96.12ms
step:71/1750 train_time:6825ms step_avg:96.13ms
step:72/1750 train_time:6923ms step_avg:96.16ms
step:73/1750 train_time:7019ms step_avg:96.15ms
step:74/1750 train_time:7116ms step_avg:96.16ms
step:75/1750 train_time:7214ms step_avg:96.18ms
step:76/1750 train_time:7311ms step_avg:96.20ms
step:77/1750 train_time:7407ms step_avg:96.20ms
step:78/1750 train_time:7505ms step_avg:96.22ms
step:79/1750 train_time:7601ms step_avg:96.21ms
step:80/1750 train_time:7697ms step_avg:96.21ms
step:81/1750 train_time:7794ms step_avg:96.23ms
step:82/1750 train_time:7892ms step_avg:96.24ms
step:83/1750 train_time:7989ms step_avg:96.25ms
step:84/1750 train_time:8085ms step_avg:96.25ms
step:85/1750 train_time:8182ms step_avg:96.26ms
step:86/1750 train_time:8279ms step_avg:96.27ms
step:87/1750 train_time:8375ms step_avg:96.26ms
step:88/1750 train_time:8472ms step_avg:96.27ms
step:89/1750 train_time:8569ms step_avg:96.29ms
step:90/1750 train_time:8665ms step_avg:96.28ms
step:91/1750 train_time:8762ms step_avg:96.29ms
step:92/1750 train_time:8858ms step_avg:96.29ms
step:93/1750 train_time:8955ms step_avg:96.29ms
step:94/1750 train_time:9052ms step_avg:96.30ms
step:95/1750 train_time:9149ms step_avg:96.31ms
step:96/1750 train_time:9247ms step_avg:96.32ms
step:97/1750 train_time:9344ms step_avg:96.33ms
step:98/1750 train_time:9440ms step_avg:96.33ms
step:99/1750 train_time:9537ms step_avg:96.33ms
step:100/1750 train_time:9634ms step_avg:96.34ms
step:101/1750 train_time:9731ms step_avg:96.35ms
step:102/1750 train_time:9827ms step_avg:96.34ms
step:103/1750 train_time:9923ms step_avg:96.34ms
step:104/1750 train_time:10020ms step_avg:96.35ms
step:105/1750 train_time:10116ms step_avg:96.35ms
step:106/1750 train_time:10214ms step_avg:96.35ms
step:107/1750 train_time:10312ms step_avg:96.37ms
step:108/1750 train_time:10410ms step_avg:96.39ms
step:109/1750 train_time:10507ms step_avg:96.40ms
step:110/1750 train_time:10604ms step_avg:96.40ms
step:111/1750 train_time:10700ms step_avg:96.40ms
step:112/1750 train_time:10797ms step_avg:96.40ms
step:113/1750 train_time:10896ms step_avg:96.42ms
step:114/1750 train_time:10991ms step_avg:96.41ms
step:115/1750 train_time:11087ms step_avg:96.41ms
step:116/1750 train_time:11184ms step_avg:96.41ms
step:117/1750 train_time:11281ms step_avg:96.41ms
step:118/1750 train_time:11377ms step_avg:96.42ms
step:119/1750 train_time:11474ms step_avg:96.42ms
step:120/1750 train_time:11572ms step_avg:96.43ms
step:121/1750 train_time:11671ms step_avg:96.45ms
step:122/1750 train_time:12106ms step_avg:99.23ms
step:123/1750 train_time:12161ms step_avg:98.87ms
step:124/1750 train_time:12256ms step_avg:98.84ms
step:125/1750 train_time:12353ms step_avg:98.82ms
step:125/1750 val_loss:4.6310 train_time:12443ms step_avg:99.55ms
step:126/1750 train_time:12464ms step_avg:98.92ms
step:127/1750 train_time:12551ms step_avg:98.83ms
step:128/1750 train_time:12652ms step_avg:98.85ms
step:129/1750 train_time:12750ms step_avg:98.83ms
step:130/1750 train_time:12847ms step_avg:98.82ms
step:131/1750 train_time:12943ms step_avg:98.80ms
step:132/1750 train_time:13040ms step_avg:98.79ms
step:133/1750 train_time:13137ms step_avg:98.78ms
step:134/1750 train_time:13235ms step_avg:98.77ms
step:135/1750 train_time:13332ms step_avg:98.76ms
step:136/1750 train_time:13429ms step_avg:98.75ms
step:137/1750 train_time:13527ms step_avg:98.74ms
step:138/1750 train_time:13624ms step_avg:98.72ms
step:139/1750 train_time:13721ms step_avg:98.71ms
step:140/1750 train_time:13819ms step_avg:98.71ms
step:141/1750 train_time:13918ms step_avg:98.71ms
step:142/1750 train_time:14016ms step_avg:98.70ms
step:143/1750 train_time:14112ms step_avg:98.69ms
step:144/1750 train_time:14209ms step_avg:98.68ms
step:145/1750 train_time:14307ms step_avg:98.67ms
step:146/1750 train_time:14403ms step_avg:98.65ms
step:147/1750 train_time:14500ms step_avg:98.64ms
step:148/1750 train_time:14599ms step_avg:98.64ms
step:149/1750 train_time:14698ms step_avg:98.64ms
step:150/1750 train_time:14796ms step_avg:98.64ms
step:151/1750 train_time:14894ms step_avg:98.63ms
step:152/1750 train_time:14992ms step_avg:98.63ms
step:153/1750 train_time:15089ms step_avg:98.62ms
step:154/1750 train_time:15185ms step_avg:98.61ms
step:155/1750 train_time:15283ms step_avg:98.60ms
step:156/1750 train_time:15380ms step_avg:98.59ms
step:157/1750 train_time:15478ms step_avg:98.58ms
step:158/1750 train_time:15576ms step_avg:98.58ms
step:159/1750 train_time:15673ms step_avg:98.57ms
step:160/1750 train_time:15770ms step_avg:98.56ms
step:161/1750 train_time:15867ms step_avg:98.56ms
step:162/1750 train_time:15965ms step_avg:98.55ms
step:163/1750 train_time:16062ms step_avg:98.54ms
step:164/1750 train_time:16160ms step_avg:98.53ms
step:165/1750 train_time:16258ms step_avg:98.53ms
step:166/1750 train_time:16355ms step_avg:98.53ms
step:167/1750 train_time:16452ms step_avg:98.52ms
step:168/1750 train_time:16550ms step_avg:98.51ms
step:169/1750 train_time:16647ms step_avg:98.50ms
step:170/1750 train_time:16744ms step_avg:98.50ms
step:171/1750 train_time:16842ms step_avg:98.49ms
step:172/1750 train_time:16940ms step_avg:98.49ms
step:173/1750 train_time:17038ms step_avg:98.48ms
step:174/1750 train_time:17136ms step_avg:98.48ms
step:175/1750 train_time:17233ms step_avg:98.48ms
step:176/1750 train_time:17330ms step_avg:98.47ms
step:177/1750 train_time:17427ms step_avg:98.46ms
step:178/1750 train_time:17524ms step_avg:98.45ms
step:179/1750 train_time:17621ms step_avg:98.44ms
step:180/1750 train_time:17719ms step_avg:98.44ms
step:181/1750 train_time:17817ms step_avg:98.44ms
step:182/1750 train_time:17915ms step_avg:98.43ms
step:183/1750 train_time:18012ms step_avg:98.42ms
step:184/1750 train_time:18109ms step_avg:98.42ms
step:185/1750 train_time:18205ms step_avg:98.41ms
step:186/1750 train_time:18302ms step_avg:98.40ms
step:187/1750 train_time:18400ms step_avg:98.39ms
step:188/1750 train_time:18498ms step_avg:98.39ms
step:189/1750 train_time:18596ms step_avg:98.39ms
step:190/1750 train_time:18694ms step_avg:98.39ms
step:191/1750 train_time:18792ms step_avg:98.39ms
step:192/1750 train_time:18890ms step_avg:98.39ms
step:193/1750 train_time:18987ms step_avg:98.38ms
step:194/1750 train_time:19084ms step_avg:98.37ms
step:195/1750 train_time:19182ms step_avg:98.37ms
step:196/1750 train_time:19279ms step_avg:98.36ms
step:197/1750 train_time:19377ms step_avg:98.36ms
step:198/1750 train_time:19475ms step_avg:98.36ms
step:199/1750 train_time:19572ms step_avg:98.35ms
step:200/1750 train_time:19669ms step_avg:98.34ms
step:201/1750 train_time:19766ms step_avg:98.34ms
step:202/1750 train_time:19863ms step_avg:98.33ms
step:203/1750 train_time:19960ms step_avg:98.33ms
step:204/1750 train_time:20058ms step_avg:98.32ms
step:205/1750 train_time:20157ms step_avg:98.33ms
step:206/1750 train_time:20255ms step_avg:98.33ms
step:207/1750 train_time:20353ms step_avg:98.32ms
step:208/1750 train_time:20450ms step_avg:98.32ms
step:209/1750 train_time:20546ms step_avg:98.31ms
step:210/1750 train_time:20643ms step_avg:98.30ms
step:211/1750 train_time:20741ms step_avg:98.30ms
step:212/1750 train_time:20839ms step_avg:98.30ms
step:213/1750 train_time:20937ms step_avg:98.29ms
step:214/1750 train_time:21034ms step_avg:98.29ms
step:215/1750 train_time:21131ms step_avg:98.28ms
step:216/1750 train_time:21228ms step_avg:98.28ms
step:217/1750 train_time:21325ms step_avg:98.27ms
step:218/1750 train_time:21422ms step_avg:98.27ms
step:219/1750 train_time:21519ms step_avg:98.26ms
step:220/1750 train_time:21617ms step_avg:98.26ms
step:221/1750 train_time:21714ms step_avg:98.26ms
step:222/1750 train_time:21812ms step_avg:98.25ms
step:223/1750 train_time:21908ms step_avg:98.24ms
step:224/1750 train_time:22005ms step_avg:98.24ms
step:225/1750 train_time:22103ms step_avg:98.23ms
step:226/1750 train_time:22201ms step_avg:98.24ms
step:227/1750 train_time:22298ms step_avg:98.23ms
step:228/1750 train_time:22397ms step_avg:98.23ms
step:229/1750 train_time:22496ms step_avg:98.24ms
step:230/1750 train_time:22594ms step_avg:98.24ms
step:231/1750 train_time:22692ms step_avg:98.23ms
step:232/1750 train_time:22789ms step_avg:98.23ms
step:233/1750 train_time:22885ms step_avg:98.22ms
step:234/1750 train_time:22983ms step_avg:98.22ms
step:235/1750 train_time:23081ms step_avg:98.22ms
step:236/1750 train_time:23179ms step_avg:98.21ms
step:237/1750 train_time:23277ms step_avg:98.21ms
step:238/1750 train_time:23374ms step_avg:98.21ms
step:239/1750 train_time:23471ms step_avg:98.21ms
step:240/1750 train_time:23568ms step_avg:98.20ms
step:241/1750 train_time:23666ms step_avg:98.20ms
step:242/1750 train_time:23763ms step_avg:98.19ms
step:243/1750 train_time:23861ms step_avg:98.19ms
step:244/1750 train_time:23958ms step_avg:98.19ms
step:245/1750 train_time:24056ms step_avg:98.19ms
step:246/1750 train_time:24155ms step_avg:98.19ms
step:247/1750 train_time:24252ms step_avg:98.19ms
step:248/1750 train_time:24349ms step_avg:98.18ms
step:249/1750 train_time:24446ms step_avg:98.18ms
step:250/1750 train_time:24543ms step_avg:98.17ms
step:250/1750 val_loss:4.1155 train_time:24635ms step_avg:98.54ms
step:251/1750 train_time:24656ms step_avg:98.23ms
step:252/1750 train_time:24742ms step_avg:98.18ms
step:253/1750 train_time:24840ms step_avg:98.18ms
step:254/1750 train_time:24938ms step_avg:98.18ms
step:255/1750 train_time:25036ms step_avg:98.18ms
step:256/1750 train_time:25132ms step_avg:98.17ms
step:257/1750 train_time:25229ms step_avg:98.17ms
step:258/1750 train_time:25327ms step_avg:98.17ms
step:259/1750 train_time:25424ms step_avg:98.16ms
step:260/1750 train_time:25522ms step_avg:98.16ms
step:261/1750 train_time:25619ms step_avg:98.16ms
step:262/1750 train_time:25716ms step_avg:98.15ms
step:263/1750 train_time:25813ms step_avg:98.15ms
step:264/1750 train_time:25911ms step_avg:98.15ms
step:265/1750 train_time:26010ms step_avg:98.15ms
step:266/1750 train_time:26108ms step_avg:98.15ms
step:267/1750 train_time:26206ms step_avg:98.15ms
step:268/1750 train_time:26305ms step_avg:98.15ms
step:269/1750 train_time:26404ms step_avg:98.16ms
step:270/1750 train_time:26502ms step_avg:98.16ms
step:271/1750 train_time:26600ms step_avg:98.15ms
step:272/1750 train_time:26697ms step_avg:98.15ms
step:273/1750 train_time:26795ms step_avg:98.15ms
step:274/1750 train_time:26892ms step_avg:98.15ms
step:275/1750 train_time:26991ms step_avg:98.15ms
step:276/1750 train_time:27089ms step_avg:98.15ms
step:277/1750 train_time:27188ms step_avg:98.15ms
step:278/1750 train_time:27287ms step_avg:98.16ms
step:279/1750 train_time:27387ms step_avg:98.16ms
step:280/1750 train_time:27484ms step_avg:98.16ms
step:281/1750 train_time:27582ms step_avg:98.16ms
step:282/1750 train_time:27681ms step_avg:98.16ms
step:283/1750 train_time:27780ms step_avg:98.16ms
step:284/1750 train_time:27877ms step_avg:98.16ms
step:285/1750 train_time:27975ms step_avg:98.16ms
step:286/1750 train_time:28073ms step_avg:98.16ms
step:287/1750 train_time:28171ms step_avg:98.16ms
step:288/1750 train_time:28269ms step_avg:98.15ms
step:289/1750 train_time:28368ms step_avg:98.16ms
step:290/1750 train_time:28465ms step_avg:98.16ms
step:291/1750 train_time:28563ms step_avg:98.15ms
step:292/1750 train_time:28660ms step_avg:98.15ms
step:293/1750 train_time:28759ms step_avg:98.15ms
step:294/1750 train_time:28856ms step_avg:98.15ms
step:295/1750 train_time:28954ms step_avg:98.15ms
step:296/1750 train_time:29051ms step_avg:98.15ms
step:297/1750 train_time:29149ms step_avg:98.15ms
step:298/1750 train_time:29249ms step_avg:98.15ms
step:299/1750 train_time:29347ms step_avg:98.15ms
step:300/1750 train_time:29444ms step_avg:98.15ms
step:301/1750 train_time:29542ms step_avg:98.15ms
step:302/1750 train_time:29640ms step_avg:98.15ms
step:303/1750 train_time:29738ms step_avg:98.14ms
step:304/1750 train_time:29836ms step_avg:98.14ms
step:305/1750 train_time:29933ms step_avg:98.14ms
step:306/1750 train_time:30031ms step_avg:98.14ms
step:307/1750 train_time:30129ms step_avg:98.14ms
step:308/1750 train_time:30228ms step_avg:98.14ms
step:309/1750 train_time:30325ms step_avg:98.14ms
step:310/1750 train_time:30423ms step_avg:98.14ms
step:311/1750 train_time:30520ms step_avg:98.14ms
step:312/1750 train_time:30618ms step_avg:98.13ms
step:313/1750 train_time:30715ms step_avg:98.13ms
step:314/1750 train_time:30814ms step_avg:98.13ms
step:315/1750 train_time:30912ms step_avg:98.13ms
step:316/1750 train_time:31010ms step_avg:98.13ms
step:317/1750 train_time:31108ms step_avg:98.13ms
step:318/1750 train_time:31207ms step_avg:98.13ms
step:319/1750 train_time:31305ms step_avg:98.13ms
step:320/1750 train_time:31402ms step_avg:98.13ms
step:321/1750 train_time:31500ms step_avg:98.13ms
step:322/1750 train_time:31598ms step_avg:98.13ms
step:323/1750 train_time:31695ms step_avg:98.13ms
step:324/1750 train_time:31792ms step_avg:98.12ms
step:325/1750 train_time:31890ms step_avg:98.12ms
step:326/1750 train_time:31988ms step_avg:98.12ms
step:327/1750 train_time:32088ms step_avg:98.13ms
step:328/1750 train_time:32188ms step_avg:98.13ms
step:329/1750 train_time:32285ms step_avg:98.13ms
step:330/1750 train_time:32384ms step_avg:98.13ms
step:331/1750 train_time:32482ms step_avg:98.13ms
step:332/1750 train_time:32580ms step_avg:98.13ms
step:333/1750 train_time:32677ms step_avg:98.13ms
step:334/1750 train_time:32775ms step_avg:98.13ms
step:335/1750 train_time:32872ms step_avg:98.13ms
step:336/1750 train_time:32970ms step_avg:98.13ms
step:337/1750 train_time:33068ms step_avg:98.12ms
step:338/1750 train_time:33167ms step_avg:98.13ms
step:339/1750 train_time:33265ms step_avg:98.13ms
step:340/1750 train_time:33362ms step_avg:98.12ms
step:341/1750 train_time:33460ms step_avg:98.12ms
step:342/1750 train_time:33558ms step_avg:98.12ms
step:343/1750 train_time:33655ms step_avg:98.12ms
step:344/1750 train_time:33753ms step_avg:98.12ms
step:345/1750 train_time:33851ms step_avg:98.12ms
step:346/1750 train_time:33948ms step_avg:98.12ms
step:347/1750 train_time:34047ms step_avg:98.12ms
step:348/1750 train_time:34145ms step_avg:98.12ms
step:349/1750 train_time:34244ms step_avg:98.12ms
step:350/1750 train_time:34341ms step_avg:98.12ms
step:351/1750 train_time:34439ms step_avg:98.12ms
step:352/1750 train_time:34536ms step_avg:98.11ms
step:353/1750 train_time:34634ms step_avg:98.11ms
step:354/1750 train_time:34731ms step_avg:98.11ms
step:355/1750 train_time:34828ms step_avg:98.11ms
step:356/1750 train_time:34926ms step_avg:98.11ms
step:357/1750 train_time:35024ms step_avg:98.11ms
step:358/1750 train_time:35121ms step_avg:98.10ms
step:359/1750 train_time:35219ms step_avg:98.10ms
step:360/1750 train_time:35316ms step_avg:98.10ms
step:361/1750 train_time:35413ms step_avg:98.10ms
step:362/1750 train_time:35511ms step_avg:98.10ms
step:363/1750 train_time:35610ms step_avg:98.10ms
step:364/1750 train_time:35708ms step_avg:98.10ms
step:365/1750 train_time:35807ms step_avg:98.10ms
step:366/1750 train_time:35905ms step_avg:98.10ms
step:367/1750 train_time:36003ms step_avg:98.10ms
step:368/1750 train_time:36100ms step_avg:98.10ms
step:369/1750 train_time:36198ms step_avg:98.10ms
step:370/1750 train_time:36296ms step_avg:98.10ms
step:371/1750 train_time:36393ms step_avg:98.09ms
step:372/1750 train_time:36490ms step_avg:98.09ms
step:373/1750 train_time:36589ms step_avg:98.09ms
step:374/1750 train_time:36687ms step_avg:98.09ms
step:375/1750 train_time:36784ms step_avg:98.09ms
step:375/1750 val_loss:3.9028 train_time:36876ms step_avg:98.34ms
step:376/1750 train_time:36896ms step_avg:98.13ms
step:377/1750 train_time:36989ms step_avg:98.11ms
step:378/1750 train_time:37089ms step_avg:98.12ms
step:379/1750 train_time:37188ms step_avg:98.12ms
step:380/1750 train_time:37286ms step_avg:98.12ms
step:381/1750 train_time:37383ms step_avg:98.12ms
step:382/1750 train_time:37482ms step_avg:98.12ms
step:383/1750 train_time:37578ms step_avg:98.12ms
step:384/1750 train_time:37676ms step_avg:98.12ms
step:385/1750 train_time:37774ms step_avg:98.11ms
step:386/1750 train_time:37872ms step_avg:98.11ms
step:387/1750 train_time:37971ms step_avg:98.12ms
step:388/1750 train_time:38071ms step_avg:98.12ms
step:389/1750 train_time:38170ms step_avg:98.12ms
step:390/1750 train_time:38268ms step_avg:98.12ms
step:391/1750 train_time:38368ms step_avg:98.13ms
step:392/1750 train_time:38469ms step_avg:98.13ms
step:393/1750 train_time:38570ms step_avg:98.14ms
step:394/1750 train_time:38670ms step_avg:98.15ms
step:395/1750 train_time:38770ms step_avg:98.15ms
step:396/1750 train_time:38871ms step_avg:98.16ms
step:397/1750 train_time:38971ms step_avg:98.16ms
step:398/1750 train_time:39072ms step_avg:98.17ms
step:399/1750 train_time:39173ms step_avg:98.18ms
step:400/1750 train_time:39274ms step_avg:98.18ms
step:401/1750 train_time:39374ms step_avg:98.19ms
step:402/1750 train_time:39475ms step_avg:98.20ms
step:403/1750 train_time:39575ms step_avg:98.20ms
step:404/1750 train_time:39676ms step_avg:98.21ms
step:405/1750 train_time:39775ms step_avg:98.21ms
step:406/1750 train_time:39876ms step_avg:98.22ms
step:407/1750 train_time:39976ms step_avg:98.22ms
step:408/1750 train_time:40076ms step_avg:98.22ms
step:409/1750 train_time:40176ms step_avg:98.23ms
step:410/1750 train_time:40276ms step_avg:98.23ms
step:411/1750 train_time:40376ms step_avg:98.24ms
step:412/1750 train_time:40476ms step_avg:98.24ms
step:413/1750 train_time:40576ms step_avg:98.25ms
step:414/1750 train_time:40676ms step_avg:98.25ms
step:415/1750 train_time:40776ms step_avg:98.25ms
step:416/1750 train_time:40876ms step_avg:98.26ms
step:417/1750 train_time:40977ms step_avg:98.27ms
step:418/1750 train_time:41077ms step_avg:98.27ms
step:419/1750 train_time:41177ms step_avg:98.27ms
step:420/1750 train_time:41277ms step_avg:98.28ms
step:421/1750 train_time:41377ms step_avg:98.28ms
step:422/1750 train_time:41477ms step_avg:98.29ms
step:423/1750 train_time:41577ms step_avg:98.29ms
step:424/1750 train_time:41677ms step_avg:98.30ms
step:425/1750 train_time:41777ms step_avg:98.30ms
step:426/1750 train_time:41878ms step_avg:98.30ms
step:427/1750 train_time:41977ms step_avg:98.31ms
step:428/1750 train_time:42077ms step_avg:98.31ms
step:429/1750 train_time:42177ms step_avg:98.31ms
step:430/1750 train_time:42277ms step_avg:98.32ms
step:431/1750 train_time:42377ms step_avg:98.32ms
step:432/1750 train_time:42478ms step_avg:98.33ms
step:433/1750 train_time:42578ms step_avg:98.33ms
step:434/1750 train_time:42678ms step_avg:98.34ms
step:435/1750 train_time:42777ms step_avg:98.34ms
step:436/1750 train_time:42878ms step_avg:98.34ms
step:437/1750 train_time:42978ms step_avg:98.35ms
step:438/1750 train_time:43077ms step_avg:98.35ms
step:439/1750 train_time:43177ms step_avg:98.35ms
step:440/1750 train_time:43278ms step_avg:98.36ms
step:441/1750 train_time:43378ms step_avg:98.36ms
step:442/1750 train_time:43478ms step_avg:98.37ms
step:443/1750 train_time:43578ms step_avg:98.37ms
step:444/1750 train_time:43678ms step_avg:98.37ms
step:445/1750 train_time:43778ms step_avg:98.38ms
step:446/1750 train_time:43878ms step_avg:98.38ms
step:447/1750 train_time:43978ms step_avg:98.38ms
step:448/1750 train_time:44078ms step_avg:98.39ms
step:449/1750 train_time:44178ms step_avg:98.39ms
step:450/1750 train_time:44278ms step_avg:98.39ms
step:451/1750 train_time:44378ms step_avg:98.40ms
step:452/1750 train_time:44478ms step_avg:98.40ms
step:453/1750 train_time:44577ms step_avg:98.40ms
step:454/1750 train_time:44678ms step_avg:98.41ms
step:455/1750 train_time:44777ms step_avg:98.41ms
step:456/1750 train_time:44878ms step_avg:98.42ms
step:457/1750 train_time:44977ms step_avg:98.42ms
step:458/1750 train_time:45077ms step_avg:98.42ms
step:459/1750 train_time:45178ms step_avg:98.43ms
step:460/1750 train_time:45277ms step_avg:98.43ms
step:461/1750 train_time:45377ms step_avg:98.43ms
step:462/1750 train_time:45477ms step_avg:98.44ms
step:463/1750 train_time:45577ms step_avg:98.44ms
step:464/1750 train_time:45677ms step_avg:98.44ms
step:465/1750 train_time:45777ms step_avg:98.44ms
step:466/1750 train_time:45877ms step_avg:98.45ms
step:467/1750 train_time:45977ms step_avg:98.45ms
step:468/1750 train_time:46076ms step_avg:98.45ms
step:469/1750 train_time:46176ms step_avg:98.46ms
step:470/1750 train_time:46276ms step_avg:98.46ms
step:471/1750 train_time:46378ms step_avg:98.47ms
step:472/1750 train_time:46477ms step_avg:98.47ms
step:473/1750 train_time:46577ms step_avg:98.47ms
step:474/1750 train_time:46677ms step_avg:98.47ms
step:475/1750 train_time:46777ms step_avg:98.48ms
step:476/1750 train_time:46877ms step_avg:98.48ms
step:477/1750 train_time:46977ms step_avg:98.48ms
step:478/1750 train_time:47078ms step_avg:98.49ms
step:479/1750 train_time:47178ms step_avg:98.49ms
step:480/1750 train_time:47278ms step_avg:98.50ms
step:481/1750 train_time:47379ms step_avg:98.50ms
step:482/1750 train_time:47479ms step_avg:98.50ms
step:483/1750 train_time:47578ms step_avg:98.51ms
step:484/1750 train_time:47678ms step_avg:98.51ms
step:485/1750 train_time:47778ms step_avg:98.51ms
step:486/1750 train_time:47877ms step_avg:98.51ms
step:487/1750 train_time:47977ms step_avg:98.52ms
step:488/1750 train_time:48077ms step_avg:98.52ms
step:489/1750 train_time:48177ms step_avg:98.52ms
step:490/1750 train_time:48277ms step_avg:98.52ms
step:491/1750 train_time:48377ms step_avg:98.53ms
step:492/1750 train_time:48478ms step_avg:98.53ms
step:493/1750 train_time:48578ms step_avg:98.54ms
step:494/1750 train_time:48678ms step_avg:98.54ms
step:495/1750 train_time:48778ms step_avg:98.54ms
step:496/1750 train_time:48878ms step_avg:98.54ms
step:497/1750 train_time:48977ms step_avg:98.55ms
step:498/1750 train_time:49077ms step_avg:98.55ms
step:499/1750 train_time:49177ms step_avg:98.55ms
step:500/1750 train_time:49278ms step_avg:98.56ms
step:500/1750 val_loss:3.7506 train_time:49371ms step_avg:98.74ms
step:501/1750 train_time:49392ms step_avg:98.59ms
step:502/1750 train_time:49485ms step_avg:98.58ms
step:503/1750 train_time:49590ms step_avg:98.59ms
step:504/1750 train_time:49690ms step_avg:98.59ms
step:505/1750 train_time:49789ms step_avg:98.59ms
step:506/1750 train_time:49888ms step_avg:98.59ms
step:507/1750 train_time:49988ms step_avg:98.60ms
step:508/1750 train_time:50089ms step_avg:98.60ms
step:509/1750 train_time:50189ms step_avg:98.60ms
step:510/1750 train_time:50288ms step_avg:98.60ms
step:511/1750 train_time:50388ms step_avg:98.61ms
step:512/1750 train_time:50489ms step_avg:98.61ms
step:513/1750 train_time:50590ms step_avg:98.62ms
step:514/1750 train_time:50691ms step_avg:98.62ms
step:515/1750 train_time:50790ms step_avg:98.62ms
step:516/1750 train_time:50890ms step_avg:98.62ms
step:517/1750 train_time:50990ms step_avg:98.63ms
step:518/1750 train_time:51089ms step_avg:98.63ms
step:519/1750 train_time:51189ms step_avg:98.63ms
step:520/1750 train_time:51289ms step_avg:98.63ms
step:521/1750 train_time:51390ms step_avg:98.64ms
step:522/1750 train_time:51489ms step_avg:98.64ms
step:523/1750 train_time:51590ms step_avg:98.64ms
step:524/1750 train_time:51690ms step_avg:98.64ms
step:525/1750 train_time:51790ms step_avg:98.65ms
step:526/1750 train_time:51891ms step_avg:98.65ms
step:527/1750 train_time:51992ms step_avg:98.66ms
step:528/1750 train_time:52092ms step_avg:98.66ms
step:529/1750 train_time:52192ms step_avg:98.66ms
step:530/1750 train_time:52293ms step_avg:98.67ms
step:531/1750 train_time:52393ms step_avg:98.67ms
step:532/1750 train_time:52493ms step_avg:98.67ms
step:533/1750 train_time:52594ms step_avg:98.68ms
step:534/1750 train_time:52694ms step_avg:98.68ms
step:535/1750 train_time:52794ms step_avg:98.68ms
step:536/1750 train_time:52893ms step_avg:98.68ms
step:537/1750 train_time:52994ms step_avg:98.68ms
step:538/1750 train_time:53093ms step_avg:98.69ms
step:539/1750 train_time:53195ms step_avg:98.69ms
step:540/1750 train_time:53294ms step_avg:98.69ms
step:541/1750 train_time:53394ms step_avg:98.70ms
step:542/1750 train_time:53494ms step_avg:98.70ms
step:543/1750 train_time:53595ms step_avg:98.70ms
step:544/1750 train_time:53694ms step_avg:98.70ms
step:545/1750 train_time:53794ms step_avg:98.70ms
step:546/1750 train_time:53893ms step_avg:98.71ms
step:547/1750 train_time:53993ms step_avg:98.71ms
step:548/1750 train_time:54094ms step_avg:98.71ms
step:549/1750 train_time:54194ms step_avg:98.71ms
step:550/1750 train_time:54294ms step_avg:98.72ms
step:551/1750 train_time:54394ms step_avg:98.72ms
step:552/1750 train_time:54494ms step_avg:98.72ms
step:553/1750 train_time:54594ms step_avg:98.72ms
step:554/1750 train_time:54694ms step_avg:98.73ms
step:555/1750 train_time:54794ms step_avg:98.73ms
step:556/1750 train_time:54893ms step_avg:98.73ms
step:557/1750 train_time:54993ms step_avg:98.73ms
step:558/1750 train_time:55093ms step_avg:98.73ms
step:559/1750 train_time:55193ms step_avg:98.73ms
step:560/1750 train_time:55292ms step_avg:98.74ms
step:561/1750 train_time:55393ms step_avg:98.74ms
step:562/1750 train_time:55493ms step_avg:98.74ms
step:563/1750 train_time:55594ms step_avg:98.75ms
step:564/1750 train_time:55694ms step_avg:98.75ms
step:565/1750 train_time:55794ms step_avg:98.75ms
step:566/1750 train_time:55894ms step_avg:98.75ms
step:567/1750 train_time:55994ms step_avg:98.75ms
step:568/1750 train_time:56094ms step_avg:98.76ms
step:569/1750 train_time:56193ms step_avg:98.76ms
step:570/1750 train_time:56293ms step_avg:98.76ms
step:571/1750 train_time:56394ms step_avg:98.76ms
step:572/1750 train_time:56494ms step_avg:98.77ms
step:573/1750 train_time:56595ms step_avg:98.77ms
step:574/1750 train_time:56696ms step_avg:98.77ms
step:575/1750 train_time:56796ms step_avg:98.78ms
step:576/1750 train_time:56896ms step_avg:98.78ms
step:577/1750 train_time:56996ms step_avg:98.78ms
step:578/1750 train_time:57096ms step_avg:98.78ms
step:579/1750 train_time:57196ms step_avg:98.78ms
step:580/1750 train_time:57295ms step_avg:98.79ms
step:581/1750 train_time:57395ms step_avg:98.79ms
step:582/1750 train_time:57494ms step_avg:98.79ms
step:583/1750 train_time:57595ms step_avg:98.79ms
step:584/1750 train_time:57695ms step_avg:98.79ms
step:585/1750 train_time:57796ms step_avg:98.80ms
step:586/1750 train_time:57896ms step_avg:98.80ms
step:587/1750 train_time:57995ms step_avg:98.80ms
step:588/1750 train_time:58096ms step_avg:98.80ms
step:589/1750 train_time:58196ms step_avg:98.80ms
step:590/1750 train_time:58296ms step_avg:98.81ms
step:591/1750 train_time:58396ms step_avg:98.81ms
step:592/1750 train_time:58496ms step_avg:98.81ms
step:593/1750 train_time:58595ms step_avg:98.81ms
step:594/1750 train_time:58695ms step_avg:98.81ms
step:595/1750 train_time:58796ms step_avg:98.82ms
step:596/1750 train_time:58895ms step_avg:98.82ms
step:597/1750 train_time:58995ms step_avg:98.82ms
step:598/1750 train_time:59095ms step_avg:98.82ms
step:599/1750 train_time:59195ms step_avg:98.82ms
step:600/1750 train_time:59296ms step_avg:98.83ms
step:601/1750 train_time:59395ms step_avg:98.83ms
step:602/1750 train_time:59494ms step_avg:98.83ms
step:603/1750 train_time:59595ms step_avg:98.83ms
step:604/1750 train_time:59695ms step_avg:98.83ms
step:605/1750 train_time:59795ms step_avg:98.83ms
step:606/1750 train_time:59895ms step_avg:98.84ms
step:607/1750 train_time:59995ms step_avg:98.84ms
step:608/1750 train_time:60095ms step_avg:98.84ms
step:609/1750 train_time:60194ms step_avg:98.84ms
step:610/1750 train_time:60294ms step_avg:98.84ms
step:611/1750 train_time:60394ms step_avg:98.84ms
step:612/1750 train_time:60493ms step_avg:98.85ms
step:613/1750 train_time:60594ms step_avg:98.85ms
step:614/1750 train_time:60694ms step_avg:98.85ms
step:615/1750 train_time:60794ms step_avg:98.85ms
step:616/1750 train_time:60895ms step_avg:98.86ms
step:617/1750 train_time:60995ms step_avg:98.86ms
step:618/1750 train_time:61096ms step_avg:98.86ms
step:619/1750 train_time:61195ms step_avg:98.86ms
step:620/1750 train_time:61295ms step_avg:98.86ms
step:621/1750 train_time:61395ms step_avg:98.86ms
step:622/1750 train_time:61495ms step_avg:98.87ms
step:623/1750 train_time:61595ms step_avg:98.87ms
step:624/1750 train_time:61695ms step_avg:98.87ms
step:625/1750 train_time:61795ms step_avg:98.87ms
step:625/1750 val_loss:3.6646 train_time:61890ms step_avg:99.02ms
step:626/1750 train_time:61910ms step_avg:98.90ms
step:627/1750 train_time:62002ms step_avg:98.89ms
step:628/1750 train_time:62103ms step_avg:98.89ms
step:629/1750 train_time:62203ms step_avg:98.89ms
step:630/1750 train_time:62303ms step_avg:98.89ms
step:631/1750 train_time:62403ms step_avg:98.90ms
step:632/1750 train_time:62503ms step_avg:98.90ms
step:633/1750 train_time:62603ms step_avg:98.90ms
step:634/1750 train_time:62703ms step_avg:98.90ms
step:635/1750 train_time:62803ms step_avg:98.90ms
step:636/1750 train_time:62904ms step_avg:98.91ms
step:637/1750 train_time:63005ms step_avg:98.91ms
step:638/1750 train_time:63106ms step_avg:98.91ms
step:639/1750 train_time:63206ms step_avg:98.91ms
step:640/1750 train_time:63305ms step_avg:98.91ms
step:641/1750 train_time:63405ms step_avg:98.92ms
step:642/1750 train_time:63504ms step_avg:98.92ms
step:643/1750 train_time:63604ms step_avg:98.92ms
step:644/1750 train_time:63703ms step_avg:98.92ms
step:645/1750 train_time:63802ms step_avg:98.92ms
step:646/1750 train_time:63902ms step_avg:98.92ms
step:647/1750 train_time:64003ms step_avg:98.92ms
step:648/1750 train_time:64103ms step_avg:98.93ms
step:649/1750 train_time:64204ms step_avg:98.93ms
step:650/1750 train_time:64304ms step_avg:98.93ms
step:651/1750 train_time:64406ms step_avg:98.93ms
step:652/1750 train_time:64507ms step_avg:98.94ms
step:653/1750 train_time:64608ms step_avg:98.94ms
step:654/1750 train_time:64709ms step_avg:98.94ms
step:655/1750 train_time:64810ms step_avg:98.95ms
step:656/1750 train_time:64911ms step_avg:98.95ms
step:657/1750 train_time:65012ms step_avg:98.95ms
step:658/1750 train_time:65113ms step_avg:98.96ms
step:659/1750 train_time:65214ms step_avg:98.96ms
step:660/1750 train_time:65317ms step_avg:98.96ms
step:661/1750 train_time:65419ms step_avg:98.97ms
step:662/1750 train_time:65521ms step_avg:98.97ms
step:663/1750 train_time:65623ms step_avg:98.98ms
step:664/1750 train_time:65724ms step_avg:98.98ms
step:665/1750 train_time:65826ms step_avg:98.99ms
step:666/1750 train_time:65927ms step_avg:98.99ms
step:667/1750 train_time:66028ms step_avg:98.99ms
step:668/1750 train_time:66129ms step_avg:99.00ms
step:669/1750 train_time:66231ms step_avg:99.00ms
step:670/1750 train_time:66332ms step_avg:99.00ms
step:671/1750 train_time:66435ms step_avg:99.01ms
step:672/1750 train_time:66539ms step_avg:99.02ms
step:673/1750 train_time:66640ms step_avg:99.02ms
step:674/1750 train_time:66742ms step_avg:99.02ms
step:675/1750 train_time:66843ms step_avg:99.03ms
step:676/1750 train_time:66944ms step_avg:99.03ms
step:677/1750 train_time:67045ms step_avg:99.03ms
step:678/1750 train_time:67147ms step_avg:99.04ms
step:679/1750 train_time:67247ms step_avg:99.04ms
step:680/1750 train_time:67349ms step_avg:99.04ms
step:681/1750 train_time:67450ms step_avg:99.05ms
step:682/1750 train_time:67551ms step_avg:99.05ms
step:683/1750 train_time:67652ms step_avg:99.05ms
step:684/1750 train_time:67754ms step_avg:99.06ms
step:685/1750 train_time:67856ms step_avg:99.06ms
step:686/1750 train_time:67958ms step_avg:99.06ms
step:687/1750 train_time:68062ms step_avg:99.07ms
step:688/1750 train_time:68164ms step_avg:99.07ms
step:689/1750 train_time:68265ms step_avg:99.08ms
step:690/1750 train_time:68367ms step_avg:99.08ms
step:691/1750 train_time:68468ms step_avg:99.09ms
step:692/1750 train_time:68569ms step_avg:99.09ms
step:693/1750 train_time:68670ms step_avg:99.09ms
step:694/1750 train_time:68772ms step_avg:99.09ms
step:695/1750 train_time:68873ms step_avg:99.10ms
step:696/1750 train_time:68974ms step_avg:99.10ms
step:697/1750 train_time:69077ms step_avg:99.11ms
step:698/1750 train_time:69181ms step_avg:99.11ms
step:699/1750 train_time:69283ms step_avg:99.12ms
step:700/1750 train_time:69385ms step_avg:99.12ms
step:701/1750 train_time:69487ms step_avg:99.13ms
step:702/1750 train_time:69588ms step_avg:99.13ms
step:703/1750 train_time:69689ms step_avg:99.13ms
step:704/1750 train_time:69790ms step_avg:99.13ms
step:705/1750 train_time:69892ms step_avg:99.14ms
step:706/1750 train_time:69993ms step_avg:99.14ms
step:707/1750 train_time:70095ms step_avg:99.14ms
step:708/1750 train_time:70198ms step_avg:99.15ms
step:709/1750 train_time:70301ms step_avg:99.15ms
step:710/1750 train_time:70403ms step_avg:99.16ms
step:711/1750 train_time:70504ms step_avg:99.16ms
step:712/1750 train_time:70605ms step_avg:99.16ms
step:713/1750 train_time:70706ms step_avg:99.17ms
step:714/1750 train_time:70808ms step_avg:99.17ms
step:715/1750 train_time:70910ms step_avg:99.17ms
step:716/1750 train_time:71011ms step_avg:99.18ms
step:717/1750 train_time:71112ms step_avg:99.18ms
step:718/1750 train_time:71214ms step_avg:99.18ms
step:719/1750 train_time:71316ms step_avg:99.19ms
step:720/1750 train_time:71420ms step_avg:99.19ms
step:721/1750 train_time:71522ms step_avg:99.20ms
step:722/1750 train_time:71623ms step_avg:99.20ms
step:723/1750 train_time:71725ms step_avg:99.20ms
step:724/1750 train_time:71826ms step_avg:99.21ms
step:725/1750 train_time:71928ms step_avg:99.21ms
step:726/1750 train_time:72029ms step_avg:99.21ms
step:727/1750 train_time:72130ms step_avg:99.22ms
step:728/1750 train_time:72231ms step_avg:99.22ms
step:729/1750 train_time:72332ms step_avg:99.22ms
step:730/1750 train_time:72434ms step_avg:99.22ms
step:731/1750 train_time:72537ms step_avg:99.23ms
step:732/1750 train_time:72640ms step_avg:99.23ms
step:733/1750 train_time:72742ms step_avg:99.24ms
step:734/1750 train_time:72844ms step_avg:99.24ms
step:735/1750 train_time:72945ms step_avg:99.25ms
step:736/1750 train_time:73047ms step_avg:99.25ms
step:737/1750 train_time:73148ms step_avg:99.25ms
step:738/1750 train_time:73249ms step_avg:99.25ms
step:739/1750 train_time:73350ms step_avg:99.26ms
step:740/1750 train_time:73451ms step_avg:99.26ms
step:741/1750 train_time:73552ms step_avg:99.26ms
step:742/1750 train_time:73654ms step_avg:99.26ms
step:743/1750 train_time:73757ms step_avg:99.27ms
step:744/1750 train_time:73861ms step_avg:99.28ms
step:745/1750 train_time:73963ms step_avg:99.28ms
step:746/1750 train_time:74065ms step_avg:99.28ms
step:747/1750 train_time:74166ms step_avg:99.28ms
step:748/1750 train_time:74268ms step_avg:99.29ms
step:749/1750 train_time:74369ms step_avg:99.29ms
step:750/1750 train_time:74470ms step_avg:99.29ms
step:750/1750 val_loss:3.5989 train_time:74566ms step_avg:99.42ms
step:751/1750 train_time:74585ms step_avg:99.31ms
step:752/1750 train_time:74683ms step_avg:99.31ms
step:753/1750 train_time:74785ms step_avg:99.32ms
step:754/1750 train_time:74887ms step_avg:99.32ms
step:755/1750 train_time:74988ms step_avg:99.32ms
step:756/1750 train_time:75088ms step_avg:99.32ms
step:757/1750 train_time:75189ms step_avg:99.32ms
step:758/1750 train_time:75290ms step_avg:99.33ms
step:759/1750 train_time:75390ms step_avg:99.33ms
step:760/1750 train_time:75492ms step_avg:99.33ms
step:761/1750 train_time:75592ms step_avg:99.33ms
step:762/1750 train_time:75694ms step_avg:99.34ms
step:763/1750 train_time:75797ms step_avg:99.34ms
step:764/1750 train_time:75899ms step_avg:99.34ms
step:765/1750 train_time:76001ms step_avg:99.35ms
step:766/1750 train_time:76104ms step_avg:99.35ms
step:767/1750 train_time:76206ms step_avg:99.36ms
step:768/1750 train_time:76308ms step_avg:99.36ms
step:769/1750 train_time:76410ms step_avg:99.36ms
step:770/1750 train_time:76511ms step_avg:99.36ms
step:771/1750 train_time:76612ms step_avg:99.37ms
step:772/1750 train_time:76713ms step_avg:99.37ms
step:773/1750 train_time:76814ms step_avg:99.37ms
step:774/1750 train_time:76915ms step_avg:99.37ms
step:775/1750 train_time:77017ms step_avg:99.38ms
step:776/1750 train_time:77121ms step_avg:99.38ms
step:777/1750 train_time:77223ms step_avg:99.39ms
step:778/1750 train_time:77325ms step_avg:99.39ms
step:779/1750 train_time:77427ms step_avg:99.39ms
step:780/1750 train_time:77528ms step_avg:99.40ms
step:781/1750 train_time:77629ms step_avg:99.40ms
step:782/1750 train_time:77731ms step_avg:99.40ms
step:783/1750 train_time:77833ms step_avg:99.40ms
step:784/1750 train_time:77935ms step_avg:99.41ms
step:785/1750 train_time:78037ms step_avg:99.41ms
step:786/1750 train_time:78139ms step_avg:99.41ms
step:787/1750 train_time:78243ms step_avg:99.42ms
step:788/1750 train_time:78345ms step_avg:99.42ms
step:789/1750 train_time:78448ms step_avg:99.43ms
step:790/1750 train_time:78549ms step_avg:99.43ms
step:791/1750 train_time:78651ms step_avg:99.43ms
step:792/1750 train_time:78754ms step_avg:99.44ms
step:793/1750 train_time:78854ms step_avg:99.44ms
step:794/1750 train_time:78955ms step_avg:99.44ms
step:795/1750 train_time:79057ms step_avg:99.44ms
step:796/1750 train_time:79160ms step_avg:99.45ms
step:797/1750 train_time:79262ms step_avg:99.45ms
step:798/1750 train_time:79365ms step_avg:99.45ms
step:799/1750 train_time:79467ms step_avg:99.46ms
step:800/1750 train_time:79569ms step_avg:99.46ms
step:801/1750 train_time:79670ms step_avg:99.46ms
step:802/1750 train_time:79772ms step_avg:99.47ms
step:803/1750 train_time:79874ms step_avg:99.47ms
step:804/1750 train_time:79975ms step_avg:99.47ms
step:805/1750 train_time:80076ms step_avg:99.47ms
step:806/1750 train_time:80178ms step_avg:99.48ms
step:807/1750 train_time:80281ms step_avg:99.48ms
step:808/1750 train_time:80385ms step_avg:99.49ms
step:809/1750 train_time:80486ms step_avg:99.49ms
step:810/1750 train_time:80588ms step_avg:99.49ms
step:811/1750 train_time:80690ms step_avg:99.49ms
step:812/1750 train_time:80792ms step_avg:99.50ms
step:813/1750 train_time:80893ms step_avg:99.50ms
step:814/1750 train_time:80995ms step_avg:99.50ms
step:815/1750 train_time:81096ms step_avg:99.50ms
step:816/1750 train_time:81198ms step_avg:99.51ms
step:817/1750 train_time:81302ms step_avg:99.51ms
step:818/1750 train_time:81404ms step_avg:99.52ms
step:819/1750 train_time:81506ms step_avg:99.52ms
step:820/1750 train_time:81607ms step_avg:99.52ms
step:821/1750 train_time:81709ms step_avg:99.52ms
step:822/1750 train_time:81810ms step_avg:99.53ms
step:823/1750 train_time:81912ms step_avg:99.53ms
step:824/1750 train_time:82014ms step_avg:99.53ms
step:825/1750 train_time:82115ms step_avg:99.53ms
step:826/1750 train_time:82217ms step_avg:99.54ms
step:827/1750 train_time:82320ms step_avg:99.54ms
step:828/1750 train_time:82423ms step_avg:99.54ms
step:829/1750 train_time:82526ms step_avg:99.55ms
step:830/1750 train_time:82628ms step_avg:99.55ms
step:831/1750 train_time:82730ms step_avg:99.55ms
step:832/1750 train_time:82831ms step_avg:99.56ms
step:833/1750 train_time:82933ms step_avg:99.56ms
step:834/1750 train_time:83035ms step_avg:99.56ms
step:835/1750 train_time:83136ms step_avg:99.56ms
step:836/1750 train_time:83238ms step_avg:99.57ms
step:837/1750 train_time:83341ms step_avg:99.57ms
step:838/1750 train_time:83444ms step_avg:99.57ms
step:839/1750 train_time:83546ms step_avg:99.58ms
step:840/1750 train_time:83648ms step_avg:99.58ms
step:841/1750 train_time:83750ms step_avg:99.58ms
step:842/1750 train_time:83852ms step_avg:99.59ms
step:843/1750 train_time:83954ms step_avg:99.59ms
step:844/1750 train_time:84055ms step_avg:99.59ms
step:845/1750 train_time:84156ms step_avg:99.59ms
step:846/1750 train_time:84258ms step_avg:99.60ms
step:847/1750 train_time:84361ms step_avg:99.60ms
step:848/1750 train_time:84464ms step_avg:99.60ms
step:849/1750 train_time:84567ms step_avg:99.61ms
step:850/1750 train_time:84669ms step_avg:99.61ms
step:851/1750 train_time:84770ms step_avg:99.61ms
step:852/1750 train_time:84872ms step_avg:99.61ms
step:853/1750 train_time:84974ms step_avg:99.62ms
step:854/1750 train_time:85075ms step_avg:99.62ms
step:855/1750 train_time:85176ms step_avg:99.62ms
step:856/1750 train_time:85279ms step_avg:99.63ms
step:857/1750 train_time:85383ms step_avg:99.63ms
step:858/1750 train_time:85484ms step_avg:99.63ms
step:859/1750 train_time:85586ms step_avg:99.63ms
step:860/1750 train_time:85688ms step_avg:99.64ms
step:861/1750 train_time:85791ms step_avg:99.64ms
step:862/1750 train_time:85893ms step_avg:99.64ms
step:863/1750 train_time:85994ms step_avg:99.65ms
step:864/1750 train_time:86095ms step_avg:99.65ms
step:865/1750 train_time:86197ms step_avg:99.65ms
step:866/1750 train_time:86299ms step_avg:99.65ms
step:867/1750 train_time:86402ms step_avg:99.66ms
step:868/1750 train_time:86504ms step_avg:99.66ms
step:869/1750 train_time:86606ms step_avg:99.66ms
step:870/1750 train_time:86708ms step_avg:99.66ms
step:871/1750 train_time:86810ms step_avg:99.67ms
step:872/1750 train_time:86911ms step_avg:99.67ms
step:873/1750 train_time:87013ms step_avg:99.67ms
step:874/1750 train_time:87114ms step_avg:99.67ms
step:875/1750 train_time:87216ms step_avg:99.68ms
step:875/1750 val_loss:3.5488 train_time:87311ms step_avg:99.78ms
step:876/1750 train_time:87332ms step_avg:99.69ms
step:877/1750 train_time:87426ms step_avg:99.69ms
step:878/1750 train_time:87530ms step_avg:99.69ms
step:879/1750 train_time:87633ms step_avg:99.70ms
step:880/1750 train_time:87735ms step_avg:99.70ms
step:881/1750 train_time:87837ms step_avg:99.70ms
step:882/1750 train_time:87938ms step_avg:99.70ms
step:883/1750 train_time:88040ms step_avg:99.71ms
step:884/1750 train_time:88141ms step_avg:99.71ms
step:885/1750 train_time:88243ms step_avg:99.71ms
step:886/1750 train_time:88344ms step_avg:99.71ms
step:887/1750 train_time:88445ms step_avg:99.71ms
step:888/1750 train_time:88548ms step_avg:99.72ms
step:889/1750 train_time:88650ms step_avg:99.72ms
step:890/1750 train_time:88752ms step_avg:99.72ms
step:891/1750 train_time:88854ms step_avg:99.72ms
step:892/1750 train_time:88956ms step_avg:99.73ms
step:893/1750 train_time:89058ms step_avg:99.73ms
step:894/1750 train_time:89160ms step_avg:99.73ms
step:895/1750 train_time:89261ms step_avg:99.73ms
step:896/1750 train_time:89363ms step_avg:99.74ms
step:897/1750 train_time:89464ms step_avg:99.74ms
step:898/1750 train_time:89566ms step_avg:99.74ms
step:899/1750 train_time:89667ms step_avg:99.74ms
step:900/1750 train_time:89771ms step_avg:99.75ms
step:901/1750 train_time:89873ms step_avg:99.75ms
step:902/1750 train_time:89976ms step_avg:99.75ms
step:903/1750 train_time:90078ms step_avg:99.75ms
step:904/1750 train_time:90179ms step_avg:99.76ms
step:905/1750 train_time:90280ms step_avg:99.76ms
step:906/1750 train_time:90382ms step_avg:99.76ms
step:907/1750 train_time:90483ms step_avg:99.76ms
step:908/1750 train_time:90584ms step_avg:99.76ms
step:909/1750 train_time:90686ms step_avg:99.76ms
step:910/1750 train_time:90790ms step_avg:99.77ms
step:911/1750 train_time:90894ms step_avg:99.77ms
step:912/1750 train_time:90997ms step_avg:99.78ms
step:913/1750 train_time:91100ms step_avg:99.78ms
step:914/1750 train_time:91204ms step_avg:99.79ms
step:915/1750 train_time:91308ms step_avg:99.79ms
step:916/1750 train_time:91412ms step_avg:99.79ms
step:917/1750 train_time:91515ms step_avg:99.80ms
step:918/1750 train_time:91619ms step_avg:99.80ms
step:919/1750 train_time:91722ms step_avg:99.81ms
step:920/1750 train_time:91824ms step_avg:99.81ms
step:921/1750 train_time:91928ms step_avg:99.81ms
step:922/1750 train_time:92032ms step_avg:99.82ms
step:923/1750 train_time:92136ms step_avg:99.82ms
step:924/1750 train_time:92239ms step_avg:99.83ms
step:925/1750 train_time:92341ms step_avg:99.83ms
step:926/1750 train_time:92445ms step_avg:99.83ms
step:927/1750 train_time:92548ms step_avg:99.84ms
step:928/1750 train_time:92652ms step_avg:99.84ms
step:929/1750 train_time:92756ms step_avg:99.84ms
step:930/1750 train_time:92859ms step_avg:99.85ms
step:931/1750 train_time:92961ms step_avg:99.85ms
step:932/1750 train_time:93064ms step_avg:99.85ms
step:933/1750 train_time:93167ms step_avg:99.86ms
step:934/1750 train_time:93271ms step_avg:99.86ms
step:935/1750 train_time:93375ms step_avg:99.87ms
step:936/1750 train_time:93478ms step_avg:99.87ms
step:937/1750 train_time:93581ms step_avg:99.87ms
step:938/1750 train_time:93684ms step_avg:99.88ms
step:939/1750 train_time:93788ms step_avg:99.88ms
step:940/1750 train_time:93892ms step_avg:99.89ms
step:941/1750 train_time:93996ms step_avg:99.89ms
step:942/1750 train_time:94099ms step_avg:99.89ms
step:943/1750 train_time:94203ms step_avg:99.90ms
step:944/1750 train_time:94306ms step_avg:99.90ms
step:945/1750 train_time:94409ms step_avg:99.90ms
step:946/1750 train_time:94513ms step_avg:99.91ms
step:947/1750 train_time:94618ms step_avg:99.91ms
step:948/1750 train_time:94720ms step_avg:99.92ms
step:949/1750 train_time:94823ms step_avg:99.92ms
step:950/1750 train_time:94926ms step_avg:99.92ms
step:951/1750 train_time:95030ms step_avg:99.93ms
step:952/1750 train_time:95133ms step_avg:99.93ms
step:953/1750 train_time:95236ms step_avg:99.93ms
step:954/1750 train_time:95339ms step_avg:99.94ms
step:955/1750 train_time:95441ms step_avg:99.94ms
step:956/1750 train_time:95544ms step_avg:99.94ms
step:957/1750 train_time:95647ms step_avg:99.95ms
step:958/1750 train_time:95751ms step_avg:99.95ms
step:959/1750 train_time:95855ms step_avg:99.95ms
step:960/1750 train_time:95959ms step_avg:99.96ms
step:961/1750 train_time:96062ms step_avg:99.96ms
step:962/1750 train_time:96166ms step_avg:99.96ms
step:963/1750 train_time:96269ms step_avg:99.97ms
step:964/1750 train_time:96373ms step_avg:99.97ms
step:965/1750 train_time:96477ms step_avg:99.98ms
step:966/1750 train_time:96578ms step_avg:99.98ms
step:967/1750 train_time:96682ms step_avg:99.98ms
step:968/1750 train_time:96786ms step_avg:99.99ms
step:969/1750 train_time:96889ms step_avg:99.99ms
step:970/1750 train_time:96993ms step_avg:99.99ms
step:971/1750 train_time:97097ms step_avg:100.00ms
step:972/1750 train_time:97200ms step_avg:100.00ms
step:973/1750 train_time:97303ms step_avg:100.00ms
step:974/1750 train_time:97406ms step_avg:100.01ms
step:975/1750 train_time:97510ms step_avg:100.01ms
step:976/1750 train_time:97615ms step_avg:100.01ms
step:977/1750 train_time:97717ms step_avg:100.02ms
step:978/1750 train_time:97820ms step_avg:100.02ms
step:979/1750 train_time:97925ms step_avg:100.03ms
step:980/1750 train_time:98028ms step_avg:100.03ms
step:981/1750 train_time:98132ms step_avg:100.03ms
step:982/1750 train_time:98236ms step_avg:100.04ms
step:983/1750 train_time:98339ms step_avg:100.04ms
step:984/1750 train_time:98443ms step_avg:100.04ms
step:985/1750 train_time:98546ms step_avg:100.05ms
step:986/1750 train_time:98650ms step_avg:100.05ms
step:987/1750 train_time:98755ms step_avg:100.06ms
step:988/1750 train_time:98858ms step_avg:100.06ms
step:989/1750 train_time:98962ms step_avg:100.06ms
step:990/1750 train_time:99064ms step_avg:100.06ms
step:991/1750 train_time:99168ms step_avg:100.07ms
step:992/1750 train_time:99272ms step_avg:100.07ms
step:993/1750 train_time:99376ms step_avg:100.08ms
step:994/1750 train_time:99479ms step_avg:100.08ms
step:995/1750 train_time:99583ms step_avg:100.08ms
step:996/1750 train_time:99686ms step_avg:100.09ms
step:997/1750 train_time:99789ms step_avg:100.09ms
step:998/1750 train_time:99894ms step_avg:100.09ms
step:999/1750 train_time:99997ms step_avg:100.10ms
step:1000/1750 train_time:100101ms step_avg:100.10ms
step:1000/1750 val_loss:3.5119 train_time:100198ms step_avg:100.20ms
step:1001/1750 train_time:100218ms step_avg:100.12ms
step:1002/1750 train_time:100313ms step_avg:100.11ms
step:1003/1750 train_time:100418ms step_avg:100.12ms
step:1004/1750 train_time:100521ms step_avg:100.12ms
step:1005/1750 train_time:100624ms step_avg:100.12ms
step:1006/1750 train_time:100726ms step_avg:100.13ms
step:1007/1750 train_time:100830ms step_avg:100.13ms
step:1008/1750 train_time:100933ms step_avg:100.13ms
step:1009/1750 train_time:101036ms step_avg:100.13ms
step:1010/1750 train_time:101139ms step_avg:100.14ms
step:1011/1750 train_time:101242ms step_avg:100.14ms
step:1012/1750 train_time:101347ms step_avg:100.15ms
step:1013/1750 train_time:101452ms step_avg:100.15ms
step:1014/1750 train_time:101556ms step_avg:100.15ms
step:1015/1750 train_time:101660ms step_avg:100.16ms
step:1016/1750 train_time:101763ms step_avg:100.16ms
step:1017/1750 train_time:101868ms step_avg:100.16ms
step:1018/1750 train_time:101971ms step_avg:100.17ms
step:1019/1750 train_time:102074ms step_avg:100.17ms
step:1020/1750 train_time:102177ms step_avg:100.17ms
step:1021/1750 train_time:102280ms step_avg:100.18ms
step:1022/1750 train_time:102383ms step_avg:100.18ms
step:1023/1750 train_time:102487ms step_avg:100.18ms
step:1024/1750 train_time:102592ms step_avg:100.19ms
step:1025/1750 train_time:102695ms step_avg:100.19ms
step:1026/1750 train_time:102799ms step_avg:100.19ms
step:1027/1750 train_time:102902ms step_avg:100.20ms
step:1028/1750 train_time:103005ms step_avg:100.20ms
step:1029/1750 train_time:103109ms step_avg:100.20ms
step:1030/1750 train_time:103212ms step_avg:100.21ms
step:1031/1750 train_time:103315ms step_avg:100.21ms
step:1032/1750 train_time:103418ms step_avg:100.21ms
step:1033/1750 train_time:103521ms step_avg:100.21ms
step:1034/1750 train_time:103623ms step_avg:100.22ms
step:1035/1750 train_time:103726ms step_avg:100.22ms
step:1036/1750 train_time:103831ms step_avg:100.22ms
step:1037/1750 train_time:103934ms step_avg:100.23ms
step:1038/1750 train_time:104037ms step_avg:100.23ms
step:1039/1750 train_time:104140ms step_avg:100.23ms
step:1040/1750 train_time:104244ms step_avg:100.23ms
step:1041/1750 train_time:104347ms step_avg:100.24ms
step:1042/1750 train_time:104452ms step_avg:100.24ms
step:1043/1750 train_time:104556ms step_avg:100.25ms
step:1044/1750 train_time:104659ms step_avg:100.25ms
step:1045/1750 train_time:104763ms step_avg:100.25ms
step:1046/1750 train_time:104866ms step_avg:100.25ms
step:1047/1750 train_time:104972ms step_avg:100.26ms
step:1048/1750 train_time:105076ms step_avg:100.26ms
step:1049/1750 train_time:105180ms step_avg:100.27ms
step:1050/1750 train_time:105283ms step_avg:100.27ms
step:1051/1750 train_time:105386ms step_avg:100.27ms
step:1052/1750 train_time:105492ms step_avg:100.28ms
step:1053/1750 train_time:105595ms step_avg:100.28ms
step:1054/1750 train_time:105698ms step_avg:100.28ms
step:1055/1750 train_time:105801ms step_avg:100.29ms
step:1056/1750 train_time:105904ms step_avg:100.29ms
step:1057/1750 train_time:106008ms step_avg:100.29ms
step:1058/1750 train_time:106113ms step_avg:100.30ms
step:1059/1750 train_time:106217ms step_avg:100.30ms
step:1060/1750 train_time:106320ms step_avg:100.30ms
step:1061/1750 train_time:106423ms step_avg:100.30ms
step:1062/1750 train_time:106527ms step_avg:100.31ms
step:1063/1750 train_time:106635ms step_avg:100.31ms
step:1064/1750 train_time:106738ms step_avg:100.32ms
step:1065/1750 train_time:106841ms step_avg:100.32ms
step:1066/1750 train_time:106944ms step_avg:100.32ms
step:1067/1750 train_time:107048ms step_avg:100.33ms
step:1068/1750 train_time:107154ms step_avg:100.33ms
step:1069/1750 train_time:107257ms step_avg:100.33ms
step:1070/1750 train_time:107360ms step_avg:100.34ms
step:1071/1750 train_time:107463ms step_avg:100.34ms
step:1072/1750 train_time:107566ms step_avg:100.34ms
step:1073/1750 train_time:107671ms step_avg:100.35ms
step:1074/1750 train_time:107774ms step_avg:100.35ms
step:1075/1750 train_time:107877ms step_avg:100.35ms
step:1076/1750 train_time:107980ms step_avg:100.35ms
step:1077/1750 train_time:108083ms step_avg:100.36ms
step:1078/1750 train_time:108186ms step_avg:100.36ms
step:1079/1750 train_time:108291ms step_avg:100.36ms
step:1080/1750 train_time:108395ms step_avg:100.37ms
step:1081/1750 train_time:108499ms step_avg:100.37ms
step:1082/1750 train_time:108601ms step_avg:100.37ms
step:1083/1750 train_time:108704ms step_avg:100.37ms
step:1084/1750 train_time:108809ms step_avg:100.38ms
step:1085/1750 train_time:108912ms step_avg:100.38ms
step:1086/1750 train_time:109016ms step_avg:100.38ms
step:1087/1750 train_time:109118ms step_avg:100.38ms
step:1088/1750 train_time:109221ms step_avg:100.39ms
step:1089/1750 train_time:109324ms step_avg:100.39ms
step:1090/1750 train_time:109428ms step_avg:100.39ms
step:1091/1750 train_time:109532ms step_avg:100.40ms
step:1092/1750 train_time:109636ms step_avg:100.40ms
step:1093/1750 train_time:109740ms step_avg:100.40ms
step:1094/1750 train_time:109844ms step_avg:100.41ms
step:1095/1750 train_time:109947ms step_avg:100.41ms
step:1096/1750 train_time:110051ms step_avg:100.41ms
step:1097/1750 train_time:110155ms step_avg:100.41ms
step:1098/1750 train_time:110258ms step_avg:100.42ms
step:1099/1750 train_time:110361ms step_avg:100.42ms
step:1100/1750 train_time:110464ms step_avg:100.42ms
step:1101/1750 train_time:110567ms step_avg:100.42ms
step:1102/1750 train_time:110672ms step_avg:100.43ms
step:1103/1750 train_time:110776ms step_avg:100.43ms
step:1104/1750 train_time:110880ms step_avg:100.43ms
step:1105/1750 train_time:110983ms step_avg:100.44ms
step:1106/1750 train_time:111086ms step_avg:100.44ms
step:1107/1750 train_time:111193ms step_avg:100.44ms
step:1108/1750 train_time:111296ms step_avg:100.45ms
step:1109/1750 train_time:111400ms step_avg:100.45ms
step:1110/1750 train_time:111503ms step_avg:100.45ms
step:1111/1750 train_time:111607ms step_avg:100.46ms
step:1112/1750 train_time:111712ms step_avg:100.46ms
step:1113/1750 train_time:111815ms step_avg:100.46ms
step:1114/1750 train_time:111918ms step_avg:100.47ms
step:1115/1750 train_time:112021ms step_avg:100.47ms
step:1116/1750 train_time:112124ms step_avg:100.47ms
step:1117/1750 train_time:112228ms step_avg:100.47ms
step:1118/1750 train_time:112332ms step_avg:100.48ms
step:1119/1750 train_time:112437ms step_avg:100.48ms
step:1120/1750 train_time:112540ms step_avg:100.48ms
step:1121/1750 train_time:112642ms step_avg:100.48ms
step:1122/1750 train_time:112745ms step_avg:100.49ms
step:1123/1750 train_time:112848ms step_avg:100.49ms
step:1124/1750 train_time:112953ms step_avg:100.49ms
step:1125/1750 train_time:113056ms step_avg:100.49ms
step:1125/1750 val_loss:3.4696 train_time:113153ms step_avg:100.58ms
step:1126/1750 train_time:113173ms step_avg:100.51ms
step:1127/1750 train_time:113269ms step_avg:100.50ms
step:1128/1750 train_time:113372ms step_avg:100.51ms
step:1129/1750 train_time:113474ms step_avg:100.51ms
step:1130/1750 train_time:113578ms step_avg:100.51ms
step:1131/1750 train_time:113681ms step_avg:100.51ms
step:1132/1750 train_time:113784ms step_avg:100.52ms
step:1133/1750 train_time:113888ms step_avg:100.52ms
step:1134/1750 train_time:113991ms step_avg:100.52ms
step:1135/1750 train_time:114094ms step_avg:100.52ms
step:1136/1750 train_time:114197ms step_avg:100.53ms
step:1137/1750 train_time:114302ms step_avg:100.53ms
step:1138/1750 train_time:114406ms step_avg:100.53ms
step:1139/1750 train_time:114512ms step_avg:100.54ms
step:1140/1750 train_time:114616ms step_avg:100.54ms
step:1141/1750 train_time:114719ms step_avg:100.54ms
step:1142/1750 train_time:114822ms step_avg:100.54ms
step:1143/1750 train_time:114926ms step_avg:100.55ms
step:1144/1750 train_time:115029ms step_avg:100.55ms
step:1145/1750 train_time:115133ms step_avg:100.55ms
step:1146/1750 train_time:115237ms step_avg:100.56ms
step:1147/1750 train_time:115340ms step_avg:100.56ms
step:1148/1750 train_time:115443ms step_avg:100.56ms
step:1149/1750 train_time:115547ms step_avg:100.56ms
step:1150/1750 train_time:115651ms step_avg:100.57ms
step:1151/1750 train_time:115755ms step_avg:100.57ms
step:1152/1750 train_time:115858ms step_avg:100.57ms
step:1153/1750 train_time:115961ms step_avg:100.57ms
step:1154/1750 train_time:116064ms step_avg:100.58ms
step:1155/1750 train_time:116168ms step_avg:100.58ms
step:1156/1750 train_time:116273ms step_avg:100.58ms
step:1157/1750 train_time:116377ms step_avg:100.59ms
step:1158/1750 train_time:116481ms step_avg:100.59ms
step:1159/1750 train_time:116584ms step_avg:100.59ms
step:1160/1750 train_time:116687ms step_avg:100.59ms
step:1161/1750 train_time:116792ms step_avg:100.60ms
step:1162/1750 train_time:116896ms step_avg:100.60ms
step:1163/1750 train_time:116999ms step_avg:100.60ms
step:1164/1750 train_time:117103ms step_avg:100.60ms
step:1165/1750 train_time:117205ms step_avg:100.61ms
step:1166/1750 train_time:117310ms step_avg:100.61ms
step:1167/1750 train_time:117414ms step_avg:100.61ms
step:1168/1750 train_time:117518ms step_avg:100.61ms
step:1169/1750 train_time:117621ms step_avg:100.62ms
step:1170/1750 train_time:117726ms step_avg:100.62ms
step:1171/1750 train_time:117831ms step_avg:100.62ms
step:1172/1750 train_time:117936ms step_avg:100.63ms
step:1173/1750 train_time:118040ms step_avg:100.63ms
step:1174/1750 train_time:118144ms step_avg:100.63ms
step:1175/1750 train_time:118249ms step_avg:100.64ms
step:1176/1750 train_time:118353ms step_avg:100.64ms
step:1177/1750 train_time:118457ms step_avg:100.64ms
step:1178/1750 train_time:118562ms step_avg:100.65ms
step:1179/1750 train_time:118666ms step_avg:100.65ms
step:1180/1750 train_time:118773ms step_avg:100.65ms
step:1181/1750 train_time:118877ms step_avg:100.66ms
step:1182/1750 train_time:118982ms step_avg:100.66ms
step:1183/1750 train_time:119086ms step_avg:100.66ms
step:1184/1750 train_time:119191ms step_avg:100.67ms
step:1185/1750 train_time:119296ms step_avg:100.67ms
step:1186/1750 train_time:119401ms step_avg:100.68ms
step:1187/1750 train_time:119508ms step_avg:100.68ms
step:1188/1750 train_time:119613ms step_avg:100.68ms
step:1189/1750 train_time:119717ms step_avg:100.69ms
step:1190/1750 train_time:119822ms step_avg:100.69ms
step:1191/1750 train_time:119927ms step_avg:100.69ms
step:1192/1750 train_time:120032ms step_avg:100.70ms
step:1193/1750 train_time:120136ms step_avg:100.70ms
step:1194/1750 train_time:120240ms step_avg:100.70ms
step:1195/1750 train_time:120345ms step_avg:100.71ms
step:1196/1750 train_time:120450ms step_avg:100.71ms
step:1197/1750 train_time:120554ms step_avg:100.71ms
step:1198/1750 train_time:120658ms step_avg:100.72ms
step:1199/1750 train_time:120762ms step_avg:100.72ms
step:1200/1750 train_time:120867ms step_avg:100.72ms
step:1201/1750 train_time:120972ms step_avg:100.73ms
step:1202/1750 train_time:121076ms step_avg:100.73ms
step:1203/1750 train_time:121180ms step_avg:100.73ms
step:1204/1750 train_time:121285ms step_avg:100.73ms
step:1205/1750 train_time:121390ms step_avg:100.74ms
step:1206/1750 train_time:121496ms step_avg:100.74ms
step:1207/1750 train_time:121600ms step_avg:100.75ms
step:1208/1750 train_time:121704ms step_avg:100.75ms
step:1209/1750 train_time:121809ms step_avg:100.75ms
step:1210/1750 train_time:121914ms step_avg:100.76ms
step:1211/1750 train_time:122018ms step_avg:100.76ms
step:1212/1750 train_time:122126ms step_avg:100.76ms
step:1213/1750 train_time:122231ms step_avg:100.77ms
step:1214/1750 train_time:122335ms step_avg:100.77ms
step:1215/1750 train_time:122440ms step_avg:100.77ms
step:1216/1750 train_time:122546ms step_avg:100.78ms
step:1217/1750 train_time:122651ms step_avg:100.78ms
step:1218/1750 train_time:122755ms step_avg:100.78ms
step:1219/1750 train_time:122860ms step_avg:100.79ms
step:1220/1750 train_time:122964ms step_avg:100.79ms
step:1221/1750 train_time:123069ms step_avg:100.79ms
step:1222/1750 train_time:123176ms step_avg:100.80ms
step:1223/1750 train_time:123281ms step_avg:100.80ms
step:1224/1750 train_time:123386ms step_avg:100.81ms
step:1225/1750 train_time:123491ms step_avg:100.81ms
step:1226/1750 train_time:123595ms step_avg:100.81ms
step:1227/1750 train_time:123702ms step_avg:100.82ms
step:1228/1750 train_time:123809ms step_avg:100.82ms
step:1229/1750 train_time:123913ms step_avg:100.82ms
step:1230/1750 train_time:124018ms step_avg:100.83ms
step:1231/1750 train_time:124122ms step_avg:100.83ms
step:1232/1750 train_time:124225ms step_avg:100.83ms
step:1233/1750 train_time:124330ms step_avg:100.84ms
step:1234/1750 train_time:124435ms step_avg:100.84ms
step:1235/1750 train_time:124539ms step_avg:100.84ms
step:1236/1750 train_time:124645ms step_avg:100.85ms
step:1237/1750 train_time:124751ms step_avg:100.85ms
step:1238/1750 train_time:124856ms step_avg:100.85ms
step:1239/1750 train_time:124960ms step_avg:100.86ms
step:1240/1750 train_time:125065ms step_avg:100.86ms
step:1241/1750 train_time:125170ms step_avg:100.86ms
step:1242/1750 train_time:125275ms step_avg:100.87ms
step:1243/1750 train_time:125379ms step_avg:100.87ms
step:1244/1750 train_time:125484ms step_avg:100.87ms
step:1245/1750 train_time:125588ms step_avg:100.87ms
step:1246/1750 train_time:125694ms step_avg:100.88ms
step:1247/1750 train_time:125798ms step_avg:100.88ms
step:1248/1750 train_time:125903ms step_avg:100.88ms
step:1249/1750 train_time:126006ms step_avg:100.89ms
step:1250/1750 train_time:126113ms step_avg:100.89ms
step:1250/1750 val_loss:3.4218 train_time:126214ms step_avg:100.97ms
step:1251/1750 train_time:126234ms step_avg:100.91ms
step:1252/1750 train_time:126328ms step_avg:100.90ms
step:1253/1750 train_time:126433ms step_avg:100.90ms
step:1254/1750 train_time:126538ms step_avg:100.91ms
step:1255/1750 train_time:126643ms step_avg:100.91ms
step:1256/1750 train_time:126747ms step_avg:100.91ms
step:1257/1750 train_time:126852ms step_avg:100.92ms
step:1258/1750 train_time:126957ms step_avg:100.92ms
step:1259/1750 train_time:127062ms step_avg:100.92ms
step:1260/1750 train_time:127166ms step_avg:100.93ms
step:1261/1750 train_time:127271ms step_avg:100.93ms
step:1262/1750 train_time:127376ms step_avg:100.93ms
step:1263/1750 train_time:127482ms step_avg:100.94ms
step:1264/1750 train_time:127587ms step_avg:100.94ms
step:1265/1750 train_time:127691ms step_avg:100.94ms
step:1266/1750 train_time:127796ms step_avg:100.94ms
step:1267/1750 train_time:127901ms step_avg:100.95ms
step:1268/1750 train_time:128006ms step_avg:100.95ms
step:1269/1750 train_time:128111ms step_avg:100.95ms
step:1270/1750 train_time:128215ms step_avg:100.96ms
step:1271/1750 train_time:128320ms step_avg:100.96ms
step:1272/1750 train_time:128424ms step_avg:100.96ms
step:1273/1750 train_time:128528ms step_avg:100.96ms
step:1274/1750 train_time:128633ms step_avg:100.97ms
step:1275/1750 train_time:128739ms step_avg:100.97ms
step:1276/1750 train_time:128842ms step_avg:100.97ms
step:1277/1750 train_time:128946ms step_avg:100.98ms
step:1278/1750 train_time:129051ms step_avg:100.98ms
step:1279/1750 train_time:129156ms step_avg:100.98ms
step:1280/1750 train_time:129261ms step_avg:100.99ms
step:1281/1750 train_time:129366ms step_avg:100.99ms
step:1282/1750 train_time:129472ms step_avg:100.99ms
step:1283/1750 train_time:129576ms step_avg:100.99ms
step:1284/1750 train_time:129680ms step_avg:101.00ms
step:1285/1750 train_time:129785ms step_avg:101.00ms
step:1286/1750 train_time:129891ms step_avg:101.00ms
step:1287/1750 train_time:129996ms step_avg:101.01ms
step:1288/1750 train_time:130100ms step_avg:101.01ms
step:1289/1750 train_time:130205ms step_avg:101.01ms
step:1290/1750 train_time:130308ms step_avg:101.01ms
step:1291/1750 train_time:130413ms step_avg:101.02ms
step:1292/1750 train_time:130518ms step_avg:101.02ms
step:1293/1750 train_time:130623ms step_avg:101.02ms
step:1294/1750 train_time:130727ms step_avg:101.03ms
step:1295/1750 train_time:130832ms step_avg:101.03ms
step:1296/1750 train_time:130936ms step_avg:101.03ms
step:1297/1750 train_time:131041ms step_avg:101.03ms
step:1298/1750 train_time:131148ms step_avg:101.04ms
step:1299/1750 train_time:131251ms step_avg:101.04ms
step:1300/1750 train_time:131355ms step_avg:101.04ms
step:1301/1750 train_time:131462ms step_avg:101.05ms
step:1302/1750 train_time:131567ms step_avg:101.05ms
step:1303/1750 train_time:131671ms step_avg:101.05ms
step:1304/1750 train_time:131776ms step_avg:101.06ms
step:1305/1750 train_time:131881ms step_avg:101.06ms
step:1306/1750 train_time:131985ms step_avg:101.06ms
step:1307/1750 train_time:132089ms step_avg:101.06ms
step:1308/1750 train_time:132194ms step_avg:101.07ms
step:1309/1750 train_time:132299ms step_avg:101.07ms
step:1310/1750 train_time:132404ms step_avg:101.07ms
step:1311/1750 train_time:132508ms step_avg:101.07ms
step:1312/1750 train_time:132612ms step_avg:101.08ms
step:1313/1750 train_time:132717ms step_avg:101.08ms
step:1314/1750 train_time:132821ms step_avg:101.08ms
step:1315/1750 train_time:132926ms step_avg:101.08ms
step:1316/1750 train_time:133030ms step_avg:101.09ms
step:1317/1750 train_time:133136ms step_avg:101.09ms
step:1318/1750 train_time:133243ms step_avg:101.09ms
step:1319/1750 train_time:133348ms step_avg:101.10ms
step:1320/1750 train_time:133454ms step_avg:101.10ms
step:1321/1750 train_time:133558ms step_avg:101.10ms
step:1322/1750 train_time:133662ms step_avg:101.11ms
step:1323/1750 train_time:133767ms step_avg:101.11ms
step:1324/1750 train_time:133871ms step_avg:101.11ms
step:1325/1750 train_time:133978ms step_avg:101.12ms
step:1326/1750 train_time:134082ms step_avg:101.12ms
step:1327/1750 train_time:134189ms step_avg:101.12ms
step:1328/1750 train_time:134293ms step_avg:101.12ms
step:1329/1750 train_time:134399ms step_avg:101.13ms
step:1330/1750 train_time:134503ms step_avg:101.13ms
step:1331/1750 train_time:134607ms step_avg:101.13ms
step:1332/1750 train_time:134711ms step_avg:101.13ms
step:1333/1750 train_time:134815ms step_avg:101.14ms
step:1334/1750 train_time:134920ms step_avg:101.14ms
step:1335/1750 train_time:135025ms step_avg:101.14ms
step:1336/1750 train_time:135129ms step_avg:101.14ms
step:1337/1750 train_time:135233ms step_avg:101.15ms
step:1338/1750 train_time:135339ms step_avg:101.15ms
step:1339/1750 train_time:135444ms step_avg:101.15ms
step:1340/1750 train_time:135549ms step_avg:101.16ms
step:1341/1750 train_time:135653ms step_avg:101.16ms
step:1342/1750 train_time:135759ms step_avg:101.16ms
step:1343/1750 train_time:135865ms step_avg:101.17ms
step:1344/1750 train_time:135969ms step_avg:101.17ms
step:1345/1750 train_time:136074ms step_avg:101.17ms
step:1346/1750 train_time:136179ms step_avg:101.17ms
step:1347/1750 train_time:136284ms step_avg:101.18ms
step:1348/1750 train_time:136390ms step_avg:101.18ms
step:1349/1750 train_time:136495ms step_avg:101.18ms
step:1350/1750 train_time:136601ms step_avg:101.19ms
step:1351/1750 train_time:136706ms step_avg:101.19ms
step:1352/1750 train_time:136810ms step_avg:101.19ms
step:1353/1750 train_time:136915ms step_avg:101.19ms
step:1354/1750 train_time:137020ms step_avg:101.20ms
step:1355/1750 train_time:137124ms step_avg:101.20ms
step:1356/1750 train_time:137229ms step_avg:101.20ms
step:1357/1750 train_time:137333ms step_avg:101.20ms
step:1358/1750 train_time:137438ms step_avg:101.21ms
step:1359/1750 train_time:137542ms step_avg:101.21ms
step:1360/1750 train_time:137649ms step_avg:101.21ms
step:1361/1750 train_time:137754ms step_avg:101.21ms
step:1362/1750 train_time:137859ms step_avg:101.22ms
step:1363/1750 train_time:137963ms step_avg:101.22ms
step:1364/1750 train_time:138067ms step_avg:101.22ms
step:1365/1750 train_time:138171ms step_avg:101.22ms
step:1366/1750 train_time:138276ms step_avg:101.23ms
step:1367/1750 train_time:138381ms step_avg:101.23ms
step:1368/1750 train_time:138485ms step_avg:101.23ms
step:1369/1750 train_time:138590ms step_avg:101.23ms
step:1370/1750 train_time:138695ms step_avg:101.24ms
step:1371/1750 train_time:138800ms step_avg:101.24ms
step:1372/1750 train_time:138904ms step_avg:101.24ms
step:1373/1750 train_time:139010ms step_avg:101.25ms
step:1374/1750 train_time:139117ms step_avg:101.25ms
step:1375/1750 train_time:139222ms step_avg:101.25ms
step:1375/1750 val_loss:3.3776 train_time:139321ms step_avg:101.32ms
step:1376/1750 train_time:139341ms step_avg:101.27ms
step:1377/1750 train_time:139439ms step_avg:101.26ms
step:1378/1750 train_time:139546ms step_avg:101.27ms
step:1379/1750 train_time:139650ms step_avg:101.27ms
step:1380/1750 train_time:139755ms step_avg:101.27ms
step:1381/1750 train_time:139860ms step_avg:101.27ms
step:1382/1750 train_time:139964ms step_avg:101.28ms
step:1383/1750 train_time:140068ms step_avg:101.28ms
step:1384/1750 train_time:140173ms step_avg:101.28ms
step:1385/1750 train_time:140278ms step_avg:101.28ms
step:1386/1750 train_time:140386ms step_avg:101.29ms
step:1387/1750 train_time:140491ms step_avg:101.29ms
step:1388/1750 train_time:140595ms step_avg:101.29ms
step:1389/1750 train_time:140700ms step_avg:101.30ms
step:1390/1750 train_time:140804ms step_avg:101.30ms
step:1391/1750 train_time:140910ms step_avg:101.30ms
step:1392/1750 train_time:141014ms step_avg:101.30ms
step:1393/1750 train_time:141118ms step_avg:101.30ms
step:1394/1750 train_time:141223ms step_avg:101.31ms
step:1395/1750 train_time:141328ms step_avg:101.31ms
step:1396/1750 train_time:141434ms step_avg:101.31ms
step:1397/1750 train_time:141538ms step_avg:101.32ms
step:1398/1750 train_time:141644ms step_avg:101.32ms
step:1399/1750 train_time:141748ms step_avg:101.32ms
step:1400/1750 train_time:141853ms step_avg:101.32ms
step:1401/1750 train_time:141957ms step_avg:101.33ms
step:1402/1750 train_time:142063ms step_avg:101.33ms
step:1403/1750 train_time:142168ms step_avg:101.33ms
step:1404/1750 train_time:142273ms step_avg:101.33ms
step:1405/1750 train_time:142376ms step_avg:101.34ms
step:1406/1750 train_time:142481ms step_avg:101.34ms
step:1407/1750 train_time:142586ms step_avg:101.34ms
step:1408/1750 train_time:142691ms step_avg:101.34ms
step:1409/1750 train_time:142795ms step_avg:101.34ms
step:1410/1750 train_time:142900ms step_avg:101.35ms
step:1411/1750 train_time:143005ms step_avg:101.35ms
step:1412/1750 train_time:143109ms step_avg:101.35ms
step:1413/1750 train_time:143213ms step_avg:101.35ms
step:1414/1750 train_time:143319ms step_avg:101.36ms
step:1415/1750 train_time:143423ms step_avg:101.36ms
step:1416/1750 train_time:143529ms step_avg:101.36ms
step:1417/1750 train_time:143633ms step_avg:101.36ms
step:1418/1750 train_time:143737ms step_avg:101.37ms
step:1419/1750 train_time:143842ms step_avg:101.37ms
step:1420/1750 train_time:143946ms step_avg:101.37ms
step:1421/1750 train_time:144051ms step_avg:101.37ms
step:1422/1750 train_time:144155ms step_avg:101.38ms
step:1423/1750 train_time:144259ms step_avg:101.38ms
step:1424/1750 train_time:144364ms step_avg:101.38ms
step:1425/1750 train_time:144469ms step_avg:101.38ms
step:1426/1750 train_time:144574ms step_avg:101.38ms
step:1427/1750 train_time:144678ms step_avg:101.39ms
step:1428/1750 train_time:144785ms step_avg:101.39ms
step:1429/1750 train_time:144891ms step_avg:101.39ms
step:1430/1750 train_time:144996ms step_avg:101.40ms
step:1431/1750 train_time:145104ms step_avg:101.40ms
step:1432/1750 train_time:145210ms step_avg:101.40ms
step:1433/1750 train_time:145315ms step_avg:101.41ms
step:1434/1750 train_time:145420ms step_avg:101.41ms
step:1435/1750 train_time:145527ms step_avg:101.41ms
step:1436/1750 train_time:145635ms step_avg:101.42ms
step:1437/1750 train_time:145741ms step_avg:101.42ms
step:1438/1750 train_time:145847ms step_avg:101.42ms
step:1439/1750 train_time:145952ms step_avg:101.43ms
step:1440/1750 train_time:146058ms step_avg:101.43ms
step:1441/1750 train_time:146167ms step_avg:101.43ms
step:1442/1750 train_time:146271ms step_avg:101.44ms
