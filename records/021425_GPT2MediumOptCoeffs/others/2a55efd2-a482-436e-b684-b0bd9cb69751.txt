import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for a, b, c in [
        (3.9090, -4.0174, 1.1084),
        (3.7495, -3.8578, 1.1084),
        (3.5931, -3.7008, 1.1077),
        (3.4398, -3.5462, 1.1064),
        (3.2882, -3.3917, 1.1035),
        (3.1156, -3.2027, 1.0871),
        (2.7208, -2.6650, 0.9442),
    ]:
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.mul_(1 - group["lr"] * group["weight_decay"])
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=0.5, w_s=2**-9, grad_s=2**-19)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 7050 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=16, num_heads=8, model_dim=1024,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.1/1024**0.5), dict(params=embed_params, lr=0.3), dict(params=scalar_params, lr=0.015)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.025, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        return (1 - x) / args.cooldown_frac

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
for _ in range(warmup_steps):
    inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device="cuda")
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Sun Feb 16 18:53:27 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   38C    P0            117W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   31C    P0            120W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   29C    P0            114W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   37C    P0            118W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   37C    P0            120W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   31C    P0            112W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   36C    P0            114W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   29C    P0            113W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/7050 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/7050 train_time:153ms step_avg:152.91ms
step:2/7050 train_time:341ms step_avg:170.33ms
step:3/7050 train_time:556ms step_avg:185.33ms
step:4/7050 train_time:779ms step_avg:194.80ms
step:5/7050 train_time:1002ms step_avg:200.50ms
step:6/7050 train_time:1225ms step_avg:204.19ms
step:7/7050 train_time:1451ms step_avg:207.33ms
step:8/7050 train_time:1679ms step_avg:209.86ms
step:9/7050 train_time:1903ms step_avg:211.48ms
step:10/7050 train_time:2127ms step_avg:212.73ms
step:11/7050 train_time:2352ms step_avg:213.78ms
step:12/7050 train_time:2579ms step_avg:214.91ms
step:13/7050 train_time:2805ms step_avg:215.77ms
step:14/7050 train_time:3030ms step_avg:216.40ms
step:15/7050 train_time:3259ms step_avg:217.24ms
step:16/7050 train_time:3485ms step_avg:217.78ms
step:17/7050 train_time:3710ms step_avg:218.23ms
step:18/7050 train_time:3936ms step_avg:218.65ms
step:19/7050 train_time:4162ms step_avg:219.04ms
step:20/7050 train_time:4389ms step_avg:219.44ms
step:21/7050 train_time:4615ms step_avg:219.78ms
step:22/7050 train_time:4839ms step_avg:219.97ms
step:23/7050 train_time:5065ms step_avg:220.23ms
step:24/7050 train_time:5291ms step_avg:220.45ms
step:25/7050 train_time:5517ms step_avg:220.68ms
step:26/7050 train_time:5741ms step_avg:220.80ms
step:27/7050 train_time:5968ms step_avg:221.02ms
step:28/7050 train_time:6191ms step_avg:221.11ms
step:29/7050 train_time:6417ms step_avg:221.27ms
step:30/7050 train_time:6642ms step_avg:221.39ms
step:31/7050 train_time:6867ms step_avg:221.53ms
step:32/7050 train_time:7092ms step_avg:221.62ms
step:33/7050 train_time:7317ms step_avg:221.72ms
step:34/7050 train_time:7542ms step_avg:221.81ms
step:35/7050 train_time:7767ms step_avg:221.91ms
step:36/7050 train_time:7992ms step_avg:221.99ms
step:37/7050 train_time:8216ms step_avg:222.05ms
step:38/7050 train_time:8442ms step_avg:222.15ms
step:39/7050 train_time:8668ms step_avg:222.26ms
step:40/7050 train_time:8892ms step_avg:222.29ms
step:41/7050 train_time:9117ms step_avg:222.36ms
step:42/7050 train_time:9341ms step_avg:222.41ms
step:43/7050 train_time:9569ms step_avg:222.53ms
step:44/7050 train_time:9793ms step_avg:222.58ms
step:45/7050 train_time:10020ms step_avg:222.66ms
step:46/7050 train_time:10247ms step_avg:222.75ms
step:47/7050 train_time:10471ms step_avg:222.79ms
step:48/7050 train_time:10696ms step_avg:222.84ms
step:49/7050 train_time:10923ms step_avg:222.91ms
step:50/7050 train_time:11148ms step_avg:222.96ms
step:51/7050 train_time:11372ms step_avg:222.99ms
step:52/7050 train_time:11599ms step_avg:223.06ms
step:53/7050 train_time:11825ms step_avg:223.11ms
step:54/7050 train_time:12051ms step_avg:223.16ms
step:55/7050 train_time:12275ms step_avg:223.19ms
step:56/7050 train_time:12502ms step_avg:223.25ms
step:57/7050 train_time:12728ms step_avg:223.29ms
step:58/7050 train_time:12952ms step_avg:223.31ms
step:59/7050 train_time:13177ms step_avg:223.34ms
step:60/7050 train_time:13403ms step_avg:223.39ms
step:61/7050 train_time:13629ms step_avg:223.42ms
step:62/7050 train_time:13854ms step_avg:223.45ms
step:63/7050 train_time:14080ms step_avg:223.49ms
step:64/7050 train_time:14305ms step_avg:223.52ms
step:65/7050 train_time:14530ms step_avg:223.54ms
step:66/7050 train_time:14755ms step_avg:223.57ms
step:67/7050 train_time:14982ms step_avg:223.62ms
step:68/7050 train_time:15207ms step_avg:223.64ms
step:69/7050 train_time:15433ms step_avg:223.67ms
step:70/7050 train_time:15660ms step_avg:223.72ms
step:71/7050 train_time:15885ms step_avg:223.74ms
step:72/7050 train_time:16109ms step_avg:223.74ms
step:73/7050 train_time:16335ms step_avg:223.76ms
step:74/7050 train_time:16560ms step_avg:223.79ms
step:75/7050 train_time:16787ms step_avg:223.83ms
step:76/7050 train_time:17010ms step_avg:223.81ms
step:77/7050 train_time:17235ms step_avg:223.83ms
step:78/7050 train_time:17461ms step_avg:223.86ms
step:79/7050 train_time:17686ms step_avg:223.87ms
step:80/7050 train_time:17910ms step_avg:223.88ms
step:81/7050 train_time:18133ms step_avg:223.87ms
step:82/7050 train_time:18359ms step_avg:223.89ms
step:83/7050 train_time:18584ms step_avg:223.90ms
step:84/7050 train_time:18808ms step_avg:223.91ms
step:85/7050 train_time:19034ms step_avg:223.93ms
step:86/7050 train_time:19258ms step_avg:223.93ms
step:87/7050 train_time:19483ms step_avg:223.94ms
step:88/7050 train_time:19709ms step_avg:223.96ms
step:89/7050 train_time:19934ms step_avg:223.98ms
step:90/7050 train_time:20159ms step_avg:223.99ms
step:91/7050 train_time:20384ms step_avg:224.00ms
step:92/7050 train_time:20610ms step_avg:224.02ms
step:93/7050 train_time:20835ms step_avg:224.04ms
step:94/7050 train_time:21060ms step_avg:224.05ms
step:95/7050 train_time:21286ms step_avg:224.06ms
step:96/7050 train_time:21511ms step_avg:224.07ms
step:97/7050 train_time:21735ms step_avg:224.07ms
step:98/7050 train_time:21960ms step_avg:224.09ms
step:99/7050 train_time:22185ms step_avg:224.10ms
step:100/7050 train_time:22410ms step_avg:224.10ms
step:101/7050 train_time:22636ms step_avg:224.12ms
step:102/7050 train_time:22863ms step_avg:224.15ms
step:103/7050 train_time:23090ms step_avg:224.17ms
step:104/7050 train_time:23313ms step_avg:224.16ms
step:105/7050 train_time:23539ms step_avg:224.18ms
step:106/7050 train_time:23764ms step_avg:224.19ms
step:107/7050 train_time:23990ms step_avg:224.20ms
step:108/7050 train_time:24214ms step_avg:224.21ms
step:109/7050 train_time:24439ms step_avg:224.21ms
step:110/7050 train_time:24665ms step_avg:224.22ms
step:111/7050 train_time:24890ms step_avg:224.23ms
step:112/7050 train_time:25114ms step_avg:224.23ms
step:113/7050 train_time:25339ms step_avg:224.24ms
step:114/7050 train_time:25565ms step_avg:224.25ms
step:115/7050 train_time:25791ms step_avg:224.27ms
step:116/7050 train_time:26015ms step_avg:224.27ms
step:117/7050 train_time:26240ms step_avg:224.27ms
step:118/7050 train_time:26465ms step_avg:224.28ms
step:119/7050 train_time:26691ms step_avg:224.29ms
step:120/7050 train_time:26916ms step_avg:224.30ms
step:121/7050 train_time:27140ms step_avg:224.30ms
step:122/7050 train_time:27364ms step_avg:224.29ms
step:123/7050 train_time:27590ms step_avg:224.31ms
step:124/7050 train_time:27814ms step_avg:224.31ms
step:125/7050 train_time:28039ms step_avg:224.31ms
step:125/7050 val_loss:4.4941 train_time:28167ms step_avg:225.34ms
step:126/7050 train_time:28264ms step_avg:224.31ms
step:127/7050 train_time:28488ms step_avg:224.32ms
step:128/7050 train_time:28717ms step_avg:224.35ms
step:129/7050 train_time:28944ms step_avg:224.37ms
step:130/7050 train_time:29168ms step_avg:224.37ms
step:131/7050 train_time:29391ms step_avg:224.36ms
step:132/7050 train_time:29617ms step_avg:224.37ms
step:133/7050 train_time:29844ms step_avg:224.39ms
step:134/7050 train_time:30068ms step_avg:224.39ms
step:135/7050 train_time:30292ms step_avg:224.39ms
step:136/7050 train_time:30516ms step_avg:224.38ms
step:137/7050 train_time:30744ms step_avg:224.41ms
step:138/7050 train_time:30968ms step_avg:224.41ms
step:139/7050 train_time:31193ms step_avg:224.41ms
step:140/7050 train_time:31418ms step_avg:224.41ms
step:141/7050 train_time:31645ms step_avg:224.43ms
step:142/7050 train_time:31870ms step_avg:224.44ms
step:143/7050 train_time:32095ms step_avg:224.44ms
step:144/7050 train_time:32320ms step_avg:224.44ms
step:145/7050 train_time:32543ms step_avg:224.43ms
step:146/7050 train_time:32767ms step_avg:224.43ms
step:147/7050 train_time:32992ms step_avg:224.44ms
step:148/7050 train_time:33219ms step_avg:224.45ms
step:149/7050 train_time:33445ms step_avg:224.46ms
step:150/7050 train_time:33668ms step_avg:224.45ms
step:151/7050 train_time:33892ms step_avg:224.45ms
step:152/7050 train_time:34119ms step_avg:224.46ms
step:153/7050 train_time:34345ms step_avg:224.48ms
step:154/7050 train_time:34568ms step_avg:224.47ms
step:155/7050 train_time:34792ms step_avg:224.46ms
step:156/7050 train_time:35020ms step_avg:224.49ms
step:157/7050 train_time:35242ms step_avg:224.47ms
step:158/7050 train_time:35466ms step_avg:224.47ms
step:159/7050 train_time:35691ms step_avg:224.47ms
step:160/7050 train_time:35914ms step_avg:224.46ms
step:161/7050 train_time:36140ms step_avg:224.47ms
step:162/7050 train_time:36362ms step_avg:224.46ms
step:163/7050 train_time:36589ms step_avg:224.47ms
step:164/7050 train_time:36813ms step_avg:224.47ms
step:165/7050 train_time:37039ms step_avg:224.48ms
step:166/7050 train_time:37261ms step_avg:224.47ms
step:167/7050 train_time:37486ms step_avg:224.47ms
step:168/7050 train_time:37712ms step_avg:224.48ms
step:169/7050 train_time:37938ms step_avg:224.49ms
step:170/7050 train_time:38163ms step_avg:224.49ms
step:171/7050 train_time:38387ms step_avg:224.48ms
step:172/7050 train_time:38611ms step_avg:224.48ms
step:173/7050 train_time:38837ms step_avg:224.49ms
step:174/7050 train_time:39062ms step_avg:224.49ms
step:175/7050 train_time:39284ms step_avg:224.48ms
step:176/7050 train_time:39509ms step_avg:224.48ms
step:177/7050 train_time:39734ms step_avg:224.48ms
step:178/7050 train_time:39957ms step_avg:224.48ms
step:179/7050 train_time:40181ms step_avg:224.48ms
step:180/7050 train_time:40406ms step_avg:224.48ms
step:181/7050 train_time:40629ms step_avg:224.47ms
step:182/7050 train_time:40853ms step_avg:224.47ms
step:183/7050 train_time:41079ms step_avg:224.47ms
step:184/7050 train_time:41303ms step_avg:224.47ms
step:185/7050 train_time:41529ms step_avg:224.48ms
step:186/7050 train_time:41753ms step_avg:224.48ms
step:187/7050 train_time:41977ms step_avg:224.47ms
step:188/7050 train_time:42200ms step_avg:224.47ms
step:189/7050 train_time:42425ms step_avg:224.47ms
step:190/7050 train_time:42650ms step_avg:224.47ms
step:191/7050 train_time:42875ms step_avg:224.48ms
step:192/7050 train_time:43099ms step_avg:224.47ms
step:193/7050 train_time:43322ms step_avg:224.47ms
step:194/7050 train_time:43545ms step_avg:224.46ms
step:195/7050 train_time:43770ms step_avg:224.46ms
step:196/7050 train_time:43995ms step_avg:224.46ms
step:197/7050 train_time:44218ms step_avg:224.45ms
step:198/7050 train_time:44444ms step_avg:224.47ms
step:199/7050 train_time:44668ms step_avg:224.46ms
step:200/7050 train_time:44893ms step_avg:224.46ms
step:201/7050 train_time:45117ms step_avg:224.47ms
step:202/7050 train_time:45342ms step_avg:224.46ms
step:203/7050 train_time:45566ms step_avg:224.46ms
step:204/7050 train_time:45790ms step_avg:224.46ms
step:205/7050 train_time:46015ms step_avg:224.46ms
step:206/7050 train_time:46239ms step_avg:224.46ms
step:207/7050 train_time:46461ms step_avg:224.45ms
step:208/7050 train_time:46685ms step_avg:224.45ms
step:209/7050 train_time:46908ms step_avg:224.44ms
step:210/7050 train_time:47135ms step_avg:224.45ms
step:211/7050 train_time:47358ms step_avg:224.45ms
step:212/7050 train_time:47580ms step_avg:224.43ms
step:213/7050 train_time:47802ms step_avg:224.42ms
step:214/7050 train_time:48027ms step_avg:224.42ms
step:215/7050 train_time:48253ms step_avg:224.43ms
step:216/7050 train_time:48475ms step_avg:224.42ms
step:217/7050 train_time:48700ms step_avg:224.42ms
step:218/7050 train_time:48922ms step_avg:224.41ms
step:219/7050 train_time:49146ms step_avg:224.41ms
step:220/7050 train_time:49370ms step_avg:224.41ms
step:221/7050 train_time:49596ms step_avg:224.41ms
step:222/7050 train_time:49818ms step_avg:224.41ms
step:223/7050 train_time:50039ms step_avg:224.39ms
step:224/7050 train_time:50263ms step_avg:224.39ms
step:225/7050 train_time:50488ms step_avg:224.39ms
step:226/7050 train_time:50712ms step_avg:224.39ms
step:227/7050 train_time:50937ms step_avg:224.39ms
step:228/7050 train_time:51160ms step_avg:224.38ms
step:229/7050 train_time:51383ms step_avg:224.38ms
step:230/7050 train_time:51608ms step_avg:224.38ms
step:231/7050 train_time:51832ms step_avg:224.38ms
step:232/7050 train_time:52056ms step_avg:224.38ms
step:233/7050 train_time:52280ms step_avg:224.38ms
step:234/7050 train_time:52503ms step_avg:224.37ms
step:235/7050 train_time:52728ms step_avg:224.38ms
step:236/7050 train_time:52953ms step_avg:224.38ms
step:237/7050 train_time:53176ms step_avg:224.37ms
step:238/7050 train_time:53400ms step_avg:224.37ms
step:239/7050 train_time:53624ms step_avg:224.37ms
step:240/7050 train_time:53849ms step_avg:224.37ms
step:241/7050 train_time:54074ms step_avg:224.37ms
step:242/7050 train_time:54297ms step_avg:224.37ms
step:243/7050 train_time:54521ms step_avg:224.36ms
step:244/7050 train_time:54744ms step_avg:224.36ms
step:245/7050 train_time:54969ms step_avg:224.36ms
step:246/7050 train_time:55192ms step_avg:224.36ms
step:247/7050 train_time:55416ms step_avg:224.36ms
step:248/7050 train_time:55640ms step_avg:224.36ms
step:249/7050 train_time:55863ms step_avg:224.35ms
step:250/7050 train_time:56088ms step_avg:224.35ms
step:250/7050 val_loss:4.1832 train_time:56214ms step_avg:224.85ms
step:251/7050 train_time:56318ms step_avg:224.37ms
step:252/7050 train_time:56535ms step_avg:224.35ms
step:253/7050 train_time:56764ms step_avg:224.36ms
step:254/7050 train_time:56989ms step_avg:224.37ms
step:255/7050 train_time:57213ms step_avg:224.36ms
step:256/7050 train_time:57434ms step_avg:224.35ms
step:257/7050 train_time:57658ms step_avg:224.35ms
step:258/7050 train_time:57884ms step_avg:224.36ms
step:259/7050 train_time:58109ms step_avg:224.36ms
step:260/7050 train_time:58332ms step_avg:224.35ms
step:261/7050 train_time:58555ms step_avg:224.35ms
step:262/7050 train_time:58779ms step_avg:224.35ms
step:263/7050 train_time:59005ms step_avg:224.35ms
step:264/7050 train_time:59229ms step_avg:224.35ms
step:265/7050 train_time:59452ms step_avg:224.35ms
step:266/7050 train_time:59676ms step_avg:224.35ms
step:267/7050 train_time:59901ms step_avg:224.35ms
step:268/7050 train_time:60125ms step_avg:224.35ms
step:269/7050 train_time:60349ms step_avg:224.34ms
step:270/7050 train_time:60572ms step_avg:224.34ms
step:271/7050 train_time:60796ms step_avg:224.34ms
step:272/7050 train_time:61020ms step_avg:224.34ms
step:273/7050 train_time:61246ms step_avg:224.34ms
step:274/7050 train_time:61469ms step_avg:224.34ms
step:275/7050 train_time:61695ms step_avg:224.35ms
step:276/7050 train_time:61921ms step_avg:224.35ms
step:277/7050 train_time:62144ms step_avg:224.35ms
step:278/7050 train_time:62367ms step_avg:224.34ms
step:279/7050 train_time:62591ms step_avg:224.34ms
step:280/7050 train_time:62814ms step_avg:224.33ms
step:281/7050 train_time:63039ms step_avg:224.34ms
step:282/7050 train_time:63262ms step_avg:224.33ms
step:283/7050 train_time:63486ms step_avg:224.33ms
step:284/7050 train_time:63709ms step_avg:224.33ms
step:285/7050 train_time:63934ms step_avg:224.33ms
step:286/7050 train_time:64158ms step_avg:224.33ms
step:287/7050 train_time:64379ms step_avg:224.32ms
step:288/7050 train_time:64603ms step_avg:224.32ms
step:289/7050 train_time:64828ms step_avg:224.32ms
step:290/7050 train_time:65052ms step_avg:224.32ms
step:291/7050 train_time:65273ms step_avg:224.31ms
step:292/7050 train_time:65498ms step_avg:224.31ms
step:293/7050 train_time:65723ms step_avg:224.31ms
step:294/7050 train_time:65947ms step_avg:224.31ms
step:295/7050 train_time:66170ms step_avg:224.30ms
step:296/7050 train_time:66396ms step_avg:224.31ms
step:297/7050 train_time:66619ms step_avg:224.31ms
step:298/7050 train_time:66843ms step_avg:224.31ms
step:299/7050 train_time:67066ms step_avg:224.30ms
step:300/7050 train_time:67290ms step_avg:224.30ms
step:301/7050 train_time:67515ms step_avg:224.30ms
step:302/7050 train_time:67738ms step_avg:224.30ms
step:303/7050 train_time:67961ms step_avg:224.29ms
step:304/7050 train_time:68185ms step_avg:224.29ms
step:305/7050 train_time:68408ms step_avg:224.29ms
step:306/7050 train_time:68635ms step_avg:224.30ms
step:307/7050 train_time:68857ms step_avg:224.29ms
step:308/7050 train_time:69080ms step_avg:224.29ms
step:309/7050 train_time:69304ms step_avg:224.28ms
step:310/7050 train_time:69527ms step_avg:224.28ms
step:311/7050 train_time:69750ms step_avg:224.28ms
step:312/7050 train_time:69973ms step_avg:224.27ms
step:313/7050 train_time:70197ms step_avg:224.27ms
step:314/7050 train_time:70422ms step_avg:224.27ms
step:315/7050 train_time:70645ms step_avg:224.27ms
step:316/7050 train_time:70869ms step_avg:224.27ms
step:317/7050 train_time:71095ms step_avg:224.27ms
step:318/7050 train_time:71317ms step_avg:224.27ms
step:319/7050 train_time:71539ms step_avg:224.26ms
step:320/7050 train_time:71763ms step_avg:224.26ms
step:321/7050 train_time:71987ms step_avg:224.26ms
step:322/7050 train_time:72211ms step_avg:224.26ms
step:323/7050 train_time:72433ms step_avg:224.25ms
step:324/7050 train_time:72656ms step_avg:224.25ms
step:325/7050 train_time:72880ms step_avg:224.25ms
step:326/7050 train_time:73104ms step_avg:224.25ms
step:327/7050 train_time:73328ms step_avg:224.24ms
step:328/7050 train_time:73552ms step_avg:224.24ms
step:329/7050 train_time:73774ms step_avg:224.24ms
step:330/7050 train_time:73998ms step_avg:224.24ms
step:331/7050 train_time:74222ms step_avg:224.24ms
step:332/7050 train_time:74447ms step_avg:224.24ms
step:333/7050 train_time:74670ms step_avg:224.23ms
step:334/7050 train_time:74894ms step_avg:224.23ms
step:335/7050 train_time:75117ms step_avg:224.23ms
step:336/7050 train_time:75341ms step_avg:224.23ms
step:337/7050 train_time:75564ms step_avg:224.23ms
step:338/7050 train_time:75789ms step_avg:224.23ms
step:339/7050 train_time:76013ms step_avg:224.23ms
step:340/7050 train_time:76236ms step_avg:224.22ms
step:341/7050 train_time:76459ms step_avg:224.22ms
step:342/7050 train_time:76684ms step_avg:224.22ms
step:343/7050 train_time:76907ms step_avg:224.22ms
step:344/7050 train_time:77131ms step_avg:224.22ms
step:345/7050 train_time:77354ms step_avg:224.21ms
step:346/7050 train_time:77577ms step_avg:224.21ms
step:347/7050 train_time:77801ms step_avg:224.21ms
step:348/7050 train_time:78025ms step_avg:224.21ms
step:349/7050 train_time:78249ms step_avg:224.21ms
step:350/7050 train_time:78473ms step_avg:224.21ms
step:351/7050 train_time:78696ms step_avg:224.21ms
step:352/7050 train_time:78921ms step_avg:224.21ms
step:353/7050 train_time:79144ms step_avg:224.21ms
step:354/7050 train_time:79368ms step_avg:224.20ms
step:355/7050 train_time:79594ms step_avg:224.21ms
step:356/7050 train_time:79817ms step_avg:224.20ms
step:357/7050 train_time:80039ms step_avg:224.20ms
step:358/7050 train_time:80263ms step_avg:224.20ms
step:359/7050 train_time:80487ms step_avg:224.20ms
step:360/7050 train_time:80711ms step_avg:224.20ms
step:361/7050 train_time:80934ms step_avg:224.19ms
step:362/7050 train_time:81157ms step_avg:224.19ms
step:363/7050 train_time:81380ms step_avg:224.19ms
step:364/7050 train_time:81606ms step_avg:224.19ms
step:365/7050 train_time:81829ms step_avg:224.19ms
step:366/7050 train_time:82054ms step_avg:224.19ms
step:367/7050 train_time:82277ms step_avg:224.19ms
step:368/7050 train_time:82501ms step_avg:224.19ms
step:369/7050 train_time:82726ms step_avg:224.19ms
step:370/7050 train_time:82949ms step_avg:224.19ms
step:371/7050 train_time:83174ms step_avg:224.19ms
step:372/7050 train_time:83398ms step_avg:224.19ms
step:373/7050 train_time:83621ms step_avg:224.19ms
step:374/7050 train_time:83844ms step_avg:224.18ms
step:375/7050 train_time:84068ms step_avg:224.18ms
step:375/7050 val_loss:4.0344 train_time:84197ms step_avg:224.52ms
step:376/7050 train_time:84295ms step_avg:224.19ms
step:377/7050 train_time:84519ms step_avg:224.19ms
step:378/7050 train_time:84748ms step_avg:224.20ms
step:379/7050 train_time:84973ms step_avg:224.20ms
step:380/7050 train_time:85195ms step_avg:224.20ms
step:381/7050 train_time:85417ms step_avg:224.19ms
step:382/7050 train_time:85641ms step_avg:224.19ms
step:383/7050 train_time:85867ms step_avg:224.20ms
step:384/7050 train_time:86089ms step_avg:224.19ms
step:385/7050 train_time:86312ms step_avg:224.19ms
step:386/7050 train_time:86534ms step_avg:224.18ms
step:387/7050 train_time:86759ms step_avg:224.18ms
step:388/7050 train_time:86984ms step_avg:224.19ms
step:389/7050 train_time:87207ms step_avg:224.18ms
step:390/7050 train_time:87430ms step_avg:224.18ms
step:391/7050 train_time:87654ms step_avg:224.18ms
step:392/7050 train_time:87878ms step_avg:224.18ms
step:393/7050 train_time:88103ms step_avg:224.18ms
step:394/7050 train_time:88326ms step_avg:224.18ms
step:395/7050 train_time:88548ms step_avg:224.17ms
step:396/7050 train_time:88774ms step_avg:224.18ms
step:397/7050 train_time:88996ms step_avg:224.17ms
step:398/7050 train_time:89219ms step_avg:224.17ms
step:399/7050 train_time:89443ms step_avg:224.17ms
step:400/7050 train_time:89666ms step_avg:224.17ms
step:401/7050 train_time:89891ms step_avg:224.17ms
step:402/7050 train_time:90114ms step_avg:224.17ms
step:403/7050 train_time:90338ms step_avg:224.16ms
step:404/7050 train_time:90562ms step_avg:224.16ms
step:405/7050 train_time:90785ms step_avg:224.16ms
step:406/7050 train_time:91009ms step_avg:224.16ms
step:407/7050 train_time:91231ms step_avg:224.16ms
step:408/7050 train_time:91456ms step_avg:224.16ms
step:409/7050 train_time:91680ms step_avg:224.16ms
step:410/7050 train_time:91904ms step_avg:224.15ms
step:411/7050 train_time:92125ms step_avg:224.15ms
step:412/7050 train_time:92349ms step_avg:224.15ms
step:413/7050 train_time:92572ms step_avg:224.15ms
step:414/7050 train_time:92796ms step_avg:224.15ms
step:415/7050 train_time:93020ms step_avg:224.14ms
step:416/7050 train_time:93244ms step_avg:224.14ms
step:417/7050 train_time:93467ms step_avg:224.14ms
step:418/7050 train_time:93691ms step_avg:224.14ms
step:419/7050 train_time:93914ms step_avg:224.14ms
step:420/7050 train_time:94138ms step_avg:224.14ms
step:421/7050 train_time:94361ms step_avg:224.14ms
step:422/7050 train_time:94584ms step_avg:224.13ms
step:423/7050 train_time:94806ms step_avg:224.13ms
step:424/7050 train_time:95030ms step_avg:224.13ms
step:425/7050 train_time:95255ms step_avg:224.13ms
step:426/7050 train_time:95478ms step_avg:224.13ms
step:427/7050 train_time:95702ms step_avg:224.13ms
step:428/7050 train_time:95924ms step_avg:224.12ms
step:429/7050 train_time:96147ms step_avg:224.12ms
step:430/7050 train_time:96373ms step_avg:224.12ms
step:431/7050 train_time:96596ms step_avg:224.12ms
step:432/7050 train_time:96819ms step_avg:224.12ms
step:433/7050 train_time:97043ms step_avg:224.12ms
step:434/7050 train_time:97267ms step_avg:224.12ms
step:435/7050 train_time:97491ms step_avg:224.12ms
step:436/7050 train_time:97715ms step_avg:224.12ms
step:437/7050 train_time:97938ms step_avg:224.11ms
step:438/7050 train_time:98164ms step_avg:224.12ms
step:439/7050 train_time:98388ms step_avg:224.12ms
step:440/7050 train_time:98611ms step_avg:224.12ms
step:441/7050 train_time:98834ms step_avg:224.11ms
step:442/7050 train_time:99057ms step_avg:224.11ms
step:443/7050 train_time:99281ms step_avg:224.11ms
step:444/7050 train_time:99505ms step_avg:224.11ms
step:445/7050 train_time:99728ms step_avg:224.11ms
step:446/7050 train_time:99951ms step_avg:224.11ms
step:447/7050 train_time:100176ms step_avg:224.11ms
step:448/7050 train_time:100399ms step_avg:224.11ms
step:449/7050 train_time:100623ms step_avg:224.10ms
step:450/7050 train_time:100847ms step_avg:224.10ms
step:451/7050 train_time:101069ms step_avg:224.10ms
step:452/7050 train_time:101294ms step_avg:224.10ms
step:453/7050 train_time:101516ms step_avg:224.10ms
step:454/7050 train_time:101740ms step_avg:224.10ms
step:455/7050 train_time:101966ms step_avg:224.10ms
step:456/7050 train_time:102187ms step_avg:224.10ms
step:457/7050 train_time:102412ms step_avg:224.10ms
step:458/7050 train_time:102636ms step_avg:224.10ms
step:459/7050 train_time:102860ms step_avg:224.10ms
step:460/7050 train_time:103085ms step_avg:224.10ms
step:461/7050 train_time:103307ms step_avg:224.09ms
step:462/7050 train_time:103531ms step_avg:224.09ms
step:463/7050 train_time:103755ms step_avg:224.09ms
step:464/7050 train_time:103979ms step_avg:224.09ms
step:465/7050 train_time:104204ms step_avg:224.10ms
step:466/7050 train_time:104427ms step_avg:224.09ms
step:467/7050 train_time:104649ms step_avg:224.09ms
step:468/7050 train_time:104874ms step_avg:224.09ms
step:469/7050 train_time:105097ms step_avg:224.09ms
step:470/7050 train_time:105321ms step_avg:224.09ms
step:471/7050 train_time:105545ms step_avg:224.09ms
step:472/7050 train_time:105769ms step_avg:224.09ms
step:473/7050 train_time:105993ms step_avg:224.09ms
step:474/7050 train_time:106216ms step_avg:224.08ms
step:475/7050 train_time:106439ms step_avg:224.08ms
step:476/7050 train_time:106664ms step_avg:224.08ms
step:477/7050 train_time:106888ms step_avg:224.08ms
step:478/7050 train_time:107111ms step_avg:224.08ms
step:479/7050 train_time:107334ms step_avg:224.08ms
step:480/7050 train_time:107558ms step_avg:224.08ms
step:481/7050 train_time:107782ms step_avg:224.08ms
step:482/7050 train_time:108006ms step_avg:224.08ms
step:483/7050 train_time:108228ms step_avg:224.07ms
step:484/7050 train_time:108452ms step_avg:224.07ms
step:485/7050 train_time:108676ms step_avg:224.07ms
step:486/7050 train_time:108899ms step_avg:224.07ms
step:487/7050 train_time:109122ms step_avg:224.07ms
step:488/7050 train_time:109344ms step_avg:224.07ms
step:489/7050 train_time:109569ms step_avg:224.07ms
step:490/7050 train_time:109791ms step_avg:224.06ms
step:491/7050 train_time:110014ms step_avg:224.06ms
step:492/7050 train_time:110239ms step_avg:224.06ms
step:493/7050 train_time:110462ms step_avg:224.06ms
step:494/7050 train_time:110685ms step_avg:224.06ms
step:495/7050 train_time:110908ms step_avg:224.06ms
step:496/7050 train_time:111131ms step_avg:224.05ms
step:497/7050 train_time:111355ms step_avg:224.05ms
step:498/7050 train_time:111579ms step_avg:224.05ms
step:499/7050 train_time:111802ms step_avg:224.05ms
step:500/7050 train_time:112024ms step_avg:224.05ms
step:500/7050 val_loss:3.9388 train_time:112150ms step_avg:224.30ms
step:501/7050 train_time:112250ms step_avg:224.05ms
step:502/7050 train_time:112473ms step_avg:224.05ms
step:503/7050 train_time:112700ms step_avg:224.06ms
step:504/7050 train_time:112924ms step_avg:224.05ms
step:505/7050 train_time:113146ms step_avg:224.05ms
step:506/7050 train_time:113368ms step_avg:224.05ms
step:507/7050 train_time:113593ms step_avg:224.05ms
step:508/7050 train_time:113817ms step_avg:224.05ms
step:509/7050 train_time:114042ms step_avg:224.05ms
step:510/7050 train_time:114263ms step_avg:224.04ms
step:511/7050 train_time:114488ms step_avg:224.05ms
step:512/7050 train_time:114713ms step_avg:224.05ms
step:513/7050 train_time:114933ms step_avg:224.04ms
step:514/7050 train_time:115159ms step_avg:224.04ms
step:515/7050 train_time:115382ms step_avg:224.04ms
step:516/7050 train_time:115606ms step_avg:224.04ms
step:517/7050 train_time:115831ms step_avg:224.04ms
step:518/7050 train_time:116053ms step_avg:224.04ms
step:519/7050 train_time:116276ms step_avg:224.04ms
step:520/7050 train_time:116500ms step_avg:224.04ms
step:521/7050 train_time:116723ms step_avg:224.04ms
step:522/7050 train_time:116947ms step_avg:224.04ms
step:523/7050 train_time:117172ms step_avg:224.04ms
step:524/7050 train_time:117398ms step_avg:224.04ms
step:525/7050 train_time:117623ms step_avg:224.04ms
step:526/7050 train_time:117849ms step_avg:224.05ms
step:527/7050 train_time:118071ms step_avg:224.04ms
step:528/7050 train_time:118296ms step_avg:224.04ms
step:529/7050 train_time:118520ms step_avg:224.05ms
step:530/7050 train_time:118746ms step_avg:224.05ms
step:531/7050 train_time:118970ms step_avg:224.05ms
step:532/7050 train_time:119195ms step_avg:224.05ms
step:533/7050 train_time:119420ms step_avg:224.05ms
step:534/7050 train_time:119646ms step_avg:224.06ms
step:535/7050 train_time:119870ms step_avg:224.06ms
step:536/7050 train_time:120095ms step_avg:224.06ms
step:537/7050 train_time:120319ms step_avg:224.06ms
step:538/7050 train_time:120545ms step_avg:224.06ms
step:539/7050 train_time:120770ms step_avg:224.06ms
step:540/7050 train_time:120996ms step_avg:224.07ms
step:541/7050 train_time:121222ms step_avg:224.07ms
step:542/7050 train_time:121447ms step_avg:224.07ms
step:543/7050 train_time:121671ms step_avg:224.07ms
step:544/7050 train_time:121897ms step_avg:224.07ms
step:545/7050 train_time:122121ms step_avg:224.08ms
step:546/7050 train_time:122349ms step_avg:224.08ms
step:547/7050 train_time:122572ms step_avg:224.08ms
step:548/7050 train_time:122795ms step_avg:224.08ms
step:549/7050 train_time:123020ms step_avg:224.08ms
step:550/7050 train_time:123246ms step_avg:224.08ms
step:551/7050 train_time:123469ms step_avg:224.08ms
step:552/7050 train_time:123694ms step_avg:224.08ms
step:553/7050 train_time:123920ms step_avg:224.09ms
step:554/7050 train_time:124145ms step_avg:224.09ms
step:555/7050 train_time:124372ms step_avg:224.09ms
step:556/7050 train_time:124595ms step_avg:224.09ms
step:557/7050 train_time:124820ms step_avg:224.09ms
step:558/7050 train_time:125045ms step_avg:224.09ms
step:559/7050 train_time:125270ms step_avg:224.10ms
step:560/7050 train_time:125495ms step_avg:224.10ms
step:561/7050 train_time:125719ms step_avg:224.10ms
step:562/7050 train_time:125945ms step_avg:224.10ms
step:563/7050 train_time:126169ms step_avg:224.10ms
step:564/7050 train_time:126395ms step_avg:224.11ms
step:565/7050 train_time:126620ms step_avg:224.11ms
step:566/7050 train_time:126843ms step_avg:224.10ms
step:567/7050 train_time:127068ms step_avg:224.11ms
step:568/7050 train_time:127292ms step_avg:224.10ms
step:569/7050 train_time:127519ms step_avg:224.11ms
step:570/7050 train_time:127743ms step_avg:224.11ms
step:571/7050 train_time:127968ms step_avg:224.11ms
step:572/7050 train_time:128193ms step_avg:224.11ms
step:573/7050 train_time:128419ms step_avg:224.12ms
step:574/7050 train_time:128645ms step_avg:224.12ms
step:575/7050 train_time:128869ms step_avg:224.12ms
step:576/7050 train_time:129095ms step_avg:224.12ms
step:577/7050 train_time:129320ms step_avg:224.13ms
step:578/7050 train_time:129546ms step_avg:224.13ms
step:579/7050 train_time:129770ms step_avg:224.13ms
step:580/7050 train_time:129994ms step_avg:224.13ms
step:581/7050 train_time:130220ms step_avg:224.13ms
step:582/7050 train_time:130445ms step_avg:224.13ms
step:583/7050 train_time:130669ms step_avg:224.13ms
step:584/7050 train_time:130895ms step_avg:224.13ms
step:585/7050 train_time:131120ms step_avg:224.14ms
step:586/7050 train_time:131346ms step_avg:224.14ms
step:587/7050 train_time:131569ms step_avg:224.14ms
step:588/7050 train_time:131794ms step_avg:224.14ms
step:589/7050 train_time:132019ms step_avg:224.14ms
step:590/7050 train_time:132245ms step_avg:224.14ms
step:591/7050 train_time:132469ms step_avg:224.14ms
step:592/7050 train_time:132694ms step_avg:224.15ms
step:593/7050 train_time:132920ms step_avg:224.15ms
step:594/7050 train_time:133145ms step_avg:224.15ms
step:595/7050 train_time:133372ms step_avg:224.15ms
step:596/7050 train_time:133599ms step_avg:224.16ms
step:597/7050 train_time:133824ms step_avg:224.16ms
step:598/7050 train_time:134048ms step_avg:224.16ms
step:599/7050 train_time:134273ms step_avg:224.16ms
step:600/7050 train_time:134498ms step_avg:224.16ms
step:601/7050 train_time:134722ms step_avg:224.16ms
step:602/7050 train_time:134951ms step_avg:224.17ms
step:603/7050 train_time:135174ms step_avg:224.17ms
step:604/7050 train_time:135399ms step_avg:224.17ms
step:605/7050 train_time:135624ms step_avg:224.17ms
step:606/7050 train_time:135849ms step_avg:224.17ms
step:607/7050 train_time:136073ms step_avg:224.17ms
step:608/7050 train_time:136299ms step_avg:224.18ms
step:609/7050 train_time:136525ms step_avg:224.18ms
step:610/7050 train_time:136749ms step_avg:224.18ms
step:611/7050 train_time:136974ms step_avg:224.18ms
step:612/7050 train_time:137197ms step_avg:224.18ms
step:613/7050 train_time:137423ms step_avg:224.18ms
step:614/7050 train_time:137648ms step_avg:224.18ms
step:615/7050 train_time:137874ms step_avg:224.19ms
step:616/7050 train_time:138098ms step_avg:224.18ms
step:617/7050 train_time:138322ms step_avg:224.18ms
step:618/7050 train_time:138550ms step_avg:224.19ms
step:619/7050 train_time:138775ms step_avg:224.19ms
step:620/7050 train_time:139001ms step_avg:224.20ms
step:621/7050 train_time:139226ms step_avg:224.20ms
step:622/7050 train_time:139449ms step_avg:224.19ms
step:623/7050 train_time:139674ms step_avg:224.20ms
step:624/7050 train_time:139899ms step_avg:224.20ms
step:625/7050 train_time:140125ms step_avg:224.20ms
step:625/7050 val_loss:3.6546 train_time:140251ms step_avg:224.40ms
step:626/7050 train_time:140351ms step_avg:224.20ms
step:627/7050 train_time:140577ms step_avg:224.21ms
step:628/7050 train_time:140802ms step_avg:224.21ms
step:629/7050 train_time:141025ms step_avg:224.21ms
step:630/7050 train_time:141249ms step_avg:224.20ms
step:631/7050 train_time:141473ms step_avg:224.20ms
step:632/7050 train_time:141699ms step_avg:224.21ms
step:633/7050 train_time:141926ms step_avg:224.21ms
step:634/7050 train_time:142149ms step_avg:224.21ms
step:635/7050 train_time:142373ms step_avg:224.21ms
step:636/7050 train_time:142598ms step_avg:224.21ms
step:637/7050 train_time:142823ms step_avg:224.21ms
step:638/7050 train_time:143048ms step_avg:224.21ms
step:639/7050 train_time:143273ms step_avg:224.21ms
step:640/7050 train_time:143498ms step_avg:224.22ms
step:641/7050 train_time:143723ms step_avg:224.22ms
step:642/7050 train_time:143949ms step_avg:224.22ms
step:643/7050 train_time:144174ms step_avg:224.22ms
step:644/7050 train_time:144399ms step_avg:224.22ms
step:645/7050 train_time:144624ms step_avg:224.22ms
step:646/7050 train_time:144848ms step_avg:224.22ms
step:647/7050 train_time:145074ms step_avg:224.22ms
step:648/7050 train_time:145299ms step_avg:224.23ms
step:649/7050 train_time:145524ms step_avg:224.23ms
step:650/7050 train_time:145748ms step_avg:224.23ms
step:651/7050 train_time:145974ms step_avg:224.23ms
step:652/7050 train_time:146199ms step_avg:224.23ms
step:653/7050 train_time:146423ms step_avg:224.23ms
step:654/7050 train_time:146647ms step_avg:224.23ms
step:655/7050 train_time:146871ms step_avg:224.23ms
step:656/7050 train_time:147099ms step_avg:224.24ms
step:657/7050 train_time:147323ms step_avg:224.24ms
step:658/7050 train_time:147546ms step_avg:224.23ms
step:659/7050 train_time:147770ms step_avg:224.23ms
step:660/7050 train_time:147997ms step_avg:224.24ms
step:661/7050 train_time:148220ms step_avg:224.24ms
step:662/7050 train_time:148445ms step_avg:224.24ms
step:663/7050 train_time:148668ms step_avg:224.23ms
step:664/7050 train_time:148894ms step_avg:224.24ms
step:665/7050 train_time:149119ms step_avg:224.24ms
step:666/7050 train_time:149344ms step_avg:224.24ms
step:667/7050 train_time:149569ms step_avg:224.24ms
step:668/7050 train_time:149792ms step_avg:224.24ms
step:669/7050 train_time:150019ms step_avg:224.24ms
step:670/7050 train_time:150242ms step_avg:224.24ms
step:671/7050 train_time:150467ms step_avg:224.24ms
step:672/7050 train_time:150692ms step_avg:224.24ms
step:673/7050 train_time:150919ms step_avg:224.25ms
step:674/7050 train_time:151143ms step_avg:224.25ms
step:675/7050 train_time:151367ms step_avg:224.25ms
step:676/7050 train_time:151593ms step_avg:224.25ms
step:677/7050 train_time:151819ms step_avg:224.25ms
step:678/7050 train_time:152043ms step_avg:224.25ms
step:679/7050 train_time:152269ms step_avg:224.25ms
step:680/7050 train_time:152493ms step_avg:224.25ms
step:681/7050 train_time:152719ms step_avg:224.26ms
step:682/7050 train_time:152943ms step_avg:224.26ms
step:683/7050 train_time:153167ms step_avg:224.26ms
step:684/7050 train_time:153393ms step_avg:224.26ms
step:685/7050 train_time:153618ms step_avg:224.26ms
step:686/7050 train_time:153843ms step_avg:224.26ms
step:687/7050 train_time:154069ms step_avg:224.26ms
step:688/7050 train_time:154293ms step_avg:224.26ms
step:689/7050 train_time:154519ms step_avg:224.27ms
step:690/7050 train_time:154743ms step_avg:224.27ms
step:691/7050 train_time:154967ms step_avg:224.27ms
step:692/7050 train_time:155193ms step_avg:224.27ms
step:693/7050 train_time:155419ms step_avg:224.27ms
step:694/7050 train_time:155642ms step_avg:224.27ms
step:695/7050 train_time:155868ms step_avg:224.27ms
step:696/7050 train_time:156091ms step_avg:224.27ms
step:697/7050 train_time:156318ms step_avg:224.27ms
step:698/7050 train_time:156542ms step_avg:224.27ms
step:699/7050 train_time:156766ms step_avg:224.27ms
step:700/7050 train_time:156992ms step_avg:224.27ms
step:701/7050 train_time:157218ms step_avg:224.28ms
step:702/7050 train_time:157442ms step_avg:224.28ms
step:703/7050 train_time:157667ms step_avg:224.28ms
step:704/7050 train_time:157891ms step_avg:224.28ms
step:705/7050 train_time:158115ms step_avg:224.28ms
step:706/7050 train_time:158339ms step_avg:224.28ms
step:707/7050 train_time:158564ms step_avg:224.28ms
step:708/7050 train_time:158790ms step_avg:224.28ms
step:709/7050 train_time:159014ms step_avg:224.28ms
step:710/7050 train_time:159239ms step_avg:224.28ms
step:711/7050 train_time:159465ms step_avg:224.28ms
step:712/7050 train_time:159689ms step_avg:224.28ms
step:713/7050 train_time:159913ms step_avg:224.28ms
step:714/7050 train_time:160138ms step_avg:224.28ms
step:715/7050 train_time:160362ms step_avg:224.28ms
step:716/7050 train_time:160588ms step_avg:224.29ms
step:717/7050 train_time:160814ms step_avg:224.29ms
step:718/7050 train_time:161037ms step_avg:224.29ms
step:719/7050 train_time:161260ms step_avg:224.28ms
step:720/7050 train_time:161486ms step_avg:224.29ms
step:721/7050 train_time:161710ms step_avg:224.29ms
step:722/7050 train_time:161934ms step_avg:224.29ms
step:723/7050 train_time:162159ms step_avg:224.29ms
step:724/7050 train_time:162384ms step_avg:224.29ms
step:725/7050 train_time:162609ms step_avg:224.29ms
step:726/7050 train_time:162833ms step_avg:224.29ms
step:727/7050 train_time:163058ms step_avg:224.29ms
step:728/7050 train_time:163282ms step_avg:224.29ms
step:729/7050 train_time:163506ms step_avg:224.29ms
step:730/7050 train_time:163730ms step_avg:224.29ms
step:731/7050 train_time:163954ms step_avg:224.29ms
step:732/7050 train_time:164180ms step_avg:224.29ms
step:733/7050 train_time:164406ms step_avg:224.29ms
step:734/7050 train_time:164630ms step_avg:224.29ms
step:735/7050 train_time:164854ms step_avg:224.29ms
step:736/7050 train_time:165079ms step_avg:224.29ms
step:737/7050 train_time:165303ms step_avg:224.29ms
step:738/7050 train_time:165528ms step_avg:224.29ms
step:739/7050 train_time:165753ms step_avg:224.29ms
step:740/7050 train_time:165978ms step_avg:224.29ms
step:741/7050 train_time:166202ms step_avg:224.29ms
step:742/7050 train_time:166426ms step_avg:224.29ms
step:743/7050 train_time:166651ms step_avg:224.29ms
step:744/7050 train_time:166874ms step_avg:224.29ms
step:745/7050 train_time:167099ms step_avg:224.29ms
step:746/7050 train_time:167325ms step_avg:224.30ms
step:747/7050 train_time:167550ms step_avg:224.30ms
step:748/7050 train_time:167774ms step_avg:224.30ms
step:749/7050 train_time:167999ms step_avg:224.30ms
step:750/7050 train_time:168224ms step_avg:224.30ms
step:750/7050 val_loss:3.5881 train_time:168349ms step_avg:224.47ms
step:751/7050 train_time:168448ms step_avg:224.30ms
step:752/7050 train_time:168672ms step_avg:224.30ms
step:753/7050 train_time:168899ms step_avg:224.30ms
step:754/7050 train_time:169126ms step_avg:224.31ms
step:755/7050 train_time:169350ms step_avg:224.30ms
step:756/7050 train_time:169574ms step_avg:224.30ms
step:757/7050 train_time:169799ms step_avg:224.30ms
step:758/7050 train_time:170026ms step_avg:224.31ms
step:759/7050 train_time:170252ms step_avg:224.31ms
step:760/7050 train_time:170475ms step_avg:224.31ms
step:761/7050 train_time:170700ms step_avg:224.31ms
step:762/7050 train_time:170926ms step_avg:224.31ms
step:763/7050 train_time:171152ms step_avg:224.31ms
step:764/7050 train_time:171376ms step_avg:224.31ms
step:765/7050 train_time:171601ms step_avg:224.31ms
step:766/7050 train_time:171826ms step_avg:224.32ms
step:767/7050 train_time:172050ms step_avg:224.32ms
step:768/7050 train_time:172276ms step_avg:224.32ms
step:769/7050 train_time:172501ms step_avg:224.32ms
step:770/7050 train_time:172725ms step_avg:224.32ms
step:771/7050 train_time:172951ms step_avg:224.32ms
step:772/7050 train_time:173176ms step_avg:224.32ms
step:773/7050 train_time:173400ms step_avg:224.32ms
step:774/7050 train_time:173626ms step_avg:224.32ms
step:775/7050 train_time:173851ms step_avg:224.32ms
step:776/7050 train_time:174076ms step_avg:224.32ms
step:777/7050 train_time:174301ms step_avg:224.33ms
step:778/7050 train_time:174527ms step_avg:224.33ms
step:779/7050 train_time:174752ms step_avg:224.33ms
step:780/7050 train_time:174975ms step_avg:224.33ms
step:781/7050 train_time:175200ms step_avg:224.33ms
step:782/7050 train_time:175426ms step_avg:224.33ms
step:783/7050 train_time:175650ms step_avg:224.33ms
step:784/7050 train_time:175877ms step_avg:224.33ms
step:785/7050 train_time:176100ms step_avg:224.33ms
step:786/7050 train_time:176326ms step_avg:224.33ms
step:787/7050 train_time:176551ms step_avg:224.33ms
step:788/7050 train_time:176776ms step_avg:224.33ms
step:789/7050 train_time:176999ms step_avg:224.33ms
step:790/7050 train_time:177223ms step_avg:224.33ms
step:791/7050 train_time:177449ms step_avg:224.33ms
step:792/7050 train_time:177673ms step_avg:224.34ms
step:793/7050 train_time:177898ms step_avg:224.34ms
step:794/7050 train_time:178122ms step_avg:224.34ms
step:795/7050 train_time:178348ms step_avg:224.34ms
step:796/7050 train_time:178574ms step_avg:224.34ms
step:797/7050 train_time:178798ms step_avg:224.34ms
step:798/7050 train_time:179022ms step_avg:224.34ms
step:799/7050 train_time:179248ms step_avg:224.34ms
step:800/7050 train_time:179472ms step_avg:224.34ms
step:801/7050 train_time:179699ms step_avg:224.34ms
step:802/7050 train_time:179925ms step_avg:224.35ms
step:803/7050 train_time:180149ms step_avg:224.35ms
step:804/7050 train_time:180374ms step_avg:224.35ms
step:805/7050 train_time:180600ms step_avg:224.35ms
step:806/7050 train_time:180825ms step_avg:224.35ms
step:807/7050 train_time:181050ms step_avg:224.35ms
step:808/7050 train_time:181275ms step_avg:224.35ms
step:809/7050 train_time:181499ms step_avg:224.35ms
step:810/7050 train_time:181726ms step_avg:224.35ms
step:811/7050 train_time:181950ms step_avg:224.35ms
step:812/7050 train_time:182176ms step_avg:224.35ms
step:813/7050 train_time:182399ms step_avg:224.35ms
step:814/7050 train_time:182627ms step_avg:224.36ms
step:815/7050 train_time:182852ms step_avg:224.36ms
step:816/7050 train_time:183078ms step_avg:224.36ms
step:817/7050 train_time:183303ms step_avg:224.36ms
step:818/7050 train_time:183528ms step_avg:224.36ms
step:819/7050 train_time:183753ms step_avg:224.36ms
step:820/7050 train_time:183977ms step_avg:224.36ms
step:821/7050 train_time:184202ms step_avg:224.36ms
step:822/7050 train_time:184428ms step_avg:224.36ms
step:823/7050 train_time:184653ms step_avg:224.37ms
step:824/7050 train_time:184876ms step_avg:224.36ms
step:825/7050 train_time:185100ms step_avg:224.36ms
step:826/7050 train_time:185324ms step_avg:224.36ms
step:827/7050 train_time:185549ms step_avg:224.36ms
step:828/7050 train_time:185774ms step_avg:224.36ms
step:829/7050 train_time:185999ms step_avg:224.37ms
step:830/7050 train_time:186224ms step_avg:224.37ms
step:831/7050 train_time:186450ms step_avg:224.37ms
step:832/7050 train_time:186674ms step_avg:224.37ms
step:833/7050 train_time:186899ms step_avg:224.37ms
step:834/7050 train_time:187124ms step_avg:224.37ms
step:835/7050 train_time:187348ms step_avg:224.37ms
step:836/7050 train_time:187575ms step_avg:224.37ms
step:837/7050 train_time:187799ms step_avg:224.37ms
step:838/7050 train_time:188025ms step_avg:224.37ms
step:839/7050 train_time:188250ms step_avg:224.37ms
step:840/7050 train_time:188474ms step_avg:224.37ms
step:841/7050 train_time:188698ms step_avg:224.37ms
step:842/7050 train_time:188924ms step_avg:224.38ms
step:843/7050 train_time:189149ms step_avg:224.38ms
step:844/7050 train_time:189373ms step_avg:224.38ms
step:845/7050 train_time:189597ms step_avg:224.38ms
step:846/7050 train_time:189822ms step_avg:224.38ms
step:847/7050 train_time:190048ms step_avg:224.38ms
step:848/7050 train_time:190272ms step_avg:224.38ms
step:849/7050 train_time:190496ms step_avg:224.38ms
step:850/7050 train_time:190723ms step_avg:224.38ms
step:851/7050 train_time:190946ms step_avg:224.38ms
step:852/7050 train_time:191171ms step_avg:224.38ms
step:853/7050 train_time:191397ms step_avg:224.38ms
step:854/7050 train_time:191623ms step_avg:224.38ms
step:855/7050 train_time:191848ms step_avg:224.38ms
step:856/7050 train_time:192072ms step_avg:224.38ms
step:857/7050 train_time:192297ms step_avg:224.38ms
step:858/7050 train_time:192521ms step_avg:224.38ms
step:859/7050 train_time:192746ms step_avg:224.38ms
step:860/7050 train_time:192971ms step_avg:224.38ms
step:861/7050 train_time:193195ms step_avg:224.38ms
step:862/7050 train_time:193420ms step_avg:224.39ms
step:863/7050 train_time:193645ms step_avg:224.39ms
step:864/7050 train_time:193873ms step_avg:224.39ms
step:865/7050 train_time:194096ms step_avg:224.39ms
step:866/7050 train_time:194319ms step_avg:224.39ms
step:867/7050 train_time:194544ms step_avg:224.39ms
step:868/7050 train_time:194769ms step_avg:224.39ms
step:869/7050 train_time:194996ms step_avg:224.39ms
step:870/7050 train_time:195218ms step_avg:224.39ms
step:871/7050 train_time:195444ms step_avg:224.39ms
step:872/7050 train_time:195669ms step_avg:224.39ms
step:873/7050 train_time:195893ms step_avg:224.39ms
step:874/7050 train_time:196118ms step_avg:224.39ms
step:875/7050 train_time:196345ms step_avg:224.39ms
step:875/7050 val_loss:3.5348 train_time:196472ms step_avg:224.54ms
step:876/7050 train_time:196570ms step_avg:224.39ms
step:877/7050 train_time:196794ms step_avg:224.39ms
step:878/7050 train_time:197021ms step_avg:224.40ms
step:879/7050 train_time:197245ms step_avg:224.40ms
step:880/7050 train_time:197471ms step_avg:224.40ms
step:881/7050 train_time:197695ms step_avg:224.40ms
step:882/7050 train_time:197919ms step_avg:224.40ms
step:883/7050 train_time:198145ms step_avg:224.40ms
step:884/7050 train_time:198370ms step_avg:224.40ms
step:885/7050 train_time:198595ms step_avg:224.40ms
step:886/7050 train_time:198819ms step_avg:224.40ms
step:887/7050 train_time:199045ms step_avg:224.40ms
step:888/7050 train_time:199269ms step_avg:224.40ms
step:889/7050 train_time:199494ms step_avg:224.40ms
step:890/7050 train_time:199719ms step_avg:224.40ms
step:891/7050 train_time:199943ms step_avg:224.40ms
step:892/7050 train_time:200169ms step_avg:224.40ms
step:893/7050 train_time:200394ms step_avg:224.41ms
step:894/7050 train_time:200618ms step_avg:224.41ms
step:895/7050 train_time:200842ms step_avg:224.40ms
step:896/7050 train_time:201066ms step_avg:224.40ms
step:897/7050 train_time:201292ms step_avg:224.41ms
step:898/7050 train_time:201516ms step_avg:224.41ms
step:899/7050 train_time:201742ms step_avg:224.41ms
step:900/7050 train_time:201965ms step_avg:224.41ms
step:901/7050 train_time:202190ms step_avg:224.41ms
step:902/7050 train_time:202416ms step_avg:224.41ms
step:903/7050 train_time:202641ms step_avg:224.41ms
step:904/7050 train_time:202864ms step_avg:224.41ms
step:905/7050 train_time:203090ms step_avg:224.41ms
step:906/7050 train_time:203314ms step_avg:224.41ms
step:907/7050 train_time:203538ms step_avg:224.41ms
step:908/7050 train_time:203763ms step_avg:224.41ms
step:909/7050 train_time:203989ms step_avg:224.41ms
step:910/7050 train_time:204214ms step_avg:224.41ms
step:911/7050 train_time:204438ms step_avg:224.41ms
step:912/7050 train_time:204663ms step_avg:224.41ms
step:913/7050 train_time:204888ms step_avg:224.41ms
step:914/7050 train_time:205114ms step_avg:224.41ms
step:915/7050 train_time:205338ms step_avg:224.41ms
step:916/7050 train_time:205563ms step_avg:224.41ms
step:917/7050 train_time:205787ms step_avg:224.41ms
step:918/7050 train_time:206014ms step_avg:224.42ms
step:919/7050 train_time:206239ms step_avg:224.42ms
step:920/7050 train_time:206464ms step_avg:224.42ms
step:921/7050 train_time:206687ms step_avg:224.42ms
step:922/7050 train_time:206913ms step_avg:224.42ms
step:923/7050 train_time:207138ms step_avg:224.42ms
step:924/7050 train_time:207363ms step_avg:224.42ms
step:925/7050 train_time:207587ms step_avg:224.42ms
step:926/7050 train_time:207811ms step_avg:224.42ms
step:927/7050 train_time:208036ms step_avg:224.42ms
step:928/7050 train_time:208261ms step_avg:224.42ms
step:929/7050 train_time:208486ms step_avg:224.42ms
step:930/7050 train_time:208711ms step_avg:224.42ms
step:931/7050 train_time:208936ms step_avg:224.42ms
step:932/7050 train_time:209162ms step_avg:224.42ms
step:933/7050 train_time:209388ms step_avg:224.42ms
step:934/7050 train_time:209612ms step_avg:224.42ms
step:935/7050 train_time:209839ms step_avg:224.43ms
step:936/7050 train_time:210063ms step_avg:224.43ms
step:937/7050 train_time:210288ms step_avg:224.43ms
step:938/7050 train_time:210514ms step_avg:224.43ms
step:939/7050 train_time:210738ms step_avg:224.43ms
step:940/7050 train_time:210963ms step_avg:224.43ms
step:941/7050 train_time:211188ms step_avg:224.43ms
step:942/7050 train_time:211413ms step_avg:224.43ms
step:943/7050 train_time:211637ms step_avg:224.43ms
step:944/7050 train_time:211862ms step_avg:224.43ms
step:945/7050 train_time:212086ms step_avg:224.43ms
step:946/7050 train_time:212312ms step_avg:224.43ms
step:947/7050 train_time:212536ms step_avg:224.43ms
step:948/7050 train_time:212761ms step_avg:224.43ms
step:949/7050 train_time:212986ms step_avg:224.43ms
step:950/7050 train_time:213211ms step_avg:224.43ms
step:951/7050 train_time:213436ms step_avg:224.43ms
step:952/7050 train_time:213658ms step_avg:224.43ms
step:953/7050 train_time:213884ms step_avg:224.43ms
step:954/7050 train_time:214108ms step_avg:224.43ms
step:955/7050 train_time:214333ms step_avg:224.43ms
step:956/7050 train_time:214558ms step_avg:224.43ms
step:957/7050 train_time:214783ms step_avg:224.43ms
step:958/7050 train_time:215008ms step_avg:224.43ms
step:959/7050 train_time:215233ms step_avg:224.43ms
step:960/7050 train_time:215459ms step_avg:224.44ms
step:961/7050 train_time:215684ms step_avg:224.44ms
step:962/7050 train_time:215909ms step_avg:224.44ms
step:963/7050 train_time:216134ms step_avg:224.44ms
step:964/7050 train_time:216359ms step_avg:224.44ms
step:965/7050 train_time:216583ms step_avg:224.44ms
step:966/7050 train_time:216808ms step_avg:224.44ms
step:967/7050 train_time:217034ms step_avg:224.44ms
step:968/7050 train_time:217259ms step_avg:224.44ms
step:969/7050 train_time:217486ms step_avg:224.44ms
step:970/7050 train_time:217710ms step_avg:224.44ms
step:971/7050 train_time:217934ms step_avg:224.44ms
step:972/7050 train_time:218159ms step_avg:224.44ms
step:973/7050 train_time:218384ms step_avg:224.44ms
step:974/7050 train_time:218609ms step_avg:224.44ms
step:975/7050 train_time:218833ms step_avg:224.44ms
step:976/7050 train_time:219058ms step_avg:224.44ms
step:977/7050 train_time:219282ms step_avg:224.44ms
step:978/7050 train_time:219507ms step_avg:224.44ms
step:979/7050 train_time:219731ms step_avg:224.44ms
step:980/7050 train_time:219955ms step_avg:224.44ms
step:981/7050 train_time:220180ms step_avg:224.44ms
step:982/7050 train_time:220405ms step_avg:224.45ms
step:983/7050 train_time:220630ms step_avg:224.45ms
step:984/7050 train_time:220854ms step_avg:224.45ms
step:985/7050 train_time:221079ms step_avg:224.45ms
step:986/7050 train_time:221304ms step_avg:224.45ms
step:987/7050 train_time:221529ms step_avg:224.45ms
step:988/7050 train_time:221754ms step_avg:224.45ms
step:989/7050 train_time:221981ms step_avg:224.45ms
step:990/7050 train_time:222205ms step_avg:224.45ms
step:991/7050 train_time:222431ms step_avg:224.45ms
step:992/7050 train_time:222655ms step_avg:224.45ms
step:993/7050 train_time:222881ms step_avg:224.45ms
step:994/7050 train_time:223105ms step_avg:224.45ms
step:995/7050 train_time:223330ms step_avg:224.45ms
step:996/7050 train_time:223553ms step_avg:224.45ms
step:997/7050 train_time:223778ms step_avg:224.45ms
step:998/7050 train_time:224004ms step_avg:224.45ms
step:999/7050 train_time:224229ms step_avg:224.45ms
step:1000/7050 train_time:224455ms step_avg:224.45ms
step:1000/7050 val_loss:3.4881 train_time:224580ms step_avg:224.58ms
step:1001/7050 train_time:224682ms step_avg:224.46ms
step:1002/7050 train_time:224905ms step_avg:224.46ms
step:1003/7050 train_time:225132ms step_avg:224.46ms
step:1004/7050 train_time:225356ms step_avg:224.46ms
step:1005/7050 train_time:225579ms step_avg:224.46ms
step:1006/7050 train_time:225803ms step_avg:224.46ms
step:1007/7050 train_time:226031ms step_avg:224.46ms
step:1008/7050 train_time:226256ms step_avg:224.46ms
step:1009/7050 train_time:226480ms step_avg:224.46ms
step:1010/7050 train_time:226703ms step_avg:224.46ms
step:1011/7050 train_time:226929ms step_avg:224.46ms
step:1012/7050 train_time:227156ms step_avg:224.46ms
step:1013/7050 train_time:227384ms step_avg:224.47ms
step:1014/7050 train_time:227607ms step_avg:224.46ms
step:1015/7050 train_time:227832ms step_avg:224.47ms
step:1016/7050 train_time:228058ms step_avg:224.47ms
step:1017/7050 train_time:228284ms step_avg:224.47ms
step:1018/7050 train_time:228511ms step_avg:224.47ms
step:1019/7050 train_time:228733ms step_avg:224.47ms
step:1020/7050 train_time:228959ms step_avg:224.47ms
step:1021/7050 train_time:229184ms step_avg:224.47ms
step:1022/7050 train_time:229408ms step_avg:224.47ms
step:1023/7050 train_time:229633ms step_avg:224.47ms
step:1024/7050 train_time:229857ms step_avg:224.47ms
step:1025/7050 train_time:230081ms step_avg:224.47ms
step:1026/7050 train_time:230307ms step_avg:224.47ms
step:1027/7050 train_time:230533ms step_avg:224.47ms
step:1028/7050 train_time:230760ms step_avg:224.48ms
step:1029/7050 train_time:230983ms step_avg:224.47ms
step:1030/7050 train_time:231208ms step_avg:224.47ms
step:1031/7050 train_time:231433ms step_avg:224.47ms
step:1032/7050 train_time:231660ms step_avg:224.48ms
step:1033/7050 train_time:231883ms step_avg:224.48ms
step:1034/7050 train_time:232108ms step_avg:224.48ms
step:1035/7050 train_time:232335ms step_avg:224.48ms
step:1036/7050 train_time:232561ms step_avg:224.48ms
step:1037/7050 train_time:232787ms step_avg:224.48ms
step:1038/7050 train_time:233012ms step_avg:224.48ms
step:1039/7050 train_time:233237ms step_avg:224.48ms
step:1040/7050 train_time:233465ms step_avg:224.49ms
step:1041/7050 train_time:233691ms step_avg:224.49ms
step:1042/7050 train_time:233915ms step_avg:224.49ms
step:1043/7050 train_time:234139ms step_avg:224.49ms
step:1044/7050 train_time:234365ms step_avg:224.49ms
step:1045/7050 train_time:234590ms step_avg:224.49ms
step:1046/7050 train_time:234816ms step_avg:224.49ms
step:1047/7050 train_time:235043ms step_avg:224.49ms
step:1048/7050 train_time:235269ms step_avg:224.49ms
step:1049/7050 train_time:235494ms step_avg:224.49ms
step:1050/7050 train_time:235722ms step_avg:224.50ms
step:1051/7050 train_time:235949ms step_avg:224.50ms
step:1052/7050 train_time:236174ms step_avg:224.50ms
step:1053/7050 train_time:236400ms step_avg:224.50ms
step:1054/7050 train_time:236627ms step_avg:224.50ms
step:1055/7050 train_time:236854ms step_avg:224.51ms
step:1056/7050 train_time:237080ms step_avg:224.51ms
step:1057/7050 train_time:237305ms step_avg:224.51ms
step:1058/7050 train_time:237531ms step_avg:224.51ms
step:1059/7050 train_time:237761ms step_avg:224.51ms
step:1060/7050 train_time:237986ms step_avg:224.52ms
step:1061/7050 train_time:238215ms step_avg:224.52ms
step:1062/7050 train_time:238441ms step_avg:224.52ms
step:1063/7050 train_time:238668ms step_avg:224.52ms
step:1064/7050 train_time:238894ms step_avg:224.52ms
step:1065/7050 train_time:239119ms step_avg:224.52ms
step:1066/7050 train_time:239345ms step_avg:224.53ms
step:1067/7050 train_time:239571ms step_avg:224.53ms
step:1068/7050 train_time:239797ms step_avg:224.53ms
step:1069/7050 train_time:240024ms step_avg:224.53ms
step:1070/7050 train_time:240252ms step_avg:224.53ms
step:1071/7050 train_time:240476ms step_avg:224.53ms
step:1072/7050 train_time:240702ms step_avg:224.54ms
step:1073/7050 train_time:240929ms step_avg:224.54ms
step:1074/7050 train_time:241156ms step_avg:224.54ms
step:1075/7050 train_time:241383ms step_avg:224.54ms
step:1076/7050 train_time:241608ms step_avg:224.54ms
step:1077/7050 train_time:241834ms step_avg:224.54ms
step:1078/7050 train_time:242061ms step_avg:224.55ms
step:1079/7050 train_time:242287ms step_avg:224.55ms
step:1080/7050 train_time:242512ms step_avg:224.55ms
step:1081/7050 train_time:242738ms step_avg:224.55ms
step:1082/7050 train_time:242965ms step_avg:224.55ms
step:1083/7050 train_time:243191ms step_avg:224.55ms
step:1084/7050 train_time:243417ms step_avg:224.55ms
step:1085/7050 train_time:243640ms step_avg:224.55ms
step:1086/7050 train_time:243867ms step_avg:224.55ms
step:1087/7050 train_time:244093ms step_avg:224.56ms
step:1088/7050 train_time:244318ms step_avg:224.56ms
step:1089/7050 train_time:244546ms step_avg:224.56ms
step:1090/7050 train_time:244772ms step_avg:224.56ms
step:1091/7050 train_time:244998ms step_avg:224.56ms
step:1092/7050 train_time:245225ms step_avg:224.56ms
step:1093/7050 train_time:245452ms step_avg:224.57ms
step:1094/7050 train_time:245677ms step_avg:224.57ms
step:1095/7050 train_time:245901ms step_avg:224.57ms
step:1096/7050 train_time:246128ms step_avg:224.57ms
step:1097/7050 train_time:246354ms step_avg:224.57ms
step:1098/7050 train_time:246582ms step_avg:224.57ms
step:1099/7050 train_time:246807ms step_avg:224.57ms
step:1100/7050 train_time:247033ms step_avg:224.58ms
step:1101/7050 train_time:247259ms step_avg:224.58ms
step:1102/7050 train_time:247487ms step_avg:224.58ms
step:1103/7050 train_time:247712ms step_avg:224.58ms
step:1104/7050 train_time:247936ms step_avg:224.58ms
step:1105/7050 train_time:248164ms step_avg:224.58ms
step:1106/7050 train_time:248390ms step_avg:224.58ms
step:1107/7050 train_time:248617ms step_avg:224.59ms
step:1108/7050 train_time:248840ms step_avg:224.58ms
step:1109/7050 train_time:249067ms step_avg:224.59ms
step:1110/7050 train_time:249292ms step_avg:224.59ms
step:1111/7050 train_time:249519ms step_avg:224.59ms
step:1112/7050 train_time:249745ms step_avg:224.59ms
step:1113/7050 train_time:249970ms step_avg:224.59ms
step:1114/7050 train_time:250196ms step_avg:224.59ms
step:1115/7050 train_time:250422ms step_avg:224.59ms
step:1116/7050 train_time:250647ms step_avg:224.59ms
step:1117/7050 train_time:250874ms step_avg:224.60ms
step:1118/7050 train_time:251098ms step_avg:224.60ms
step:1119/7050 train_time:251326ms step_avg:224.60ms
step:1120/7050 train_time:251554ms step_avg:224.60ms
step:1121/7050 train_time:251778ms step_avg:224.60ms
step:1122/7050 train_time:252004ms step_avg:224.60ms
step:1123/7050 train_time:252230ms step_avg:224.60ms
step:1124/7050 train_time:252458ms step_avg:224.61ms
step:1125/7050 train_time:252682ms step_avg:224.61ms
step:1125/7050 val_loss:3.4255 train_time:252811ms step_avg:224.72ms
step:1126/7050 train_time:252912ms step_avg:224.61ms
step:1127/7050 train_time:253134ms step_avg:224.61ms
step:1128/7050 train_time:253363ms step_avg:224.61ms
step:1129/7050 train_time:253591ms step_avg:224.62ms
step:1130/7050 train_time:253817ms step_avg:224.62ms
step:1131/7050 train_time:254041ms step_avg:224.62ms
step:1132/7050 train_time:254267ms step_avg:224.62ms
step:1133/7050 train_time:254495ms step_avg:224.62ms
step:1134/7050 train_time:254723ms step_avg:224.62ms
step:1135/7050 train_time:254948ms step_avg:224.62ms
step:1136/7050 train_time:255176ms step_avg:224.63ms
step:1137/7050 train_time:255403ms step_avg:224.63ms
step:1138/7050 train_time:255630ms step_avg:224.63ms
step:1139/7050 train_time:255856ms step_avg:224.63ms
step:1140/7050 train_time:256081ms step_avg:224.63ms
step:1141/7050 train_time:256306ms step_avg:224.63ms
step:1142/7050 train_time:256534ms step_avg:224.64ms
step:1143/7050 train_time:256761ms step_avg:224.64ms
step:1144/7050 train_time:256987ms step_avg:224.64ms
step:1145/7050 train_time:257213ms step_avg:224.64ms
step:1146/7050 train_time:257440ms step_avg:224.64ms
step:1147/7050 train_time:257665ms step_avg:224.64ms
step:1148/7050 train_time:257891ms step_avg:224.64ms
step:1149/7050 train_time:258119ms step_avg:224.65ms
step:1150/7050 train_time:258345ms step_avg:224.65ms
step:1151/7050 train_time:258570ms step_avg:224.65ms
step:1152/7050 train_time:258796ms step_avg:224.65ms
step:1153/7050 train_time:259023ms step_avg:224.65ms
step:1154/7050 train_time:259246ms step_avg:224.65ms
step:1155/7050 train_time:259473ms step_avg:224.65ms
step:1156/7050 train_time:259699ms step_avg:224.65ms
step:1157/7050 train_time:259926ms step_avg:224.66ms
step:1158/7050 train_time:260151ms step_avg:224.66ms
step:1159/7050 train_time:260376ms step_avg:224.66ms
step:1160/7050 train_time:260606ms step_avg:224.66ms
step:1161/7050 train_time:260832ms step_avg:224.66ms
step:1162/7050 train_time:261057ms step_avg:224.66ms
step:1163/7050 train_time:261283ms step_avg:224.66ms
step:1164/7050 train_time:261510ms step_avg:224.66ms
step:1165/7050 train_time:261735ms step_avg:224.67ms
step:1166/7050 train_time:261961ms step_avg:224.67ms
step:1167/7050 train_time:262185ms step_avg:224.67ms
step:1168/7050 train_time:262412ms step_avg:224.67ms
step:1169/7050 train_time:262639ms step_avg:224.67ms
step:1170/7050 train_time:262865ms step_avg:224.67ms
step:1171/7050 train_time:263091ms step_avg:224.67ms
step:1172/7050 train_time:263318ms step_avg:224.67ms
step:1173/7050 train_time:263545ms step_avg:224.68ms
step:1174/7050 train_time:263771ms step_avg:224.68ms
step:1175/7050 train_time:263999ms step_avg:224.68ms
step:1176/7050 train_time:264223ms step_avg:224.68ms
step:1177/7050 train_time:264450ms step_avg:224.68ms
step:1178/7050 train_time:264675ms step_avg:224.68ms
step:1179/7050 train_time:264904ms step_avg:224.68ms
step:1180/7050 train_time:265128ms step_avg:224.68ms
step:1181/7050 train_time:265354ms step_avg:224.69ms
step:1182/7050 train_time:265580ms step_avg:224.69ms
step:1183/7050 train_time:265808ms step_avg:224.69ms
step:1184/7050 train_time:266034ms step_avg:224.69ms
step:1185/7050 train_time:266259ms step_avg:224.69ms
step:1186/7050 train_time:266486ms step_avg:224.69ms
step:1187/7050 train_time:266712ms step_avg:224.69ms
step:1188/7050 train_time:266937ms step_avg:224.69ms
step:1189/7050 train_time:267163ms step_avg:224.70ms
step:1190/7050 train_time:267390ms step_avg:224.70ms
step:1191/7050 train_time:267617ms step_avg:224.70ms
step:1192/7050 train_time:267845ms step_avg:224.70ms
step:1193/7050 train_time:268070ms step_avg:224.70ms
step:1194/7050 train_time:268297ms step_avg:224.70ms
step:1195/7050 train_time:268524ms step_avg:224.71ms
step:1196/7050 train_time:268750ms step_avg:224.71ms
step:1197/7050 train_time:268976ms step_avg:224.71ms
step:1198/7050 train_time:269202ms step_avg:224.71ms
step:1199/7050 train_time:269429ms step_avg:224.71ms
step:1200/7050 train_time:269655ms step_avg:224.71ms
step:1201/7050 train_time:269880ms step_avg:224.71ms
step:1202/7050 train_time:270107ms step_avg:224.71ms
step:1203/7050 train_time:270332ms step_avg:224.72ms
step:1204/7050 train_time:270558ms step_avg:224.72ms
step:1205/7050 train_time:270783ms step_avg:224.72ms
step:1206/7050 train_time:271009ms step_avg:224.72ms
step:1207/7050 train_time:271236ms step_avg:224.72ms
step:1208/7050 train_time:271459ms step_avg:224.72ms
step:1209/7050 train_time:271686ms step_avg:224.72ms
step:1210/7050 train_time:271912ms step_avg:224.72ms
step:1211/7050 train_time:272139ms step_avg:224.72ms
step:1212/7050 train_time:272364ms step_avg:224.72ms
step:1213/7050 train_time:272591ms step_avg:224.72ms
step:1214/7050 train_time:272817ms step_avg:224.73ms
step:1215/7050 train_time:273043ms step_avg:224.73ms
step:1216/7050 train_time:273269ms step_avg:224.73ms
step:1217/7050 train_time:273495ms step_avg:224.73ms
step:1218/7050 train_time:273723ms step_avg:224.73ms
step:1219/7050 train_time:273948ms step_avg:224.73ms
step:1220/7050 train_time:274174ms step_avg:224.73ms
step:1221/7050 train_time:274400ms step_avg:224.73ms
step:1222/7050 train_time:274628ms step_avg:224.74ms
step:1223/7050 train_time:274850ms step_avg:224.73ms
step:1224/7050 train_time:275077ms step_avg:224.74ms
step:1225/7050 train_time:275305ms step_avg:224.74ms
step:1226/7050 train_time:275531ms step_avg:224.74ms
step:1227/7050 train_time:275758ms step_avg:224.74ms
step:1228/7050 train_time:275983ms step_avg:224.74ms
step:1229/7050 train_time:276210ms step_avg:224.74ms
step:1230/7050 train_time:276436ms step_avg:224.74ms
step:1231/7050 train_time:276662ms step_avg:224.75ms
step:1232/7050 train_time:276888ms step_avg:224.75ms
step:1233/7050 train_time:277114ms step_avg:224.75ms
step:1234/7050 train_time:277341ms step_avg:224.75ms
step:1235/7050 train_time:277568ms step_avg:224.75ms
step:1236/7050 train_time:277793ms step_avg:224.75ms
step:1237/7050 train_time:278021ms step_avg:224.75ms
step:1238/7050 train_time:278246ms step_avg:224.75ms
step:1239/7050 train_time:278472ms step_avg:224.76ms
step:1240/7050 train_time:278699ms step_avg:224.76ms
step:1241/7050 train_time:278925ms step_avg:224.76ms
step:1242/7050 train_time:279149ms step_avg:224.76ms
step:1243/7050 train_time:279375ms step_avg:224.76ms
step:1244/7050 train_time:279603ms step_avg:224.76ms
step:1245/7050 train_time:279827ms step_avg:224.76ms
step:1246/7050 train_time:280056ms step_avg:224.76ms
step:1247/7050 train_time:280281ms step_avg:224.76ms
step:1248/7050 train_time:280508ms step_avg:224.77ms
step:1249/7050 train_time:280735ms step_avg:224.77ms
step:1250/7050 train_time:280960ms step_avg:224.77ms
step:1250/7050 val_loss:3.3942 train_time:281088ms step_avg:224.87ms
step:1251/7050 train_time:281187ms step_avg:224.77ms
step:1252/7050 train_time:281416ms step_avg:224.77ms
step:1253/7050 train_time:281645ms step_avg:224.78ms
step:1254/7050 train_time:281872ms step_avg:224.78ms
step:1255/7050 train_time:282096ms step_avg:224.78ms
step:1256/7050 train_time:282321ms step_avg:224.78ms
step:1257/7050 train_time:282547ms step_avg:224.78ms
step:1258/7050 train_time:282775ms step_avg:224.78ms
step:1259/7050 train_time:283000ms step_avg:224.78ms
step:1260/7050 train_time:283225ms step_avg:224.78ms
step:1261/7050 train_time:283452ms step_avg:224.78ms
step:1262/7050 train_time:283678ms step_avg:224.78ms
step:1263/7050 train_time:283905ms step_avg:224.79ms
step:1264/7050 train_time:284131ms step_avg:224.79ms
step:1265/7050 train_time:284359ms step_avg:224.79ms
step:1266/7050 train_time:284583ms step_avg:224.79ms
step:1267/7050 train_time:284810ms step_avg:224.79ms
step:1268/7050 train_time:285036ms step_avg:224.79ms
step:1269/7050 train_time:285262ms step_avg:224.79ms
step:1270/7050 train_time:285488ms step_avg:224.79ms
step:1271/7050 train_time:285714ms step_avg:224.79ms
step:1272/7050 train_time:285941ms step_avg:224.80ms
step:1273/7050 train_time:286166ms step_avg:224.80ms
step:1274/7050 train_time:286393ms step_avg:224.80ms
step:1275/7050 train_time:286618ms step_avg:224.80ms
step:1276/7050 train_time:286845ms step_avg:224.80ms
step:1277/7050 train_time:287071ms step_avg:224.80ms
step:1278/7050 train_time:287298ms step_avg:224.80ms
step:1279/7050 train_time:287523ms step_avg:224.80ms
step:1280/7050 train_time:287749ms step_avg:224.80ms
step:1281/7050 train_time:287976ms step_avg:224.81ms
step:1282/7050 train_time:288202ms step_avg:224.81ms
step:1283/7050 train_time:288428ms step_avg:224.81ms
step:1284/7050 train_time:288654ms step_avg:224.81ms
step:1285/7050 train_time:288880ms step_avg:224.81ms
step:1286/7050 train_time:289107ms step_avg:224.81ms
step:1287/7050 train_time:289334ms step_avg:224.81ms
step:1288/7050 train_time:289559ms step_avg:224.81ms
step:1289/7050 train_time:289787ms step_avg:224.82ms
step:1290/7050 train_time:290012ms step_avg:224.82ms
step:1291/7050 train_time:290238ms step_avg:224.82ms
step:1292/7050 train_time:290463ms step_avg:224.82ms
step:1293/7050 train_time:290689ms step_avg:224.82ms
step:1294/7050 train_time:290915ms step_avg:224.82ms
step:1295/7050 train_time:291141ms step_avg:224.82ms
step:1296/7050 train_time:291368ms step_avg:224.82ms
step:1297/7050 train_time:291594ms step_avg:224.82ms
step:1298/7050 train_time:291819ms step_avg:224.82ms
step:1299/7050 train_time:292045ms step_avg:224.82ms
step:1300/7050 train_time:292273ms step_avg:224.83ms
step:1301/7050 train_time:292497ms step_avg:224.83ms
step:1302/7050 train_time:292724ms step_avg:224.83ms
step:1303/7050 train_time:292950ms step_avg:224.83ms
step:1304/7050 train_time:293177ms step_avg:224.83ms
step:1305/7050 train_time:293401ms step_avg:224.83ms
step:1306/7050 train_time:293628ms step_avg:224.83ms
step:1307/7050 train_time:293855ms step_avg:224.83ms
step:1308/7050 train_time:294082ms step_avg:224.83ms
step:1309/7050 train_time:294307ms step_avg:224.83ms
step:1310/7050 train_time:294533ms step_avg:224.83ms
step:1311/7050 train_time:294762ms step_avg:224.84ms
step:1312/7050 train_time:294988ms step_avg:224.84ms
step:1313/7050 train_time:295213ms step_avg:224.84ms
step:1314/7050 train_time:295438ms step_avg:224.84ms
step:1315/7050 train_time:295665ms step_avg:224.84ms
step:1316/7050 train_time:295891ms step_avg:224.84ms
step:1317/7050 train_time:296118ms step_avg:224.84ms
step:1318/7050 train_time:296343ms step_avg:224.84ms
step:1319/7050 train_time:296569ms step_avg:224.84ms
step:1320/7050 train_time:296795ms step_avg:224.84ms
step:1321/7050 train_time:297021ms step_avg:224.85ms
step:1322/7050 train_time:297247ms step_avg:224.85ms
step:1323/7050 train_time:297471ms step_avg:224.85ms
step:1324/7050 train_time:297698ms step_avg:224.85ms
step:1325/7050 train_time:297922ms step_avg:224.85ms
step:1326/7050 train_time:298150ms step_avg:224.85ms
step:1327/7050 train_time:298377ms step_avg:224.85ms
step:1328/7050 train_time:298602ms step_avg:224.85ms
step:1329/7050 train_time:298829ms step_avg:224.85ms
step:1330/7050 train_time:299055ms step_avg:224.85ms
step:1331/7050 train_time:299282ms step_avg:224.85ms
step:1332/7050 train_time:299507ms step_avg:224.86ms
step:1333/7050 train_time:299733ms step_avg:224.86ms
step:1334/7050 train_time:299961ms step_avg:224.86ms
step:1335/7050 train_time:300188ms step_avg:224.86ms
step:1336/7050 train_time:300414ms step_avg:224.86ms
step:1337/7050 train_time:300640ms step_avg:224.86ms
step:1338/7050 train_time:300867ms step_avg:224.86ms
step:1339/7050 train_time:301095ms step_avg:224.87ms
step:1340/7050 train_time:301321ms step_avg:224.87ms
step:1341/7050 train_time:301548ms step_avg:224.87ms
step:1342/7050 train_time:301775ms step_avg:224.87ms
step:1343/7050 train_time:302000ms step_avg:224.87ms
step:1344/7050 train_time:302226ms step_avg:224.87ms
step:1345/7050 train_time:302452ms step_avg:224.87ms
step:1346/7050 train_time:302679ms step_avg:224.87ms
step:1347/7050 train_time:302907ms step_avg:224.87ms
step:1348/7050 train_time:303132ms step_avg:224.88ms
step:1349/7050 train_time:303358ms step_avg:224.88ms
step:1350/7050 train_time:303585ms step_avg:224.88ms
step:1351/7050 train_time:303810ms step_avg:224.88ms
step:1352/7050 train_time:304035ms step_avg:224.88ms
step:1353/7050 train_time:304261ms step_avg:224.88ms
step:1354/7050 train_time:304488ms step_avg:224.88ms
step:1355/7050 train_time:304714ms step_avg:224.88ms
step:1356/7050 train_time:304941ms step_avg:224.88ms
step:1357/7050 train_time:305167ms step_avg:224.88ms
step:1358/7050 train_time:305392ms step_avg:224.88ms
step:1359/7050 train_time:305617ms step_avg:224.88ms
step:1360/7050 train_time:305844ms step_avg:224.89ms
step:1361/7050 train_time:306073ms step_avg:224.89ms
step:1362/7050 train_time:306298ms step_avg:224.89ms
step:1363/7050 train_time:306522ms step_avg:224.89ms
step:1364/7050 train_time:306749ms step_avg:224.89ms
step:1365/7050 train_time:306975ms step_avg:224.89ms
step:1366/7050 train_time:307200ms step_avg:224.89ms
step:1367/7050 train_time:307427ms step_avg:224.89ms
step:1368/7050 train_time:307654ms step_avg:224.89ms
step:1369/7050 train_time:307883ms step_avg:224.90ms
step:1370/7050 train_time:308108ms step_avg:224.90ms
step:1371/7050 train_time:308334ms step_avg:224.90ms
step:1372/7050 train_time:308560ms step_avg:224.90ms
step:1373/7050 train_time:308789ms step_avg:224.90ms
step:1374/7050 train_time:309013ms step_avg:224.90ms
step:1375/7050 train_time:309240ms step_avg:224.90ms
step:1375/7050 val_loss:3.3710 train_time:309367ms step_avg:224.99ms
step:1376/7050 train_time:309467ms step_avg:224.90ms
step:1377/7050 train_time:309691ms step_avg:224.90ms
step:1378/7050 train_time:309918ms step_avg:224.90ms
step:1379/7050 train_time:310144ms step_avg:224.91ms
step:1380/7050 train_time:310369ms step_avg:224.90ms
step:1381/7050 train_time:310596ms step_avg:224.91ms
step:1382/7050 train_time:310823ms step_avg:224.91ms
step:1383/7050 train_time:311049ms step_avg:224.91ms
step:1384/7050 train_time:311274ms step_avg:224.91ms
step:1385/7050 train_time:311500ms step_avg:224.91ms
step:1386/7050 train_time:311725ms step_avg:224.91ms
step:1387/7050 train_time:311953ms step_avg:224.91ms
step:1388/7050 train_time:312180ms step_avg:224.91ms
step:1389/7050 train_time:312404ms step_avg:224.91ms
step:1390/7050 train_time:312632ms step_avg:224.91ms
step:1391/7050 train_time:312857ms step_avg:224.91ms
step:1392/7050 train_time:313083ms step_avg:224.92ms
step:1393/7050 train_time:313310ms step_avg:224.92ms
step:1394/7050 train_time:313536ms step_avg:224.92ms
step:1395/7050 train_time:313762ms step_avg:224.92ms
step:1396/7050 train_time:313988ms step_avg:224.92ms
step:1397/7050 train_time:314217ms step_avg:224.92ms
step:1398/7050 train_time:314443ms step_avg:224.92ms
step:1399/7050 train_time:314669ms step_avg:224.92ms
step:1400/7050 train_time:314896ms step_avg:224.93ms
step:1401/7050 train_time:315120ms step_avg:224.93ms
step:1402/7050 train_time:315345ms step_avg:224.93ms
step:1403/7050 train_time:315573ms step_avg:224.93ms
step:1404/7050 train_time:315799ms step_avg:224.93ms
step:1405/7050 train_time:316026ms step_avg:224.93ms
step:1406/7050 train_time:316252ms step_avg:224.93ms
step:1407/7050 train_time:316479ms step_avg:224.93ms
step:1408/7050 train_time:316706ms step_avg:224.93ms
step:1409/7050 train_time:316931ms step_avg:224.93ms
step:1410/7050 train_time:317157ms step_avg:224.93ms
step:1411/7050 train_time:317384ms step_avg:224.94ms
step:1412/7050 train_time:317612ms step_avg:224.94ms
step:1413/7050 train_time:317838ms step_avg:224.94ms
step:1414/7050 train_time:318064ms step_avg:224.94ms
step:1415/7050 train_time:318289ms step_avg:224.94ms
step:1416/7050 train_time:318518ms step_avg:224.94ms
step:1417/7050 train_time:318744ms step_avg:224.94ms
step:1418/7050 train_time:318969ms step_avg:224.94ms
step:1419/7050 train_time:319193ms step_avg:224.94ms
step:1420/7050 train_time:319420ms step_avg:224.94ms
step:1421/7050 train_time:319646ms step_avg:224.94ms
step:1422/7050 train_time:319872ms step_avg:224.94ms
step:1423/7050 train_time:320098ms step_avg:224.95ms
step:1424/7050 train_time:320325ms step_avg:224.95ms
step:1425/7050 train_time:320550ms step_avg:224.95ms
step:1426/7050 train_time:320776ms step_avg:224.95ms
step:1427/7050 train_time:321003ms step_avg:224.95ms
step:1428/7050 train_time:321228ms step_avg:224.95ms
step:1429/7050 train_time:321455ms step_avg:224.95ms
step:1430/7050 train_time:321680ms step_avg:224.95ms
step:1431/7050 train_time:321907ms step_avg:224.95ms
step:1432/7050 train_time:322133ms step_avg:224.95ms
step:1433/7050 train_time:322359ms step_avg:224.95ms
step:1434/7050 train_time:322585ms step_avg:224.95ms
step:1435/7050 train_time:322809ms step_avg:224.95ms
step:1436/7050 train_time:323036ms step_avg:224.96ms
step:1437/7050 train_time:323262ms step_avg:224.96ms
step:1438/7050 train_time:323487ms step_avg:224.96ms
step:1439/7050 train_time:323715ms step_avg:224.96ms
step:1440/7050 train_time:323940ms step_avg:224.96ms
step:1441/7050 train_time:324168ms step_avg:224.96ms
step:1442/7050 train_time:324393ms step_avg:224.96ms
step:1443/7050 train_time:324618ms step_avg:224.96ms
step:1444/7050 train_time:324844ms step_avg:224.96ms
step:1445/7050 train_time:325070ms step_avg:224.96ms
step:1446/7050 train_time:325299ms step_avg:224.96ms
step:1447/7050 train_time:325524ms step_avg:224.96ms
step:1448/7050 train_time:325749ms step_avg:224.96ms
step:1449/7050 train_time:325974ms step_avg:224.96ms
step:1450/7050 train_time:326200ms step_avg:224.97ms
step:1451/7050 train_time:326428ms step_avg:224.97ms
step:1452/7050 train_time:326652ms step_avg:224.97ms
step:1453/7050 train_time:326878ms step_avg:224.97ms
step:1454/7050 train_time:327105ms step_avg:224.97ms
step:1455/7050 train_time:327333ms step_avg:224.97ms
step:1456/7050 train_time:327558ms step_avg:224.97ms
step:1457/7050 train_time:327785ms step_avg:224.97ms
step:1458/7050 train_time:328011ms step_avg:224.97ms
step:1459/7050 train_time:328239ms step_avg:224.98ms
step:1460/7050 train_time:328463ms step_avg:224.97ms
step:1461/7050 train_time:328688ms step_avg:224.97ms
step:1462/7050 train_time:328917ms step_avg:224.98ms
step:1463/7050 train_time:329146ms step_avg:224.98ms
step:1464/7050 train_time:329372ms step_avg:224.98ms
step:1465/7050 train_time:329598ms step_avg:224.98ms
step:1466/7050 train_time:329825ms step_avg:224.98ms
step:1467/7050 train_time:330050ms step_avg:224.98ms
step:1468/7050 train_time:330276ms step_avg:224.98ms
step:1469/7050 train_time:330501ms step_avg:224.98ms
step:1470/7050 train_time:330728ms step_avg:224.98ms
step:1471/7050 train_time:330954ms step_avg:224.99ms
step:1472/7050 train_time:331182ms step_avg:224.99ms
step:1473/7050 train_time:331408ms step_avg:224.99ms
step:1474/7050 train_time:331634ms step_avg:224.99ms
step:1475/7050 train_time:331860ms step_avg:224.99ms
step:1476/7050 train_time:332086ms step_avg:224.99ms
step:1477/7050 train_time:332312ms step_avg:224.99ms
step:1478/7050 train_time:332539ms step_avg:224.99ms
step:1479/7050 train_time:332764ms step_avg:224.99ms
step:1480/7050 train_time:332990ms step_avg:224.99ms
step:1481/7050 train_time:333218ms step_avg:225.00ms
step:1482/7050 train_time:333443ms step_avg:225.00ms
step:1483/7050 train_time:333668ms step_avg:225.00ms
step:1484/7050 train_time:333894ms step_avg:225.00ms
step:1485/7050 train_time:334121ms step_avg:225.00ms
step:1486/7050 train_time:334345ms step_avg:225.00ms
step:1487/7050 train_time:334571ms step_avg:225.00ms
step:1488/7050 train_time:334797ms step_avg:225.00ms
step:1489/7050 train_time:335024ms step_avg:225.00ms
step:1490/7050 train_time:335250ms step_avg:225.00ms
step:1491/7050 train_time:335475ms step_avg:225.00ms
step:1492/7050 train_time:335700ms step_avg:225.00ms
step:1493/7050 train_time:335927ms step_avg:225.00ms
step:1494/7050 train_time:336153ms step_avg:225.00ms
step:1495/7050 train_time:336378ms step_avg:225.00ms
step:1496/7050 train_time:336606ms step_avg:225.00ms
step:1497/7050 train_time:336833ms step_avg:225.01ms
step:1498/7050 train_time:337059ms step_avg:225.01ms
step:1499/7050 train_time:337285ms step_avg:225.01ms
step:1500/7050 train_time:337511ms step_avg:225.01ms
step:1500/7050 val_loss:3.3503 train_time:337640ms step_avg:225.09ms
step:1501/7050 train_time:337741ms step_avg:225.01ms
step:1502/7050 train_time:337964ms step_avg:225.01ms
step:1503/7050 train_time:338196ms step_avg:225.01ms
step:1504/7050 train_time:338423ms step_avg:225.02ms
step:1505/7050 train_time:338648ms step_avg:225.02ms
step:1506/7050 train_time:338873ms step_avg:225.02ms
step:1507/7050 train_time:339100ms step_avg:225.02ms
step:1508/7050 train_time:339326ms step_avg:225.02ms
step:1509/7050 train_time:339553ms step_avg:225.02ms
step:1510/7050 train_time:339778ms step_avg:225.02ms
step:1511/7050 train_time:340002ms step_avg:225.02ms
step:1512/7050 train_time:340229ms step_avg:225.02ms
step:1513/7050 train_time:340457ms step_avg:225.02ms
step:1514/7050 train_time:340682ms step_avg:225.02ms
step:1515/7050 train_time:340908ms step_avg:225.02ms
step:1516/7050 train_time:341134ms step_avg:225.02ms
step:1517/7050 train_time:341361ms step_avg:225.02ms
step:1518/7050 train_time:341588ms step_avg:225.03ms
step:1519/7050 train_time:341814ms step_avg:225.03ms
step:1520/7050 train_time:342038ms step_avg:225.03ms
