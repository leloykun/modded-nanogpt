import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
# use of FlexAttention contributed by @KoszarskyB
from torch.nn.attention.flex_attention import BlockMask, flex_attention
#torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Muon optimizer

@torch.compile
def zeropower_via_newtonschulz5(G: Tensor, steps: int) -> Tensor:
    """
    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a
    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose
    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at
    zero even beyond the point where the iteration no longer converges all the way to one everywhere
    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T
    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model
    performance at all relative to UV^T, where USV^T = G is the SVD.
    """
    assert G.ndim >= 2 # batched Muon implementation by @scottjmaddox, and put into practice in the record by @YouJiacheng
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) + 1e-7)
    # Perform the NS iterations
    for a, b, c in [
        (4.0848, -6.8946, 2.9270),
        (3.9505, -6.3029, 2.6377),
        (3.7418, -5.5913, 2.3037),
        (2.8769, -3.1427, 1.2046),
        (2.8366, -3.0525, 1.2012),
    ]:
        A = X @ X.mT
        B = b * A + c * A @ A # quintic computation strategy adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng
        X = a * X + B @ X

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X

class Muon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Some warnings:
    - This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).
    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.

    Arguments:
        lr: The learning rate used by the internal SGD.
        momentum: The momentum used by the internal SGD.
        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)
        ns_steps: The number of Newton-Schulz iteration steps to use.
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, nesterov=True, ns_steps=5, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, nesterov=nesterov, ns_steps=ns_steps)
        params: list[Tensor] = [*params]
        param_groups = []
        for size in {p.numel() for p in params}:
            b = torch.empty(world_size, size, dtype=torch.bfloat16, device="cuda")
            group = dict(params=[p for p in params if p.numel() == size],
                         update_buffer=b, update_buffer_views=[b[i] for i in range(world_size)])
            param_groups.append(group)
        super().__init__(param_groups, defaults)

    @torch.no_grad()
    def step(self):
        for group in self.param_groups:
            update_buffer: Tensor = group["update_buffer"]
            update_buffer_views: list[Tensor] = group["update_buffer_views"]
            # generate weight updates in distributed fashion
            params: list[Tensor] = group["params"]
            handle = None
            params_world = None
            def update_prev(): # optimized Muon implementation contributed by @YouJiacheng
                handle.wait()
                for p_world, g_world in zip(params_world, update_buffer_views):
                    p_world.mul_(1 - group["lr"] * group["weight_decay"])
                    p_world.add_(g_world.view_as(p_world),
                                 alpha=-group["lr"] * max(1, p_world.size(-2) / p_world.size(-1))**0.5)
            for base_i in range(len(params))[::self.world_size]:
                if base_i + self.rank < len(params):
                    p = params[base_i + self.rank]
                    g = p.grad
                    assert g is not None
                    state = self.state[p]
                    if "momentum_buffer" not in state:
                        state["momentum_buffer"] = torch.zeros_like(g)
                    buf: Tensor = state["momentum_buffer"]
                    buf.lerp_(g, 1 - group["momentum"])
                    g = g.lerp_(buf, group["momentum"]) if group["nesterov"] else buf
                    g = zeropower_via_newtonschulz5(g, steps=group["ns_steps"]).flatten()
                else:
                    g = update_buffer_views[self.rank]
                if base_i > 0:
                    update_prev() # async all_gather instead of sync all_reduce by @YouJiacheng
                handle = dist.all_gather_into_tensor(update_buffer, g, async_op=True)
                params_world = params[base_i : base_i + self.world_size]
            update_prev()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8: bool = False, x_s: float = 1.0, w_s: float = 1.0, grad_s: float = 1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        std = 0.5 * (self.in_features ** -0.5) # 0.5 is a bit better than the default 1/sqrt(3)
        bound = (3 ** 0.5) * std
        with torch.no_grad():
            self.weight.uniform_(-bound, bound)

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim//4, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim//4)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq)
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos + x2 * sin
        y2 = x1 * (-sin) + x2 * cos
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=128):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        hdim = num_heads * head_dim
        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKV weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        self.qkv_w = nn.Parameter(torch.empty(3, hdim, dim).uniform_(-bound, bound))
        self.lambdas = nn.Parameter(torch.tensor([0.5, 0.5]))
        self.rotary = Rotary(head_dim, max_seq_len)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977
        # scale the attention logits by given constant, instead of the default head_dim**-0.5, by @leloykun
        # inspired by learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.12

    def forward(self, x: Tensor, ve: Tensor | None, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        q, k, v = F.linear(x, self.qkv_w.flatten(end_dim=1).type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = self.rotary(q), self.rotary(k)
        if ve is not None:
            v = self.lambdas[0] * v + self.lambdas[1] * ve.view_as(v) # @KoszarskyB & @Grad62304977
        else: # skip mid-layers token value embeddings by @YouJiacheng
            v = self.lambdas[0] * v
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=self.attn_scale).transpose(1, 2)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = self.c_proj(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        self.c_fc = CastedLinear(dim, hdim)
        self.c_proj = CastedLinear(hdim, dim)
        self.c_proj.weight.detach().zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = self.c_fc(x)
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = self.c_proj(x)
        return x

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        # skip attention of blocks.7 (the 8th layer) by @YouJiacheng
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len) if layer_idx != 7 else None
        self.mlp = MLP(dim)
        self.lambdas = nn.Parameter(torch.tensor([1., 0.]))

    def forward(self, x: Tensor, ve: Tensor | None, x0: Tensor, block_mask: BlockMask):
        x = self.lambdas[0] * x + self.lambdas[1] * x0
        if self.attn is not None:
            x = x + self.attn(norm(x), ve, block_mask)
        x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, model_dim)
        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        self.lm_head = CastedLinear(model_dim, next_multiple_of_n(vocab_size, n=128), use_fp8=True, x_s=0.5, w_s=2**-9, grad_s=2**-19)
        self.lm_head.weight.detach().zero_() # @Grad62304977
        # Add learnable skip connection weights for decoder layers
        assert num_layers % 2 == 0
        self.skip_weights = nn.Parameter(torch.ones(num_layers//2))

    def create_blockmasks(self, input_seq: Tensor, sliding_window_num_blocks: Tensor):
        BLOCK_SIZE = 128
        docs = (input_seq == 50256).cumsum(0)

        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask

        def dense_to_ordered(dense_blockmask: Tensor):
            num_blocks = dense_blockmask.sum(dim=-1, dtype=torch.int32)
            indices = dense_blockmask.argsort(dim=-1, descending=False, stable=True).flip(-1).to(torch.int32)
            return num_blocks[None, None].contiguous(), indices[None, None].contiguous()

        # manual block mask creation by @YouJiacheng
        assert len(input_seq) % BLOCK_SIZE == 0
        NUM_BLOCKS = len(input_seq) // BLOCK_SIZE
        block_idx = torch.arange(NUM_BLOCKS, dtype=torch.int32, device="cuda")
        causal_blockmask_any = block_idx[:, None] >= block_idx
        causal_blockmask_all = block_idx[:, None] > block_idx
        docs_low = docs.view(-1, BLOCK_SIZE)[:, 0].contiguous()
        docs_high = docs.view(-1, BLOCK_SIZE)[:, -1].contiguous()
        document_blockmask_any = (docs_low[:, None] <= docs_high) & (docs_high[:, None] >= docs_low)
        document_blockmask_all = (docs_low[:, None] == docs_high) & (docs_high[:, None] == docs_low)
        blockmask_any = causal_blockmask_any & document_blockmask_any
        blockmask_all = causal_blockmask_all & document_blockmask_all
        partial_kv_num_blocks, partial_kv_indices = dense_to_ordered(blockmask_any & ~blockmask_all)
        full_kv_num_blocks, full_kv_indices = dense_to_ordered(blockmask_all)
        def build_bm(window_size_blocks: Tensor) -> BlockMask:
            return BlockMask.from_kv_blocks(
                torch.clamp_max(partial_kv_num_blocks, torch.clamp_min(window_size_blocks - full_kv_num_blocks, 1)),
                partial_kv_indices,
                torch.clamp_max(full_kv_num_blocks, window_size_blocks - 1),
                full_kv_indices,
                BLOCK_SIZE=BLOCK_SIZE,
                mask_mod=document_causal,
            )
        # Long-short SWA block masks by @leloykun & @YouJiacheng, adapated from suggestion by @Grad62304977, following Gemma 2 paper
        return build_bm(sliding_window_num_blocks), build_bm(sliding_window_num_blocks // 2)

    def forward(self, input_seq: Tensor, target_seq: Tensor, sliding_window_num_blocks: Tensor):
        assert input_seq.ndim == 1

        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        ve = [ve[0], ve[1], ve[2]] + [None] * (len(self.blocks) - 6) + [ve[0], ve[1], ve[2]]
        assert len(ve) == len(self.blocks)

        long_bm, short_bm = self.create_blockmasks(input_seq, sliding_window_num_blocks)
        block_masks = [long_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, short_bm, long_bm]
        assert len(block_masks) == len(self.blocks)

        x = x0 = norm(self.embed(input_seq)[None]) # use of norm here by @Grad62304977

        # U-net design by @brendanh0gan
        skip_connections = []
        n = len(self.skip_weights)
        for i in range(len(self.blocks)):
            if i >= n:
                x = x + self.skip_weights[i - n] * skip_connections.pop()
            x = self.blocks[i](x, ve[i], x0, block_masks[i])
            if i < n:
                skip_connections.append(x)

        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15, @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1)
        logits = 30 * torch.sigmoid(logits.float() / 7.5)
        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_seq)
        return loss

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):
    files = sorted(Path.cwd().glob(filename_pattern))
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training
    tokens, pos = _load_data_shard(next(file_iter)), 0
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 64*1024 # FlexAttention sequence length
    val_seq_len = 4*64*1024 # FlexAttention sequence length for validation
    # optimization
    num_iterations = 7050 # number of iterations to run
    cooldown_frac = 0.4 # fraction of training spent cooling down the learning rate
    # architecture
    vocab_size = 50257
    # evaluation and logging
    val_loss_every = 125 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert world_size == 8 # this code is designed for 8xH100
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, num_layers=16, num_heads=8, model_dim=1024,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len)).cuda()
for m in model.modules():
    if isinstance(m, nn.Embedding):
        m.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# collect the parameters to optimize
hidden_matrix_params = [p for n, p in model.blocks.named_parameters() if p.ndim >= 2 and "embed" not in n]
embed_params = [p for n, p in model.named_parameters() if "embed" in n]
scalar_params = [p for p in model.parameters() if p.ndim < 2]
head_params = [model.lm_head.weight]

# init the optimizer(s)
adam_params = [dict(params=head_params, lr=0.1/1024**0.5), dict(params=embed_params, lr=0.3), dict(params=scalar_params, lr=0.015)]
# small adam epsilon by @YouJiacheng. this is an alternate method of fixing the world_size dependence
# discovered by @fernbear.bsky.social https://x.com/hi_tysam/status/1879692937589875094
optimizer1 = torch.optim.Adam(adam_params, betas=(0.8, 0.95), eps=1e-10, fused=True)
optimizer2 = Muon(hidden_matrix_params, lr=0.025, momentum=0.95, rank=rank, world_size=world_size)
optimizers = [optimizer1, optimizer2]
for opt in optimizers:
    for group in opt.param_groups:
        group["initial_lr"] = group["lr"]

# learning rate schedule: stable then decay
def get_lr(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x < 1
    if x < 1 - args.cooldown_frac:
        return 1.0
    else:
        return (1 - x) / args.cooldown_frac

# attention window size schedule: linearly increase
@lru_cache(1)
def get_window_size_blocks_helper(window_size: int):
    return torch.tensor(window_size // 128, dtype=torch.int32, pin_memory=True).cuda(non_blocking=True)
def get_window_size_blocks(step: int):
    x = step / args.num_iterations # progress in training
    assert 0 <= x <= 1
    # Linearly increase the block-wise sliding window size over training 128 -> 1792
    # increase by @fernbear.bsky.social; block-wise by @YouJiacheng
    window_size = next_multiple_of_n(1728 * x, n=128)
    return get_window_size_blocks_helper(window_size)

model: nn.Module = torch.compile(model, dynamic=False)

########################################
#            Warmup kernels            #
########################################

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=[copy.deepcopy(opt.state_dict()) for opt in optimizers]) # save the initial state
for _ in range(warmup_steps):
    inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device="cuda")
    model(inputs.to(torch.int32), targets, get_window_size_blocks(0)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    for opt in optimizers:
        opt.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
for opt, opt_state in zip(optimizers, initial_state["optimizers"]):
    opt.load_state_dict(opt_state)
del initial_state

########################################
#        Training and validation       #
########################################

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)
training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        val_batch_size = world_size * args.val_seq_len
        assert args.val_tokens % val_batch_size == 0
        val_steps = args.val_tokens // val_batch_size
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets = next(val_loader)
                val_loss += model(inputs, targets, get_window_size_blocks(step))
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    model(inputs, targets, get_window_size_blocks(step)).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # set optimization hyperparameters
    for opt in optimizers:
        for group in opt.param_groups:
            group["lr"] = group["initial_lr"] * get_lr(step)
    for group in optimizer2.param_groups:
        frac = min(step / 300, 1) # momentum warmup for muon
        group["momentum"] = (1 - frac) * 0.85 + frac * 0.95
    # step the optimizers
    for opt in optimizers:
        opt.step()
    # null the gradients
    model.zero_grad(set_to_none=True)
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.12 (main, Nov  6 2024, 20:22:13) [GCC 11.4.0]
Running PyTorch 2.7.0.dev20250125+cu126 compiled for CUDA 12.6
Mon Feb 17 16:21:45 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:19:00.0 Off |                    0 |
| N/A   28C    P0            111W /  700W |    7714MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:3B:00.0 Off |                    0 |
| N/A   27C    P0            119W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:4C:00.0 Off |                    0 |
| N/A   25C    P0            112W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:5D:00.0 Off |                    0 |
| N/A   28C    P0            111W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:9B:00.0 Off |                    0 |
| N/A   28C    P0            114W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:BB:00.0 Off |                    0 |
| N/A   26C    P0            109W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:CB:00.0 Off |                    0 |
| N/A   27C    P0            110W /  700W |    3452MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:DB:00.0 Off |                    0 |
| N/A   25C    P0            111W /  700W |    3212MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
step:0/7050 val_loss:10.8258 train_time:0ms step_avg:0.02ms
step:1/7050 train_time:136ms step_avg:136.50ms
step:2/7050 train_time:298ms step_avg:149.09ms
step:3/7050 train_time:510ms step_avg:170.06ms
step:4/7050 train_time:732ms step_avg:182.96ms
step:5/7050 train_time:955ms step_avg:191.08ms
step:6/7050 train_time:1177ms step_avg:196.19ms
step:7/7050 train_time:1402ms step_avg:200.32ms
step:8/7050 train_time:1626ms step_avg:203.29ms
step:9/7050 train_time:1850ms step_avg:205.56ms
step:10/7050 train_time:2073ms step_avg:207.30ms
step:11/7050 train_time:2298ms step_avg:208.93ms
step:12/7050 train_time:2521ms step_avg:210.06ms
step:13/7050 train_time:2744ms step_avg:211.07ms
step:14/7050 train_time:2968ms step_avg:211.97ms
step:15/7050 train_time:3191ms step_avg:212.76ms
step:16/7050 train_time:3418ms step_avg:213.60ms
step:17/7050 train_time:3640ms step_avg:214.12ms
step:18/7050 train_time:3865ms step_avg:214.73ms
step:19/7050 train_time:4088ms step_avg:215.16ms
step:20/7050 train_time:4313ms step_avg:215.66ms
step:21/7050 train_time:4538ms step_avg:216.09ms
step:22/7050 train_time:4760ms step_avg:216.37ms
step:23/7050 train_time:4985ms step_avg:216.75ms
step:24/7050 train_time:5210ms step_avg:217.07ms
step:25/7050 train_time:5435ms step_avg:217.39ms
step:26/7050 train_time:5660ms step_avg:217.69ms
step:27/7050 train_time:5882ms step_avg:217.84ms
step:28/7050 train_time:6106ms step_avg:218.08ms
step:29/7050 train_time:6331ms step_avg:218.32ms
step:30/7050 train_time:6556ms step_avg:218.52ms
step:31/7050 train_time:6781ms step_avg:218.75ms
step:32/7050 train_time:7003ms step_avg:218.86ms
step:33/7050 train_time:7228ms step_avg:219.04ms
step:34/7050 train_time:7453ms step_avg:219.21ms
step:35/7050 train_time:7678ms step_avg:219.36ms
step:36/7050 train_time:7903ms step_avg:219.53ms
step:37/7050 train_time:8125ms step_avg:219.61ms
step:38/7050 train_time:8348ms step_avg:219.69ms
step:39/7050 train_time:8573ms step_avg:219.83ms
step:40/7050 train_time:8797ms step_avg:219.92ms
step:41/7050 train_time:9019ms step_avg:219.98ms
step:42/7050 train_time:9243ms step_avg:220.08ms
step:43/7050 train_time:9468ms step_avg:220.19ms
step:44/7050 train_time:9693ms step_avg:220.29ms
step:45/7050 train_time:9918ms step_avg:220.39ms
step:46/7050 train_time:10141ms step_avg:220.46ms
step:47/7050 train_time:10365ms step_avg:220.52ms
step:48/7050 train_time:10589ms step_avg:220.60ms
step:49/7050 train_time:10814ms step_avg:220.70ms
step:50/7050 train_time:11037ms step_avg:220.73ms
step:51/7050 train_time:11259ms step_avg:220.77ms
step:52/7050 train_time:11484ms step_avg:220.84ms
step:53/7050 train_time:11707ms step_avg:220.89ms
step:54/7050 train_time:11931ms step_avg:220.94ms
step:55/7050 train_time:12153ms step_avg:220.96ms
step:56/7050 train_time:12376ms step_avg:221.01ms
step:57/7050 train_time:12599ms step_avg:221.04ms
step:58/7050 train_time:12824ms step_avg:221.11ms
step:59/7050 train_time:13048ms step_avg:221.15ms
step:60/7050 train_time:13270ms step_avg:221.16ms
step:61/7050 train_time:13493ms step_avg:221.20ms
step:62/7050 train_time:13717ms step_avg:221.23ms
step:63/7050 train_time:13941ms step_avg:221.28ms
step:64/7050 train_time:14165ms step_avg:221.32ms
step:65/7050 train_time:14389ms step_avg:221.36ms
step:66/7050 train_time:14613ms step_avg:221.41ms
step:67/7050 train_time:14837ms step_avg:221.45ms
step:68/7050 train_time:15062ms step_avg:221.50ms
step:69/7050 train_time:15285ms step_avg:221.53ms
step:70/7050 train_time:15509ms step_avg:221.56ms
step:71/7050 train_time:15733ms step_avg:221.59ms
step:72/7050 train_time:15957ms step_avg:221.63ms
step:73/7050 train_time:16181ms step_avg:221.66ms
step:74/7050 train_time:16404ms step_avg:221.68ms
step:75/7050 train_time:16628ms step_avg:221.71ms
step:76/7050 train_time:16853ms step_avg:221.76ms
step:77/7050 train_time:17078ms step_avg:221.79ms
step:78/7050 train_time:17302ms step_avg:221.81ms
step:79/7050 train_time:17525ms step_avg:221.83ms
step:80/7050 train_time:17749ms step_avg:221.86ms
step:81/7050 train_time:17974ms step_avg:221.90ms
step:82/7050 train_time:18198ms step_avg:221.92ms
step:83/7050 train_time:18420ms step_avg:221.93ms
step:84/7050 train_time:18644ms step_avg:221.96ms
step:85/7050 train_time:18868ms step_avg:221.97ms
step:86/7050 train_time:19092ms step_avg:222.00ms
step:87/7050 train_time:19317ms step_avg:222.03ms
step:88/7050 train_time:19539ms step_avg:222.04ms
step:89/7050 train_time:19762ms step_avg:222.05ms
step:90/7050 train_time:19988ms step_avg:222.09ms
step:91/7050 train_time:20215ms step_avg:222.14ms
step:92/7050 train_time:20438ms step_avg:222.15ms
step:93/7050 train_time:20660ms step_avg:222.15ms
step:94/7050 train_time:20884ms step_avg:222.17ms
step:95/7050 train_time:21109ms step_avg:222.19ms
step:96/7050 train_time:21335ms step_avg:222.24ms
step:97/7050 train_time:21556ms step_avg:222.23ms
step:98/7050 train_time:21780ms step_avg:222.24ms
step:99/7050 train_time:22005ms step_avg:222.27ms
step:100/7050 train_time:22228ms step_avg:222.28ms
step:101/7050 train_time:22453ms step_avg:222.31ms
step:102/7050 train_time:22675ms step_avg:222.30ms
step:103/7050 train_time:22897ms step_avg:222.30ms
step:104/7050 train_time:23121ms step_avg:222.32ms
step:105/7050 train_time:23345ms step_avg:222.33ms
step:106/7050 train_time:23569ms step_avg:222.35ms
step:107/7050 train_time:23793ms step_avg:222.36ms
step:108/7050 train_time:24017ms step_avg:222.38ms
step:109/7050 train_time:24241ms step_avg:222.39ms
step:110/7050 train_time:24465ms step_avg:222.41ms
step:111/7050 train_time:24688ms step_avg:222.42ms
step:112/7050 train_time:24913ms step_avg:222.44ms
step:113/7050 train_time:25134ms step_avg:222.43ms
step:114/7050 train_time:25359ms step_avg:222.45ms
step:115/7050 train_time:25582ms step_avg:222.46ms
step:116/7050 train_time:25806ms step_avg:222.47ms
step:117/7050 train_time:26030ms step_avg:222.48ms
step:118/7050 train_time:26253ms step_avg:222.48ms
step:119/7050 train_time:26475ms step_avg:222.48ms
step:120/7050 train_time:26701ms step_avg:222.51ms
step:121/7050 train_time:26925ms step_avg:222.52ms
step:122/7050 train_time:27148ms step_avg:222.52ms
step:123/7050 train_time:27372ms step_avg:222.54ms
step:124/7050 train_time:27595ms step_avg:222.54ms
step:125/7050 train_time:27819ms step_avg:222.55ms
step:125/7050 val_loss:4.5135 train_time:27996ms step_avg:223.96ms
step:126/7050 train_time:28044ms step_avg:222.57ms
step:127/7050 train_time:28267ms step_avg:222.57ms
step:128/7050 train_time:28498ms step_avg:222.64ms
step:129/7050 train_time:28721ms step_avg:222.64ms
step:130/7050 train_time:28943ms step_avg:222.64ms
step:131/7050 train_time:29167ms step_avg:222.65ms
step:132/7050 train_time:29390ms step_avg:222.65ms
step:133/7050 train_time:29615ms step_avg:222.67ms
step:134/7050 train_time:29838ms step_avg:222.68ms
step:135/7050 train_time:30060ms step_avg:222.67ms
step:136/7050 train_time:30284ms step_avg:222.67ms
step:137/7050 train_time:30508ms step_avg:222.69ms
step:138/7050 train_time:30733ms step_avg:222.71ms
step:139/7050 train_time:30957ms step_avg:222.71ms
step:140/7050 train_time:31179ms step_avg:222.71ms
step:141/7050 train_time:31404ms step_avg:222.73ms
step:142/7050 train_time:31628ms step_avg:222.73ms
step:143/7050 train_time:31854ms step_avg:222.76ms
step:144/7050 train_time:32075ms step_avg:222.74ms
step:145/7050 train_time:32299ms step_avg:222.75ms
step:146/7050 train_time:32524ms step_avg:222.77ms
step:147/7050 train_time:32748ms step_avg:222.77ms
step:148/7050 train_time:32970ms step_avg:222.77ms
step:149/7050 train_time:33193ms step_avg:222.77ms
step:150/7050 train_time:33417ms step_avg:222.78ms
step:151/7050 train_time:33642ms step_avg:222.79ms
step:152/7050 train_time:33865ms step_avg:222.80ms
step:153/7050 train_time:34088ms step_avg:222.80ms
step:154/7050 train_time:34311ms step_avg:222.80ms
step:155/7050 train_time:34533ms step_avg:222.80ms
step:156/7050 train_time:34759ms step_avg:222.81ms
step:157/7050 train_time:34981ms step_avg:222.81ms
step:158/7050 train_time:35204ms step_avg:222.81ms
step:159/7050 train_time:35428ms step_avg:222.82ms
step:160/7050 train_time:35651ms step_avg:222.82ms
step:161/7050 train_time:35875ms step_avg:222.83ms
step:162/7050 train_time:36098ms step_avg:222.82ms
step:163/7050 train_time:36320ms step_avg:222.82ms
step:164/7050 train_time:36544ms step_avg:222.83ms
step:165/7050 train_time:36767ms step_avg:222.83ms
step:166/7050 train_time:36989ms step_avg:222.82ms
step:167/7050 train_time:37212ms step_avg:222.83ms
step:168/7050 train_time:37436ms step_avg:222.83ms
step:169/7050 train_time:37660ms step_avg:222.84ms
step:170/7050 train_time:37882ms step_avg:222.84ms
step:171/7050 train_time:38105ms step_avg:222.84ms
step:172/7050 train_time:38329ms step_avg:222.84ms
step:173/7050 train_time:38553ms step_avg:222.85ms
step:174/7050 train_time:38776ms step_avg:222.85ms
step:175/7050 train_time:38999ms step_avg:222.85ms
step:176/7050 train_time:39224ms step_avg:222.86ms
step:177/7050 train_time:39447ms step_avg:222.87ms
step:178/7050 train_time:39670ms step_avg:222.87ms
step:179/7050 train_time:39893ms step_avg:222.86ms
step:180/7050 train_time:40116ms step_avg:222.87ms
step:181/7050 train_time:40341ms step_avg:222.88ms
step:182/7050 train_time:40564ms step_avg:222.88ms
step:183/7050 train_time:40786ms step_avg:222.87ms
step:184/7050 train_time:41009ms step_avg:222.87ms
step:185/7050 train_time:41233ms step_avg:222.88ms
step:186/7050 train_time:41456ms step_avg:222.88ms
step:187/7050 train_time:41678ms step_avg:222.88ms
step:188/7050 train_time:41901ms step_avg:222.88ms
step:189/7050 train_time:42125ms step_avg:222.89ms
step:190/7050 train_time:42348ms step_avg:222.89ms
step:191/7050 train_time:42572ms step_avg:222.89ms
step:192/7050 train_time:42794ms step_avg:222.88ms
step:193/7050 train_time:43017ms step_avg:222.89ms
step:194/7050 train_time:43239ms step_avg:222.88ms
step:195/7050 train_time:43463ms step_avg:222.89ms
step:196/7050 train_time:43687ms step_avg:222.89ms
step:197/7050 train_time:43909ms step_avg:222.89ms
step:198/7050 train_time:44133ms step_avg:222.90ms
step:199/7050 train_time:44356ms step_avg:222.89ms
step:200/7050 train_time:44578ms step_avg:222.89ms
step:201/7050 train_time:44802ms step_avg:222.90ms
step:202/7050 train_time:45025ms step_avg:222.90ms
step:203/7050 train_time:45250ms step_avg:222.90ms
step:204/7050 train_time:45472ms step_avg:222.90ms
step:205/7050 train_time:45695ms step_avg:222.90ms
step:206/7050 train_time:45919ms step_avg:222.91ms
step:207/7050 train_time:46141ms step_avg:222.90ms
step:208/7050 train_time:46363ms step_avg:222.90ms
step:209/7050 train_time:46585ms step_avg:222.90ms
step:210/7050 train_time:46809ms step_avg:222.90ms
step:211/7050 train_time:47032ms step_avg:222.90ms
step:212/7050 train_time:47255ms step_avg:222.90ms
step:213/7050 train_time:47478ms step_avg:222.90ms
step:214/7050 train_time:47701ms step_avg:222.90ms
step:215/7050 train_time:47923ms step_avg:222.90ms
step:216/7050 train_time:48145ms step_avg:222.89ms
step:217/7050 train_time:48369ms step_avg:222.90ms
step:218/7050 train_time:48591ms step_avg:222.89ms
step:219/7050 train_time:48814ms step_avg:222.90ms
step:220/7050 train_time:49038ms step_avg:222.90ms
step:221/7050 train_time:49260ms step_avg:222.89ms
step:222/7050 train_time:49483ms step_avg:222.90ms
step:223/7050 train_time:49706ms step_avg:222.90ms
step:224/7050 train_time:49929ms step_avg:222.90ms
step:225/7050 train_time:50150ms step_avg:222.89ms
step:226/7050 train_time:50373ms step_avg:222.89ms
step:227/7050 train_time:50596ms step_avg:222.89ms
step:228/7050 train_time:50819ms step_avg:222.89ms
step:229/7050 train_time:51044ms step_avg:222.90ms
step:230/7050 train_time:51267ms step_avg:222.90ms
step:231/7050 train_time:51490ms step_avg:222.90ms
step:232/7050 train_time:51712ms step_avg:222.90ms
step:233/7050 train_time:51936ms step_avg:222.90ms
step:234/7050 train_time:52158ms step_avg:222.90ms
step:235/7050 train_time:52380ms step_avg:222.89ms
step:236/7050 train_time:52604ms step_avg:222.90ms
step:237/7050 train_time:52827ms step_avg:222.90ms
step:238/7050 train_time:53050ms step_avg:222.90ms
step:239/7050 train_time:53274ms step_avg:222.90ms
step:240/7050 train_time:53496ms step_avg:222.90ms
step:241/7050 train_time:53718ms step_avg:222.90ms
step:242/7050 train_time:53941ms step_avg:222.90ms
step:243/7050 train_time:54163ms step_avg:222.89ms
step:244/7050 train_time:54387ms step_avg:222.90ms
step:245/7050 train_time:54609ms step_avg:222.89ms
step:246/7050 train_time:54832ms step_avg:222.89ms
step:247/7050 train_time:55055ms step_avg:222.90ms
step:248/7050 train_time:55277ms step_avg:222.89ms
step:249/7050 train_time:55499ms step_avg:222.89ms
step:250/7050 train_time:55723ms step_avg:222.89ms
step:250/7050 val_loss:4.1889 train_time:55899ms step_avg:223.60ms
step:251/7050 train_time:55946ms step_avg:222.89ms
step:252/7050 train_time:56167ms step_avg:222.89ms
step:253/7050 train_time:56396ms step_avg:222.91ms
step:254/7050 train_time:56621ms step_avg:222.92ms
step:255/7050 train_time:56842ms step_avg:222.91ms
step:256/7050 train_time:57064ms step_avg:222.91ms
step:257/7050 train_time:57287ms step_avg:222.91ms
step:258/7050 train_time:57510ms step_avg:222.91ms
step:259/7050 train_time:57733ms step_avg:222.91ms
step:260/7050 train_time:57956ms step_avg:222.91ms
step:261/7050 train_time:58179ms step_avg:222.91ms
step:262/7050 train_time:58402ms step_avg:222.91ms
step:263/7050 train_time:58626ms step_avg:222.91ms
step:264/7050 train_time:58848ms step_avg:222.91ms
step:265/7050 train_time:59070ms step_avg:222.91ms
step:266/7050 train_time:59294ms step_avg:222.91ms
step:267/7050 train_time:59516ms step_avg:222.91ms
step:268/7050 train_time:59739ms step_avg:222.91ms
step:269/7050 train_time:59961ms step_avg:222.90ms
step:270/7050 train_time:60184ms step_avg:222.90ms
step:271/7050 train_time:60406ms step_avg:222.90ms
step:272/7050 train_time:60630ms step_avg:222.90ms
step:273/7050 train_time:60851ms step_avg:222.90ms
step:274/7050 train_time:61073ms step_avg:222.90ms
step:275/7050 train_time:61297ms step_avg:222.90ms
step:276/7050 train_time:61520ms step_avg:222.90ms
step:277/7050 train_time:61743ms step_avg:222.90ms
step:278/7050 train_time:61965ms step_avg:222.90ms
step:279/7050 train_time:62189ms step_avg:222.90ms
step:280/7050 train_time:62411ms step_avg:222.90ms
step:281/7050 train_time:62634ms step_avg:222.90ms
step:282/7050 train_time:62855ms step_avg:222.89ms
step:283/7050 train_time:63076ms step_avg:222.88ms
step:284/7050 train_time:63300ms step_avg:222.89ms
step:285/7050 train_time:63522ms step_avg:222.88ms
step:286/7050 train_time:63745ms step_avg:222.88ms
step:287/7050 train_time:63967ms step_avg:222.88ms
step:288/7050 train_time:64190ms step_avg:222.88ms
step:289/7050 train_time:64415ms step_avg:222.89ms
step:290/7050 train_time:64637ms step_avg:222.89ms
step:291/7050 train_time:64859ms step_avg:222.88ms
step:292/7050 train_time:65081ms step_avg:222.88ms
step:293/7050 train_time:65306ms step_avg:222.89ms
step:294/7050 train_time:65526ms step_avg:222.88ms
step:295/7050 train_time:65749ms step_avg:222.88ms
step:296/7050 train_time:65972ms step_avg:222.88ms
step:297/7050 train_time:66195ms step_avg:222.88ms
step:298/7050 train_time:66416ms step_avg:222.87ms
step:299/7050 train_time:66638ms step_avg:222.87ms
step:300/7050 train_time:66861ms step_avg:222.87ms
step:301/7050 train_time:67084ms step_avg:222.87ms
step:302/7050 train_time:67306ms step_avg:222.87ms
step:303/7050 train_time:67528ms step_avg:222.86ms
step:304/7050 train_time:67750ms step_avg:222.86ms
step:305/7050 train_time:67972ms step_avg:222.86ms
step:306/7050 train_time:68195ms step_avg:222.86ms
step:307/7050 train_time:68416ms step_avg:222.85ms
step:308/7050 train_time:68639ms step_avg:222.86ms
step:309/7050 train_time:68862ms step_avg:222.85ms
step:310/7050 train_time:69086ms step_avg:222.86ms
step:311/7050 train_time:69309ms step_avg:222.86ms
step:312/7050 train_time:69531ms step_avg:222.86ms
step:313/7050 train_time:69754ms step_avg:222.86ms
step:314/7050 train_time:69977ms step_avg:222.86ms
step:315/7050 train_time:70200ms step_avg:222.86ms
step:316/7050 train_time:70422ms step_avg:222.85ms
step:317/7050 train_time:70645ms step_avg:222.85ms
step:318/7050 train_time:70867ms step_avg:222.85ms
step:319/7050 train_time:71090ms step_avg:222.85ms
step:320/7050 train_time:71313ms step_avg:222.85ms
step:321/7050 train_time:71534ms step_avg:222.85ms
step:322/7050 train_time:71756ms step_avg:222.84ms
step:323/7050 train_time:71980ms step_avg:222.85ms
step:324/7050 train_time:72201ms step_avg:222.84ms
step:325/7050 train_time:72424ms step_avg:222.84ms
step:326/7050 train_time:72646ms step_avg:222.84ms
step:327/7050 train_time:72869ms step_avg:222.84ms
step:328/7050 train_time:73093ms step_avg:222.84ms
step:329/7050 train_time:73315ms step_avg:222.84ms
step:330/7050 train_time:73537ms step_avg:222.84ms
step:331/7050 train_time:73759ms step_avg:222.84ms
step:332/7050 train_time:73985ms step_avg:222.85ms
step:333/7050 train_time:74206ms step_avg:222.84ms
step:334/7050 train_time:74428ms step_avg:222.84ms
step:335/7050 train_time:74651ms step_avg:222.84ms
step:336/7050 train_time:74874ms step_avg:222.84ms
step:337/7050 train_time:75099ms step_avg:222.85ms
step:338/7050 train_time:75320ms step_avg:222.84ms
step:339/7050 train_time:75542ms step_avg:222.84ms
step:340/7050 train_time:75764ms step_avg:222.84ms
step:341/7050 train_time:75987ms step_avg:222.84ms
step:342/7050 train_time:76209ms step_avg:222.83ms
step:343/7050 train_time:76430ms step_avg:222.83ms
step:344/7050 train_time:76652ms step_avg:222.83ms
step:345/7050 train_time:76875ms step_avg:222.83ms
step:346/7050 train_time:77097ms step_avg:222.82ms
step:347/7050 train_time:77320ms step_avg:222.82ms
step:348/7050 train_time:77541ms step_avg:222.82ms
step:349/7050 train_time:77765ms step_avg:222.82ms
step:350/7050 train_time:77986ms step_avg:222.82ms
step:351/7050 train_time:78209ms step_avg:222.82ms
step:352/7050 train_time:78430ms step_avg:222.81ms
step:353/7050 train_time:78652ms step_avg:222.81ms
step:354/7050 train_time:78876ms step_avg:222.81ms
step:355/7050 train_time:79100ms step_avg:222.82ms
step:356/7050 train_time:79322ms step_avg:222.81ms
step:357/7050 train_time:79545ms step_avg:222.81ms
step:358/7050 train_time:79766ms step_avg:222.81ms
step:359/7050 train_time:79989ms step_avg:222.81ms
step:360/7050 train_time:80212ms step_avg:222.81ms
step:361/7050 train_time:80435ms step_avg:222.81ms
step:362/7050 train_time:80657ms step_avg:222.81ms
step:363/7050 train_time:80881ms step_avg:222.81ms
step:364/7050 train_time:81103ms step_avg:222.81ms
step:365/7050 train_time:81326ms step_avg:222.81ms
step:366/7050 train_time:81548ms step_avg:222.81ms
step:367/7050 train_time:81770ms step_avg:222.81ms
step:368/7050 train_time:81992ms step_avg:222.81ms
step:369/7050 train_time:82214ms step_avg:222.80ms
step:370/7050 train_time:82436ms step_avg:222.80ms
step:371/7050 train_time:82658ms step_avg:222.80ms
step:372/7050 train_time:82881ms step_avg:222.80ms
step:373/7050 train_time:83104ms step_avg:222.80ms
step:374/7050 train_time:83326ms step_avg:222.80ms
step:375/7050 train_time:83548ms step_avg:222.79ms
step:375/7050 val_loss:4.0431 train_time:83725ms step_avg:223.27ms
step:376/7050 train_time:83773ms step_avg:222.80ms
step:377/7050 train_time:83995ms step_avg:222.80ms
step:378/7050 train_time:84224ms step_avg:222.81ms
step:379/7050 train_time:84447ms step_avg:222.82ms
step:380/7050 train_time:84668ms step_avg:222.81ms
step:381/7050 train_time:84890ms step_avg:222.81ms
step:382/7050 train_time:85111ms step_avg:222.80ms
step:383/7050 train_time:85336ms step_avg:222.81ms
step:384/7050 train_time:85559ms step_avg:222.81ms
step:385/7050 train_time:85780ms step_avg:222.81ms
step:386/7050 train_time:86003ms step_avg:222.80ms
step:387/7050 train_time:86227ms step_avg:222.81ms
step:388/7050 train_time:86449ms step_avg:222.81ms
step:389/7050 train_time:86672ms step_avg:222.81ms
step:390/7050 train_time:86893ms step_avg:222.80ms
step:391/7050 train_time:87116ms step_avg:222.80ms
step:392/7050 train_time:87339ms step_avg:222.80ms
step:393/7050 train_time:87563ms step_avg:222.81ms
step:394/7050 train_time:87785ms step_avg:222.81ms
step:395/7050 train_time:88007ms step_avg:222.80ms
step:396/7050 train_time:88230ms step_avg:222.80ms
step:397/7050 train_time:88452ms step_avg:222.80ms
step:398/7050 train_time:88676ms step_avg:222.80ms
step:399/7050 train_time:88896ms step_avg:222.80ms
step:400/7050 train_time:89119ms step_avg:222.80ms
step:401/7050 train_time:89342ms step_avg:222.80ms
step:402/7050 train_time:89564ms step_avg:222.80ms
step:403/7050 train_time:89786ms step_avg:222.79ms
step:404/7050 train_time:90008ms step_avg:222.79ms
step:405/7050 train_time:90231ms step_avg:222.79ms
step:406/7050 train_time:90453ms step_avg:222.79ms
step:407/7050 train_time:90676ms step_avg:222.79ms
step:408/7050 train_time:90898ms step_avg:222.79ms
step:409/7050 train_time:91122ms step_avg:222.79ms
step:410/7050 train_time:91344ms step_avg:222.79ms
step:411/7050 train_time:91567ms step_avg:222.79ms
step:412/7050 train_time:91788ms step_avg:222.79ms
step:413/7050 train_time:92009ms step_avg:222.78ms
step:414/7050 train_time:92233ms step_avg:222.78ms
step:415/7050 train_time:92455ms step_avg:222.78ms
step:416/7050 train_time:92677ms step_avg:222.78ms
step:417/7050 train_time:92899ms step_avg:222.78ms
step:418/7050 train_time:93122ms step_avg:222.78ms
step:419/7050 train_time:93344ms step_avg:222.78ms
step:420/7050 train_time:93567ms step_avg:222.78ms
step:421/7050 train_time:93788ms step_avg:222.77ms
step:422/7050 train_time:94011ms step_avg:222.77ms
step:423/7050 train_time:94234ms step_avg:222.77ms
step:424/7050 train_time:94456ms step_avg:222.77ms
step:425/7050 train_time:94679ms step_avg:222.77ms
step:426/7050 train_time:94900ms step_avg:222.77ms
step:427/7050 train_time:95123ms step_avg:222.77ms
step:428/7050 train_time:95345ms step_avg:222.77ms
step:429/7050 train_time:95568ms step_avg:222.77ms
step:430/7050 train_time:95791ms step_avg:222.77ms
step:431/7050 train_time:96013ms step_avg:222.77ms
step:432/7050 train_time:96236ms step_avg:222.77ms
step:433/7050 train_time:96459ms step_avg:222.77ms
step:434/7050 train_time:96680ms step_avg:222.77ms
step:435/7050 train_time:96903ms step_avg:222.77ms
step:436/7050 train_time:97124ms step_avg:222.76ms
step:437/7050 train_time:97347ms step_avg:222.76ms
step:438/7050 train_time:97568ms step_avg:222.76ms
step:439/7050 train_time:97790ms step_avg:222.76ms
step:440/7050 train_time:98013ms step_avg:222.76ms
step:441/7050 train_time:98235ms step_avg:222.76ms
step:442/7050 train_time:98459ms step_avg:222.76ms
step:443/7050 train_time:98682ms step_avg:222.76ms
step:444/7050 train_time:98905ms step_avg:222.76ms
step:445/7050 train_time:99127ms step_avg:222.76ms
step:446/7050 train_time:99351ms step_avg:222.76ms
step:447/7050 train_time:99572ms step_avg:222.76ms
step:448/7050 train_time:99795ms step_avg:222.76ms
step:449/7050 train_time:100019ms step_avg:222.76ms
step:450/7050 train_time:100243ms step_avg:222.76ms
step:451/7050 train_time:100465ms step_avg:222.76ms
step:452/7050 train_time:100687ms step_avg:222.76ms
step:453/7050 train_time:100909ms step_avg:222.76ms
step:454/7050 train_time:101133ms step_avg:222.76ms
step:455/7050 train_time:101356ms step_avg:222.76ms
step:456/7050 train_time:101577ms step_avg:222.76ms
step:457/7050 train_time:101800ms step_avg:222.76ms
step:458/7050 train_time:102022ms step_avg:222.76ms
step:459/7050 train_time:102247ms step_avg:222.76ms
step:460/7050 train_time:102468ms step_avg:222.76ms
step:461/7050 train_time:102690ms step_avg:222.76ms
step:462/7050 train_time:102912ms step_avg:222.75ms
step:463/7050 train_time:103138ms step_avg:222.76ms
step:464/7050 train_time:103360ms step_avg:222.76ms
step:465/7050 train_time:103582ms step_avg:222.76ms
step:466/7050 train_time:103804ms step_avg:222.75ms
step:467/7050 train_time:104028ms step_avg:222.76ms
step:468/7050 train_time:104253ms step_avg:222.76ms
step:469/7050 train_time:104474ms step_avg:222.76ms
step:470/7050 train_time:104696ms step_avg:222.76ms
step:471/7050 train_time:104920ms step_avg:222.76ms
step:472/7050 train_time:105144ms step_avg:222.76ms
step:473/7050 train_time:105366ms step_avg:222.76ms
step:474/7050 train_time:105588ms step_avg:222.76ms
step:475/7050 train_time:105810ms step_avg:222.76ms
step:476/7050 train_time:106033ms step_avg:222.76ms
step:477/7050 train_time:106257ms step_avg:222.76ms
step:478/7050 train_time:106479ms step_avg:222.76ms
step:479/7050 train_time:106701ms step_avg:222.76ms
step:480/7050 train_time:106924ms step_avg:222.76ms
step:481/7050 train_time:107146ms step_avg:222.76ms
step:482/7050 train_time:107369ms step_avg:222.76ms
step:483/7050 train_time:107590ms step_avg:222.75ms
step:484/7050 train_time:107812ms step_avg:222.75ms
step:485/7050 train_time:108034ms step_avg:222.75ms
step:486/7050 train_time:108257ms step_avg:222.75ms
step:487/7050 train_time:108479ms step_avg:222.75ms
step:488/7050 train_time:108700ms step_avg:222.75ms
step:489/7050 train_time:108924ms step_avg:222.75ms
step:490/7050 train_time:109147ms step_avg:222.75ms
step:491/7050 train_time:109371ms step_avg:222.75ms
step:492/7050 train_time:109592ms step_avg:222.75ms
step:493/7050 train_time:109815ms step_avg:222.75ms
step:494/7050 train_time:110039ms step_avg:222.75ms
step:495/7050 train_time:110261ms step_avg:222.75ms
step:496/7050 train_time:110484ms step_avg:222.75ms
step:497/7050 train_time:110705ms step_avg:222.75ms
step:498/7050 train_time:110927ms step_avg:222.74ms
step:499/7050 train_time:111150ms step_avg:222.75ms
step:500/7050 train_time:111372ms step_avg:222.74ms
step:500/7050 val_loss:3.9463 train_time:111547ms step_avg:223.09ms
step:501/7050 train_time:111595ms step_avg:222.74ms
step:502/7050 train_time:111816ms step_avg:222.74ms
step:503/7050 train_time:112047ms step_avg:222.76ms
step:504/7050 train_time:112268ms step_avg:222.75ms
step:505/7050 train_time:112488ms step_avg:222.75ms
step:506/7050 train_time:112710ms step_avg:222.75ms
step:507/7050 train_time:112934ms step_avg:222.75ms
step:508/7050 train_time:113157ms step_avg:222.75ms
step:509/7050 train_time:113379ms step_avg:222.75ms
step:510/7050 train_time:113601ms step_avg:222.75ms
step:511/7050 train_time:113824ms step_avg:222.75ms
step:512/7050 train_time:114048ms step_avg:222.75ms
step:513/7050 train_time:114272ms step_avg:222.75ms
step:514/7050 train_time:114493ms step_avg:222.75ms
step:515/7050 train_time:114714ms step_avg:222.75ms
step:516/7050 train_time:114937ms step_avg:222.75ms
step:517/7050 train_time:115160ms step_avg:222.75ms
step:518/7050 train_time:115384ms step_avg:222.75ms
step:519/7050 train_time:115606ms step_avg:222.75ms
step:520/7050 train_time:115829ms step_avg:222.75ms
step:521/7050 train_time:116051ms step_avg:222.75ms
step:522/7050 train_time:116273ms step_avg:222.75ms
step:523/7050 train_time:116496ms step_avg:222.75ms
step:524/7050 train_time:116718ms step_avg:222.74ms
step:525/7050 train_time:116942ms step_avg:222.75ms
step:526/7050 train_time:117167ms step_avg:222.75ms
step:527/7050 train_time:117389ms step_avg:222.75ms
step:528/7050 train_time:117612ms step_avg:222.75ms
step:529/7050 train_time:117835ms step_avg:222.75ms
step:530/7050 train_time:118060ms step_avg:222.75ms
step:531/7050 train_time:118285ms step_avg:222.76ms
step:532/7050 train_time:118509ms step_avg:222.76ms
step:533/7050 train_time:118732ms step_avg:222.76ms
step:534/7050 train_time:118955ms step_avg:222.76ms
step:535/7050 train_time:119181ms step_avg:222.77ms
step:536/7050 train_time:119405ms step_avg:222.77ms
step:537/7050 train_time:119628ms step_avg:222.77ms
step:538/7050 train_time:119851ms step_avg:222.77ms
step:539/7050 train_time:120075ms step_avg:222.77ms
step:540/7050 train_time:120302ms step_avg:222.78ms
step:541/7050 train_time:120525ms step_avg:222.78ms
step:542/7050 train_time:120747ms step_avg:222.78ms
step:543/7050 train_time:120971ms step_avg:222.78ms
step:544/7050 train_time:121195ms step_avg:222.79ms
step:545/7050 train_time:121418ms step_avg:222.79ms
step:546/7050 train_time:121641ms step_avg:222.79ms
step:547/7050 train_time:121865ms step_avg:222.79ms
step:548/7050 train_time:122089ms step_avg:222.79ms
step:549/7050 train_time:122314ms step_avg:222.79ms
step:550/7050 train_time:122538ms step_avg:222.80ms
step:551/7050 train_time:122761ms step_avg:222.80ms
step:552/7050 train_time:122985ms step_avg:222.80ms
step:553/7050 train_time:123209ms step_avg:222.80ms
step:554/7050 train_time:123434ms step_avg:222.81ms
step:555/7050 train_time:123658ms step_avg:222.81ms
step:556/7050 train_time:123882ms step_avg:222.81ms
step:557/7050 train_time:124105ms step_avg:222.81ms
step:558/7050 train_time:124330ms step_avg:222.81ms
step:559/7050 train_time:124553ms step_avg:222.81ms
step:560/7050 train_time:124776ms step_avg:222.81ms
step:561/7050 train_time:124999ms step_avg:222.81ms
step:562/7050 train_time:125221ms step_avg:222.81ms
step:563/7050 train_time:125444ms step_avg:222.81ms
step:564/7050 train_time:125669ms step_avg:222.82ms
step:565/7050 train_time:125893ms step_avg:222.82ms
step:566/7050 train_time:126117ms step_avg:222.82ms
step:567/7050 train_time:126340ms step_avg:222.82ms
step:568/7050 train_time:126564ms step_avg:222.82ms
step:569/7050 train_time:126790ms step_avg:222.83ms
step:570/7050 train_time:127014ms step_avg:222.83ms
step:571/7050 train_time:127238ms step_avg:222.83ms
step:572/7050 train_time:127461ms step_avg:222.83ms
step:573/7050 train_time:127685ms step_avg:222.84ms
step:574/7050 train_time:127910ms step_avg:222.84ms
step:575/7050 train_time:128133ms step_avg:222.84ms
step:576/7050 train_time:128357ms step_avg:222.84ms
step:577/7050 train_time:128581ms step_avg:222.84ms
step:578/7050 train_time:128806ms step_avg:222.85ms
step:579/7050 train_time:129029ms step_avg:222.85ms
step:580/7050 train_time:129252ms step_avg:222.85ms
step:581/7050 train_time:129477ms step_avg:222.85ms
step:582/7050 train_time:129700ms step_avg:222.85ms
step:583/7050 train_time:129922ms step_avg:222.85ms
step:584/7050 train_time:130146ms step_avg:222.85ms
step:585/7050 train_time:130370ms step_avg:222.85ms
step:586/7050 train_time:130594ms step_avg:222.86ms
step:587/7050 train_time:130818ms step_avg:222.86ms
step:588/7050 train_time:131042ms step_avg:222.86ms
step:589/7050 train_time:131266ms step_avg:222.86ms
step:590/7050 train_time:131491ms step_avg:222.87ms
step:591/7050 train_time:131714ms step_avg:222.87ms
step:592/7050 train_time:131938ms step_avg:222.87ms
step:593/7050 train_time:132163ms step_avg:222.87ms
step:594/7050 train_time:132388ms step_avg:222.87ms
step:595/7050 train_time:132612ms step_avg:222.88ms
step:596/7050 train_time:132835ms step_avg:222.88ms
step:597/7050 train_time:133059ms step_avg:222.88ms
step:598/7050 train_time:133283ms step_avg:222.88ms
step:599/7050 train_time:133509ms step_avg:222.89ms
step:600/7050 train_time:133734ms step_avg:222.89ms
step:601/7050 train_time:133956ms step_avg:222.89ms
step:602/7050 train_time:134179ms step_avg:222.89ms
step:603/7050 train_time:134403ms step_avg:222.89ms
step:604/7050 train_time:134628ms step_avg:222.89ms
step:605/7050 train_time:134851ms step_avg:222.89ms
step:606/7050 train_time:135076ms step_avg:222.90ms
step:607/7050 train_time:135299ms step_avg:222.90ms
step:608/7050 train_time:135523ms step_avg:222.90ms
step:609/7050 train_time:135746ms step_avg:222.90ms
step:610/7050 train_time:135970ms step_avg:222.90ms
step:611/7050 train_time:136194ms step_avg:222.90ms
step:612/7050 train_time:136417ms step_avg:222.90ms
step:613/7050 train_time:136641ms step_avg:222.91ms
step:614/7050 train_time:136865ms step_avg:222.91ms
step:615/7050 train_time:137090ms step_avg:222.91ms
step:616/7050 train_time:137313ms step_avg:222.91ms
step:617/7050 train_time:137538ms step_avg:222.91ms
step:618/7050 train_time:137760ms step_avg:222.91ms
step:619/7050 train_time:137984ms step_avg:222.91ms
step:620/7050 train_time:138207ms step_avg:222.92ms
step:621/7050 train_time:138430ms step_avg:222.92ms
step:622/7050 train_time:138653ms step_avg:222.92ms
step:623/7050 train_time:138878ms step_avg:222.92ms
step:624/7050 train_time:139102ms step_avg:222.92ms
step:625/7050 train_time:139326ms step_avg:222.92ms
step:625/7050 val_loss:3.6611 train_time:139502ms step_avg:223.20ms
step:626/7050 train_time:139550ms step_avg:222.92ms
step:627/7050 train_time:139771ms step_avg:222.92ms
step:628/7050 train_time:140000ms step_avg:222.93ms
step:629/7050 train_time:140223ms step_avg:222.93ms
step:630/7050 train_time:140446ms step_avg:222.93ms
step:631/7050 train_time:140670ms step_avg:222.93ms
step:632/7050 train_time:140894ms step_avg:222.93ms
step:633/7050 train_time:141120ms step_avg:222.94ms
step:634/7050 train_time:141343ms step_avg:222.94ms
step:635/7050 train_time:141567ms step_avg:222.94ms
step:636/7050 train_time:141790ms step_avg:222.94ms
step:637/7050 train_time:142014ms step_avg:222.94ms
step:638/7050 train_time:142237ms step_avg:222.94ms
step:639/7050 train_time:142461ms step_avg:222.94ms
step:640/7050 train_time:142683ms step_avg:222.94ms
step:641/7050 train_time:142906ms step_avg:222.94ms
step:642/7050 train_time:143131ms step_avg:222.95ms
step:643/7050 train_time:143355ms step_avg:222.95ms
step:644/7050 train_time:143577ms step_avg:222.95ms
step:645/7050 train_time:143800ms step_avg:222.95ms
step:646/7050 train_time:144025ms step_avg:222.95ms
step:647/7050 train_time:144248ms step_avg:222.95ms
step:648/7050 train_time:144472ms step_avg:222.95ms
step:649/7050 train_time:144696ms step_avg:222.95ms
step:650/7050 train_time:144920ms step_avg:222.95ms
step:651/7050 train_time:145145ms step_avg:222.96ms
step:652/7050 train_time:145367ms step_avg:222.96ms
step:653/7050 train_time:145590ms step_avg:222.95ms
step:654/7050 train_time:145814ms step_avg:222.96ms
step:655/7050 train_time:146038ms step_avg:222.96ms
step:656/7050 train_time:146262ms step_avg:222.96ms
step:657/7050 train_time:146485ms step_avg:222.96ms
step:658/7050 train_time:146708ms step_avg:222.96ms
step:659/7050 train_time:146934ms step_avg:222.96ms
step:660/7050 train_time:147157ms step_avg:222.97ms
step:661/7050 train_time:147381ms step_avg:222.97ms
step:662/7050 train_time:147604ms step_avg:222.97ms
step:663/7050 train_time:147826ms step_avg:222.97ms
step:664/7050 train_time:148049ms step_avg:222.97ms
step:665/7050 train_time:148273ms step_avg:222.97ms
step:666/7050 train_time:148497ms step_avg:222.97ms
step:667/7050 train_time:148722ms step_avg:222.97ms
step:668/7050 train_time:148947ms step_avg:222.97ms
step:669/7050 train_time:149169ms step_avg:222.97ms
step:670/7050 train_time:149394ms step_avg:222.98ms
step:671/7050 train_time:149617ms step_avg:222.98ms
step:672/7050 train_time:149842ms step_avg:222.98ms
step:673/7050 train_time:150068ms step_avg:222.98ms
step:674/7050 train_time:150291ms step_avg:222.98ms
step:675/7050 train_time:150514ms step_avg:222.98ms
step:676/7050 train_time:150738ms step_avg:222.98ms
step:677/7050 train_time:150962ms step_avg:222.99ms
step:678/7050 train_time:151185ms step_avg:222.99ms
step:679/7050 train_time:151409ms step_avg:222.99ms
step:680/7050 train_time:151633ms step_avg:222.99ms
step:681/7050 train_time:151858ms step_avg:222.99ms
step:682/7050 train_time:152083ms step_avg:223.00ms
step:683/7050 train_time:152306ms step_avg:223.00ms
step:684/7050 train_time:152530ms step_avg:223.00ms
step:685/7050 train_time:152753ms step_avg:223.00ms
step:686/7050 train_time:152977ms step_avg:223.00ms
step:687/7050 train_time:153201ms step_avg:223.00ms
step:688/7050 train_time:153425ms step_avg:223.00ms
step:689/7050 train_time:153649ms step_avg:223.00ms
step:690/7050 train_time:153873ms step_avg:223.01ms
step:691/7050 train_time:154097ms step_avg:223.01ms
step:692/7050 train_time:154322ms step_avg:223.01ms
step:693/7050 train_time:154546ms step_avg:223.01ms
step:694/7050 train_time:154768ms step_avg:223.01ms
step:695/7050 train_time:154992ms step_avg:223.01ms
step:696/7050 train_time:155217ms step_avg:223.01ms
step:697/7050 train_time:155443ms step_avg:223.02ms
step:698/7050 train_time:155666ms step_avg:223.02ms
step:699/7050 train_time:155889ms step_avg:223.02ms
step:700/7050 train_time:156112ms step_avg:223.02ms
step:701/7050 train_time:156336ms step_avg:223.02ms
step:702/7050 train_time:156561ms step_avg:223.02ms
step:703/7050 train_time:156783ms step_avg:223.02ms
step:704/7050 train_time:157006ms step_avg:223.02ms
step:705/7050 train_time:157229ms step_avg:223.02ms
step:706/7050 train_time:157454ms step_avg:223.02ms
step:707/7050 train_time:157678ms step_avg:223.02ms
step:708/7050 train_time:157900ms step_avg:223.02ms
step:709/7050 train_time:158123ms step_avg:223.02ms
step:710/7050 train_time:158347ms step_avg:223.02ms
step:711/7050 train_time:158572ms step_avg:223.03ms
step:712/7050 train_time:158797ms step_avg:223.03ms
step:713/7050 train_time:159021ms step_avg:223.03ms
step:714/7050 train_time:159245ms step_avg:223.03ms
step:715/7050 train_time:159468ms step_avg:223.03ms
step:716/7050 train_time:159691ms step_avg:223.03ms
step:717/7050 train_time:159917ms step_avg:223.04ms
step:718/7050 train_time:160139ms step_avg:223.04ms
step:719/7050 train_time:160362ms step_avg:223.04ms
step:720/7050 train_time:160586ms step_avg:223.04ms
step:721/7050 train_time:160809ms step_avg:223.04ms
step:722/7050 train_time:161036ms step_avg:223.04ms
step:723/7050 train_time:161257ms step_avg:223.04ms
step:724/7050 train_time:161479ms step_avg:223.04ms
step:725/7050 train_time:161703ms step_avg:223.04ms
step:726/7050 train_time:161926ms step_avg:223.04ms
step:727/7050 train_time:162152ms step_avg:223.04ms
step:728/7050 train_time:162373ms step_avg:223.04ms
step:729/7050 train_time:162598ms step_avg:223.04ms
step:730/7050 train_time:162821ms step_avg:223.04ms
step:731/7050 train_time:163046ms step_avg:223.04ms
step:732/7050 train_time:163269ms step_avg:223.05ms
step:733/7050 train_time:163491ms step_avg:223.04ms
step:734/7050 train_time:163715ms step_avg:223.05ms
step:735/7050 train_time:163938ms step_avg:223.05ms
step:736/7050 train_time:164161ms step_avg:223.05ms
step:737/7050 train_time:164386ms step_avg:223.05ms
step:738/7050 train_time:164609ms step_avg:223.05ms
step:739/7050 train_time:164834ms step_avg:223.05ms
step:740/7050 train_time:165059ms step_avg:223.05ms
step:741/7050 train_time:165281ms step_avg:223.05ms
step:742/7050 train_time:165508ms step_avg:223.06ms
step:743/7050 train_time:165730ms step_avg:223.06ms
step:744/7050 train_time:165953ms step_avg:223.06ms
step:745/7050 train_time:166177ms step_avg:223.06ms
step:746/7050 train_time:166401ms step_avg:223.06ms
step:747/7050 train_time:166629ms step_avg:223.06ms
step:748/7050 train_time:166849ms step_avg:223.06ms
step:749/7050 train_time:167073ms step_avg:223.06ms
step:750/7050 train_time:167297ms step_avg:223.06ms
step:750/7050 val_loss:3.5936 train_time:167475ms step_avg:223.30ms
step:751/7050 train_time:167523ms step_avg:223.07ms
step:752/7050 train_time:167745ms step_avg:223.07ms
step:753/7050 train_time:167974ms step_avg:223.07ms
step:754/7050 train_time:168198ms step_avg:223.07ms
step:755/7050 train_time:168420ms step_avg:223.07ms
step:756/7050 train_time:168643ms step_avg:223.07ms
step:757/7050 train_time:168868ms step_avg:223.08ms
step:758/7050 train_time:169094ms step_avg:223.08ms
step:759/7050 train_time:169317ms step_avg:223.08ms
step:760/7050 train_time:169538ms step_avg:223.08ms
step:761/7050 train_time:169761ms step_avg:223.08ms
step:762/7050 train_time:169986ms step_avg:223.08ms
step:763/7050 train_time:170209ms step_avg:223.08ms
step:764/7050 train_time:170433ms step_avg:223.08ms
step:765/7050 train_time:170656ms step_avg:223.08ms
step:766/7050 train_time:170879ms step_avg:223.08ms
step:767/7050 train_time:171104ms step_avg:223.08ms
step:768/7050 train_time:171327ms step_avg:223.08ms
step:769/7050 train_time:171550ms step_avg:223.08ms
step:770/7050 train_time:171774ms step_avg:223.08ms
step:771/7050 train_time:171998ms step_avg:223.08ms
step:772/7050 train_time:172221ms step_avg:223.08ms
step:773/7050 train_time:172445ms step_avg:223.09ms
step:774/7050 train_time:172669ms step_avg:223.09ms
step:775/7050 train_time:172894ms step_avg:223.09ms
step:776/7050 train_time:173117ms step_avg:223.09ms
step:777/7050 train_time:173341ms step_avg:223.09ms
step:778/7050 train_time:173564ms step_avg:223.09ms
step:779/7050 train_time:173789ms step_avg:223.09ms
step:780/7050 train_time:174013ms step_avg:223.09ms
step:781/7050 train_time:174236ms step_avg:223.09ms
step:782/7050 train_time:174460ms step_avg:223.09ms
step:783/7050 train_time:174683ms step_avg:223.10ms
step:784/7050 train_time:174907ms step_avg:223.10ms
step:785/7050 train_time:175131ms step_avg:223.10ms
step:786/7050 train_time:175355ms step_avg:223.10ms
step:787/7050 train_time:175578ms step_avg:223.10ms
step:788/7050 train_time:175802ms step_avg:223.10ms
step:789/7050 train_time:176026ms step_avg:223.10ms
step:790/7050 train_time:176249ms step_avg:223.10ms
step:791/7050 train_time:176473ms step_avg:223.10ms
step:792/7050 train_time:176696ms step_avg:223.10ms
step:793/7050 train_time:176920ms step_avg:223.10ms
step:794/7050 train_time:177144ms step_avg:223.10ms
step:795/7050 train_time:177368ms step_avg:223.10ms
step:796/7050 train_time:177592ms step_avg:223.11ms
step:797/7050 train_time:177815ms step_avg:223.11ms
step:798/7050 train_time:178038ms step_avg:223.10ms
step:799/7050 train_time:178261ms step_avg:223.11ms
step:800/7050 train_time:178486ms step_avg:223.11ms
step:801/7050 train_time:178709ms step_avg:223.11ms
step:802/7050 train_time:178931ms step_avg:223.11ms
step:803/7050 train_time:179156ms step_avg:223.11ms
step:804/7050 train_time:179380ms step_avg:223.11ms
step:805/7050 train_time:179605ms step_avg:223.11ms
step:806/7050 train_time:179828ms step_avg:223.11ms
step:807/7050 train_time:180051ms step_avg:223.11ms
step:808/7050 train_time:180276ms step_avg:223.11ms
step:809/7050 train_time:180500ms step_avg:223.11ms
step:810/7050 train_time:180724ms step_avg:223.12ms
step:811/7050 train_time:180948ms step_avg:223.12ms
step:812/7050 train_time:181170ms step_avg:223.12ms
step:813/7050 train_time:181394ms step_avg:223.12ms
step:814/7050 train_time:181616ms step_avg:223.12ms
step:815/7050 train_time:181841ms step_avg:223.12ms
step:816/7050 train_time:182066ms step_avg:223.12ms
step:817/7050 train_time:182290ms step_avg:223.12ms
step:818/7050 train_time:182513ms step_avg:223.12ms
step:819/7050 train_time:182737ms step_avg:223.12ms
step:820/7050 train_time:182962ms step_avg:223.12ms
step:821/7050 train_time:183185ms step_avg:223.12ms
step:822/7050 train_time:183408ms step_avg:223.12ms
step:823/7050 train_time:183631ms step_avg:223.12ms
step:824/7050 train_time:183856ms step_avg:223.13ms
step:825/7050 train_time:184079ms step_avg:223.13ms
step:826/7050 train_time:184303ms step_avg:223.13ms
step:827/7050 train_time:184526ms step_avg:223.13ms
step:828/7050 train_time:184749ms step_avg:223.13ms
step:829/7050 train_time:184975ms step_avg:223.13ms
step:830/7050 train_time:185199ms step_avg:223.13ms
step:831/7050 train_time:185422ms step_avg:223.13ms
step:832/7050 train_time:185645ms step_avg:223.13ms
step:833/7050 train_time:185870ms step_avg:223.13ms
step:834/7050 train_time:186094ms step_avg:223.13ms
step:835/7050 train_time:186317ms step_avg:223.13ms
step:836/7050 train_time:186540ms step_avg:223.13ms
step:837/7050 train_time:186765ms step_avg:223.14ms
step:838/7050 train_time:186989ms step_avg:223.14ms
step:839/7050 train_time:187213ms step_avg:223.14ms
step:840/7050 train_time:187436ms step_avg:223.14ms
step:841/7050 train_time:187659ms step_avg:223.14ms
step:842/7050 train_time:187883ms step_avg:223.14ms
step:843/7050 train_time:188107ms step_avg:223.14ms
step:844/7050 train_time:188330ms step_avg:223.14ms
step:845/7050 train_time:188554ms step_avg:223.14ms
step:846/7050 train_time:188778ms step_avg:223.14ms
step:847/7050 train_time:189002ms step_avg:223.14ms
step:848/7050 train_time:189225ms step_avg:223.14ms
step:849/7050 train_time:189450ms step_avg:223.14ms
step:850/7050 train_time:189674ms step_avg:223.15ms
step:851/7050 train_time:189897ms step_avg:223.15ms
step:852/7050 train_time:190121ms step_avg:223.15ms
step:853/7050 train_time:190346ms step_avg:223.15ms
step:854/7050 train_time:190569ms step_avg:223.15ms
step:855/7050 train_time:190793ms step_avg:223.15ms
step:856/7050 train_time:191015ms step_avg:223.15ms
step:857/7050 train_time:191237ms step_avg:223.15ms
step:858/7050 train_time:191462ms step_avg:223.15ms
step:859/7050 train_time:191687ms step_avg:223.15ms
step:860/7050 train_time:191910ms step_avg:223.15ms
step:861/7050 train_time:192133ms step_avg:223.15ms
step:862/7050 train_time:192356ms step_avg:223.15ms
step:863/7050 train_time:192581ms step_avg:223.15ms
step:864/7050 train_time:192806ms step_avg:223.16ms
step:865/7050 train_time:193028ms step_avg:223.15ms
step:866/7050 train_time:193251ms step_avg:223.15ms
step:867/7050 train_time:193475ms step_avg:223.15ms
step:868/7050 train_time:193699ms step_avg:223.16ms
step:869/7050 train_time:193922ms step_avg:223.16ms
step:870/7050 train_time:194145ms step_avg:223.16ms
step:871/7050 train_time:194370ms step_avg:223.16ms
step:872/7050 train_time:194593ms step_avg:223.16ms
step:873/7050 train_time:194817ms step_avg:223.16ms
step:874/7050 train_time:195040ms step_avg:223.16ms
step:875/7050 train_time:195264ms step_avg:223.16ms
step:875/7050 val_loss:3.5372 train_time:195441ms step_avg:223.36ms
step:876/7050 train_time:195490ms step_avg:223.16ms
step:877/7050 train_time:195712ms step_avg:223.16ms
step:878/7050 train_time:195940ms step_avg:223.17ms
step:879/7050 train_time:196164ms step_avg:223.17ms
step:880/7050 train_time:196385ms step_avg:223.16ms
step:881/7050 train_time:196609ms step_avg:223.17ms
step:882/7050 train_time:196834ms step_avg:223.17ms
step:883/7050 train_time:197059ms step_avg:223.17ms
step:884/7050 train_time:197284ms step_avg:223.17ms
step:885/7050 train_time:197507ms step_avg:223.17ms
step:886/7050 train_time:197733ms step_avg:223.17ms
step:887/7050 train_time:197958ms step_avg:223.18ms
step:888/7050 train_time:198182ms step_avg:223.18ms
step:889/7050 train_time:198405ms step_avg:223.18ms
step:890/7050 train_time:198628ms step_avg:223.18ms
step:891/7050 train_time:198851ms step_avg:223.18ms
step:892/7050 train_time:199075ms step_avg:223.18ms
step:893/7050 train_time:199300ms step_avg:223.18ms
step:894/7050 train_time:199524ms step_avg:223.18ms
step:895/7050 train_time:199749ms step_avg:223.18ms
step:896/7050 train_time:199971ms step_avg:223.18ms
step:897/7050 train_time:200194ms step_avg:223.18ms
step:898/7050 train_time:200418ms step_avg:223.18ms
step:899/7050 train_time:200645ms step_avg:223.19ms
step:900/7050 train_time:200869ms step_avg:223.19ms
step:901/7050 train_time:201092ms step_avg:223.19ms
step:902/7050 train_time:201317ms step_avg:223.19ms
step:903/7050 train_time:201542ms step_avg:223.19ms
step:904/7050 train_time:201764ms step_avg:223.19ms
step:905/7050 train_time:201988ms step_avg:223.19ms
step:906/7050 train_time:202212ms step_avg:223.19ms
step:907/7050 train_time:202436ms step_avg:223.19ms
step:908/7050 train_time:202661ms step_avg:223.19ms
step:909/7050 train_time:202886ms step_avg:223.20ms
step:910/7050 train_time:203108ms step_avg:223.20ms
step:911/7050 train_time:203331ms step_avg:223.20ms
step:912/7050 train_time:203555ms step_avg:223.20ms
step:913/7050 train_time:203779ms step_avg:223.20ms
step:914/7050 train_time:204004ms step_avg:223.20ms
step:915/7050 train_time:204228ms step_avg:223.20ms
step:916/7050 train_time:204453ms step_avg:223.20ms
step:917/7050 train_time:204676ms step_avg:223.20ms
step:918/7050 train_time:204899ms step_avg:223.20ms
step:919/7050 train_time:205124ms step_avg:223.20ms
step:920/7050 train_time:205347ms step_avg:223.20ms
step:921/7050 train_time:205572ms step_avg:223.20ms
step:922/7050 train_time:205796ms step_avg:223.21ms
step:923/7050 train_time:206020ms step_avg:223.21ms
step:924/7050 train_time:206244ms step_avg:223.21ms
step:925/7050 train_time:206467ms step_avg:223.21ms
step:926/7050 train_time:206691ms step_avg:223.21ms
step:927/7050 train_time:206914ms step_avg:223.21ms
step:928/7050 train_time:207138ms step_avg:223.21ms
step:929/7050 train_time:207362ms step_avg:223.21ms
step:930/7050 train_time:207585ms step_avg:223.21ms
step:931/7050 train_time:207810ms step_avg:223.21ms
step:932/7050 train_time:208033ms step_avg:223.21ms
step:933/7050 train_time:208256ms step_avg:223.21ms
step:934/7050 train_time:208480ms step_avg:223.21ms
step:935/7050 train_time:208703ms step_avg:223.21ms
step:936/7050 train_time:208929ms step_avg:223.21ms
step:937/7050 train_time:209153ms step_avg:223.22ms
step:938/7050 train_time:209377ms step_avg:223.22ms
step:939/7050 train_time:209600ms step_avg:223.22ms
step:940/7050 train_time:209825ms step_avg:223.22ms
step:941/7050 train_time:210050ms step_avg:223.22ms
step:942/7050 train_time:210273ms step_avg:223.22ms
step:943/7050 train_time:210497ms step_avg:223.22ms
step:944/7050 train_time:210721ms step_avg:223.22ms
step:945/7050 train_time:210945ms step_avg:223.22ms
step:946/7050 train_time:211168ms step_avg:223.22ms
step:947/7050 train_time:211392ms step_avg:223.22ms
step:948/7050 train_time:211615ms step_avg:223.22ms
step:949/7050 train_time:211839ms step_avg:223.22ms
step:950/7050 train_time:212063ms step_avg:223.22ms
step:951/7050 train_time:212286ms step_avg:223.22ms
step:952/7050 train_time:212508ms step_avg:223.22ms
step:953/7050 train_time:212734ms step_avg:223.23ms
step:954/7050 train_time:212958ms step_avg:223.23ms
step:955/7050 train_time:213181ms step_avg:223.23ms
step:956/7050 train_time:213403ms step_avg:223.23ms
step:957/7050 train_time:213626ms step_avg:223.22ms
step:958/7050 train_time:213850ms step_avg:223.23ms
step:959/7050 train_time:214075ms step_avg:223.23ms
step:960/7050 train_time:214298ms step_avg:223.23ms
step:961/7050 train_time:214521ms step_avg:223.23ms
step:962/7050 train_time:214745ms step_avg:223.23ms
step:963/7050 train_time:214969ms step_avg:223.23ms
step:964/7050 train_time:215194ms step_avg:223.23ms
step:965/7050 train_time:215416ms step_avg:223.23ms
step:966/7050 train_time:215641ms step_avg:223.23ms
step:967/7050 train_time:215865ms step_avg:223.23ms
step:968/7050 train_time:216089ms step_avg:223.23ms
step:969/7050 train_time:216312ms step_avg:223.23ms
step:970/7050 train_time:216536ms step_avg:223.23ms
step:971/7050 train_time:216759ms step_avg:223.23ms
step:972/7050 train_time:216982ms step_avg:223.23ms
step:973/7050 train_time:217205ms step_avg:223.23ms
step:974/7050 train_time:217428ms step_avg:223.23ms
step:975/7050 train_time:217651ms step_avg:223.23ms
step:976/7050 train_time:217876ms step_avg:223.23ms
step:977/7050 train_time:218099ms step_avg:223.23ms
step:978/7050 train_time:218324ms step_avg:223.24ms
step:979/7050 train_time:218548ms step_avg:223.24ms
step:980/7050 train_time:218773ms step_avg:223.24ms
step:981/7050 train_time:218997ms step_avg:223.24ms
step:982/7050 train_time:219220ms step_avg:223.24ms
step:983/7050 train_time:219443ms step_avg:223.24ms
step:984/7050 train_time:219666ms step_avg:223.24ms
step:985/7050 train_time:219890ms step_avg:223.24ms
step:986/7050 train_time:220112ms step_avg:223.24ms
step:987/7050 train_time:220337ms step_avg:223.24ms
step:988/7050 train_time:220561ms step_avg:223.24ms
step:989/7050 train_time:220784ms step_avg:223.24ms
step:990/7050 train_time:221008ms step_avg:223.24ms
step:991/7050 train_time:221233ms step_avg:223.24ms
step:992/7050 train_time:221456ms step_avg:223.24ms
step:993/7050 train_time:221680ms step_avg:223.24ms
step:994/7050 train_time:221903ms step_avg:223.24ms
step:995/7050 train_time:222128ms step_avg:223.24ms
step:996/7050 train_time:222350ms step_avg:223.24ms
step:997/7050 train_time:222573ms step_avg:223.24ms
step:998/7050 train_time:222798ms step_avg:223.24ms
step:999/7050 train_time:223022ms step_avg:223.25ms
step:1000/7050 train_time:223245ms step_avg:223.25ms
step:1000/7050 val_loss:3.4934 train_time:223422ms step_avg:223.42ms
step:1001/7050 train_time:223469ms step_avg:223.25ms
step:1002/7050 train_time:223691ms step_avg:223.24ms
step:1003/7050 train_time:223921ms step_avg:223.25ms
step:1004/7050 train_time:224145ms step_avg:223.25ms
step:1005/7050 train_time:224367ms step_avg:223.25ms
step:1006/7050 train_time:224592ms step_avg:223.25ms
step:1007/7050 train_time:224816ms step_avg:223.25ms
step:1008/7050 train_time:225040ms step_avg:223.25ms
step:1009/7050 train_time:225264ms step_avg:223.25ms
step:1010/7050 train_time:225487ms step_avg:223.25ms
step:1011/7050 train_time:225711ms step_avg:223.26ms
step:1012/7050 train_time:225935ms step_avg:223.26ms
step:1013/7050 train_time:226160ms step_avg:223.26ms
step:1014/7050 train_time:226386ms step_avg:223.26ms
step:1015/7050 train_time:226607ms step_avg:223.26ms
step:1016/7050 train_time:226830ms step_avg:223.26ms
step:1017/7050 train_time:227054ms step_avg:223.26ms
step:1018/7050 train_time:227278ms step_avg:223.26ms
step:1019/7050 train_time:227502ms step_avg:223.26ms
step:1020/7050 train_time:227724ms step_avg:223.26ms
step:1021/7050 train_time:227948ms step_avg:223.26ms
step:1022/7050 train_time:228172ms step_avg:223.26ms
step:1023/7050 train_time:228395ms step_avg:223.26ms
step:1024/7050 train_time:228618ms step_avg:223.26ms
step:1025/7050 train_time:228840ms step_avg:223.26ms
step:1026/7050 train_time:229065ms step_avg:223.26ms
step:1027/7050 train_time:229289ms step_avg:223.26ms
step:1028/7050 train_time:229513ms step_avg:223.26ms
step:1029/7050 train_time:229736ms step_avg:223.26ms
step:1030/7050 train_time:229961ms step_avg:223.26ms
step:1031/7050 train_time:230186ms step_avg:223.26ms
step:1032/7050 train_time:230409ms step_avg:223.26ms
step:1033/7050 train_time:230632ms step_avg:223.26ms
step:1034/7050 train_time:230855ms step_avg:223.26ms
step:1035/7050 train_time:231080ms step_avg:223.27ms
step:1036/7050 train_time:231303ms step_avg:223.27ms
step:1037/7050 train_time:231527ms step_avg:223.27ms
step:1038/7050 train_time:231749ms step_avg:223.26ms
step:1039/7050 train_time:231973ms step_avg:223.27ms
step:1040/7050 train_time:232200ms step_avg:223.27ms
step:1041/7050 train_time:232424ms step_avg:223.27ms
step:1042/7050 train_time:232647ms step_avg:223.27ms
step:1043/7050 train_time:232871ms step_avg:223.27ms
step:1044/7050 train_time:233096ms step_avg:223.27ms
step:1045/7050 train_time:233320ms step_avg:223.27ms
step:1046/7050 train_time:233543ms step_avg:223.27ms
step:1047/7050 train_time:233766ms step_avg:223.27ms
step:1048/7050 train_time:233992ms step_avg:223.28ms
step:1049/7050 train_time:234220ms step_avg:223.28ms
step:1050/7050 train_time:234444ms step_avg:223.28ms
step:1051/7050 train_time:234668ms step_avg:223.28ms
step:1052/7050 train_time:234893ms step_avg:223.28ms
step:1053/7050 train_time:235119ms step_avg:223.28ms
step:1054/7050 train_time:235344ms step_avg:223.29ms
step:1055/7050 train_time:235568ms step_avg:223.29ms
step:1056/7050 train_time:235793ms step_avg:223.29ms
step:1057/7050 train_time:236017ms step_avg:223.29ms
step:1058/7050 train_time:236243ms step_avg:223.29ms
step:1059/7050 train_time:236469ms step_avg:223.30ms
step:1060/7050 train_time:236694ms step_avg:223.30ms
step:1061/7050 train_time:236918ms step_avg:223.30ms
step:1062/7050 train_time:237144ms step_avg:223.30ms
step:1063/7050 train_time:237369ms step_avg:223.30ms
step:1064/7050 train_time:237595ms step_avg:223.30ms
step:1065/7050 train_time:237821ms step_avg:223.31ms
step:1066/7050 train_time:238044ms step_avg:223.31ms
step:1067/7050 train_time:238268ms step_avg:223.31ms
step:1068/7050 train_time:238494ms step_avg:223.31ms
step:1069/7050 train_time:238720ms step_avg:223.31ms
step:1070/7050 train_time:238947ms step_avg:223.31ms
step:1071/7050 train_time:239173ms step_avg:223.32ms
step:1072/7050 train_time:239397ms step_avg:223.32ms
step:1073/7050 train_time:239620ms step_avg:223.32ms
step:1074/7050 train_time:239845ms step_avg:223.32ms
step:1075/7050 train_time:240071ms step_avg:223.32ms
step:1076/7050 train_time:240297ms step_avg:223.32ms
step:1077/7050 train_time:240521ms step_avg:223.32ms
step:1078/7050 train_time:240747ms step_avg:223.33ms
step:1079/7050 train_time:240971ms step_avg:223.33ms
step:1080/7050 train_time:241195ms step_avg:223.33ms
step:1081/7050 train_time:241420ms step_avg:223.33ms
step:1082/7050 train_time:241645ms step_avg:223.33ms
step:1083/7050 train_time:241870ms step_avg:223.33ms
step:1084/7050 train_time:242094ms step_avg:223.33ms
step:1085/7050 train_time:242320ms step_avg:223.34ms
step:1086/7050 train_time:242545ms step_avg:223.34ms
step:1087/7050 train_time:242770ms step_avg:223.34ms
step:1088/7050 train_time:242995ms step_avg:223.34ms
step:1089/7050 train_time:243219ms step_avg:223.34ms
step:1090/7050 train_time:243445ms step_avg:223.34ms
step:1091/7050 train_time:243670ms step_avg:223.35ms
step:1092/7050 train_time:243895ms step_avg:223.35ms
step:1093/7050 train_time:244119ms step_avg:223.35ms
step:1094/7050 train_time:244344ms step_avg:223.35ms
step:1095/7050 train_time:244570ms step_avg:223.35ms
step:1096/7050 train_time:244795ms step_avg:223.35ms
step:1097/7050 train_time:245021ms step_avg:223.36ms
step:1098/7050 train_time:245246ms step_avg:223.36ms
step:1099/7050 train_time:245470ms step_avg:223.36ms
step:1100/7050 train_time:245695ms step_avg:223.36ms
step:1101/7050 train_time:245921ms step_avg:223.36ms
step:1102/7050 train_time:246146ms step_avg:223.36ms
step:1103/7050 train_time:246371ms step_avg:223.36ms
step:1104/7050 train_time:246597ms step_avg:223.37ms
step:1105/7050 train_time:246822ms step_avg:223.37ms
step:1106/7050 train_time:247046ms step_avg:223.37ms
step:1107/7050 train_time:247271ms step_avg:223.37ms
step:1108/7050 train_time:247495ms step_avg:223.37ms
step:1109/7050 train_time:247720ms step_avg:223.37ms
step:1110/7050 train_time:247945ms step_avg:223.37ms
step:1111/7050 train_time:248170ms step_avg:223.37ms
step:1112/7050 train_time:248394ms step_avg:223.38ms
step:1113/7050 train_time:248619ms step_avg:223.38ms
step:1114/7050 train_time:248844ms step_avg:223.38ms
step:1115/7050 train_time:249070ms step_avg:223.38ms
step:1116/7050 train_time:249295ms step_avg:223.38ms
step:1117/7050 train_time:249520ms step_avg:223.38ms
step:1118/7050 train_time:249745ms step_avg:223.39ms
step:1119/7050 train_time:249970ms step_avg:223.39ms
step:1120/7050 train_time:250194ms step_avg:223.39ms
step:1121/7050 train_time:250420ms step_avg:223.39ms
step:1122/7050 train_time:250644ms step_avg:223.39ms
step:1123/7050 train_time:250868ms step_avg:223.39ms
step:1124/7050 train_time:251093ms step_avg:223.39ms
step:1125/7050 train_time:251318ms step_avg:223.39ms
step:1125/7050 val_loss:3.4308 train_time:251494ms step_avg:223.55ms
step:1126/7050 train_time:251543ms step_avg:223.40ms
step:1127/7050 train_time:251767ms step_avg:223.40ms
step:1128/7050 train_time:251994ms step_avg:223.40ms
step:1129/7050 train_time:252220ms step_avg:223.40ms
step:1130/7050 train_time:252444ms step_avg:223.40ms
step:1131/7050 train_time:252668ms step_avg:223.40ms
step:1132/7050 train_time:252894ms step_avg:223.40ms
step:1133/7050 train_time:253122ms step_avg:223.41ms
step:1134/7050 train_time:253347ms step_avg:223.41ms
step:1135/7050 train_time:253570ms step_avg:223.41ms
step:1136/7050 train_time:253795ms step_avg:223.41ms
step:1137/7050 train_time:254022ms step_avg:223.41ms
step:1138/7050 train_time:254247ms step_avg:223.42ms
step:1139/7050 train_time:254471ms step_avg:223.42ms
step:1140/7050 train_time:254695ms step_avg:223.42ms
step:1141/7050 train_time:254923ms step_avg:223.42ms
step:1142/7050 train_time:255147ms step_avg:223.42ms
step:1143/7050 train_time:255371ms step_avg:223.42ms
step:1144/7050 train_time:255597ms step_avg:223.42ms
step:1145/7050 train_time:255822ms step_avg:223.43ms
step:1146/7050 train_time:256047ms step_avg:223.43ms
step:1147/7050 train_time:256272ms step_avg:223.43ms
step:1148/7050 train_time:256498ms step_avg:223.43ms
step:1149/7050 train_time:256723ms step_avg:223.43ms
step:1150/7050 train_time:256948ms step_avg:223.43ms
step:1151/7050 train_time:257175ms step_avg:223.44ms
step:1152/7050 train_time:257399ms step_avg:223.44ms
step:1153/7050 train_time:257624ms step_avg:223.44ms
step:1154/7050 train_time:257848ms step_avg:223.44ms
step:1155/7050 train_time:258073ms step_avg:223.44ms
step:1156/7050 train_time:258298ms step_avg:223.44ms
step:1157/7050 train_time:258523ms step_avg:223.44ms
step:1158/7050 train_time:258748ms step_avg:223.44ms
step:1159/7050 train_time:258973ms step_avg:223.44ms
step:1160/7050 train_time:259198ms step_avg:223.45ms
step:1161/7050 train_time:259422ms step_avg:223.45ms
step:1162/7050 train_time:259646ms step_avg:223.45ms
step:1163/7050 train_time:259870ms step_avg:223.45ms
step:1164/7050 train_time:260095ms step_avg:223.45ms
step:1165/7050 train_time:260320ms step_avg:223.45ms
step:1166/7050 train_time:260546ms step_avg:223.45ms
step:1167/7050 train_time:260771ms step_avg:223.45ms
step:1168/7050 train_time:260995ms step_avg:223.45ms
step:1169/7050 train_time:261223ms step_avg:223.46ms
step:1170/7050 train_time:261445ms step_avg:223.46ms
step:1171/7050 train_time:261670ms step_avg:223.46ms
step:1172/7050 train_time:261896ms step_avg:223.46ms
step:1173/7050 train_time:262123ms step_avg:223.46ms
step:1174/7050 train_time:262348ms step_avg:223.47ms
step:1175/7050 train_time:262573ms step_avg:223.47ms
step:1176/7050 train_time:262797ms step_avg:223.47ms
step:1177/7050 train_time:263023ms step_avg:223.47ms
step:1178/7050 train_time:263249ms step_avg:223.47ms
step:1179/7050 train_time:263477ms step_avg:223.47ms
step:1180/7050 train_time:263702ms step_avg:223.48ms
step:1181/7050 train_time:263927ms step_avg:223.48ms
step:1182/7050 train_time:264152ms step_avg:223.48ms
step:1183/7050 train_time:264376ms step_avg:223.48ms
step:1184/7050 train_time:264602ms step_avg:223.48ms
step:1185/7050 train_time:264828ms step_avg:223.48ms
step:1186/7050 train_time:265052ms step_avg:223.48ms
step:1187/7050 train_time:265277ms step_avg:223.49ms
step:1188/7050 train_time:265503ms step_avg:223.49ms
step:1189/7050 train_time:265729ms step_avg:223.49ms
step:1190/7050 train_time:265953ms step_avg:223.49ms
step:1191/7050 train_time:266176ms step_avg:223.49ms
step:1192/7050 train_time:266403ms step_avg:223.49ms
step:1193/7050 train_time:266628ms step_avg:223.49ms
step:1194/7050 train_time:266852ms step_avg:223.49ms
step:1195/7050 train_time:267078ms step_avg:223.50ms
step:1196/7050 train_time:267303ms step_avg:223.50ms
step:1197/7050 train_time:267531ms step_avg:223.50ms
step:1198/7050 train_time:267753ms step_avg:223.50ms
step:1199/7050 train_time:267978ms step_avg:223.50ms
step:1200/7050 train_time:268203ms step_avg:223.50ms
step:1201/7050 train_time:268429ms step_avg:223.50ms
step:1202/7050 train_time:268654ms step_avg:223.51ms
step:1203/7050 train_time:268880ms step_avg:223.51ms
step:1204/7050 train_time:269104ms step_avg:223.51ms
step:1205/7050 train_time:269329ms step_avg:223.51ms
step:1206/7050 train_time:269553ms step_avg:223.51ms
step:1207/7050 train_time:269779ms step_avg:223.51ms
step:1208/7050 train_time:270004ms step_avg:223.51ms
step:1209/7050 train_time:270229ms step_avg:223.51ms
step:1210/7050 train_time:270454ms step_avg:223.52ms
step:1211/7050 train_time:270679ms step_avg:223.52ms
step:1212/7050 train_time:270904ms step_avg:223.52ms
step:1213/7050 train_time:271128ms step_avg:223.52ms
step:1214/7050 train_time:271352ms step_avg:223.52ms
step:1215/7050 train_time:271579ms step_avg:223.52ms
step:1216/7050 train_time:271804ms step_avg:223.52ms
step:1217/7050 train_time:272028ms step_avg:223.52ms
step:1218/7050 train_time:272253ms step_avg:223.52ms
step:1219/7050 train_time:272478ms step_avg:223.53ms
step:1220/7050 train_time:272705ms step_avg:223.53ms
step:1221/7050 train_time:272929ms step_avg:223.53ms
step:1222/7050 train_time:273153ms step_avg:223.53ms
step:1223/7050 train_time:273378ms step_avg:223.53ms
step:1224/7050 train_time:273602ms step_avg:223.53ms
step:1225/7050 train_time:273828ms step_avg:223.53ms
step:1226/7050 train_time:274054ms step_avg:223.53ms
step:1227/7050 train_time:274277ms step_avg:223.54ms
step:1228/7050 train_time:274502ms step_avg:223.54ms
step:1229/7050 train_time:274727ms step_avg:223.54ms
step:1230/7050 train_time:274951ms step_avg:223.54ms
step:1231/7050 train_time:275176ms step_avg:223.54ms
step:1232/7050 train_time:275400ms step_avg:223.54ms
step:1233/7050 train_time:275624ms step_avg:223.54ms
step:1234/7050 train_time:275850ms step_avg:223.54ms
step:1235/7050 train_time:276075ms step_avg:223.54ms
step:1236/7050 train_time:276298ms step_avg:223.54ms
step:1237/7050 train_time:276522ms step_avg:223.54ms
step:1238/7050 train_time:276747ms step_avg:223.54ms
step:1239/7050 train_time:276972ms step_avg:223.54ms
step:1240/7050 train_time:277198ms step_avg:223.55ms
step:1241/7050 train_time:277422ms step_avg:223.55ms
step:1242/7050 train_time:277647ms step_avg:223.55ms
step:1243/7050 train_time:277871ms step_avg:223.55ms
step:1244/7050 train_time:278095ms step_avg:223.55ms
step:1245/7050 train_time:278322ms step_avg:223.55ms
step:1246/7050 train_time:278548ms step_avg:223.55ms
step:1247/7050 train_time:278771ms step_avg:223.55ms
step:1248/7050 train_time:278996ms step_avg:223.55ms
step:1249/7050 train_time:279221ms step_avg:223.56ms
step:1250/7050 train_time:279447ms step_avg:223.56ms
step:1250/7050 val_loss:3.4002 train_time:279623ms step_avg:223.70ms
step:1251/7050 train_time:279672ms step_avg:223.56ms
step:1252/7050 train_time:279896ms step_avg:223.56ms
step:1253/7050 train_time:280124ms step_avg:223.56ms
step:1254/7050 train_time:280349ms step_avg:223.56ms
step:1255/7050 train_time:280574ms step_avg:223.56ms
step:1256/7050 train_time:280799ms step_avg:223.57ms
step:1257/7050 train_time:281024ms step_avg:223.57ms
step:1258/7050 train_time:281251ms step_avg:223.57ms
step:1259/7050 train_time:281477ms step_avg:223.57ms
step:1260/7050 train_time:281701ms step_avg:223.57ms
step:1261/7050 train_time:281926ms step_avg:223.57ms
step:1262/7050 train_time:282151ms step_avg:223.57ms
step:1263/7050 train_time:282376ms step_avg:223.58ms
step:1264/7050 train_time:282602ms step_avg:223.58ms
step:1265/7050 train_time:282826ms step_avg:223.58ms
step:1266/7050 train_time:283051ms step_avg:223.58ms
step:1267/7050 train_time:283277ms step_avg:223.58ms
step:1268/7050 train_time:283499ms step_avg:223.58ms
step:1269/7050 train_time:283724ms step_avg:223.58ms
step:1270/7050 train_time:283949ms step_avg:223.58ms
step:1271/7050 train_time:284174ms step_avg:223.58ms
step:1272/7050 train_time:284399ms step_avg:223.58ms
step:1273/7050 train_time:284624ms step_avg:223.59ms
step:1274/7050 train_time:284847ms step_avg:223.58ms
step:1275/7050 train_time:285073ms step_avg:223.59ms
step:1276/7050 train_time:285300ms step_avg:223.59ms
step:1277/7050 train_time:285524ms step_avg:223.59ms
step:1278/7050 train_time:285748ms step_avg:223.59ms
step:1279/7050 train_time:285972ms step_avg:223.59ms
step:1280/7050 train_time:286198ms step_avg:223.59ms
step:1281/7050 train_time:286426ms step_avg:223.60ms
step:1282/7050 train_time:286651ms step_avg:223.60ms
step:1283/7050 train_time:286875ms step_avg:223.60ms
step:1284/7050 train_time:287100ms step_avg:223.60ms
step:1285/7050 train_time:287325ms step_avg:223.60ms
step:1286/7050 train_time:287550ms step_avg:223.60ms
step:1287/7050 train_time:287775ms step_avg:223.60ms
step:1288/7050 train_time:287999ms step_avg:223.60ms
step:1289/7050 train_time:288225ms step_avg:223.60ms
step:1290/7050 train_time:288450ms step_avg:223.60ms
step:1291/7050 train_time:288675ms step_avg:223.61ms
step:1292/7050 train_time:288900ms step_avg:223.61ms
step:1293/7050 train_time:289125ms step_avg:223.61ms
step:1294/7050 train_time:289350ms step_avg:223.61ms
step:1295/7050 train_time:289575ms step_avg:223.61ms
step:1296/7050 train_time:289801ms step_avg:223.61ms
step:1297/7050 train_time:290025ms step_avg:223.61ms
step:1298/7050 train_time:290251ms step_avg:223.61ms
step:1299/7050 train_time:290476ms step_avg:223.62ms
step:1300/7050 train_time:290700ms step_avg:223.62ms
step:1301/7050 train_time:290925ms step_avg:223.62ms
step:1302/7050 train_time:291151ms step_avg:223.62ms
step:1303/7050 train_time:291376ms step_avg:223.62ms
step:1304/7050 train_time:291602ms step_avg:223.62ms
step:1305/7050 train_time:291827ms step_avg:223.62ms
step:1306/7050 train_time:292051ms step_avg:223.62ms
step:1307/7050 train_time:292277ms step_avg:223.62ms
step:1308/7050 train_time:292502ms step_avg:223.63ms
step:1309/7050 train_time:292727ms step_avg:223.63ms
step:1310/7050 train_time:292953ms step_avg:223.63ms
step:1311/7050 train_time:293178ms step_avg:223.63ms
step:1312/7050 train_time:293404ms step_avg:223.63ms
step:1313/7050 train_time:293627ms step_avg:223.63ms
step:1314/7050 train_time:293851ms step_avg:223.63ms
step:1315/7050 train_time:294076ms step_avg:223.63ms
step:1316/7050 train_time:294300ms step_avg:223.63ms
step:1317/7050 train_time:294526ms step_avg:223.63ms
step:1318/7050 train_time:294749ms step_avg:223.63ms
step:1319/7050 train_time:294975ms step_avg:223.64ms
step:1320/7050 train_time:295201ms step_avg:223.64ms
step:1321/7050 train_time:295427ms step_avg:223.64ms
step:1322/7050 train_time:295651ms step_avg:223.64ms
step:1323/7050 train_time:295876ms step_avg:223.64ms
step:1324/7050 train_time:296102ms step_avg:223.64ms
step:1325/7050 train_time:296326ms step_avg:223.64ms
step:1326/7050 train_time:296552ms step_avg:223.64ms
step:1327/7050 train_time:296776ms step_avg:223.64ms
step:1328/7050 train_time:297001ms step_avg:223.65ms
step:1329/7050 train_time:297227ms step_avg:223.65ms
step:1330/7050 train_time:297452ms step_avg:223.65ms
step:1331/7050 train_time:297676ms step_avg:223.65ms
step:1332/7050 train_time:297900ms step_avg:223.65ms
step:1333/7050 train_time:298125ms step_avg:223.65ms
step:1334/7050 train_time:298349ms step_avg:223.65ms
step:1335/7050 train_time:298575ms step_avg:223.65ms
step:1336/7050 train_time:298800ms step_avg:223.65ms
step:1337/7050 train_time:299025ms step_avg:223.65ms
step:1338/7050 train_time:299251ms step_avg:223.66ms
step:1339/7050 train_time:299475ms step_avg:223.66ms
step:1340/7050 train_time:299700ms step_avg:223.66ms
step:1341/7050 train_time:299925ms step_avg:223.66ms
step:1342/7050 train_time:300151ms step_avg:223.66ms
step:1343/7050 train_time:300376ms step_avg:223.66ms
step:1344/7050 train_time:300602ms step_avg:223.66ms
step:1345/7050 train_time:300827ms step_avg:223.66ms
step:1346/7050 train_time:301050ms step_avg:223.66ms
step:1347/7050 train_time:301276ms step_avg:223.66ms
step:1348/7050 train_time:301501ms step_avg:223.67ms
step:1349/7050 train_time:301726ms step_avg:223.67ms
step:1350/7050 train_time:301951ms step_avg:223.67ms
step:1351/7050 train_time:302176ms step_avg:223.67ms
step:1352/7050 train_time:302400ms step_avg:223.67ms
step:1353/7050 train_time:302626ms step_avg:223.67ms
step:1354/7050 train_time:302851ms step_avg:223.67ms
step:1355/7050 train_time:303077ms step_avg:223.67ms
step:1356/7050 train_time:303302ms step_avg:223.67ms
step:1357/7050 train_time:303527ms step_avg:223.68ms
step:1358/7050 train_time:303752ms step_avg:223.68ms
step:1359/7050 train_time:303977ms step_avg:223.68ms
step:1360/7050 train_time:304201ms step_avg:223.68ms
step:1361/7050 train_time:304427ms step_avg:223.68ms
step:1362/7050 train_time:304651ms step_avg:223.68ms
step:1363/7050 train_time:304877ms step_avg:223.68ms
step:1364/7050 train_time:305102ms step_avg:223.68ms
step:1365/7050 train_time:305328ms step_avg:223.68ms
step:1366/7050 train_time:305552ms step_avg:223.68ms
step:1367/7050 train_time:305779ms step_avg:223.69ms
step:1368/7050 train_time:306005ms step_avg:223.69ms
step:1369/7050 train_time:306229ms step_avg:223.69ms
step:1370/7050 train_time:306453ms step_avg:223.69ms
step:1371/7050 train_time:306678ms step_avg:223.69ms
step:1372/7050 train_time:306902ms step_avg:223.69ms
step:1373/7050 train_time:307126ms step_avg:223.69ms
step:1374/7050 train_time:307350ms step_avg:223.69ms
step:1375/7050 train_time:307576ms step_avg:223.69ms
step:1375/7050 val_loss:3.3770 train_time:307754ms step_avg:223.82ms
step:1376/7050 train_time:307803ms step_avg:223.69ms
step:1377/7050 train_time:308025ms step_avg:223.69ms
step:1378/7050 train_time:308254ms step_avg:223.70ms
step:1379/7050 train_time:308481ms step_avg:223.70ms
step:1380/7050 train_time:308705ms step_avg:223.70ms
step:1381/7050 train_time:308930ms step_avg:223.70ms
step:1382/7050 train_time:309156ms step_avg:223.70ms
step:1383/7050 train_time:309383ms step_avg:223.70ms
step:1384/7050 train_time:309607ms step_avg:223.70ms
step:1385/7050 train_time:309831ms step_avg:223.71ms
step:1386/7050 train_time:310056ms step_avg:223.71ms
step:1387/7050 train_time:310281ms step_avg:223.71ms
step:1388/7050 train_time:310507ms step_avg:223.71ms
step:1389/7050 train_time:310731ms step_avg:223.71ms
step:1390/7050 train_time:310955ms step_avg:223.71ms
step:1391/7050 train_time:311182ms step_avg:223.71ms
step:1392/7050 train_time:311408ms step_avg:223.71ms
step:1393/7050 train_time:311632ms step_avg:223.71ms
step:1394/7050 train_time:311856ms step_avg:223.71ms
step:1395/7050 train_time:312081ms step_avg:223.71ms
step:1396/7050 train_time:312306ms step_avg:223.72ms
step:1397/7050 train_time:312532ms step_avg:223.72ms
step:1398/7050 train_time:312756ms step_avg:223.72ms
step:1399/7050 train_time:312980ms step_avg:223.72ms
step:1400/7050 train_time:313205ms step_avg:223.72ms
step:1401/7050 train_time:313429ms step_avg:223.72ms
step:1402/7050 train_time:313655ms step_avg:223.72ms
step:1403/7050 train_time:313881ms step_avg:223.72ms
step:1404/7050 train_time:314107ms step_avg:223.72ms
step:1405/7050 train_time:314331ms step_avg:223.72ms
step:1406/7050 train_time:314556ms step_avg:223.72ms
step:1407/7050 train_time:314781ms step_avg:223.73ms
step:1408/7050 train_time:315007ms step_avg:223.73ms
step:1409/7050 train_time:315234ms step_avg:223.73ms
step:1410/7050 train_time:315458ms step_avg:223.73ms
step:1411/7050 train_time:315684ms step_avg:223.73ms
step:1412/7050 train_time:315909ms step_avg:223.73ms
step:1413/7050 train_time:316134ms step_avg:223.73ms
step:1414/7050 train_time:316358ms step_avg:223.73ms
step:1415/7050 train_time:316582ms step_avg:223.73ms
step:1416/7050 train_time:316807ms step_avg:223.73ms
step:1417/7050 train_time:317032ms step_avg:223.73ms
step:1418/7050 train_time:317258ms step_avg:223.74ms
step:1419/7050 train_time:317483ms step_avg:223.74ms
step:1420/7050 train_time:317708ms step_avg:223.74ms
step:1421/7050 train_time:317933ms step_avg:223.74ms
step:1422/7050 train_time:318159ms step_avg:223.74ms
step:1423/7050 train_time:318384ms step_avg:223.74ms
step:1424/7050 train_time:318609ms step_avg:223.74ms
step:1425/7050 train_time:318834ms step_avg:223.74ms
step:1426/7050 train_time:319058ms step_avg:223.74ms
