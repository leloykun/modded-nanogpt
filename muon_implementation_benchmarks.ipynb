{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.benchmark as benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_in_us(f, *args, **kwargs):\n",
    "    t0 = benchmark.Timer(\n",
    "        stmt=\"f(*args, **kwargs)\", globals={\"args\": args, \"kwargs\": kwargs, \"f\": f}\n",
    "    )\n",
    "    return t0.blocked_autorange().mean * 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = [\n",
    "    *[torch.rand(768, 4*768, dtype=torch.float32, device=\"cuda\") for _ in range(12)],\n",
    "    *[torch.rand(4*768, 768, dtype=torch.float32, device=\"cuda\") for _ in range(12)],\n",
    "    *[torch.rand(768, 768, dtype=torch.float32, device=\"cuda\") for _ in range(4*12)],\n",
    "]\n",
    "\n",
    "s1 = torch.cuda.Stream()\n",
    "s2 = torch.cuda.Stream()\n",
    "s3 = torch.cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeropower_via_newtonschulz5(G: torch.Tensor, n: int) -> torch.Tensor:\n",
    "    ...\n",
    "\n",
    "\n",
    "def single(gs: list[torch.Tensor], n: int):\n",
    "    g = gs[0]\n",
    "    zeropower_via_newtonschulz5(g, n)\n",
    "\n",
    "\n",
    "def multiple(gs: list[torch.Tensor], n: int):\n",
    "    for g in gs:\n",
    "        zeropower_via_newtonschulz5(g, n)\n",
    "\n",
    "\n",
    "def multiplexed_2_streams(gs: list[torch.Tensor], n: int):\n",
    "    with torch.cuda.stream(s1):\n",
    "        for g in gs[:9]:\n",
    "            zeropower_via_newtonschulz5(g, n)\n",
    "    with torch.cuda.stream(s2):\n",
    "        for g in gs[9:]:\n",
    "            zeropower_via_newtonschulz5(g, n)\n",
    "\n",
    "\n",
    "def multiplexed_3_streams(gs: list[torch.Tensor], n: int):\n",
    "    with torch.cuda.stream(s1):\n",
    "        for g in gs[:6]:\n",
    "            zeropower_via_newtonschulz5(g, n)\n",
    "    with torch.cuda.stream(s2):\n",
    "        for g in gs[6:12]:\n",
    "            zeropower_via_newtonschulz5(g, n)\n",
    "    with torch.cuda.stream(s3):\n",
    "        for g in gs[12:]:\n",
    "            zeropower_via_newtonschulz5(g, n)\n",
    "\n",
    "\n",
    "def run_benchmarks():\n",
    "    benchmark_in_us(single, gs, 4)\n",
    "    benchmark_in_us(single, gs, 5)\n",
    "\n",
    "    runtime_single_4_steps = benchmark_in_us(single, gs, 4)\n",
    "    runtime_single_5_steps = benchmark_in_us(single, gs, 5)\n",
    "    runtime_multiple_4_steps = benchmark_in_us(multiple, gs, 4) / len(gs)\n",
    "    runtime_multiple_5_steps = benchmark_in_us(multiple, gs, 5) / len(gs)\n",
    "    runtime_mux_2_streams_4_steps = benchmark_in_us(multiplexed_2_streams, gs, 4) / len(gs)\n",
    "    runtime_mux_2_streams_5_steps = benchmark_in_us(multiplexed_2_streams, gs, 5) / len(gs)\n",
    "    runtime_mux_3_streams_4_steps = benchmark_in_us(multiplexed_3_streams, gs, 4) / len(gs)\n",
    "    runtime_mux_3_streams_5_steps = benchmark_in_us(multiplexed_3_streams, gs, 5) / len(gs)\n",
    "    print(f\"Single, 4 steps: {runtime_single_4_steps} ms\")\n",
    "    print(f\"Single, 5 steps: {runtime_single_5_steps} ms\")\n",
    "    print(f\"Multiple, 4 steps: {runtime_multiple_4_steps} ms\")\n",
    "    print(f\"Multiple, 5 steps: {runtime_multiple_5_steps} ms\")\n",
    "    print(f\"Multiplexed 2 streams, 4 steps: {runtime_mux_2_streams_4_steps} ms\")\n",
    "    print(f\"Multiplexed 2 streams, 5 steps: {runtime_mux_2_streams_5_steps} ms\")\n",
    "    print(f\"Multiplexed 3 streams, 4 steps: {runtime_mux_3_streams_4_steps} ms\")\n",
    "    print(f\"Multiplexed 3 streams, 5 steps: {runtime_mux_3_streams_5_steps} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile\n",
    "def zeropower_via_newtonschulz5(G: torch.Tensor, n: int):\n",
    "    assert len(G.shape) == 2\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    X = G.bfloat16()\n",
    "    X.div_(X.norm() + 1e-7)\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    for _ in range(n):\n",
    "        A = X @ X.T\n",
    "        B = A @ X\n",
    "        X = a * X + b * B + c * A @ B\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "\n",
    "print(\"Original - naive compile\")\n",
    "run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leloykun/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1604: UserWarning: NVIDIA GeForce RTX 2060 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single, 4 steps: 17.88294709995171 ms\n",
      "Single, 5 steps: 22.460382799999934 ms\n",
      "Multiple, 4 steps: 35.80253222222988 ms\n",
      "Multiple, 5 steps: 44.54966661109615 ms\n",
      "Multiplexed 2 streams, 4 steps: 35.27657105557106 ms\n",
      "Multiplexed 2 streams, 5 steps: 43.89762205553578 ms\n",
      "Multiplexed 3 streams, 4 steps: 35.296148777787394 ms\n",
      "Multiplexed 3 streams, 5 steps: 43.86552161112276 ms\n"
     ]
    }
   ],
   "source": [
    "# reduce-overhead is slower\n",
    "# @torch.compile\n",
    "@torch.compile(mode=\"max-autotune-no-cudagraphs\")\n",
    "def zeropower_via_newtonschulz5(G: torch.Tensor, n: int):\n",
    "    assert len(G.shape) == 2\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    X = G.bfloat16()\n",
    "    X.div_(X.norm() + 1e-7)\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    for _ in range(n):\n",
    "        A = X @ X.T\n",
    "        B = A @ X\n",
    "        X = a * X + b * B + c * A @ B\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "\n",
    "print(\"Original\")\n",
    "run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leloykun/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1604: UserWarning: NVIDIA GeForce RTX 2060 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single, 4 steps: 13330.02134997514 us\n",
      "Single, 5 steps: 16657.34174998761 us\n",
      "Multiple, 4 steps: 28866.108499995687 us\n",
      "Multiple, 5 steps: 36011.61483331655 us\n",
      "Multiplexed 2 streams, 4 steps: 28352.225111120788 us\n",
      "Multiplexed 2 streams, 5 steps: 35578.03144440186 us\n",
      "Multiplexed 3 streams, 4 steps: 28533.46405557507 us\n",
      "Multiplexed 3 streams, 5 steps: 35852.94805558684 us\n"
     ]
    }
   ],
   "source": [
    "# reduce-overhead is slower\n",
    "# @torch.compile\n",
    "@torch.compile(mode=\"max-autotune-no-cudagraphs\")\n",
    "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int):\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    n, m = G.size()\n",
    "    X = G.bfloat16()\n",
    "    I = torch.eye(min(n, m), dtype=X.dtype, device=X.device)\n",
    "    X.div_(X.norm() + 1e-7)\n",
    "    if n > m:\n",
    "        X = X.T\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        X = a * X + (b * A + c * A @ A) @ X\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "\n",
    "print(\"X = a * X + (b * A + c * A @ A) @ X\")\n",
    "run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leloykun/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1604: UserWarning: NVIDIA GeForce RTX 2060 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single, 4 steps: 12.982452750020457 ms\n",
      "Single, 5 steps: 16.230022550053036 ms\n",
      "Multiple, 4 steps: 29.205880777782212 ms\n",
      "Multiple, 5 steps: 36.75411722224453 ms\n",
      "Multiplexed 2 streams, 4 steps: 28.635804111066438 ms\n",
      "Multiplexed 2 streams, 5 steps: 35.807331499957705 ms\n",
      "Multiplexed 3 streams, 4 steps: 28.552707444456853 ms\n",
      "Multiplexed 3 streams, 5 steps: 35.63938483335328 ms\n"
     ]
    }
   ],
   "source": [
    "# reduce-overhead is slower\n",
    "# @torch.compile\n",
    "@torch.compile(mode=\"max-autotune-no-cudagraphs\")\n",
    "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int):\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    n, m = G.size()\n",
    "    X = G.bfloat16()\n",
    "    I = torch.eye(min(n, m), dtype=X.dtype, device=X.device)\n",
    "    X.div_(X.norm() + 1e-7)\n",
    "    if n > m:\n",
    "        X = X.T\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        X = (a * I + A @ (b * I + c * A)) @ X\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "\n",
    "print(\"X = (a * I + A @ (b * I + c * A)) @ X\")\n",
    "run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leloykun/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_inductor/lowering.py:1532: FutureWarning: `torch._prims_common.check` is deprecated and will be removed in the future. Please use `torch._check*` functions instead.\n",
      "  check(\n",
      "/home/leloykun/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1604: UserWarning: NVIDIA GeForce RTX 2060 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single, 4 steps: 13.24205756000083 ms\n",
      "Single, 5 steps: 16.41230694999649 ms\n",
      "Multiple, 4 steps: 29.21538677780215 ms\n",
      "Multiple, 5 steps: 37.094243277756 ms\n",
      "Multiplexed 2 streams, 4 steps: 28.628497833324218 ms\n",
      "Multiplexed 2 streams, 5 steps: 35.851054277777116 ms\n",
      "Multiplexed 3 streams, 4 steps: 28.66707005558864 ms\n",
      "Multiplexed 3 streams, 5 steps: 35.92424205554481 ms\n"
     ]
    }
   ],
   "source": [
    "# reduce-overhead is slower\n",
    "# @torch.compile\n",
    "@torch.compile(mode=\"max-autotune-no-cudagraphs\")\n",
    "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int):\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    n, m = G.size()\n",
    "    X = G.bfloat16()\n",
    "    I = torch.eye(min(n, m), dtype=X.dtype, device=X.device)\n",
    "    X.div_(X.norm() + 1e-7)\n",
    "    if n > m:\n",
    "        X = X.T\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        S = A @ (b * I + c * A)\n",
    "        torch.diagonal(S).add_(a)\n",
    "        X = S @ X\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "\n",
    "print(\"w/ S\")\n",
    "run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leloykun/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1604: UserWarning: NVIDIA GeForce RTX 2060 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single, 4 steps: 18210.466850041485 us\n",
      "Single, 5 steps: 18138.601850023402 us\n",
      "Multiple, 4 steps: 35475.30911110799 us\n",
      "Multiple, 5 steps: 36041.44805553409 us\n",
      "Multiplexed 2 streams, 4 steps: 35088.7146666739 us\n",
      "Multiplexed 2 streams, 5 steps: 35514.98688890812 us\n",
      "Multiplexed 3 streams, 4 steps: 35314.020166677234 us\n",
      "Multiplexed 3 streams, 5 steps: 35532.948000006094 us\n"
     ]
    }
   ],
   "source": [
    "# reduce-overhead is slower\n",
    "# @torch.compile\n",
    "@torch.compile(mode=\"max-autotune-no-cudagraphs\")\n",
    "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int):\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    n, m = G.size()\n",
    "    X = G.bfloat16()\n",
    "    I = torch.eye(min(n, m), dtype=X.dtype, device=X.device)\n",
    "    X.div_(X.norm() + 1e-7)\n",
    "    if n > m:\n",
    "        X = X.T\n",
    "    for a, b, c in (\n",
    "        (4.8969, -14.0610, 10.1415),\n",
    "        (4.7285, -10.0664, 5.4487),\n",
    "        (4.0968, -5.9557, 2.3200),\n",
    "        (3.0319, -3.3993, 1.1814),\n",
    "    ):\n",
    "        A = X @ X.T\n",
    "        B = A @ X\n",
    "        X = a * X + b * B + c * A @ B\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "\n",
    "print(\"Original - 4-step\")\n",
    "run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leloykun/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_inductor/compile_fx.py:1604: UserWarning: NVIDIA GeForce RTX 2060 does not support bfloat16 compilation natively, skipping\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single, 4 steps: 13137.565900024128 us\n",
      "Single, 5 steps: 13093.40089997022 us\n",
      "Multiple, 4 steps: 13150.539777775015 us\n",
      "Multiple, 5 steps: 13062.95088887863 us\n",
      "Multiplexed 2 streams, 4 steps: 12591.63422221516 us\n",
      "Multiplexed 2 streams, 5 steps: 12495.095277775667 us\n",
      "Multiplexed 3 streams, 4 steps: 12366.734222218332 us\n",
      "Multiplexed 3 streams, 5 steps: 12408.445333322663 us\n"
     ]
    }
   ],
   "source": [
    "# reduce-overhead is slower\n",
    "# @torch.compile\n",
    "@torch.compile(mode=\"max-autotune-no-cudagraphs\")\n",
    "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int):\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    n, m = G.size()\n",
    "    X = G.bfloat16()\n",
    "    I = torch.eye(min(n, m), dtype=X.dtype, device=X.device)\n",
    "    X.div_(X.norm() + 1e-7)\n",
    "    if n > m:\n",
    "        X = X.T\n",
    "    for a, b, c in (\n",
    "        (4.8969, -14.0610, 10.1415),\n",
    "        (4.7285, -10.0664, 5.4487),\n",
    "        (4.0968, -5.9557, 2.3200),\n",
    "        (3.0319, -3.3993, 1.1814),\n",
    "    ):\n",
    "        A = X @ X.T\n",
    "        X = (a * I + b * A @ (I + c/b * A)) @ X\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "\n",
    "print(\"X = (a * I + b * A @ (I + c/b * A)) @ X - 4-step\")\n",
    "run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce-overhead is slower\n",
    "# @torch.compile\n",
    "@torch.compile(mode=\"max-autotune-no-cudagraphs\")\n",
    "def zeropower_via_newtonschulz5(G: torch.Tensor, steps: int):\n",
    "    n, m = G.size()\n",
    "    X = G.bfloat16()\n",
    "    I = torch.eye(min(n, m), dtype=X.dtype, device=X.device)\n",
    "    X.div_(X.norm() + 1e-7)\n",
    "    if n > m:\n",
    "        X = X.T\n",
    "    for a, b, c in (\n",
    "        (4.8969, -14.0610, 10.1415),\n",
    "        (4.7285, -10.0664, 5.4487),\n",
    "        (4.0968, -5.9557, 2.3200),\n",
    "        (3.0319, -3.3993, 1.1814),\n",
    "    ):\n",
    "        A = X @ X.T\n",
    "        S = A @ (b * I + c * A)\n",
    "        torch.diagonal(S).add_(a)\n",
    "        X = S @ X\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "\n",
    "print(\"w/ S - 4-step\")\n",
    "run_benchmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeropower_via_svd(G, steps=None, eps=1e-7):\n",
    "    U, _, V = G.svd()\n",
    "    return U @ V.T\n",
    "\n",
    "\n",
    "@torch.compile\n",
    "def zeropower_via_newtonschulz5(G, steps=10, eps=1e-7, coeffs=(3.4445, -4.7750,  2.0315)):\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' \\sim Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert len(G.shape) == 2\n",
    "    a, b, c = coeffs\n",
    "    X = G.bfloat16()\n",
    "    X /= (X.norm() + eps) # ensure top singular value <= 1\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        B = A @ X\n",
    "        X = a * X + b * B + c * A @ B\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X\n",
    "\n",
    "zeropower_backends = dict(svd=zeropower_via_svd, newtonschulz5=zeropower_via_newtonschulz5)\n",
    "\n",
    "class Muon(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Muon - MomentUm Orthogonalized by Newton-schulz\n",
    "\n",
    "    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-\n",
    "    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal\n",
    "    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has\n",
    "    the advantage that it can be stably run in bfloat16 on the GPU.\n",
    "\n",
    "    Some warnings:\n",
    "    - This optimizer assumes that all parameters passed in are 2D.\n",
    "    - It should not be used for the embedding layer, the final fully connected layer, or any {0,1}-D\n",
    "    parameters; those should all be optimized by a standard method (e.g., AdamW).\n",
    "    - To use it with 4D convolutional filters, it works well to just flatten their last 3 dimensions.\n",
    "    - We believe it is unlikely to work well for training with small batch size.\n",
    "    - We believe it may not work well for finetuning pretrained models, but we haven't tested this.\n",
    "    - We have not yet tried this optimizer for training scenarios larger than NanoGPT (124M).\n",
    "\n",
    "    Arguments:\n",
    "        lr: The learning rate used by the internal SGD.\n",
    "        momentum: The momentum used by the internal SGD.\n",
    "        nesterov: Whether to use Nesterov-style momentum in the internal SGD. (recommended)\n",
    "        backend: The chosen backend for the orthogonalization step. (recommended: 'newtonschulz5')\n",
    "        backend_steps: The number of iteration steps to use in the backend, if it is iterative.\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=3e-4, momentum=0.95, nesterov=True,\n",
    "                 backend='newtonschulz5', backend_steps=5, backend_coeffs=(3.4445, -4.7750,  2.0315),\n",
    "                 rank=0, world_size=1):\n",
    "        defaults = dict(lr=lr, momentum=momentum, nesterov=nesterov, backend=backend, backend_steps=backend_steps, backend_coeffs=backend_coeffs)\n",
    "        super().__init__(params, defaults)\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "\n",
    "    @torch.compile\n",
    "    def step(self):\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            lr = group['lr']\n",
    "            momentum = group['momentum']\n",
    "            zeropower_backend = zeropower_backends[group['backend']]\n",
    "\n",
    "            # generate weight updates in distributed fashion\n",
    "            total_params = sum(p.numel() for p in group['params'])\n",
    "            updates_flat = torch.zeros(total_params, device='cuda', dtype=torch.bfloat16)\n",
    "            curr_idx = 0\n",
    "            for i, p in enumerate(group['params']):\n",
    "                # luckily this will perfectly distribute a transformer with multiple of 4 layers to 8 GPUs\n",
    "                if i % self.world_size == self.rank:\n",
    "                    g = p.grad\n",
    "                    if g is None:\n",
    "                        continue\n",
    "                    state = self.state[p]\n",
    "                    if 'momentum_buffer' not in state:\n",
    "                        state['momentum_buffer'] = torch.zeros_like(g)\n",
    "                    buf = state['momentum_buffer']\n",
    "                    buf.mul_(momentum).add_(g)\n",
    "                    if group['nesterov']:\n",
    "                        g = g.add(buf, alpha=momentum)\n",
    "                    g = zeropower_backend(g, steps=group['backend_steps'])\n",
    "                    g *= max(g.size(0), g.size(1))**0.5 # scale to have update.square().mean() == 1\n",
    "                    updates_flat[curr_idx:curr_idx+p.numel()] = g.flatten()\n",
    "                curr_idx += p.numel()\n",
    "\n",
    "            # sync updates across devices. we are not memory-constrained so can do this simple deserialization\n",
    "            dist.all_reduce(updates_flat, op=dist.ReduceOp.SUM)\n",
    "\n",
    "            # deserialize and apply updates\n",
    "            curr_idx = 0\n",
    "            for p in group['params']:\n",
    "                g = updates_flat[curr_idx:curr_idx+p.numel()].view_as(p.data).type_as(p.data)\n",
    "                p.data.add_(g, alpha=-lr)\n",
    "                curr_idx += p.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rotary(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim, base=10000):\n",
    "        super().__init__()\n",
    "        self.inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.seq_len_cached = None\n",
    "        self.cos_cached = None\n",
    "        self.sin_cached = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.shape[1]\n",
    "        if seq_len != self.seq_len_cached:\n",
    "            self.seq_len_cached = seq_len\n",
    "            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
    "            freqs = torch.outer(t, self.inv_freq).to(x.device)\n",
    "            self.cos_cached = freqs.cos().bfloat16()\n",
    "            self.sin_cached = freqs.sin().bfloat16()\n",
    "        return self.cos_cached[None, :, None, :], self.sin_cached[None, :, None, :]\n",
    "\n",
    "def apply_rotary_emb(x, cos, sin):\n",
    "    assert x.ndim == 4 # multihead attention\n",
    "    d = x.shape[3]//2\n",
    "    x1 = x[..., :d]\n",
    "    x2 = x[..., d:]\n",
    "    y1 = x1 * cos + x2 * sin\n",
    "    y2 = x1 * (-sin) + x2 * cos\n",
    "    return torch.cat([y1, y2], 3).type_as(x)\n",
    "\n",
    "class CausalSelfAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = self.n_embd // self.n_head\n",
    "        assert self.n_embd % self.n_head == 0\n",
    "        self.c_q = torch.nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.c_k = torch.nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.c_v = torch.nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        # output projection\n",
    "        self.c_proj = torch.nn.Linear(self.n_embd, self.n_embd, bias=False)\n",
    "        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n",
    "        self.rotary = Rotary(self.head_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        q = self.c_q(x).view(B, T, self.n_head, self.head_dim)\n",
    "        k = self.c_k(x).view(B, T, self.n_head, self.head_dim)\n",
    "        v = self.c_v(x).view(B, T, self.n_head, self.head_dim)\n",
    "        cos, sin = self.rotary(q)\n",
    "        q, k = F.rms_norm(q, (q.size(-1),)), F.rms_norm(k, (k.size(-1),)) # QK norm suggested by @Grad62304977\n",
    "        q, k = apply_rotary_emb(q, cos, sin), apply_rotary_emb(k, cos, sin)\n",
    "        y = F.scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), is_causal=True)\n",
    "        y = y.transpose(1, 2).contiguous().view_as(x) # re-assemble all head outputs side by side\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = torch.nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
    "        self.c_proj  = torch.nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
    "        self.c_proj.weight.data.zero_() # zero init suggested by @Grad62304977\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(F.rms_norm(x, (x.size(-1),)))\n",
    "        x = x + self.mlp(F.rms_norm(x, (x.size(-1),)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size : int = 50304\n",
    "    n_layer : int = 12\n",
    "    n_head : int = 6 # head dim 128 suggested by @Grad62304977\n",
    "    n_embd : int = 768\n",
    "    sequence_length : int = 1024\n",
    "\n",
    "class GPT(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = torch.nn.ModuleDict(dict(\n",
    "            wte = torch.nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            h = torch.nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "        ))\n",
    "        self.lm_head = torch.nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "    def forward(self, idx, targets=None, return_logits=True):\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        x = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = F.rms_norm(x, (x.size(-1),))\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            logits = logits.float() # use tf32/fp32 for logits\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            logits = logits.float() # use tf32/fp32 for logits\n",
    "            loss = None\n",
    "\n",
    "        # there are performance reasons why not returning logits is prudent, if not needed\n",
    "        if not return_logits:\n",
    "            logits = None\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vocab = 50304\n",
    "model = GPT(GPTConfig(vocab_size=num_vocab, n_layer=12, n_head=6, n_embd=768))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): GPT(\n",
       "    (transformer): ModuleDict(\n",
       "      (wte): Embedding(50304, 768)\n",
       "      (h): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (attn): CausalSelfAttention(\n",
       "            (c_q): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (c_k): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (c_v): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (c_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (rotary): Rotary()\n",
       "          )\n",
       "          (mlp): MLP(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=False)\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "model = torch.compile(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params: 123568128\n"
     ]
    }
   ],
   "source": [
    "model_param_size = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Num params: {model_param_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Muon(model.transformer.h.parameters(),           lr=0.02,  momentum=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(10).unsqueeze(0).expand(2, -1)\n",
    "y = torch.arange(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TorchRuntimeError",
     "evalue": "Failed running call_function <function cross_entropy at 0x7f507b23e700>(*(FakeTensor(..., device='cuda:0', size=(20, 50304), grad_fn=<ViewBackward0>), FakeTensor(..., size=(20,), dtype=torch.int64)), **{'ignore_index': -1}):\nUnhandled FakeTensor Device Propagation for aten.gather.default, found two different devices cuda:0, cpu\n\nfrom user code:\n   File \"/tmp/ipykernel_3468/802198899.py\", line 34, in forward\n    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTorchRuntimeError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m _, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:465\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    461\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    469\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    470\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1269\u001b[0m, in \u001b[0;36mCatchErrorsWrapper.__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1263\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[1;32m   1264\u001b[0m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks, frame_state\n\u001b[1;32m   1265\u001b[0m             )\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m-> 1269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:1064\u001b[0m, in \u001b[0;36mConvertFrame.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m   1062\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:526\u001b[0m, in \u001b[0;36mConvertFrameAssert.__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    510\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    512\u001b[0m signpost_event(\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m     },\n\u001b[1;32m    524\u001b[0m )\n\u001b[0;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_torchdynamo_orig_callable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_one_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_export_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:924\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    922\u001b[0m guarded_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 924\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:666\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_compile.compile_inner\u001b[39m\u001b[38;5;124m\"\u001b[39m, phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentire_frame_compile\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    665\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CompileTimeInstructionCounter\u001b[38;5;241m.\u001b[39mrecord():\n\u001b[0;32m--> 666\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_utils_internal.py:87\u001b[0m, in \u001b[0;36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler\u001b[38;5;241m.\u001b[39mprofile_compile_time(\n\u001b[1;32m     90\u001b[0m     function, phase_name, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m     91\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:699\u001b[0m, in \u001b[0;36m_compile.<locals>._compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    697\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 699\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py:1322\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1319\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1320\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1322\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:219\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m exit_stack\u001b[38;5;241m.\u001b[39menter_context(\n\u001b[1;32m    216\u001b[0m     torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39m_symbolic_trace\u001b[38;5;241m.\u001b[39m_maybe_revert_all_patches()\n\u001b[1;32m    217\u001b[0m )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py:634\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 634\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    636\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2796\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2796\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:983\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 983\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:895\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_block_stack(inst)\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopcode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mObservedException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:582\u001b[0m, in \u001b[0;36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation\u001b[38;5;241m.\u001b[39mreason)\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 582\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneric_context_manager_depth \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    585\u001b[0m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[1;32m    586\u001b[0m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2279\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.CALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2277\u001b[0m \u001b[38;5;129m@break_graph_if_unsupported\u001b[39m(push\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mCALL\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[0;32m-> 2279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:2273\u001b[0m, in \u001b[0;36mInstructionTranslatorBase._call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   2268\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2271\u001b[0m     \u001b[38;5;66;03m# if call_function fails, need to set kw_names to None, otherwise\u001b[39;00m\n\u001b[1;32m   2272\u001b[0m     \u001b[38;5;66;03m# a subsequent call may have self.kw_names set to an old value\u001b[39;00m\n\u001b[0;32m-> 2273\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkw_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py:830\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.call_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 830\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpush(\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/variables/torch.py:897\u001b[0m, in \u001b[0;36mTorchInGraphFunctionVariable.call_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m], variables\u001b[38;5;241m.\u001b[39mTensorVariable):\n\u001b[1;32m    889\u001b[0m                 \u001b[38;5;66;03m# Calling fake tensor propagation can mutate the out= tensor in\u001b[39;00m\n\u001b[1;32m    890\u001b[0m                 \u001b[38;5;66;03m# tx.output.tracked_fakes. tracked_fakes are used to apply\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    893\u001b[0m                 \u001b[38;5;66;03m# guards. So save the shape now, and check later if it has\u001b[39;00m\n\u001b[1;32m    894\u001b[0m                 \u001b[38;5;66;03m# changed. If it has, graph break.\u001b[39;00m\n\u001b[1;32m    895\u001b[0m                 fake_out_shape \u001b[38;5;241m=\u001b[39m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mmeta[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_value\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 897\u001b[0m             tensor_variable \u001b[38;5;241m=\u001b[39m \u001b[43mwrap_fx_proxy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m                \u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_proxy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcall_function\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfn_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mproxy_args_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    907\u001b[0m                 \u001b[38;5;28misinstance\u001b[39m(tensor_variable, TensorVariable)\n\u001b[1;32m    908\u001b[0m                 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[1;32m    909\u001b[0m                 \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mas_python_constant()\n\u001b[1;32m    910\u001b[0m             ):\n\u001b[1;32m    911\u001b[0m                 unimplemented(\n\u001b[1;32m    912\u001b[0m \u001b[38;5;250m                    \u001b[39m\u001b[38;5;124;03m\"\"\"factory functions that return tensors that require grad are not supported.\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03mEither create the tensor outside the compiled region, or do not set the tensor to require_grad\"\"\"\u001b[39;00m\n\u001b[1;32m    914\u001b[0m                 )\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py:2037\u001b[0m, in \u001b[0;36mwrap_fx_proxy\u001b[0;34m(tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   2029\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2030\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtx\u001b[39m\u001b[38;5;124m\"\u001b[39m: tx,\n\u001b[1;32m   2031\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproxy\u001b[39m\u001b[38;5;124m\"\u001b[39m: proxy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2034\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions,\n\u001b[1;32m   2035\u001b[0m }\n\u001b[1;32m   2036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subclass_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrap_fx_proxy_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTensorVariable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2039\u001b[0m     result \u001b[38;5;241m=\u001b[39m wrap_fx_proxy_cls(target_cls\u001b[38;5;241m=\u001b[39mTensorWithTFOverrideVariable, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py:2124\u001b[0m, in \u001b[0;36mwrap_fx_proxy_cls\u001b[0;34m(target_cls, tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_disable_saved_tensors_hooks_during_tracing():\n\u001b[1;32m   2120\u001b[0m     \u001b[38;5;66;03m# with preserve_rng_state():\u001b[39;00m\n\u001b[1;32m   2121\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m example_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2122\u001b[0m         \u001b[38;5;66;03m# only allow_non_graph_fake in this instance because we handle the non-fake\u001b[39;00m\n\u001b[1;32m   2123\u001b[0m         \u001b[38;5;66;03m# cases properly below.\u001b[39;00m\n\u001b[0;32m-> 2124\u001b[0m         example_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_fake_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_non_graph_fake\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2126\u001b[0m     \u001b[38;5;66;03m# Handle recursive calls here\u001b[39;00m\n\u001b[1;32m   2127\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m maybe_get_fake_mode(example_value) \u001b[38;5;129;01mis\u001b[39;00m tx\u001b[38;5;241m.\u001b[39mfake_mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/utils.py:2082\u001b[0m, in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx, allow_non_graph_fake)\u001b[0m\n\u001b[1;32m   2079\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cause, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margument\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(cause):\n\u001b[1;32m   2080\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTypeError \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcause\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2082\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TorchRuntimeError(\u001b[38;5;28mstr\u001b[39m(e))\u001b[38;5;241m.\u001b[39mwith_traceback(e\u001b[38;5;241m.\u001b[39m__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_non_graph_fake:\n\u001b[1;32m   2085\u001b[0m     _ \u001b[38;5;241m=\u001b[39m pytree\u001b[38;5;241m.\u001b[39mtree_map_only(\n\u001b[1;32m   2086\u001b[0m         torch\u001b[38;5;241m.\u001b[39mTensor, functools\u001b[38;5;241m.\u001b[39mpartial(ensure_graph_fake, tx\u001b[38;5;241m=\u001b[39mtx), ret_val\n\u001b[1;32m   2087\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/utils.py:2017\u001b[0m, in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx, allow_non_graph_fake)\u001b[0m\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2016\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tx\u001b[38;5;241m.\u001b[39mfake_mode, enable_python_dispatcher():\n\u001b[0;32m-> 2017\u001b[0m         ret_val \u001b[38;5;241m=\u001b[39m \u001b[43mwrap_fake_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n\u001b[1;32m   2021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/utils.py:1574\u001b[0m, in \u001b[0;36mwrap_fake_exception\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrap_fake_exception\u001b[39m(fn):\n\u001b[1;32m   1573\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1574\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m UnsupportedFakeTensorException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1576\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unimplemented\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/utils.py:2018\u001b[0m, in \u001b[0;36mget_fake_value.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2016\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tx\u001b[38;5;241m.\u001b[39mfake_mode, enable_python_dispatcher():\n\u001b[1;32m   2017\u001b[0m         ret_val \u001b[38;5;241m=\u001b[39m wrap_fake_exception(\n\u001b[0;32m-> 2018\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[43mrun_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnnmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2019\u001b[0m         )\n\u001b[1;32m   2020\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n\u001b[1;32m   2021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/utils.py:2150\u001b[0m, in \u001b[0;36mrun_node\u001b[0;34m(tracer, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         unimplemented(make_error_message(e), from_exc\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   2149\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2150\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(make_error_message(e))\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[1;32m   2151\u001b[0m             e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m   2152\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   2154\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(op)\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/utils.py:2132\u001b[0m, in \u001b[0;36mrun_node\u001b[0;34m(tracer, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   2130\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_function\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2133\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m op \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_method\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], node\u001b[38;5;241m.\u001b[39mtarget)(\u001b[38;5;241m*\u001b[39margs[\u001b[38;5;241m1\u001b[39m:], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/nn/functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/utils/_stats.py:21\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1238\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   1235\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_dispatch_mode(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_TorchDispatchModeKey\u001b[38;5;241m.\u001b[39mFAKE) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m ), func\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1240\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake tensor raised TypeError\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1692\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_enabled:\n\u001b[0;32m-> 1692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_impl(func, types, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1339\u001b[0m, in \u001b[0;36mFakeTensorMode._cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_cache_key(func, args, kwargs)\n\u001b[0;32m-> 1339\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1340\u001b[0m     entry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_cache_entry(state, key, func, args, kwargs, output)\n\u001b[1;32m   1341\u001b[0m     key\u001b[38;5;241m.\u001b[39mstrip_shape_env()\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1943\u001b[0m, in \u001b[0;36mFakeTensorMode._dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m decomposition_table \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m   1934\u001b[0m     has_symbolic_sizes\n\u001b[1;32m   1935\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1940\u001b[0m     )\n\u001b[1;32m   1941\u001b[0m ):\n\u001b[1;32m   1942\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m-> 1943\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdecomposition_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1945\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   1946\u001b[0m     \u001b[38;5;66;03m# Decomposes CompositeImplicitAutograd ops\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m     r \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39mdecompose(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_decomp/__init__.py:88\u001b[0m, in \u001b[0;36m_convert_out_params.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m is_none \u001b[38;5;241m=\u001b[39m out_kwargs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m((o \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m is_none \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m out_kwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_none\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mout_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_prims_common/wrappers.py:273\u001b[0m, in \u001b[0;36mout_wrapper.<locals>._out_wrapper.<locals>._fn\u001b[0;34m(out, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m     result \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, is_out\u001b[38;5;241m=\u001b[39m(out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(result, TensorLike)\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_tensor\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Tuple)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(out_names)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    279\u001b[0m )\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# Naively you might expect this assert to be true, but\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# it's not:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;66;03m# be a normal meta tensor, but this is perfectly\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# harmless.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_decomp/decompositions.py:3942\u001b[0m, in \u001b[0;36mnll_loss_forward\u001b[0;34m(self, target, weight, reduction, ignore_index)\u001b[0m\n\u001b[1;32m   3936\u001b[0m n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3938\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3939\u001b[0m     weight\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m weight\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m n_classes\n\u001b[1;32m   3940\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight tensor should be defined either for all \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_classes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m classes or no classes but got weight tensor of shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa: B950\u001b[39;00m\n\u001b[0;32m-> 3942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nll_loss_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_decomp/decompositions.py:3893\u001b[0m, in \u001b[0;36m_nll_loss_forward\u001b[0;34m(self, target, weight, reduction, ignore_index)\u001b[0m\n\u001b[1;32m   3890\u001b[0m safe_target_ \u001b[38;5;241m=\u001b[39m safe_target\u001b[38;5;241m.\u001b[39munsqueeze(channel_dim)\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;66;03m# target can be [N, 1] or [1]\u001b[39;00m\n\u001b[0;32m-> 3893\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_target_\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(channel_dim)\n\u001b[1;32m   3895\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(target \u001b[38;5;241m!=\u001b[39m ignore_index, result, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   3897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m==\u001b[39m Reduction\u001b[38;5;241m.\u001b[39mNONE\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mand\u001b[39;00m n_dims \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/utils/_stats.py:21\u001b[0m, in \u001b[0;36mcount.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m     simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     20\u001b[0m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m=\u001b[39m simple_call_counter[fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1238\u001b[0m, in \u001b[0;36mFakeTensorMode.__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   1235\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_dispatch_mode(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_TorchDispatchModeKey\u001b[38;5;241m.\u001b[39mFAKE) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m ), func\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   1240\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfake tensor raised TypeError\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1692\u001b[0m, in \u001b[0;36mFakeTensorMode.dispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_enabled:\n\u001b[0;32m-> 1692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatch_impl(func, types, args, kwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:1339\u001b[0m, in \u001b[0;36mFakeTensorMode._cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_cache_key(func, args, kwargs)\n\u001b[0;32m-> 1339\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1340\u001b[0m     entry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_cache_entry(state, key, func, args, kwargs, output)\n\u001b[1;32m   1341\u001b[0m     key\u001b[38;5;241m.\u001b[39mstrip_shape_env()\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:2021\u001b[0m, in \u001b[0;36mFakeTensorMode._dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   2017\u001b[0m     log\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed while attempting to run meta for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, func)\n\u001b[1;32m   2018\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   2020\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_propagate_real_tensors(\n\u001b[0;32m-> 2021\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_meta_outputs_with_default_device_logic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdevice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2024\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:2143\u001b[0m, in \u001b[0;36mFakeTensorMode.wrap_meta_outputs_with_default_device_logic\u001b[0;34m(self, r, func, flat_args, device)\u001b[0m\n\u001b[1;32m   2140\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m e\n\u001b[0;32m-> 2143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtree_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/utils/_pytree.py:964\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(func, tree, is_leaf, *rests)\u001b[0m\n\u001b[1;32m    962\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf\u001b[38;5;241m=\u001b[39mis_leaf)\n\u001b[1;32m    963\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[0;32m--> 964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreespec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/utils/_pytree.py:803\u001b[0m, in \u001b[0;36mTreeSpec.unflatten\u001b[0;34m(self, leaves)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munflatten\u001b[39m(\u001b[38;5;28mself\u001b[39m, leaves: Iterable[Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PyTree:\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(leaves, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 803\u001b[0m         leaves \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(leaves)\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(leaves) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_leaves:\n\u001b[1;32m    805\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    806\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtreespec.unflatten(leaves): `leaves` has length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(leaves)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut the spec refers to a pytree that holds \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_leaves\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitems (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    809\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:2121\u001b[0m, in \u001b[0;36mFakeTensorMode.wrap_meta_outputs_with_default_device_logic.<locals>.wrap\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m e\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m common_device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2118\u001b[0m     (\n\u001b[1;32m   2119\u001b[0m         common_device,\n\u001b[1;32m   2120\u001b[0m         has_scalar_only_inputs,\n\u001b[0;32m-> 2121\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[43mFakeTensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_find_common_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2123\u001b[0m is_our_fake \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_our_fake(e)\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_our_fake:\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:872\u001b[0m, in \u001b[0;36mFakeTensor._find_common_device\u001b[0;34m(func, flat_args)\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    868\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnhandled FakeTensor Device Propagation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found two different devices \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_device\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    869\u001b[0m     )\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m flat_args:\n\u001b[0;32m--> 872\u001b[0m     \u001b[43mmerge_devices\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;66;03m# some functions that allow Python numbers to bind to Tensors\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# if we have failed to find a device, and we're running one of these operators,\u001b[39;00m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# we must have scalar only inputs\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_allow_numbers_as_tensors(func) \u001b[38;5;129;01mand\u001b[39;00m common_device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;66;03m# ops with scalar only inputs always have result on cpu\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_subclasses/fake_tensor.py:867\u001b[0m, in \u001b[0;36mFakeTensor._find_common_device.<locals>.merge_devices\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# mismatching devices of non-zero dim tensors, throw\u001b[39;00m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;66;03m# This might be valid behavior and need to be explicitly modeled, e.g. reshape_as\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    868\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnhandled FakeTensor Device Propagation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found two different devices \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_device\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    869\u001b[0m )\n",
      "\u001b[0;31mTorchRuntimeError\u001b[0m: Failed running call_function <function cross_entropy at 0x7f507b23e700>(*(FakeTensor(..., device='cuda:0', size=(20, 50304), grad_fn=<ViewBackward0>), FakeTensor(..., size=(20,), dtype=torch.int64)), **{'ignore_index': -1}):\nUnhandled FakeTensor Device Propagation for aten.gather.default, found two different devices cuda:0, cpu\n\nfrom user code:\n   File \"/tmp/ipykernel_3468/802198899.py\", line 34, in forward\n    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "_, loss = model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:465\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    461\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    469\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    470\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[4], line 91\u001b[0m, in \u001b[0;36mMuon.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnesterov\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     90\u001b[0m     g \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39madd(buf, alpha\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[0;32m---> 91\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mzeropower_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackend_steps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m g \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(g\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), g\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;66;03m# scale to have update.square().mean() == 1\u001b[39;00m\n\u001b[1;32m     93\u001b[0m updates_flat[curr_idx:curr_idx\u001b[38;5;241m+\u001b[39mp\u001b[38;5;241m.\u001b[39mnumel()] \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:465\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m saved_dynamic_layer_stack_depth \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    461\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mget_dynamic_layer_stack_depth()\n\u001b[1;32m    462\u001b[0m )\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_functorch\u001b[38;5;241m.\u001b[39mpop_dynamic_layer_stack_and_undo_to_depth(\n\u001b[1;32m    469\u001b[0m         saved_dynamic_layer_stack_depth\n\u001b[1;32m    470\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mzeropower_via_newtonschulz5\u001b[0;34m(G, steps, eps, coeffs)\u001b[0m\n\u001b[1;32m      2\u001b[0m     U, _, V \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39msvd()\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m U \u001b[38;5;241m@\u001b[39m V\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mcompile\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzeropower_via_newtonschulz5\u001b[39m(G, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-7\u001b[39m, coeffs\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3.4445\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4.7750\u001b[39m,  \u001b[38;5;241m2.0315\u001b[39m)):\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    performance at all relative to UV^T, where USV^T = G is the SVD.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(G\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m prior \u001b[38;5;241m=\u001b[39m _maybe_set_eval_frame(callback)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m     _maybe_set_eval_frame(prior)\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_functorch/aot_autograd.py:1100\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1098\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m   1099\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 1100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:321\u001b[0m, in \u001b[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n\u001b[1;32m    320\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 321\u001b[0m     all_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/utils.py:124\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 124\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    128\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    129\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:667\u001b[0m, in \u001b[0;36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    664\u001b[0m     args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m([\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_tokens), \u001b[38;5;241m*\u001b[39margs]\n\u001b[1;32m    665\u001b[0m     old_args\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 667\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:488\u001b[0m, in \u001b[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    481\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functionalized_rng_runtime_epilogue(\n\u001b[1;32m    482\u001b[0m         runtime_metadata,\n\u001b[1;32m    483\u001b[0m         out,\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[1;32m    485\u001b[0m         runtime_metadata\u001b[38;5;241m.\u001b[39mnum_forward_returns,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 488\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nanogpt/lib/python3.11/site-packages/torch/_inductor/codecache.py:1478\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: List[Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1477\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/torchinductor_leloykun/xi/cxi2fslrvxhcmqy6oukk6oiyeuqigjamigch7na7vszd6rs7c5yj.py:568\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    566\u001b[0m buf21 \u001b[38;5;241m=\u001b[39m buf13; \u001b[38;5;28;01mdel\u001b[39;00m buf13  \u001b[38;5;66;03m# reuse\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [mul_9, mul_10, add_7, mul_11], Original ATen: [aten.mul, aten.add]\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m \u001b[43mextern_kernels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf20\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf19\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf18\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuf21\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m buf22 \u001b[38;5;241m=\u001b[39m buf19; \u001b[38;5;28;01mdel\u001b[39;00m buf19  \u001b[38;5;66;03m# reuse\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [A_4], Original ATen: [aten.mm]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.9\n",
    "ddp_rank = 1\n",
    "ddp_world_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_params = [\n",
    "    param\n",
    "    for param in model.transformer.h.parameters()\n",
    "    if param.shape[0] == param.shape[1]\n",
    "]\n",
    "nonsquare_params = [\n",
    "    param\n",
    "    for param in model.transformer.h.parameters()\n",
    "    if param.shape[0] != param.shape[1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = torch.optim.AdamW(model.lm_head.parameters(), lr=learning_rate, betas=(0.9, 0.95),\n",
    "                               weight_decay=weight_decay, fused=True)\n",
    "optimizer2 = Muon(square_params, lr=0.1*learning_rate, momentum=0.95,\n",
    "                  rank=ddp_rank, world_size=ddp_world_size)\n",
    "optimizer3 = Muon(nonsquare_params, lr=0.1*learning_rate, momentum=0.95,\n",
    "                  rank=ddp_rank, world_size=ddp_world_size,\n",
    "                  backend_steps=3, backend_coeffs=(3.5981, -5.1223,  2.2324))\n",
    "optimizers = [optimizer1, optimizer2, optimizer3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params: 56623104 or 0.46\n"
     ]
    }
   ],
   "source": [
    "nonsquare_param_size = sum(p.numel() for p in nonsquare_params)\n",
    "print(f\"Num params: {nonsquare_param_size} or {nonsquare_param_size/model_param_size:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num params: 28311552 or 0.23\n"
     ]
    }
   ],
   "source": [
    "square_param_size = sum(p.numel() for p in square_params)\n",
    "print(f\"Num params: {square_param_size} or {square_param_size/model_param_size:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "for N, M in MATRIX_SHAPES:\n",
    "    errors = run_experiment(key, N, M, iterator_newton_schulz, 10, 10)\n",
    "    plt.plot(errors, label=f\"{N}x{M}\")\n",
    "\n",
    "plt.xlabel(\"Iteration steps\")\n",
    "plt.ylabel(\"Mean squared distance from 1.0\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "for N, M in MATRIX_SHAPES:\n",
    "    errors = run_experiment(key, N, M, iterator_keller_jordan_opt, 10, 10)\n",
    "    print(f\"{N}x{M}: {errors[3]}\")\n",
    "    plt.plot(errors, label=f\"{N}x{M}\")\n",
    "\n",
    "plt.xlabel(\"Iteration steps\")\n",
    "plt.ylabel(\"Mean squared distance from 1.0\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "gamma = 2.5\n",
    "basin_radius = 0.15\n",
    "roots = [\n",
    "    1 - basin_radius,\n",
    "    1 + basin_radius,\n",
    "    0,\n",
    "    -1 - basin_radius,\n",
    "    -1 + basin_radius\n",
    "]\n",
    "h = lambda x: x + gamma * (x - roots[0])*(x - roots[1])*(x - roots[2])*(x - roots[3])*(x - roots[4])\n",
    "\n",
    "for N, M in MATRIX_SHAPES:\n",
    "    errors = run_experiment(key, N, M, h, 10, 10)\n",
    "    print(f\"{N}x{M}: {errors[3]}, {errors[5]}, {errors[7]}\")\n",
    "    plt.plot(errors, label=f\"{N}x{M}\")\n",
    "\n",
    "plt.xlabel(\"Iteration steps\")\n",
    "plt.ylabel(\"Mean squared distance from 1.0\")\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peak, trough = get_odd_degree_iterator_stats(2.4, 0.13)\n",
    "peak, trough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = jnp.linspace(0, 1.2, 100)\n",
    "\n",
    "plt.plot(x, x, label=f\"$y = x$\")\n",
    "\n",
    "# y = g(x)\n",
    "# plt.plot(x, y, label=\"g(x)\")\n",
    "\n",
    "# dy = jax.vmap(jax.grad(g))(x)\n",
    "# plt.plot(x, dy, label=\"g'(x)\")\n",
    "\n",
    "for gamma, r in [[-0.7530, 0.13], [-0.9223, 0.11], [2.4, 0.13], [1.2284936824712378, 0.05], [1.25, 0]]:\n",
    "    b1 = 1 - r\n",
    "    b2 = 1 + r\n",
    "    h = lambda x: gamma * (x - b1)*(x - b2)*(x)*(x + b1)*(x + b2) + x\n",
    "\n",
    "    y = h(x)\n",
    "    plt.plot(x, y, label=f\"$h(x)$ w/ $\\gamma = {gamma}$, $b1 = {b1}$, $b2 = {b2}$\")\n",
    "\n",
    "    # dy = jax.vmap(jax.grad(h))(x)\n",
    "    # plt.plot(x, dy, label=f\"$h'(x)$ w/ $\\gamma = {gamma}$, $b1 = {b1}$, $b2 = {b2}$\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1.3, 1.3, 100)\n",
    "y = g(x)\n",
    "\n",
    "plt.plot(x, x, label='y=x', color=\"black\", linestyle=\"--\")\n",
    "plt.plot(x, y, label='y=g(x)', color=\"blue\")\n",
    "\n",
    "for i, (in_features, out_features) in enumerate(MATRIX_SHAPES[-1:]):\n",
    "    y_ = h_iterators[i](x)\n",
    "    plt.plot(x, y_, label=f\"y=h(x) for {out_features}x{in_features}\")\n",
    "\n",
    "gamma = -3\n",
    "r = 0.11\n",
    "fp = [\n",
    "    0,\n",
    "    1 - r,\n",
    "    1 + r,\n",
    "    -1 - r,\n",
    "    -1 + r,\n",
    "    -1,\n",
    "    1,\n",
    "]\n",
    "h = lambda x: x + gamma * (x - fp[0])*(x - fp[1])*(x - fp[2])*(x - fp[3])*(x - fp[4])*(x - fp[5])*(x - fp[6])\n",
    "\n",
    "y_ = h(x)\n",
    "plt.plot(x, y_, label=f\"test\")\n",
    "\n",
    "plt.xlim(-(1.26373+0.1), 1.26373+0.1)\n",
    "plt.ylim(-(1.26373+0.1), 1.26373+0.1)\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features, out_features = 2**14, 768\n",
    "K = jnp.sqrt(1/in_features)\n",
    "\n",
    "X = jax.random.uniform(\n",
    "    key,\n",
    "    shape=(out_features, in_features),\n",
    "    minval=-K,\n",
    "    maxval=K,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = jnp.linalg.svd(X, compute_uv=False)\n",
    "jnp.median(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma, r = 0.8, 0.11\n",
    "fp = [1+r, 1-r, -1-r, -1+r]\n",
    "h = lambda x: x + gamma * (x - fp[0])*(x - fp[1])*(x - fp[2])*(x - fp[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b, c = 3.4445, -4.7750, 2.0315\n",
    "a, b, c = 3.3196, -4.8811, 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = X / (jnp.linalg.norm(X, ord='fro') + 1e-6)\n",
    "\n",
    "S_1 = jnp.linalg.svd(X_1, compute_uv=False)\n",
    "print(jnp.median(S_1))\n",
    "\n",
    "for _ in range(5):\n",
    "    A = X_1 @ X_1.T\n",
    "    B = A @ X_1\n",
    "    X_1 = a * X_1 + b * B + c * A @ B\n",
    "    S_1 = jnp.linalg.svd(X_1, compute_uv=False)\n",
    "    print(jnp.median(S_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = X / (jnp.linalg.norm(X) + 1e-6)\n",
    "\n",
    "S_1 = jnp.linalg.svd(X_1, compute_uv=False)\n",
    "print(jnp.median(S_1))\n",
    "\n",
    "for _ in range(5):\n",
    "    S_1 = h(S_1)\n",
    "    print(jnp.median(S_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features, out_features = 768, 768\n",
    "K = jnp.sqrt(1/in_features)\n",
    "\n",
    "X = jax.random.uniform(\n",
    "    key,\n",
    "    shape=(out_features, in_features),\n",
    "    minval=-K,\n",
    "    maxval=K,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = jnp.linalg.svd(X, compute_uv=False)\n",
    "jnp.median(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma, r = 0.8, 0.11\n",
    "fp = [1+r, 1-r, -1+r, -1-r]\n",
    "h = lambda x: x + gamma * (x - fp[0])*(x - fp[1])*(x - fp[2])*(x-fp[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = X / (jnp.linalg.norm(X, ord='fro') + 1e-6)\n",
    "\n",
    "S_1 = jnp.linalg.svd(X_1, compute_uv=False)\n",
    "print(jnp.median(S_1))\n",
    "\n",
    "for i in range(5):\n",
    "    if i % 2 == 0:\n",
    "        A = X_1 @ X_1.T\n",
    "    else:\n",
    "        A = X_1.T @ X_1\n",
    "        X_1 = X_1.T\n",
    "    X_1 = A @ A.T - 2.0242 * A + X_1 + 0.97594641\n",
    "    S_1 = jnp.linalg.svd(X_1, compute_uv=False)\n",
    "    print(jnp.median(S_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = X / (jnp.linalg.norm(X) + 1e-6)\n",
    "\n",
    "S_1 = jnp.linalg.svd(X_1, compute_uv=False)\n",
    "print(jnp.median(S_1))\n",
    "\n",
    "for _ in range(5):\n",
    "    S_1 = h(S_1)\n",
    "    print(jnp.median(S_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "\n",
    "gamma = sp.Symbol(\"gamma\", positive=True)\n",
    "r = sp.Symbol(\"r\", interval=(0, 1), left_open=False, right_open=True)\n",
    "x = sp.Symbol(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = [1 - r, 1 + r, 0, -(1 + r), -(1 - r)]\n",
    "# h = x + gamma * (x - fp[0])*(x - fp[1])*(x - fp[2])*(x - fp[3])*(x - fp[4])\n",
    "fp = [1 - r, 1 + r, -1-r, -1+r]\n",
    "h = x + gamma * (x - fp[0])*(x - fp[1])*(x - fp[2])*(x-fp[3])\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_simplified = sp.collect(sp.expand(h), x)\n",
    "h_simplified = h_simplified.refine(sp.Q.positive(gamma)).refine(sp.Q.positive(r))\n",
    "h_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_simplified.subs({gamma: 0.8, r: 0.11})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.collect(\n",
    "    sp.expand(\n",
    "        x*(gamma*(x - 1)*(x**2 + (-r**2 - 1)) + (-2*gamma*r**2+1))\n",
    "    ),\n",
    "    x,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_div = sp.collect(sp.simplify((h_simplified - x) / (x - 1)), x)\n",
    "h_div.subs({gamma: 0.8, r: 0.11})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = sp.symbols(\"a b c\")\n",
    "\n",
    "f = (a*x + b*x**3 + c*x**5).subs({a: 3.4445, b: -4.7750, c: 2.0315})\n",
    "\n",
    "dfdx = sp.simplify(sp.expand(sp.diff(f, x, real=True)))\n",
    "dfdx = dfdx.refine(sp.Q.real(x)).refine(sp.Q.real(gamma)).refine(sp.Q.real(r))\n",
    "\n",
    "all_roots = sp.nroots(dfdx)\n",
    "real_roots = [root.evalf() for root in all_roots if root.as_real_imag()[1] == 0]\n",
    "for root in real_roots:\n",
    "    print(f.subs({x: root}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.collect(sp.expand(h_exp), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.collect(sp.collect(sp.expand(h_exp), x), x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = [1 - r, 1 + r, -1 - r, -1 + r]\n",
    "h_exp = x + gamma * (x - fp[0])*(x - fp[1])*(x - fp[2])*(x - fp[3])\n",
    "sp.collect(sp.expand(h_exp), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1_exp = h_exp.subs({\n",
    "    gamma: best_hyperparams[3][0],\n",
    "    r: best_hyperparams[3][1],\n",
    "})\n",
    "sp.collect(sp.expand(h1_exp), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer (1D) | Steepest Descent in Norm\n",
    "--- | ---\n",
    "SGD | Euclidean norm ($\\|\\cdot\\|_2$)\n",
    "Adam w/o accumulation | $\\infty$-norm ($\\|\\cdot\\|_\\infty$)\n",
    "Adam | Dynamically-learned norm from $\\infty$-norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer (2D) | Steepest Descent in Norm\n",
    "--- | ---\n",
    "SGD | Euclidean norm = Frobenius norm ($\\|\\|\\cdot\\|\\|_F$) = Schatten-$2$ norm ($\\|\\|\\cdot\\|\\|_{S_2}$)\n",
    "Adam w/o accumulation | Max-of-Max norm ($\\|\\|W\\|\\| = \\max_{l}\\max_{r} \\| M_{l,r} \\|$)\n",
    "Adam | Dynamically-learned norm from Max-of-Max norm\n",
    "Shampoo w/o accumulation | Spectral norm = Schatten-$\\infty$ norm ($\\|\\|\\cdot\\|\\|_{S_\\infty}$)\n",
    "Shampoo | Dynamically-learned (approx.) $S_p$-norm from Spectral norm\n",
    "SOAP | Same as above, but with momentum on $\\Delta p$\n",
    "**Muon** | **Static range of $S_p$-norms for large $p$**\n",
    "Shape-Aware Muon (mine) | Same as above, but range of $p$ depends on matrix shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marchenko–Pastur distribution\n",
    "\n",
    "Let $X \\in R^{d_{out} \\times d_{in}}$ be a random matrix with i.i.d. entries drawn from $\\mathcal{N}(0, \\sigma^2)$.\n",
    "\n",
    "Then, the lower and upper bounds of the Marchenko-Pastur distribution are $\\lambda_{\\pm} = \\sigma^2(1 \\pm \\sqrt{M/N})^2$.\n",
    "\n",
    "### Expected Frobenius Norm of Random Matrix\n",
    "\n",
    "Recall that if each $x_i$ is drawn from $\\mathcal{N}(0, \\sigma^2)$, then $\\mathbb{E}[|x_i|^2] = \\sigma^2$. Thus,\n",
    "\n",
    "$$\\mathbb{E}[\\|X\\|_F^2] = \\Sigma_{c=1}^{M}\\Sigma_{r=1}^{N} \\mathbb{E}[|a_{ij}|^2] = MN\\sigma^2$$\n",
    "$$\\mathbb{E}[\\|X\\|_F] = \\sigma\\sqrt{MN}$$\n",
    "\n",
    "### Expected Initial range of Singular Values\n",
    "\n",
    "$$\\lambda'_{\\pm} = \\frac{\\sigma^2(1 \\pm \\sqrt{M/N})^2}{\\sigma\\sqrt{MN}}$$\n",
    "$$\\lambda'_{\\pm} = \\sigma\\frac{(1 \\pm \\sqrt{M/N})^2}{\\sqrt{MN}}$$\n",
    "$$\\lambda'_{\\pm} = \\sigma\\frac{1 \\pm 2\\sqrt{M/N} + M/N}{\\sqrt{MN}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "sigma = sp.Symbol(\"sigma\")\n",
    "M = sp.Symbol(\"M\", positive=True)\n",
    "N = sp.Symbol(\"N\", positive=True)\n",
    "k = sp.Symbol(\"k\", positive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle N \\leq k$"
      ],
      "text/plain": [
       "N <= k"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N <= k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_min = sigma * (1 - 2*sp.sqrt(M/N) + M/N) / sp.sqrt(M*N)\n",
    "lambda_max = sigma * (1 + 2*sp.sqrt(M/N) + M/N) / sp.sqrt(M*N)\n",
    "lambda_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.radsimp(sp.simplify(sp.solve(lambda_min - 1/2, sigma)[0].subs({M: k*N})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_min.subs({M: k*N})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_min.subs({M: 4*N, sigma: N})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_min.subs({M: k*N, k: 1}), lambda_min.subs({M: k*N, k: 1/4}), lambda_min.subs({M: k*N, k: 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_max.subs({M: k*N, k: 1}), lambda_max.subs({M: k*N, k: 1/4}), lambda_max.subs({M: k*N, k: 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_max.subs({M: 4*N, sigma: N/9})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\|W\\|_{\\alpha \\to \\beta} = \\max_{x \\in \\mathbb{R}^{d_{in}}} \\frac{\\|Wx\\|_\\beta}{\\|x\\|_\\alpha}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\|W\\|_{2 \\to 2} = \\max_{x \\in \\mathbb{R}^{d}} \\frac{\\|Wx\\|_2}{\\|x\\|_2} = \\|W\\|_{\\text{spectral}} = \\|W\\|_{S_{\\infty}}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanogpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
